<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-09-15T17:57:57-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Accelerated CPU Inference with PyTorch Inductor using torch.compile</title>
      <link href="https://pytorch.org/blog/accelerated-cpu-inference/" rel="alternate" type="text/html" title="Accelerated CPU Inference with PyTorch Inductor using torch.compile" />
      <published>2023-09-13T00:00:00-07:00</published>
      <updated>2023-09-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-cpu-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-cpu-inference/">&lt;h2 id=&quot;story-at-a-glance&quot;&gt;Story at a Glance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Although the PyTorch* Inductor C++/OpenMP* backend has enabled users to take advantage of modern CPU architectures and parallel processing, it has lacked optimizations, resulting in the backend performing worse than eager mode in terms of end-to-end performance.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Intel optimized the Inductor backend using a hybrid strategy that classified operations into two categories: Conv/GEMM and non-Conv/GEMM element-wise and reduction ops.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;For popular deep learning models, this hybrid strategy demonstrates promising performance improvements compared to eager mode and improves the C++/OpenMP backend’s efficiency and reliability for PyTorch models.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;inductor-backend-challenges&quot;&gt;Inductor Backend Challenges&lt;/h2&gt;

&lt;p&gt;The PyTorch Inductor C++/OpenMP backend enables users to take advantage of modern CPU architectures and parallel processing to accelerate computations.&lt;/p&gt;

&lt;p&gt;However, during the early stages of its development, the backend lacked some optimizations, which prevented it from fully utilizing the CPU computation capabilities. As a result, for most models the C++/OpenMP backend performed worse than eager mode in terms of end-to-end performance, with 45% of TorchBench, 100% of Hugging Face, and 75% of TIMM models performing worse than eager mode.&lt;/p&gt;

&lt;p&gt;In this post, we highlight Intel’s optimizations to the Inductor CPU backend, including the technologies and results.&lt;/p&gt;

&lt;p&gt;We optimized the backend by using a hybrid strategy that classified operations into two categories: Conv/GEMM and non-Conv/GEMM element-wise and reduction ops. Post-op fusion and weight prepacking using the oneDNN performance library were utilized to optimize the former, while explicit vectorization in C++ codegen was used to optimize the latter.&lt;/p&gt;

&lt;p&gt;This hybrid strategy demonstrated promising performance improvements compared to eager mode, particularly on popular deep learning models such as Inductor Hugging Face, Inductor TorchBench and Inductor TIMM. Overall, Intel’s optimizations improve the C++/OpenMP backend’s efficiency and reliability for PyTorch models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f1-pytorch-inference-speedup-ratio-trend-multi.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 1. Performance Speedup Ratio Trend&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Performance Speedup Ratio Trend&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;performance-status-of-intel-hybrid-optimizations&quot;&gt;Performance Status of Intel Hybrid Optimizations&lt;/h3&gt;

&lt;p&gt;Compared to eager mode with the hybrid optimizations, the C++/OpenMP backend shows promising performance improvements. We measured the performance of the three Inductor benchmark suites—TorchBench, Hugging Face, and TIMM—and the results are as follows. (&lt;em&gt;Note: we publish our performance data twice per week on &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/93531&quot;&gt;GitHub&lt;/a&gt;.&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Overall, these optimizations help to ensure that the C++/OpenMP backend provides efficient and reliable support for PyTorch models.&lt;/p&gt;

&lt;h3 id=&quot;passrate&quot;&gt;Passrate&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor | 93%, 56/60 | 96%, 44/46  | 100%, 61/61 |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;geometric-mean-speedup-single-socket-multi-threads&quot;&gt;Geometric mean speedup (Single-Socket Multi-threads)&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor |   1.39x    |    1.20x    |    1.73x    |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;individual-model-performance&quot;&gt;Individual Model Performance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f2-torchbench-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 2. TorchBench FP32 Performance (Single-Socket Multi-threads)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: TorchBench FP32 Performance (Single-Socket Multi-threads)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f3-huggingface-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 3. Hugging Face FP32 Performance (Single-Socket Multi-thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Hugging Face FP32 Performance (Single-Socket Multi-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f4-timm-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 4. TIMM FP32 Performance (Single-Socket Multi-threads)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: TIMM FP32 Performance (Single-Socket Multi-threads)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;geometric-mean-speedup-single-core-single-thread&quot;&gt;Geometric mean speedup (Single-core Single-thread)&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor |    1.29x   |    1.15x    |    1.37x    |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f5-torchbench-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 5. TorchBench FP32 Performance (Single-Socket Single-thread)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: TorchBench FP32 Performance (Single-Socket Single-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f6-huggingface-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 6. Hugging Face FP32 Performance (Single-Socket Single Thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: Hugging Face FP32 Performance (Single-Socket Single Thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f7-timm-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 7. TIMM FP32 Performance (Single-Socket Single-thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;: TIMM FP32 Performance (Single-Socket Single-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;technical-deep-dive&quot;&gt;Technical Deep Dive&lt;/h2&gt;

&lt;p&gt;Now, let’s take a closer look at the two primary optimizations used in the Inductor C++/OpenMP backend:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;weight prepacking and post-operation fusion via oneDNN library&lt;/li&gt;
  &lt;li&gt;explicit vectorization in Inductor C++ codegen&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;weight-prepackaging--post-op-fusion-via-onednn&quot;&gt;Weight Prepackaging &amp;amp; Post-op Fusion via oneDNN&lt;/h3&gt;

&lt;p&gt;Shorthand for Intel® oneAPI Deep Neural Network Library, oneDNN library provides a range of post-op fusions (i.e., fuse convolution and matmal with its consecutive operation) that can benefit popular models. The &lt;a href=&quot;https://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; has implemented most of these fusions and has achieved significant performance improvements. As a result, we have upstreamed all of these fusions that have been applied in Intel’s PyTorch extension to Inductor, enabling a wider range of models to benefit from these optimizations. We have defined these fusions as operators under the mkldnn namespace. This allows the Python module to invoke these mkldnn operations directly.&lt;/p&gt;

&lt;p&gt;Currently, the defined fused operations are as follows. You can find these defined fused operations at &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/aten/src/ATen/native/mkldnn/RegisterMkldnnOpContextClass.cpp#L35-#L48&quot;&gt;RegisterMkldnnOpContextClass.cpp&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_linear_pointwise&lt;/code&gt;: Fuses Linear and its post-unary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_linear_pointwise.binary&lt;/code&gt;: Fuses Linear and its post-binary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_convolution_pointwise&lt;/code&gt;: Fuses Convolution and its post-unary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_convolution_pointwise.binary&lt;/code&gt;: Fuses Convolution and its post-binary element-wise operations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The detailed fusion patterns are defined in the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/mkldnn.py#L774-#L818&quot;&gt;mkldnn.py&lt;/a&gt; file: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;convolution/linear + sigmoid/hardsigmoid/tanh/hardtanh/hardswish/leaky_relu/gelu/relu/relu6/siluconvolution/linear + add/add_/iadd/sub/sub_&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;On the Inductor side, we apply these fusions on the FX graph that has been lowered. We have defined &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/mkldnn.py#L491&quot;&gt;mkldnn_fuse_fx&lt;/a&gt; as the entry point to apply all the fusions. The code snippet for this is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def mkldnn_fuse_fx(gm: torch.fx.GraphModule, example_inputs):
    ...
    gm = fuse_unary(gm)
    gm = fuse_binary(gm)
    ...
    if config.cpp.weight_prepack:
        gm = pack_module(gm)
    return gm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkldnn_fuse_fx&lt;/code&gt; function, we apply fusion on the FX graph that hasn’t been lowered yet. To fuse convolution/linear and its consecutive elementwise operations, we invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fuse_unary&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fuse_binary&lt;/code&gt; as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   gm = fuse_unary(gm)
   gm = fuse_binary(gm)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to the post-op fusion, we apply weight prepacking to improve the Conv/GEMM performance further:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   gm = pack_module(gm)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Weight prepacking involves rearranging the weight tensor in a blocked layout, which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;can improve vectorization and cache reuse compared to plain formats like NCHW or NHWC and;&lt;/li&gt;
  &lt;li&gt;can help avoid weight reordering at runtime, which can reduce overhead and improve performance and;&lt;/li&gt;
  &lt;li&gt;increases memory usage as the tradeoff.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For these reasons, we provide &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config.cpp.weight_prepack&lt;/code&gt; flag in Inductor to provide users with more control over this optimization, allowing them to enable it based on their specific needs.&lt;/p&gt;

&lt;h3 id=&quot;explicit-vectorization-in-inductor-c-codegen&quot;&gt;Explicit Vectorization in Inductor C++ Codegen&lt;/h3&gt;

&lt;p&gt;Vectorization is a key optimization technique that can significantly improve the performance of numerical computations. By utilizing SIMD (Single Instruction, Multiple Data) instructions, vectorization enables multiple computations to be performed simultaneously on a single processor core, which can lead to significant performance improvements.&lt;/p&gt;

&lt;p&gt;In the Inductor C++/OpenMP backend, we use &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L372&quot;&gt;Intel® AVX2&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L359&quot;&gt;Intel® AVX-512&lt;/a&gt; ISA (Instruction Set Architecture) options for vectorization by leveraging the aten vectorization library to facilitate the implementation. Aten vectorization supports multiple platforms, including x86 and Arm, as well as multiple data types. It can be extended to support other ISAs easily by adding more &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L275&quot;&gt;VecISA&lt;/a&gt; sub-classes. This allows Inductor to easily support other platforms and data types in the future.&lt;/p&gt;

&lt;p&gt;Due to differences in platforms, the C++/OpenMP backend of Inductor starts by detecting the CPU features to determine the vectorization bit width at the beginning of code generation. By default, if the machine supports both AVX-512 and AVX2, the backend will choose 512-bit vectorization.&lt;/p&gt;

&lt;p&gt;If the hardware supports vectorization, the C++/OpenMP backend first detects if the loop body can be vectorized or not. There are primarily three scenarios that we are not able to generate kernel with vectorization:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Loop body lacks vector intrinsics support, e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rand&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;atomic_add&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Loop body lacks efficient vector intrinsics support, e.g., non-contiguous &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load/store&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Data types with vectorization not yet supported but work in progress, e.g., integer, double, half, and bfloat16.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To address this issue, the C++/OpenMP backend uses &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1396&quot;&gt;CppVecKernelChecker&lt;/a&gt; to detect whether all operations in a particular loop body can be vectorized or not. In general, we classified the operations into two categories by identifying if they depend on the context.&lt;/p&gt;

&lt;p&gt;For most elementwise operations such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relu&lt;/code&gt;, vectorization is straightforward, and their execution does not depend on context.&lt;/p&gt;

&lt;p&gt;However, for certain other operations, their semantics are more complex and their execution depends on context through static analysis.&lt;/p&gt;

&lt;p&gt;For example, let’s consider the where operation that takes in mask, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true_value&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false_value&lt;/code&gt; while the mask value is loaded from a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8&lt;/code&gt; tensor. The fx graph could be as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;graph():
    %ops : [#users=9] = placeholder[target=ops]
    %get_index : [#users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
    %load : [#users=1] = call_method[target=load](args = (%ops, arg1_1, %get_index), kwargs = {})
    %to_dtype : [#users=1] = call_method[target=to_dtype](args = (%ops, %load, torch.bool), kwargs = {})
    ...
    %where : [#users=1] = call_method[target=where](args = (%ops, %to_dtype, %to_dtype_2, %to_dtype_3), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Regarding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8&lt;/code&gt;, it is a general data type and could be used for computation but is not limited to being used as Boolean for mask. Hence, we need to analyze its context statically. In particular, the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1396&quot;&gt;CppVecKernelChecker&lt;/a&gt; will check whether a uint8 tensor is only used by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_dtype&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_dtype&lt;/code&gt; is only used by where. If yes, it could be vectorized. Otherwise, it will fall back to the scalar version. The generated code could be as follows:&lt;/p&gt;

&lt;p&gt;Scalar Version&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto tmp0 = in_ptr0[i1 + (17*i0)];
auto tmp3 = in_ptr1[i1 + (17*i0)];
auto tmp1 = static_cast&amp;lt;bool&amp;gt;(tmp0);
auto tmp2 = static_cast&amp;lt;float&amp;gt;(-33.0);
auto tmp4 = tmp1 ? tmp2 : tmp3;
tmp5 = std::max(tmp5, tmp4);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Vectorization Version&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float g_tmp_buffer_in_ptr0[16] = {0};
// Convert the flag to float for vectorization. 
flag_to_float(in_ptr0 + (16*i1) + (17*i0), g_tmp_buffer_in_ptr0, 16);
auto tmp0 = at::vec::Vectorized&amp;lt;float&amp;gt;::loadu(g_tmp_buffer_in_ptr0);
auto tmp3 = at::vec::Vectorized&amp;lt;float&amp;gt;::loadu(in_ptr1 + (16*i1) + (17*i0));
auto tmp1 = (tmp0);
auto tmp2 = at::vec::Vectorized&amp;lt;float&amp;gt;(static_cast&amp;lt;float&amp;gt;(-33.0));
auto tmp4 = decltype(tmp2)::blendv(tmp3, tmp2, tmp1);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to context analysis, the C++/OpenMP backend also incorporates several other vectorization-related optimizations. These include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tiled kernel implementation for supporting transpose load - &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1211&quot;&gt;cpp.py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Data type demotion based on value range - &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1647-#L1672&quot;&gt;cpp.py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Replacement of &lt;a href=&quot;http://github.com/shibatch/sleef/tree/e0a003ee838b75d11763aa9c3ef17bf71a725bff&quot;&gt;sleef&lt;/a&gt; implementation with oneDNN/oneMKL implementation for optimizing aten vectorization - &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/94577&quot;&gt;#94577&lt;/a&gt;, &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/92289&quot;&gt;#92289&lt;/a&gt;, &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/91613&quot;&gt;#91613&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, we examined vectorization optimization in Inductor C++ backend for FP32 training and inference of 150 benchmark models with 90% of inference kernels and 71% of training kernels being vectorized.&lt;/p&gt;

&lt;p&gt;In terms of inference, a total of 28,185 CPP kernels were generated, with 25,579 (90%) of them being vectorized, while the remaining 10% were scalar. As for training, 103,084 kernels were generated, with 73,909 (71%) being vectorized and 29% not vectorized.&lt;/p&gt;

&lt;p&gt;The results indicate that &lt;strong&gt;the vectorization of inference kernels is quite impressive&lt;/strong&gt; (there is still some work to be done in training kernels since we just started to work on the training). The remaining non-vectorized kernels are analyzed in different categories, highlighting the next steps to improve vectorization coverage: index-related operations, int64 support, vertical reduction, vectorization with fallback, and more.&lt;/p&gt;

&lt;p&gt;In addition, we also optimized the C++/OpenMP backend with other optimizations like buffer-reuse and CppWrapper.&lt;/p&gt;

&lt;h4 id=&quot;future-work&quot;&gt;Future Work&lt;/h4&gt;

&lt;p&gt;The next step, we will continue optimizing the C++/OpenMP backend and extend it to support more data types as the next step. This includes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Improve vectorization coverage&lt;/li&gt;
  &lt;li&gt;Support and optimize low precision kernel including BF16, FP16, Quantization&lt;/li&gt;
  &lt;li&gt;Training optimization&lt;/li&gt;
  &lt;li&gt;Loop tiling&lt;/li&gt;
  &lt;li&gt;Autotune&lt;/li&gt;
  &lt;li&gt;Further fusion optimization of Conv/GEMM kernels.&lt;/li&gt;
  &lt;li&gt;Explore alternative codegen paths: clang/llvm/triton&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Inductor C++/OpenMP backend is a flexible and efficient backend for the CPU. This blog describes the optimizations used in the C++/OpenMP backend of Inductor for inference and training of three benchmark suites – TorchBench, Hugging&lt;/p&gt;

&lt;p&gt;Face and TIMM. The primary optimizations include weight prepacking and post-operation fusion via the oneDNN library, as well as explicit vectorization in Inductor C++ codegen using AVX2 and AVX-512 instructions.&lt;/p&gt;

&lt;p&gt;The results show that 90% of inference kernels and 71% of training kernels are vectorized, indicating impressive vectorization for inference and room for improvement in training. In addition, we also applied other optimizations like buffer-reuse and CppWrapper. And we will continuously focus on the future work mentioned above to further improve the performance.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;The results presented in this blog post are the culmination of a collaborative effort between the Intel PyTorch team and Meta. We would like to express our sincere gratitude to &lt;a href=&quot;http://dev-discuss.pytorch.org/u/jansel&quot;&gt;@jansel&lt;/a&gt;, &lt;a href=&quot;http://dev-discuss.pytorch.org/u/desertfire&quot;&gt;@desertfire&lt;/a&gt;, and &lt;a href=&quot;http://dev-discuss.pytorch.org/u/chillee&quot;&gt;@Chillee&lt;/a&gt; for their invaluable contributions and unwavering support throughout the development process. Their expertise and dedication have been instrumental in achieving the optimizations and performance improvements discussed here.&lt;/p&gt;

&lt;h3 id=&quot;configuration-details&quot;&gt;Configuration Details&lt;/h3&gt;

&lt;h4 id=&quot;hardware-details&quot;&gt;Hardware Details&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Item &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Value &lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Manufacturer 
   &lt;/td&gt;
   &lt;td&gt;
Amazon EC2 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Product Name 
   &lt;/td&gt;
   &lt;td&gt;
c6i.16xlarge 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
CPU Model 
   &lt;/td&gt;
   &lt;td&gt;
Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Installed Memory 
   &lt;/td&gt;
   &lt;td&gt;
128GB (1x128GB DDR4 3200 MT/s [Unknown]) 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
OS 
   &lt;/td&gt;
   &lt;td&gt;
Ubuntu 22.04.2 LTS 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Kernel 
   &lt;/td&gt;
   &lt;td&gt;
5.19.0-1022-aws 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Microcode 
   &lt;/td&gt;
   &lt;td&gt;
0xd000389 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
GCC 
   &lt;/td&gt;
   &lt;td&gt;
gcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
GLIBC 
   &lt;/td&gt;
   &lt;td&gt;
ldd (Ubuntu GLIBC 2.35-0ubuntu3.1) 2.35 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Binutils 
   &lt;/td&gt;
   &lt;td&gt;
GNU ld (GNU Binutils for Ubuntu) 2.38 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Python 
   &lt;/td&gt;
   &lt;td&gt;
Python 3.10.6 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
OpenSSL 
   &lt;/td&gt;
   &lt;td&gt;
OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022) 
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;software-details&quot;&gt;Software Details&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;SW&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Nightly commit&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Main commit&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Pytorch
   &lt;/td&gt;
   &lt;td&gt;
a977a12
   &lt;/td&gt;
   &lt;td&gt;
0b1b063
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Torchbench
   &lt;/td&gt;
   &lt;td&gt;
/
   &lt;/td&gt;
   &lt;td&gt;
a0848e19
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchaudio
   &lt;/td&gt;
   &lt;td&gt;
0a652f5
   &lt;/td&gt;
   &lt;td&gt;
d5b2996
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchtext
   &lt;/td&gt;
   &lt;td&gt;
c4ad5dd
   &lt;/td&gt;
   &lt;td&gt;
79100a6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchvision
   &lt;/td&gt;
   &lt;td&gt;
f2009ab
   &lt;/td&gt;
   &lt;td&gt;
b78d98b
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchdata
   &lt;/td&gt;
   &lt;td&gt;
5cb3e6d
   &lt;/td&gt;
   &lt;td&gt;
f2bfd3d
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
dynamo_benchmarks
   &lt;/td&gt;
   &lt;td&gt;
fea73cb
   &lt;/td&gt;
   &lt;td&gt;
/
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;configuration&quot;&gt;Configuration&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Intel OpenMP&lt;/li&gt;
  &lt;li&gt;Jemalloc - oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:-1,muzzy_decay_ms:-1&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-Socket Multi-threads:&lt;/strong&gt; #of Instances: 1; Cores/Instance: 32&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-Core Single-thread:&lt;/strong&gt; #of Instances: 1; Cores/Instance: 1&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Story at a Glance</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">One Year of PyTorch Foundation</title>
      <link href="https://pytorch.org/blog/one-year-pytorch/" rel="alternate" type="text/html" title="One Year of PyTorch Foundation" />
      <published>2023-09-12T00:00:00-07:00</published>
      <updated>2023-09-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/one-year-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/one-year-pytorch/">&lt;p&gt;It’s been one year since we announced the formation of the PyTorch Foundation! 🎉&lt;/p&gt;

&lt;p&gt;In its inaugural year, the PyTorch Foundation made a significant impact by launching PyTorch 2.0, growing contributors and adding new member companies. We’re grateful to our founding members for their support to move the foundation forward.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A few milestones in the past year include:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;💻 Over 600,000 repositories on GitHub&lt;br /&gt;
✅ 60% of AI implementations choosing PyTorch&lt;br /&gt;
📈 More than 20% year over year growth in new repositories&lt;br /&gt;
🤝 Over 12,000 commits since last year&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;And a look at what the foundation has been up to this past year:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-timeline.svg&quot; alt=&quot;PyTorch project timeline&quot; style=&quot;width:100%; max-width: 662px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We look forward to growing our community for the years to come through supporting our contributors, democratizing the AI field, and creating new innovations.&lt;/p&gt;

&lt;p&gt;We invite you to join us at this year’s &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference&lt;/a&gt; on October 16-17 in San Francisco. Conference registration is filling up quickly, so take advantage of your chance to be part of this exciting event.&lt;/p&gt;

&lt;p&gt;Join us to stay informed about the latest announcements and have the opportunity to connect with both the founding members and new additions to the PyTorch community.&lt;/p&gt;

&lt;p&gt;With thanks and gratitude,&lt;br /&gt;
The PyTorch Foundation Team&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">It’s been one year since we announced the formation of the PyTorch Foundation! 🎉</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Graphcore Joins the PyTorch Foundation as a General Member</title>
      <link href="https://pytorch.org/blog/graphcore-joins-pytorch/" rel="alternate" type="text/html" title="Graphcore Joins the PyTorch Foundation as a General Member" />
      <published>2023-09-06T00:00:00-07:00</published>
      <updated>2023-09-06T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/graphcore-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/graphcore-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/graphcore-logo.jpg&quot; alt=&quot;Graphcore logo&quot; style=&quot;max-width:350px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Graphcore has joined as a general member.&lt;/p&gt;

&lt;p&gt;Graphcore is a UK-based company that specializes in designing and manufacturing AI accelerators, hardware and software specifically tailored for artificial intelligence and machine learning workloads.&lt;/p&gt;

&lt;p&gt;“We’re thrilled that PyTorch is the leading framework for development on the Graphcore  platform,” said Executive Director of the PyTorch Foundation Ibrahim Haddad. “Graphcore has played  an important role in the hardware and open source space, and we look forward to their continued contributions to PyTorch.”&lt;/p&gt;

&lt;p&gt;Graphcore has contributed to the PyTorch ecosystem by developing integrations to run on their IPU hardware. These integrations enable researchers and practitioners to use their preferred frameworks while taking advantage of Graphcore’s specialized hardware.&lt;/p&gt;

&lt;p&gt;“At Graphcore we’re truly aligned with PyTorch’s objective of reducing the barrier of entry to AI practitioners. By supporting a native PyTorch software environment for IPUs we are giving developers access to new underlying hardware, designed from the ground up for AI, to help unlock new AI techniques to improve efficiency or performance and to drive breakthroughs in AI research and applications, with the same user-friendly PyTorch framework they know and expect. We look forward to contributing to and growing the global AI community as an active member of the PyTorch Foundation and are proud to be the first general member.” Anthony Barbier, Software Frameworks Lead at Graphcore.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-graphcore&quot;&gt;About Graphcore&lt;/h2&gt;

&lt;p&gt;Graphcore compute systems are accelerating the AI revolution.  Powered by the groundbreaking Intelligence Processing Unit (IPU), Graphcore delivers leading-edge AI performance with unprecedented efficiency. IPUs are used around the world by organisations building their intelligent compute capabilities, including AI-centric startups, large multinational corporations and both public and private research institutions.  Graphcore is backed by some of the world’s leading investors and has attracted more than $700m of funding. The company is based in Bristol, UK, with offices across Europe, Asia and North America.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Automated trace collection and analysis</title>
      <link href="https://pytorch.org/blog/automated-trace-collection/" rel="alternate" type="text/html" title="Automated trace collection and analysis" />
      <published>2023-09-05T00:00:00-07:00</published>
      <updated>2023-09-05T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/automated-trace-collection</id>
      <content type="html" xml:base="https://pytorch.org/blog/automated-trace-collection/">&lt;p&gt;In this blog, we share how we enabled the collection and analysis of PyTorch Profiler traces for training workloads &lt;strong&gt;without any user side code instrumentation&lt;/strong&gt;. We leveraged Dynolog - an open source daemon for CPU and GPU telemetry to collect PyTorch Profiler traces, and analyzed the collected traces using Holistic Trace Analysis - an open source library for analyzing PyTorch Profiler traces. This toolchain has allowed engineers at Meta to accelerate their performance optimization workflows. The keystone to our solution was implementing pre and post hooks for the base Optimizer class in PyTorch. We demo PyTorch trace collection using Dynolog in a short video.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;Software developers at Meta run a large number of distributed training runs daily. In order to ensure that GPUs are being used effectively it is necessary to measure and analyze GPU performance for all jobs. Moreover, developers need the capability to introspect models and understand how CPUs and GPUs interact to debug performance issues. Developers build initial prototypes using a handful of GPUs and the production versions scale out to hundreds or thousands of GPUs, serving numerous business use cases such as generative AI, recommendation systems, ad ranking etc.&lt;/p&gt;

&lt;p&gt;Given the scale at Meta, it is necessary to have toolchains for performance measurement and monitoring which have low overhead and operate seamlessly with each other, to maintain high developer efficiency.&lt;/p&gt;

&lt;p&gt;In this blog, we describe how we use the PyTorch Profiler, Dynolog (a telemetry daemon) and Holistic Trace Analysis (a performance debugging library) to collect traces without any user side code instrumentation and analyze them to identify jobs with low GPU utilization.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;The diagram below shares an overview of how the toolchain works together.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;User launches a PyTorch application.&lt;/li&gt;
  &lt;li&gt;A training service or user triggers a profiling session using the Dynolog CLI which sends a request over the network to the Dynolog daemon.&lt;/li&gt;
  &lt;li&gt;Dynolog daemon relays the profiling configuration to the PyTorch application, setting it temporarily in a profiling mode.&lt;/li&gt;
  &lt;li&gt;PyTorch Profiler collects a trace and stores it to the database (e.g., network file system or S3 bucket).&lt;/li&gt;
  &lt;li&gt;The collected traces are then analyzed using Holistic Trace Analysis (HTA).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dyno_hta.png&quot; alt=&quot;Figure 1: Dynolog, PyTorch Profiler and HTA toolchain workflow&quot; style=&quot;width:100%; max-width: 662px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;mb-3&quot; style=&quot;text-align: center&quot;&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Dynolog, PyTorch Profiler and HTA toolchain workflow&lt;/em&gt;&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Let’s dig a bit deeper in each of the components.&lt;/p&gt;

&lt;h3 id=&quot;dynolog&quot;&gt;Dynolog&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://developers.facebook.com/blog/post/2022/11/16/dynolog-open-source-system-observability/&quot;&gt;Dynolog&lt;/a&gt; is a lightweight monitoring daemon for heterogeneous CPU-GPU systems. It supports continuous monitoring of &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/docs/Metrics.md&quot;&gt;performance metrics&lt;/a&gt; from the CPU (utilization, network bandwidth, instructions/second) and GPU (SM Occupancy, DRAM bandwidth, GPU power draw). Additionally, dynolog exports APIs to collect deep-dive profiling data that can be accessed via the dyno CLI.&lt;/p&gt;

&lt;p&gt;One of the chief integrations Dynolog offers is interfacing with the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html&quot;&gt;PyTorch Profiler&lt;/a&gt;. This enables &lt;a href=&quot;https://pytorch.org/blog/performance-debugging-of-production-pytorch-models-at-meta/&quot;&gt;on-demand remote tracing&lt;/a&gt; using a single command to trace thousands of servers. This can be accomplished by using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; command.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-profiler&quot;&gt;PyTorch Profiler&lt;/h3&gt;

&lt;p&gt;GPU kernels execute asynchronously, and GPU-side support is needed to create the trace. NVIDIA provides this visibility via the CUPTI library. Kineto is the subsystem within Profiler that interfaces with CUPTI. The &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/&quot;&gt;PyTorch Profiler&lt;/a&gt; leverages the &lt;a href=&quot;https://github.com/pytorch/kineto&quot;&gt;Kineto library&lt;/a&gt; to collect GPU traces. To enable automated profiling of training workloads at scale &lt;strong&gt;without any user side code instrumentation&lt;/strong&gt; we made a few fundamental changes to PyTorch. These changes enable trace collection without any user intervention.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Registration:** **First, we modified PyTorch to register with the Dynolog daemon on start up. This feature is switched on by setting the environment variable KINETO_USE_DAEMON=True. With this environment variable set to True, the PyTorch Profiler periodically polls Dynolog to check for on-demand tracing requests.&lt;/li&gt;
  &lt;li&gt;Iteration hooks: Then, we &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/89176&quot;&gt;implemented pre and post hooks for the base Optimizer class&lt;/a&gt;. This allowed us to annotate start/end of training iterations. The profiler is then aware of the iteration count and can safely capture a fixed number of iterations in the trace.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;holistic-trace-analysis-hta&quot;&gt;Holistic Trace Analysis (HTA)&lt;/h3&gt;

&lt;p&gt;ML researchers and engineers often struggle to computationally scale up their models as they are unaware of the performance bottlenecks in their workloads. Large distributed training jobs could generate thousands of traces, containing way too much data for a human to inspect. This is where &lt;a href=&quot;https://pytorch.org/blog/trace-analysis-for-masses/&quot;&gt;Holistic Trace Analysis&lt;/a&gt; comes in. HTA is an open source library for performance analysis - it takes as input PyTorch Profiler traces and up-levels the performance information contained in them. Its goal is to help researchers and engineers achieve the best performance from the hardware stack. To aid performance debugging HTA provides the following features (partial list):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/temporal_breakdown.html&quot;&gt;Temporal Breakdown&lt;/a&gt;: Breakdown of GPU time in terms of time spent in computation, communication, memory events, and idle time on a single node and across all ranks.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/idle_time_breakdown.html&quot;&gt;Idle Time Breakdown&lt;/a&gt;: Breakdown of GPU idle time into waiting for the host, waiting for another kernel or attributed to an unknown cause.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/kernel_breakdown.html&quot;&gt;Kernel Breakdown&lt;/a&gt;: Find kernels with the longest duration on each rank.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/kernel_breakdown.html#kernel-duration-distribution&quot;&gt;Kernel Duration Distribution&lt;/a&gt;: Distribution of average time taken by longest kernels across different ranks.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/comm_comp_overlap.html&quot;&gt;Communication Computation Overlap&lt;/a&gt;: Calculate the percentage of time when communication overlaps computation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We invite you to check out these &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/tree/main/examples&quot;&gt;Jupyter notebooks&lt;/a&gt; to see what HTA can do for you. If you are a first time user we recommend starting with the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/trace_analysis_demo.ipynb&quot;&gt;trace_analysis_demo&lt;/a&gt; notebook.&lt;/p&gt;

&lt;p&gt;To summarize, Dynolog allows us to collect PyTorch Profiler traces on-the-fly in a scalable manner. Furthermore, by leveraging HTA we can automate performance analysis and identify bottlenecks. At Meta, we use the Dynolog, PyTorch Profiler and HTA toolchain to accelerate our performance optimization workflows.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;We share a screencast showcasing trace collection without any user side code instrumentation for a toy PyTorch program. The demo runs in a docker container and the trace collection is triggered using Dynolog. HTA can be used to subsequently analyze the collected trace.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FjmHYMJLIdw?si=xahelamoBIja94Ox&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;faqs&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Q. What else can &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; do for me?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; command supports several custom PyTorch Profiler options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;capturing python stacks&lt;/li&gt;
  &lt;li&gt;memory profiling&lt;/li&gt;
  &lt;li&gt;record input shapes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace --help&lt;/code&gt; for all the options.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Q. Does Dynolog collect hardware performance metrics?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Dynolog can also be used for always-on monitoring:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It incorporates out-of-box &lt;a href=&quot;https://github.com/facebookincubator/dynolog/tree/main#gpu-monitoring&quot;&gt;GPU performance monitoring&lt;/a&gt; for NVIDIA GPUs using &lt;a href=&quot;https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/index.html#&quot;&gt;DCGM&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Dynolog provides basic Linux kernel &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/docs/Metrics.md&quot;&gt;performance metrics&lt;/a&gt; including CPU, network and IO resource usage.&lt;/li&gt;
  &lt;li&gt;Dynolog manages hardware performance counters for micro-architecture specific events related to CPU Cache, TLBs etc on Intel and AMD CPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Q: How can I build the Docker image used in the demo?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The dockerfile is available &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/dynolog_hta.dockerfile&quot;&gt;here&lt;/a&gt;. Use the command below to build the Docker image.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -f /path/to/dynolog_repo/dynolog_hta.dockerfile -t &amp;lt;image_name:tag&amp;gt; .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Q. How can I run the docker image?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You can refer to this &lt;a href=&quot;https://gist.github.com/anupambhatnagar/07ebff374bc45e4b63eb42893cca7e87&quot;&gt;cheat sheet&lt;/a&gt; to run the Docker image.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Anupam Bhatnagar, Brian Coutinho</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we share how we enabled the collection and analysis of PyTorch Profiler traces for training workloads without any user side code instrumentation. We leveraged Dynolog - an open source daemon for CPU and GPU telemetry to collect PyTorch Profiler traces, and analyzed the collected traces using Holistic Trace Analysis - an open source library for analyzing PyTorch Profiler traces. This toolchain has allowed engineers at Meta to accelerate their performance optimization workflows. The keystone to our solution was implementing pre and post hooks for the base Optimizer class in PyTorch. We demo PyTorch trace collection using Dynolog in a short video.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization</title>
      <link href="https://pytorch.org/blog/pytorch-xla-spmd/" rel="alternate" type="text/html" title="PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization" />
      <published>2023-08-31T00:00:00-07:00</published>
      <updated>2023-08-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-xla-spmd</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-xla-spmd/">&lt;p&gt;Today, we are delighted to announce PyTorch/XLA SPMD: the integration of &lt;a href=&quot;https://arxiv.org/pdf/2105.04663.pdf&quot;&gt;GSPMD&lt;/a&gt; into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; is an automatic parallelization system for ML workloads. The XLA compiler transforms the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. This allows developers to write PyTorch programs as if they are on a single large device without any custom sharded computation and/or collective communication ops to scale models.&lt;/p&gt;

&lt;p&gt;PyTorch/XLA SPMD allows PyTorch users to parallelize their ML workloads with GSPMD with less effort and with better performance. Some of the key highlights are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Better developer experience. Everything happens with a few &lt;a href=&quot;#simple-example-with-sharding-annotation&quot;&gt;sharding annotations&lt;/a&gt; from the user, and PyTorch/XLA SPMD achieves comparable performance to the most efficient PyTorch sharding implementation (see the Examples and Results section below). PyTorch/XLA SPMD separates the task of programming an ML model from the challenge of parallelization. Its automated approach to model sharding frees up the user from implementing the sharded version of ops with proper collectives in place.&lt;/li&gt;
  &lt;li&gt;A single API that enables a large variety of parallelism algorithms (including data parallelism, fully sharded data parallelism, spatial partitioning tensor and pipeline parallelism, as well as combinations of these algorithms) for different ML workloads and model architectures.&lt;/li&gt;
  &lt;li&gt;Industry-leading performance in large model training. PyTorch/XLA SPMD brings the powerful XLA GSPMD to PyTorch, enabling users to harness the full power of Google Cloud TPUs.&lt;/li&gt;
  &lt;li&gt;Enabling PyTorch and JAX developers take advantage of the same underlying XLA API to scale models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-concepts&quot;&gt;Key Concepts&lt;/h2&gt;

&lt;p&gt;The key concepts behind the sharding annotation API are: 1) Mesh, 2) Partition Spec, and 3) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API to express sharding intent using Mesh and Partition Spec. A more detailed design overview is available as a user guide &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/spmd.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mesh&quot;&gt;Mesh&lt;/h3&gt;

&lt;p&gt;For a given cluster of devices, a physical mesh is a representation of the interconnect topology.&lt;/p&gt;

&lt;p&gt;We derive a logical mesh based on this topology to create sub-groups of devices which can be used for partitioning different axes of tensors in a model. We apply sharding annotations to map the program across the logical mesh; this automatically inserts communication collectives in the program graph to support functional correctness (see the figure below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig1.png&quot; alt=&quot;SPMD on PyTorch/XLA&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We abstract logical mesh with &lt;a href=&quot;https://github.com/pytorch/xla/blob/028df4da388468fa9a41b1f98ea08bfce13b4c63/torch_xla/experimental/xla_sharding.py#L16&quot;&gt;Mesh API&lt;/a&gt;. The axes of the logical Mesh can be named. Here is an example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch_xla.runtime as xr
import torch_xla.experimental.xla_sharding as xs
from torch_xla.experimental.xla_sharding import Mesh

# Enable XLA SPMD execution mode.
xr.use_spmd()

# Assuming you are running on a TPU host that has 8 devices attached
num_devices = xr.global_runtime_device_count()
# mesh shape will be (4,2) in this example
mesh_shape = (num_devices // 2, 2)
device_ids = np.array(range(num_devices))
# axis_names 'x' nad 'y' are optional
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))

mesh.get_logical_mesh()
&amp;gt;&amp;gt; array([[0, 1],
          [2, 3],
          [4, 5],
          [6, 7]])
mesh.shape()
&amp;gt;&amp;gt; OrderedDict([('x', 4), ('y', 2)])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;partition-spec&quot;&gt;Partition Spec&lt;/h3&gt;

&lt;p&gt;partition_spec has the same rank as the input tensor. Each dimension describes how the corresponding input tensor dimension is sharded across the device mesh (logically defined by mesh_shape). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition_spec&lt;/code&gt; is a tuple of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device_mesh&lt;/code&gt; dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt;, None, or a tuple of mesh dimension indices. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt; can be an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;str&lt;/code&gt; if the corresponding mesh dimension is named. This specifies how each input rank is sharded (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mesh_shape&lt;/code&gt;) or replicated (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Provide optional mesh axis names and use them in the partition spec
mesh = Mesh(device_ids, (4, 2), ('data', 'model'))
partition_spec = ('model', 'data')
xs.mark_sharding(input_tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We support all three types of sharding described in the original &lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; paper. For instance, one can specify partial replication like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Provide optional mesh axis names and use them in the partition spec
mesh = Mesh(device_ids, (2, 2, 2), ('x', 'y', 'z'))

# evenly shard across x and z and replicate among y
partition_spec = ('x', 'z')  # equivalent to ('x', None, 'z')
xs.mark_sharding(input_tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;simple-example-with-sharding-annotation&quot;&gt;Simple Example With Sharding Annotation&lt;/h3&gt;

&lt;p&gt;Users can annotate native PyTorch tensors using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API (&lt;a href=&quot;https://github.com/pytorch/xla/blob/9a5fdf3920c18275cf7dba785193636f1b39ced9/torch_xla/experimental/xla_sharding.py#L388&quot;&gt;src&lt;/a&gt;). This takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt; as input and returns a &lt;a href=&quot;https://github.com/pytorch/xla/blob/03991d44a0a0297ced3ba9fc10ba451a4b6c94ab/torch_xla/experimental/xla_sharded_tensor.py#L55-L62&quot;&gt;XLAShardedTensor&lt;/a&gt; as output.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def mark_sharding(t: Union[torch.Tensor, XLAShardedTensor], mesh: Mesh, partition_spec: Tuple[Union[int, None]]) -&amp;gt; XLAShardedTensor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Invoking &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API takes a user defined logical &lt;a href=&quot;#mesh&quot;&gt;mesh&lt;/a&gt; and &lt;a href=&quot;#partition-spec&quot;&gt;partition_spec&lt;/a&gt; and generates a sharding annotation for the XLA compiler. The sharding specification is attached to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XLATensor&lt;/code&gt;, as well as the original input tensor. Here is a simple usage example from the [&lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt;], to illustrate how the sharding annotation API works:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch
import torch_xla.core.xla_model as xm
import torch_xla.runtime as xr
import torch_xla.experimental.xla_sharding as xs
from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
from torch_xla.experimental.xla_sharding import Mesh

# Enable XLA SPMD execution mode.
xr.use_spmd()

# Device mesh, this and partition spec as well as the input tensor shape define the individual shard shape.
num_devices = xr.global_runtime_device_count()
mesh_shape = (2, num_devicese // 2)  # 2x4 on v3-8, 2x2 on v4-8  
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))

t = torch.randn(8, 4).to(xm.xla_device())

# Mesh partitioning, each device holds 1/8-th of the input
partition_spec = (0, 1)
m1_sharded = xs.mark_sharding(t, mesh, partition_spec)
assert isinstance(m1_sharded, XLAShardedTensor) == True
# Note that the sharding annotation is also in-placed updated to t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can annotate different tensors in the PyTorch program to enable different parallelism techniques, as described in the comment below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Sharding annotate the linear layer weights. SimpleLinear() is a nn.Module.
model = SimpleLinear().to(xm.xla_device())
xs.mark_sharding(model.fc1.weight, mesh, partition_spec)

# Training loop
model.train()
for step, (data, target) in enumerate(loader):
  # Assumes `loader` returns data, target on XLA device
  optimizer.zero_grad()
  # Sharding annotate input data, we can shard any input
  # dimensions. Sharding the batch dimension enables 
  # data parallelism, sharding the feature dimension enables
  # spatial partitioning.
  xs.mark_sharding(data, mesh, partition_spec)
  ouput = model(data)
  loss = loss_fn(output, target)
  optimizer.step()
  xm.mark_step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More complete unit test cases and integration test examples are available in the PyTorch/XLA &lt;a href=&quot;https://github.com/pytorch/xla/tree/r2.0/test/spmd&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;We measured the performance of PyTorch/XLA SPMD using a GPT-2 model (&lt;a href=&quot;https://github.com/pytorch-tpu/transformers/tree/yeounoh_gpt2_spmd&quot;&gt;src&lt;/a&gt;) and compared it with &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;user-mode FSDP&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here, SPMD applies the same sharding scheme as the FSDP plot (i.e. 1D sharding). Users are expected to achieve better MFU results by exploring more advanced SPMD sharding schemes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig2.png&quot; alt=&quot;SPMD vs. FSDP&quot; style=&quot;width:100%; max-width: 600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We use Model FLOPS Utilization (MFU) as a metric for comparison. MFU is “the ratio of the observed throughput relative to the theoretical maximum throughput of a system operating at peak FLOPs” (&lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM paper&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;flops_per_step = 6 * global_batch_size * seq_len * num_params
model_flops_utilization = flops_per_step / step_time(s) / chip_count / flops_per_chip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This estimation assumes that the input dimensionality is much larger than the input sequence length (d_model » seq_len). If this assumption is violated the self-attention FLOPs start to be significant enough and this expression will underestimate the true MFU.&lt;/p&gt;

&lt;h3 id=&quot;scalability&quot;&gt;Scalability&lt;/h3&gt;

&lt;p&gt;One of the core benefits of SPMD is the flexible partitioning which can be used to save accelerator memory (HBM) usage and improve scalability. For scalability analysis, we present two studies: 1) we examine the peak HBM across 4 model sizes using Hugging Face transformers (GPT-2) as the base implementation; 2) we examine the peak HBM usage with &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus&quot;&gt;spatial partitioning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig3.png&quot; alt=&quot;Peak HBM Utilization&quot; style=&quot;width:100%; max-width: 600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above figure illustrates the unsharded 2B parameters model peak memory footprint stands at 26GB (red dashed line). harding model weights (model parallelism) reduces the peak memory footprint, and thus, enables larger model training with a given TPU pod slice. In  these experiments, we achieved up to 39.75% MFU on a 4B parameters model on Google Cloud TPU v4-16.&lt;/p&gt;

&lt;p&gt;We also ran an input batch scalability test using &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus&quot;&gt;spatial partitioning&lt;/a&gt; and a simple ResNet50 example (&lt;a href=&quot;https://github.com/pytorch/xla/blob/master/test/spmd/test_train_spmd_imagenet.py&quot;&gt;src&lt;/a&gt;) on Cloud TPU v4-8. Input batch is commonly sharded across the batch dimension for data parallelism (DDP, FSDP), but PyTorch/XLA SPMD enables input sharding across input feature dimensions for spatial sharding. As shown in the below figure, one can push the per-device batch size to 512 with spatial partitioning which is not possible with other data parallelism techniques.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig4.png&quot; alt=&quot;Batch size scaling with spatial partitioning&quot; style=&quot;width:100%; max-width: 741px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-road-forward-for-pytorchxla-spmd&quot;&gt;The Road Forward for PyTorch/XLA SPMD&lt;/h2&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. SPMD is still experimental, and we continuously add new features to it. In future releases, we plan to address async dataloading, partially replicated sharding, and other improvements. We’d love to &lt;a href=&quot;https://github.com/pytorch/xla#providing-feedback&quot;&gt;hear from you&lt;/a&gt;, answer your questions about PyTorch/XLA SPMD, and learn how you use SPMD.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yeounoh Chung, Jon Bolin, Milad Mohammadi, Jiewen Tan, Jack Cao, Joe Spisak, Alex Spiridonov, Shauheen Zahirazami, Steven Krawczyk, Wonjoo Lee Mohit Khatwani, Wanchao Liang, Vaibhav Singh</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we are delighted to announce PyTorch/XLA SPMD: the integration of GSPMD into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP</title>
      <link href="https://pytorch.org/blog/large-scale-training-hugging-face/" rel="alternate" type="text/html" title="Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP" />
      <published>2023-08-24T00:00:00-07:00</published>
      <updated>2023-08-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/large-scale-training-hugging-face</id>
      <content type="html" xml:base="https://pytorch.org/blog/large-scale-training-hugging-face/">&lt;p&gt;AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.&lt;/p&gt;

&lt;p&gt;To meet the growing demands of AI training at scale, last year we introduced &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;Fully Sharded Data Parallel (FSDP)&lt;/a&gt; in PyTorch/XLA. FSDP is a model parallelism architecture that unlocks the ability to easily and efficiently scale AI models into hundreds of billions of parameters. With &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/fsdp.md&quot;&gt;PyTorch/XLA FSDP&lt;/a&gt;, during distributed training, each device can store a specific model shard, and all-gather the full model weights when it is time to perform the forward pass. Nested FSDP further optimizes performance by only using a given layer’s full parameters during its forward pass.&lt;/p&gt;

&lt;p&gt;We are excited to announce that PyTorch/XLA FSDP has &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v4.27.0&quot;&gt;landed&lt;/a&gt; in &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Hugging Face Transformers&lt;/a&gt;. Now, Hugging Face users can train PyTorch models with up to 20 times more parameters using the same amount of computing power as before.&lt;/p&gt;

&lt;p&gt;We built PyTorch/XLA FSDP support directly into the Hugging Face Trainer class, so that any model using Trainer can leverage FSDP. And with the &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla/#fsdp-beta&quot;&gt;addition of automatic wrapping to PyTorch/XLA FSDP&lt;/a&gt;, nested FSDP wrapping is both flexible and simple to apply. These new features make it easy to train a wide range of Hugging Face models at large scales. In this guide, we demonstrate training GPT-2 models with up to 128B parameters on Google Cloud TPUs. PyTorch/XLA FSDP training on TPUs is highly efficient, achieving up to 45.1% model FLOPS utilization (MFU) for GPT-2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hugging_face_transformers.svg&quot; alt=&quot;Figure 1: Model FLOPS utilization for Hugging Face GPT-2 on Google Cloud TPU v4&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Model FLOPS utilization for Hugging Face GPT-2 on Google Cloud TPU v4&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;configuring-pytorchxla-fsdp-in-the-hugging-face-trainer&quot;&gt;Configuring PyTorch/XLA FSDP in the Hugging Face Trainer&lt;/h2&gt;

&lt;p&gt;First, follow your preferred method to create your TPU(s) and install PyTorch and PyTorch/XLA. You need versions &amp;gt;= 2.0 for PyTorch and PyTorch/XLA.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    pip3 install https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torc h-2.0-cp38-cp38-linux_x86_64.whl --user

    pip3 install https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torc h_xla-2.0-cp38-cp38-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, clone and install the Hugging Face Transformers repo. Install all necessary dependencies (e.g., datasets, evaluate, scikit-learn, accelerate).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    cd $HOME
    git clone https://github.com/huggingface/transformers.git cd transformers
    git checkout v4.31-release
    pip3 install -e .
    pip3 install datasets evaluate scikit-learn
    pip3 install accelerate==0.21.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/transformers&lt;/code&gt;, create any model-specific configuration files you might need. Here is an example of a configuration file for a GPT-2 model with 2B parameters, which we later refer to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt2_config.json&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;activation_function&quot;: &quot;gelu_new&quot;, 
    &quot;architectures&quot;: [
        &quot;GPT2LMHeadModel&quot;
    ],
    &quot;attn_pdrop&quot;: 0.1,
    &quot;bos_token_id&quot;: 50256, &quot;embd_pdrop&quot;: 0.1, &quot;eos_token_id&quot;: 50256, &quot;initializer_range&quot;: 0.02, &quot;layer_norm_epsilon&quot;: 1e-05, &quot;model_type&quot;: &quot;gpt2&quot;,
    &quot;n_embd&quot;: 3072,
    &quot;n_head&quot;: 24,
    &quot;n_layer&quot;: 18,
    &quot;n_positions&quot;: 1024,
    &quot;resid_pdrop&quot;: 0.1,
    &quot;summary_activation&quot;: null,
    &quot;summary_first_dropout&quot;: 0.1,
    &quot;summary_proj_to_labels&quot;: true,
    &quot;summary_type&quot;: &quot;cls_index&quot;,
    &quot;summary_use_proj&quot;: true,
    &quot;task_specific_params&quot;: {
        &quot;text-generation&quot;: {
            &quot;do_sample&quot;: true,
            &quot;max_length&quot;: 50
        }
    },
    &quot;vocab_size&quot;: 50257
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With PyTorch/XLA FSDP, it is possible to train model sizes much bigger than this on large accelerator slices. We have trained GPT-2 models as large as 128B parameters with these techniques; for expert tips on how to replicate this scale, see the appendix.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/transformers&lt;/code&gt;, create your FSDP configuration file, a JSON file containing all of the configurable aspects of your XLA FSDP wrapping stored as a dictionary. Following the &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;official Hugging Face Transformers XLA FSDP documentation&lt;/a&gt;, the following arguments are available to set:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla (bool, \*optional\*, defaults to False)&lt;/code&gt;: This is a boolean which determines whether or not you use XLA FSDP. Make sure to set this to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_fsdp_settings (dict, \*optional\*)&lt;/code&gt;: This is a dictionary which stores all of the XLA FSDP wrapping parameters you want to set; note that you do not have to specify settings for parameters where you are using the default value. For a complete list of settings, see &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_dtype&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buffer_dtype&lt;/code&gt;, enter these as strings which contain the corresponding torch data type, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bfloat16&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params (int, \*optional\*, defaults to 0)&lt;/code&gt;: An integer which sets the minimum number of parameters for size-based auto wrapping. Every module with at least as many parameters as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; will be XLA FSDP wrapped.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap (List[str], \*optional\*)&lt;/code&gt;: A list of (case-sensitive) transformer layer class names to wrap. Note that this is mutually exclusive with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt;. Example: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[&quot;GPT2Block&quot;, &quot;GPT2MLP&quot;]&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_fsdp_grad_ckpt (bool, \*optional\*, defaults to False)&lt;/code&gt;: This is a boolean which determines whether to use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be used when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla&lt;/code&gt; flag is set to true, and an auto wrapping policy is specified through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For transformer-based models, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; when performing automatic nested FSDP wrapping. Layers which share weights should not belong to separate FSDP wrapped units, and the input and output embedding layers in transformer-based models share weights.&lt;/p&gt;

&lt;p&gt;For this GPT-2 example, here is what the corresponding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_config.json&lt;/code&gt; file looks like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    {
        &quot;fsdp_transformer_layer_cls_to_wrap&quot;: [
            &quot;GPT2Block&quot;
        ],
        &quot;xla&quot;: true,
        &quot;xla_fsdp_settings&quot;: {
            &quot;compute_dtype&quot;: &quot;bfloat16&quot;,
            &quot;shard_param_on_dim_0&quot;: true,
            &quot;pin_layout_in_collective_ops&quot;: true
        },
       &quot;xla_fsdp_grad_ckpt&quot;: true
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Now, it’s time to train your model! First, ensure that you have your PyTorch/XLA runtime set up appropriately by setting&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    export PJRT_DEVICE=TPU
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When running training, the key flags to pass are:&lt;/p&gt;

&lt;p&gt;a) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fsdp &quot;full_shard&quot;&lt;/code&gt;
b) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fsdp_config fsdp_config.json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where you should replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_config.json&lt;/code&gt; with whatever you named your FSDP configuration file. Here is a sample command to train our example 2B GPT-2 model, where training is started by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_spawn.py&lt;/code&gt;, a &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/examples/pytorch/xla_spawn.py&quot;&gt;launcher script for&lt;/a&gt; distributed TPU training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    python3 -u examples/pytorch/xla_spawn.py --num_cores 4 examples/pytorch/language-modeling/run_clm.py \
    --num_train_epochs 1 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \ --per_device_train_batch_size 32 \ --per_device_eval_batch_size 32 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm \
    --overwrite_output_dir \
    --config_name gpt2_config.json \
    --cache_dir /tmp \
    --tokenizer_name gpt2 \
    --block_size 1024 \
    --optim adafactor \
    --adafactor true \
    --save_strategy no \
    --logging_strategy no \
    --fsdp &quot;full_shard&quot; \
    --fsdp_config fsdp_config.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;measuring-model-flops-utilization-mfu-for-gpt-2&quot;&gt;Measuring Model FLOPS Utilization (MFU) for GPT-2&lt;/h2&gt;

&lt;p&gt;Model FLOPS are the floating point operations required to perform a single forward and backward pass. Model FLOPS are hardware- and implementation- independent, and only depend on the underlying model. In each step, the number of FLOPS is computed via the following formulas:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tokens_per_batch = global_batch_size \* seq_len

FLOPS_per_step = 6 \* tokens_per_batch \* num_params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq_len&lt;/code&gt; is the sequence length and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_params&lt;/code&gt; is the number of parameters in the model. We note that this estimation assumes that the input dimensionality is much larger than the input sequence length (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d_model &amp;gt;&amp;gt; seq_len&lt;/code&gt;). If this assumption is violated the self-attention FLOPs start to be significant enough and this expression will underestimate the true MFU.&lt;/p&gt;

&lt;p&gt;Based on the step time and the hardware details (numbers of chips and the peak FLOPS per chip), we can compute Model FLOPS Utilization (MFU), which measures how effectively our implementation is using the underlying hardware. Achieving 100% MFU means that the hardware is being used perfectly by that model. We calculate MFU using the following formula:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model_FLOPS_utilization = FLOPS_per_step / step_time(s) / chip_count / FLOPS_per_chip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When training a GPT-2 model with 2B parameters with the XLA FSDP configuration file above on a Cloud TPU v4-8, we measure a step time of 4.191s. Using the above formula, we calculate 35.7% MFU on a v4-8. For further details on calculating MFU, refer to the &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The table below presents MFU for GPT-2 models with sizes between 2B and 128B, with a sequence length of 1024.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;TPU NumCores&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-8&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-64&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-128&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-128&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-256&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-512&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;# of Tokens / Batch&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;131,072&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;1,048,576&lt;/td&gt;
      &lt;td&gt;1,048,576&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;# of Parameters&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;2B&lt;/td&gt;
      &lt;td&gt;16B&lt;/td&gt;
      &lt;td&gt;20B&lt;/td&gt;
      &lt;td&gt;32B&lt;/td&gt;
      &lt;td&gt;64B&lt;/td&gt;
      &lt;td&gt;128B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Step Time (ms)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;4,191&lt;/td&gt;
      &lt;td&gt;14,592&lt;/td&gt;
      &lt;td&gt;7,824&lt;/td&gt;
      &lt;td&gt;12,970&lt;/td&gt;
      &lt;td&gt;25,653&lt;/td&gt;
      &lt;td&gt;30,460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PFLOPS / Step&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;404&lt;/td&gt;
      &lt;td&gt;809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;MFU&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;35.7%&lt;/td&gt;
      &lt;td&gt;38.8%&lt;/td&gt;
      &lt;td&gt;45.1%&lt;/td&gt;
      &lt;td&gt;44.4%&lt;/td&gt;
      &lt;td&gt;44.7%&lt;/td&gt;
      &lt;td&gt;37.7%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1: GPT-2 model FLOPS utilization calculation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Among these configurations, MFU peaks at 45.1% for the 20B parameter model on v4-128. This result compares favorably to, for example, 41.5% MFU for &lt;a href=&quot;https://arxiv.org/pdf/2205.05198.pdf&quot;&gt;a 22B Megatron-like model&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are two actionable insights from these experiments:&lt;/p&gt;

&lt;p&gt;First, simply increasing the number of chips without increasing the batch size generally means lower FLOPS utilization, because more time is spent on sharing the model shards. FSDP uses all-reduce communication collectives which are not asynchronous, which means that chip-to-chip communication cannot be overlapped with computation. As the number of chips increases, the number of model shards that must be communicated increases, and so we should expect the portion of the step time spent on communication to increase with the number of chips.&lt;/p&gt;

&lt;p&gt;Second, increasing the batch size generally means better FLOPS utilization. As the number of chips increases, the memory footprint of the model decreases, which often frees up high bandwidth memory (HBM) to scale up the global batch size. With a larger global batch size, the number of tokens processed in each step increases, and thus, so does the FLOPS per step. As long as the step time does not increase proportionally, we expect a larger global batch size to improve MFU.&lt;/p&gt;

&lt;p&gt;Therefore, to maximize the MFU, we recommend training with the largest global batch size possible that can fit in the HBM of the TPU slice, using FSDP to reduce memory required for the model parameters.&lt;/p&gt;

&lt;h2 id=&quot;training-very-large-models-tested-to-128b-parameters&quot;&gt;Training Very Large Models (tested to 128B parameters)&lt;/h2&gt;

&lt;p&gt;When using PyTorch/XLA, tensors must be initialized on the CPU before being moved to the XLA device. This means one may encounter host-side out-of-memory errors if the model is sufficiently large, even though the model can fit in the device HBM after sharding. To avoid this, we must defer each submodule’s initialization until it is FSDP wrapped, which ensures that submodules are sharded as soon as their values are populated, avoiding host-side limitations.&lt;/p&gt;

&lt;p&gt;Below, we explain how to modify a local copy of the Hugging Face transformers repository to train a GPT-2 model with up to 128B parameters using this technique.&lt;/p&gt;

&lt;p&gt;First, using the commands below, install torchdistX, which is a library containing experimental PyTorch Distributed features. This is the engine behind deferred initialization, and allows you to create tensors that don’t require immediate storage and can be materialized later. You also need to install a specific PyTorch/XLA 2.0 version that takes advantage of this package; note that you must uninstall PyTorch and PyTorch/XLA first, if you installed them earlier.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install torch==2.0 --index-url [https://download.pytorch.org/whl/test/cpu](https://download.pytorch.org/whl/test/cpu) --user
pip3 install torch_xla[torchdistx] -f https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/experimen tal/torch_xla-2.0-cp38-cp38-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, apply the following changes to your local copy of Hugging Face Transformers:&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src/transformers/trainer.py&lt;/code&gt;, add the following function in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_wrap_model&lt;/code&gt; on the line immediately prior to PyTorch/XLA FSDP wrapping:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchdistx import deferred_init

def _init_with_torchdistX(module):
    def check_fn(k):
        return not isinstance(k, FSDP)
    deferred_init.materialize_module(module, check_fn=check_fn)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;materialize_module&lt;/code&gt; will initialize the model tensors if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_fn&lt;/code&gt; returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt;. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_fn&lt;/code&gt; checks whether the module has been FSDP wrapped.&lt;/p&gt;

&lt;p&gt;Within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_wrap_model&lt;/code&gt;, modify your FSDP wrapping to accept the additional argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;param_init_fn=_init_with_torchdistX&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model = model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        auto_wrapper_callable=auto_wrapper_callable,
        param_init_fn=_init_with_torchdistX,
        \*\*fsdp_kwargs,
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;examples/pytorch/language-modeling/run_clm.py&lt;/code&gt;, add the following import statement at the beginning of the file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchdistx import deferred_init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Edit the model initialization so that the model is wrapped with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deferred_init.deferred_init&lt;/code&gt; by replacing the line&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = AutoModelForCausalLM.from_config(config)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = deferred_init.deferred_init(AutoModelForCausalLM.from_config, config)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that this assumes you are supplying your own model configuration file. Otherwise, you should modify your model initialization statement accordingly.&lt;/p&gt;

&lt;p&gt;You should also comment out these two lines which immediately follow the line above:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values()) logger.info(f&quot;Training new model from scratch - Total size={n_params/2\*\*20:.2f}M params&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;They will cause an error if left unmodified, since the model tensors do not actually have storage when these lines are executed.&lt;/p&gt;

&lt;p&gt;With these changes, you can now run GPT-2 models with as many as 128B parameters, provided the accelerator size is suitably large.&lt;/p&gt;

&lt;h2 id=&quot;next-steps--acknowledgements&quot;&gt;Next Steps &amp;amp; Acknowledgements&lt;/h2&gt;

&lt;p&gt;To learn more, the docs can be found &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;here&lt;/a&gt;. We’d love to &lt;a href=&quot;https://github.com/pytorch/xla#providing-feedback&quot;&gt;hear from you&lt;/a&gt; if you run into any issues with FSDP in PyTorch/XLA, or just want to tell us about how you are using it.&lt;/p&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; so that we can openly collaborate.&lt;/p&gt;

&lt;p&gt;We’d like to thank Ronghang Hu and Ross Girshick at Meta AI and Lysandre Debut, Sourab Mangrulkar, Sylvain Gugger and Arthur Zucker for all the support and collaboration. We’d also like to thank Jiewen Tan, Liyang Lu, Will Cromar, Vaibhav Singh, and Chandra Devarakonda for their assistance in preparing this post.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Alex Wertheim, Milad Mohammadi, Jack Cao, Alex Spiridonov, Joe Spisak, Lysandre Debut, Sylvain Gugger, Sourab Mangrulkar</name>
        
        
      </author>

      

      

      
        <summary type="html">AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Intel Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/intel-joins-pytorch/" rel="alternate" type="text/html" title="Intel Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-08-10T00:00:00-07:00</published>
      <updated>2023-08-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/intel-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/intel-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/intel-new-logo.svg&quot; alt=&quot;Intel logo&quot; style=&quot;max-width:250px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Intel has joined as a premier member.&lt;/p&gt;

&lt;p&gt;“The PyTorch Foundation is thrilled to welcome Intel as a premier member, marking a significant milestone in our mission to empower the global AI community. Intel’s extensive expertise and commitment to advancing cutting-edge technologies align perfectly with our vision of fostering open-source innovation,” said PyTorch Foundation Executive Director Ibrahim Haddad. “Together, we will accelerate the development and democratization of PyTorch, and use the collaboration to shape a vibrant future of AI for all.”&lt;/p&gt;

&lt;p&gt;Intel has developed and released several PyTorch-based tools and libraries to enable developers to accelerate their AI workflows, and is actively working on optimizing PyTorch to leverage Intel hardware capabilities.&lt;/p&gt;

&lt;p&gt;“At Intel, we believe in the power of collaboration and open-source innovation to propel the ecosystem towards an AI Everywhere future. Joining the Governing Board of the PyTorch Foundation is a testament to Intel’s commitment to advancing and democratizing AI,” said Wei Li, Vice President and General Manager of Artificial Intelligence and Analytics (AIA) at Intel. “By harnessing the collective expertise and resources within the deep learning community, we aim to accelerate the development of PyTorch and continue to drive breakthroughs in AI research and applications.”&lt;/p&gt;

&lt;p&gt;Intel fosters industry collaboration, co-engineering, and open source contributions to accelerate software innovation and develop new technologies that bring benefits to the open source community. By working together with other member companies and under the guidance of the PyTorch Foundation, Intel remains committed to actively contributing to and advocating for the community.&lt;/p&gt;

&lt;p&gt;As a premier member, Intel is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/wei-li.jpg&quot; alt=&quot;Wei Li&quot; style=&quot;max-width:250px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Wei Li, Vice President and General Manager of Artificial Intelligence and Analytics (AIA)  at Intel, to our board.  Dr. Wei Li is Vice President and General Manager of Artificial Intelligence and Analytics (AIA) at Intel, where he leads a world-wide team of engineering “magicians” who make AI Everywhere a reality by supercharging machine performance and developer productivity.  Wei and his team have been instrumental in Intel’s recent multi-billion-dollar AI revenue growth by delivering 10-100X software acceleration, across deep learning, statistical machine learning and big data analytics, to complement Intel’s AI-optimized hardware portfolio.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read more about Intel’s commitment to the PyTorch Community &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/ai-everywhere-intel-joins-pytorch-foundation.html#gs.4984sj&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-intel&quot;&gt;About Intel&lt;/h2&gt;

&lt;p&gt;Intel (Nasdaq: INTC) is an industry leader, creating world-changing technology that enables global progress and enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and manufacturing of semiconductors to help address our customers’ greatest challenges. By embedding intelligence in the cloud, network, edge and every kind of computing device, we unleash the potential of data to transform business and society for the better. To learn more about Intel’s innovations, go to&lt;a href=&quot;https://newsroom.intel.com/&quot;&gt; newsroom.intel.com&lt;/a&gt; and &lt;a href=&quot;https://intel.com/&quot;&gt;intel.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;© Intel Corporation. Intel, the Intel logo and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/legal/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">INT8 Quantization for x86 CPU in PyTorch</title>
      <link href="https://pytorch.org/blog/int8-quantization/" rel="alternate" type="text/html" title="INT8 Quantization for x86 CPU in PyTorch" />
      <published>2023-08-07T00:00:00-07:00</published>
      <updated>2023-08-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/int8-quantization</id>
      <content type="html" xml:base="https://pytorch.org/blog/int8-quantization/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;INT8 quantization is a powerful technique for speeding up deep learning inference on x86 CPU platforms. By reducing the precision of the model’s weights and activations from 32-bit floating-point (FP32) to 8-bit integer (INT8), INT8 quantization can significantly improve the inference speed and reduce memory requirements without sacrificing accuracy.&lt;/p&gt;

&lt;p&gt;In this blog, we will discuss the recent progress on INT8 quantization for x86 CPU in PyTorch, focusing on the new x86 quantization backend. We will also briefly look at the new quantization path with PyTorch 2.0 Export (PT2E) and TorchInductor.&lt;/p&gt;

&lt;h2 id=&quot;x86-quantization-backend&quot;&gt;X86 Quantization Backend&lt;/h2&gt;

&lt;p&gt;The current recommended way of quantization in PyTorch is &lt;a href=&quot;http://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html?highlight=fx&quot;&gt;FX&lt;/a&gt;. Before PyTorch 2.0, the default quantization backend (a.k.a. QEngine) on x86 CPUs was FBGEMM, which leveraged the FBGEMM performance library to achieve the performance speedup. In the PyTorch 2.0 release, a new quantization backend called X86 was introduced to replace FBGEMM. The x86 quantization backend offers improved INT8 inference performance when compared to the original FBGEMM backend by leveraging the strengths of both FBGEMM and the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/onednn.html&quot;&gt;Intel® oneAPI Deep Neural Network Library (oneDNN)&lt;/a&gt; kernel libraries.&lt;/p&gt;

&lt;h2 id=&quot;performance-benefit-from-x86-backend&quot;&gt;Performance Benefit from X86 Backend&lt;/h2&gt;

&lt;p&gt;To measure the performance benefits of the new X86 backend, we ran INT8 inference on 69 popular deep learning models (shown in &lt;strong&gt;Figures 1-3&lt;/strong&gt; below) using &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;4th Gen Intel® Xeon® Scalable processors&lt;/a&gt;. The results showed a 2.97X geomean performance speedup compared to FP32 inference performance, while the speedup was 1.43X with the FBGEMM backend. The charts below show the per-model performance speedup comparing the x86 backend and the FBGEMM backend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_1.jpg&quot; alt=&quot;Figure 1: Models with less than 2x performance boost with x86 backend1&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Models with less than 2x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_2.jpg&quot; alt=&quot;Figure 2: Models with 2x-4x performance boost with x86 backend1&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Models with 2x-4x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_3.jpg&quot; alt=&quot;Figure 3: Models with larger than 4x performance boost with x86 backend1&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Models with larger than 4x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;usage-of-x86-backend&quot;&gt;Usage of x86 Backend&lt;/h2&gt;

&lt;p&gt;By default in 2.0, users on x86 platforms will use the x86 quantization backend and their PyTorch programs will remain unchanged when using the default backend. Alternatively, users can specify x86 as the quantization backend explicitly. &lt;br /&gt;
Below is an example code snippet of PyTorch static post-training quantization with x86 quantization backend.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torch.ao.quantization import get_default_qconfig_mapping
from torch.quantization.quantize_fx import prepare_fx, convert_fx

qconfig_mapping = get_default_qconfig_mapping()
# Or explicity specify the qengine
# qengine = 'x86'
# torch.backends.quantized.engine = qengine
# qconfig_mapping = get_default_qconfig_mapping(qengine)

model_fp32 = MyModel().eval()
x = torch.randn((1, 3, 224, 224), dtype=torch.float)
x = x.to(memory_format=torch.channels_last)

# Insert observers according to qconfig and backend config
prepared_model = prepare_fx(model_fp32, qconfig_mapping, example_inputs=x)

# Calibration code not shown

# Convert to quantized model
quantized_model = convert_fx(prepared_model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;technical-details-of-x86-backend&quot;&gt;Technical Details of x86 Backend&lt;/h2&gt;

&lt;p&gt;We devised heuristic dispatching rules according to the performance numbers from the models we benchmarked to decide whether to invoke oneDNN or FBGEMM performance library to execute the convolution or matrix multiplication operations. The rules are a combination of operation kinds, shapes, CPU architecture information, etc. Detailed logic is available &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/93ff71ec37e3c946603600a46edef70b42f81213/aten/src/ATen/native/quantized/cpu/OnednnUtils.h#L396&quot;&gt;here&lt;/a&gt;. For more design and technical discussion, please refer to the &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;Request for Comments&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps-with-a-new-quantization-path-pytorch-20-export&quot;&gt;Next Steps With a New Quantization Path PyTorch 2.0 Export&lt;/h2&gt;

&lt;p&gt;Although still far from finalized, a new quantization path, PyTorch 2.0 Export (PT2E), is in early design and PoC stage. The new approach is slated to replace the FX quantization path in the future. It is built upon the capabilities of TorchDynamo Export, a feature introduced in the PyTorch 2.0 release for FX graph capturing. This graph is then quantized and lowered to different backends. TorchInductor, the new DL compiler of PyTorch, has shown promising results in terms of FP32 inference speedup on x86 CPU. We are working actively to enable it as one of the quantization backends of PT2E. We believe the new path will lead to further improvements in INT8 inference performance due to more flexibility of fusion at different levels.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The x86 backend introduced in PyTorch 2.0 release has demonstrated a remarkable improvement in INT8 inference speed on x86 CPU platforms. It offers a 1.43X speedup compared to the original FBGEMM backend while maintaining backward compatibility. This enhancement can benefit end users with minimal or no modifications to their programs. Furthermore, a new quantization path, PT2E, is currently in development and is expected to provide even more possibilities in the future.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;Special thanks to Nikita Shulga, Vasiliy Kuznetsov, Supriya Rao, and Jongsoo Park. Together, we made one more step forward on the path of improving the PyTorch CPU ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; AWS EC2 r7iz.metal-16xl instance (Intel(R) Xeon(R) Gold 6455B, 32-core/64-thread, Turbo Boost On, Hyper-Threading On, Memory: 8x64GB, Storage: 192GB); OS: Ubuntu 22.04.1 LTS; Kernel: 5.15.0-1028-aws; Batch Size: 1; Core per Instance: 4; PyTorch 2.0 RC3; TorchVision 0.15.0+cpu, test by Intel on 3/77/2023. May not reflect all publicly available security updates.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Hugging Face Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/hugging-face-joins/" rel="alternate" type="text/html" title="Hugging Face Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-08-03T00:00:00-07:00</published>
      <updated>2023-08-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hugging-face-joins</id>
      <content type="html" xml:base="https://pytorch.org/blog/hugging-face-joins/">&lt;p&gt;&lt;img src=&quot;/assets/images/huggingface-joins-1.jpg&quot; alt=&quot;Smiling hugging face&quot; style=&quot;max-width:250px;float:right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Hugging Face has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Hugging Face has been a long time supporter and contributor to the PyTorch Ecosystem by providing powerful models and resources that accelerate research, development, and adoption of AI technologies, particularly in the field of natural language processing.&lt;/p&gt;

&lt;p&gt;“Our mission has always been to democratize AI and make it accessible to everyone. We’re truly aligned with PyTorch’s objective of reducing the barrier of entry to practitioners. By joining the PyTorch Foundation, we can further amplify that impact and support this very important framework of the ecosystem that is PyTorch,” said Lysandre Debut, Head of Open Source at Hugging Face. “We believe the two ecosystems have significant overlap, and collaborating with the foundation will allow us to bridge the gap to provide the best software, the best tools to the machine learning community at large.”&lt;/p&gt;

&lt;p&gt;Hugging Face’s Model Hub and open source libraries promote collaboration and knowledge sharing within the AI open source community, making Hugging Face a great match to the growing PyTorch Foundation. They continue to drive industry adoption and collaboration by creating user-friendly tools and resources and providing accessible and well-documented libraries.&lt;/p&gt;

&lt;p&gt;“Hugging Face’s commitment to open source development and their exceptional contributions to the PyTorch ecosystem have truly impressed us. With their help, we will drive innovation, foster collaboration, and empower the global AI community to create transformative solutions for the AI community,” said PyTorch Foundation Executive Director Ibrahim Haddad. “We welcome Hugging Face to the PyTorch Foundation and look forward to the achievements that lie ahead.”&lt;/p&gt;

&lt;p&gt;As a premier member, Hugging Face is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/huggingface-joins-2.jpg&quot; alt=&quot;Lysandre Debut&quot; style=&quot;max-width:250px;float:right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Lysandre Debut, Head of Open Source at Hugging Face to our board.  Lysandre has been at Hugging Face since the company’s pivot to open-source, and was the first engineer to focus entirely on the open-source mission. Now leading the open-source part of the organization, Lysandre remains technically involved by being a core maintainer of the Transformers library.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-hugging-face&quot;&gt;About Hugging Face&lt;/h2&gt;

&lt;p&gt;Hugging Face is a community and company dedicated to lowering the barrier of entry to Machine Learning and Deep Learning. Strong advocates for open-source and open-science, their model Hub hosts more than 250,000 public models and 50,000 public datasets that are very simple to use. Transformers, Diffusers, PEFT, Accelerate, and Datasets are some of the open-source tools made available by Hugging Face.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its trademark usage page: www.linuxfoundation.org/trademark-usage. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AMD’s Journey to Openness and Performance</title>
      <link href="https://pytorch.org/blog/amd-journey/" rel="alternate" type="text/html" title="AMD's Journey to Openness and Performance" />
      <published>2023-08-01T00:00:00-07:00</published>
      <updated>2023-08-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/amd-journey</id>
      <content type="html" xml:base="https://pytorch.org/blog/amd-journey/">&lt;p&gt;AMD has gained progress in building a robust software stack that supports an open ecosystem of models, libraries, frameworks, and tools. With proven platforms gaining momentum, there is significance of a leadership software stack and an optimized ecosystem for achieving application performance. PyTorch is a key part of AMD’s AI  journey, and AMD’s Victor Peng, AMD President and Soumith Chintala, founder of PyTorch discussed the latest progress at the DC &amp;amp; AI Keynote on June 12.&lt;/p&gt;

&lt;h2 id=&quot;building-a-powerful-sw-stack-with-rocm&quot;&gt;Building a Powerful SW Stack with ROCm&lt;/h2&gt;

&lt;p&gt;Victor introduced ROCm, AMD’s SW stack for Instinct Data Center GPUs. It offers a comprehensive set of open-source libraries, runtime, compilers, and tools for developing, running, and fine-tuning AI models. The fifth generation ROCm incorporates optimizations for AI and high-performance computing workloads, including tailored kernels for low-latency memory systems, support for new data types, and integration with OpenAI Triton. With tools for porting AI software to AMD Instinct platforms, ROCm ensures quality and robustness, tested extensively and compliant with PyTorch and TensorFlow frameworks.&lt;/p&gt;

&lt;h2 id=&quot;collaboration-with-pytorch&quot;&gt;Collaboration with PyTorch&lt;/h2&gt;

&lt;p&gt;To shed light on the partnership between AMD and PyTorch, Victor invited &lt;a href=&quot;https://www.linkedin.com/in/soumith/&quot;&gt;Soumith Chintala&lt;/a&gt;, the founder of PyTorch, to discuss the advancements and integration between the two. PyTorch, the industry’s most famous AI framework, boasts a vibrant developer community and is known for its continuous innovation and incorporation of cutting-edge research.&lt;/p&gt;

&lt;p&gt;To highlight the AMD and PyTorch partnership, Victor hosted a discussion with Soumith Chintala, the founder of PyTorch. PyTorch, renowned for its innovation and community, is the industry’s leading AI framework. The latest version, PyTorch 2.0, integrates with hardware-agnostic software compilers like OpenAI Triton, enabling efficient training and deployment of AI models. With optimized techniques, PyTorch 2.0 enhances productivity and offers remarkable speed improvements. The collaboration between AMD and the PyTorch Foundation ensures seamless utilization of AMD GPUs, expanding AI accelerator accessibility worldwide and paving the way for future optimizations and broader hardware support.&lt;/p&gt;

&lt;h2 id=&quot;empowering-the-developer-community&quot;&gt;Empowering the Developer Community&lt;/h2&gt;

&lt;p&gt;The partnership between AMD and PyTorch benefits the developer community by democratizing access to AI accelerators. Support for AMD GPUs in PyTorch allows developers to train and deploy models across various platforms, including CPUs like EPYC and Ryzen, GPUs like Instinct and Radeon, and embedded devices like Versal SoCs. By ensuring immediate compatibility of new models on AMD platforms, the collaboration streamlines the development process and empowers developers to leverage the full potential of AMD’s hardware. This increased accessibility and flexibility enable developers worldwide to push the boundaries of AI innovation.&lt;/p&gt;

&lt;h2 id=&quot;hugging-face-and-ai-model-innovation&quot;&gt;Hugging Face and AI Model Innovation&lt;/h2&gt;

&lt;p&gt;Victor praised Hugging Face as the leading force behind open-source AI model innovation, empowering generative AI with transformative transformers. AMD’s optimized software enables a high-performing development stack, supporting groundbreaking AI advancements for customers and developers through scalable real-world deployments.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At the DC &amp;amp; AI Keynote, AMD demonstrated its dedication to openness, performance, and collaboration. The ROCm SW stack, PyTorch integration, and support for Hugging Face exemplify AMD’s commitment to empowering developers and researchers to achieve AI breakthroughs. By offering accessible, high-performing solutions, AMD fuels the future of AI as a leading GPU platform integrated with PyTorch.&lt;/p&gt;

&lt;p&gt;To listen to the full keynote visit the &lt;a href=&quot;https://www.youtube.com/watch?v=l3pe_qx95E0&quot;&gt;AMD Youtube&lt;/a&gt; channel&lt;/p&gt;

&lt;p&gt;To listen to Soumith Chintala’s section of the &lt;a href=&quot;https://www.youtube.com/watch?v=RgQEG2G1iaY&quot;&gt;keynote&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">AMD has gained progress in building a robust software stack that supports an open ecosystem of models, libraries, frameworks, and tools. With proven platforms gaining momentum, there is significance of a leadership software stack and an optimized ecosystem for achieving application performance. PyTorch is a key part of AMD’s AI journey, and AMD’s Victor Peng, AMD President and Soumith Chintala, founder of PyTorch discussed the latest progress at the DC &amp;amp; AI Keynote on June 12.</summary>
      

      
      
    </entry>
  
</feed>


