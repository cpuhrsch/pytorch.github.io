<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-05-16T09:12:08-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">PyTorch Conference 2023: Join us in San Francisco October 16-17</title>
      <link href="https://pytorch.org/blog/pytorch-conference-2023/" rel="alternate" type="text/html" title="PyTorch Conference 2023: Join us in San Francisco October 16-17" />
      <published>2023-05-16T00:00:00-07:00</published>
      <updated>2023-05-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-conference-2023</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-conference-2023/">&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conf-2023.png&quot; alt=&quot;PyTorch Conference 2023&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re thrilled to announce the upcoming &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference 2023&lt;/a&gt;! On October 16-17, the conference will showcase PyTorch 2.0, the next-generation release of the popular machine learning framework. As part of the Linux Foundation, the PyTorch Foundation Conference continues the tradition of bringing together leading researchers, developers, and academic communities to advance the education and development of end-to-end machine learning.&lt;/p&gt;

&lt;p&gt;The conference agenda features an engaging lineup of events, including an opening reception, engaging community and partner discussions, informative panels, poster sessions, enlightening use cases and community stories, as well as discussions on the latest trends in machine learning and deep learning development and deployment.&lt;/p&gt;

&lt;h2 id=&quot;call-for-proposals&quot;&gt;Call for Proposals&lt;/h2&gt;

&lt;p&gt;We are now accepting speaker proposals for the conference until &lt;strong&gt;July 21&lt;/strong&gt;. The program committee will carefully review all submissions, and selected speakers will be notified by &lt;strong&gt;August 8&lt;/strong&gt;. We strongly encourage both experienced and first-time speakers to submit their proposals. This conference provides an excellent opportunity to connect with the PyTorch community, share your ideas, and showcase your work.&lt;/p&gt;

&lt;p&gt;When preparing your proposal, please consider the following guidelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are you hoping to get from your presentation?&lt;/li&gt;
  &lt;li&gt;What do you expect the audience to gain from your presentation?&lt;/li&gt;
  &lt;li&gt;How will your presentation help better the open source ecosystem?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To help you shape your proposal, here are some suggested topics for the conference:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deployments on AWS, Azure&lt;/li&gt;
  &lt;li&gt;Use cases and real-world applications&lt;/li&gt;
  &lt;li&gt;Foundational models&lt;/li&gt;
  &lt;li&gt;AI practices&lt;/li&gt;
  &lt;li&gt;Production considerations&lt;/li&gt;
  &lt;li&gt;PyTorch 2.X features and updates&lt;/li&gt;
  &lt;li&gt;Training techniques and best practices&lt;/li&gt;
  &lt;li&gt;Inference methodologies&lt;/li&gt;
  &lt;li&gt;Hardware advancements and optimizations&lt;/li&gt;
  &lt;li&gt;Edge computing applications&lt;/li&gt;
  &lt;li&gt;Scalability solutions&lt;/li&gt;
  &lt;li&gt;Latest research breakthroughs&lt;/li&gt;
  &lt;li&gt;Optimization strategies&lt;/li&gt;
  &lt;li&gt;Extending PyTorch through customizations and plugins&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We kindly request that you refrain from submitting sales or marketing pitches and avoid discussing unlicensed or closed-source technologies. Such talks tend to detract from the integrity of our events and are not well-received by conference attendees.&lt;/p&gt;

&lt;h2 id=&quot;register-today&quot;&gt;Register Today&lt;/h2&gt;

&lt;p&gt;Registration is now open! Get your ticket today and secure your spot: &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/register/&quot;&gt;https://events.linuxfoundation.org/pytorch-conference/register/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you for your interest, and we look forward to a successful PyTorch Conference 2023!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Language Identification: Building an End-to-End AI Solution using PyTorch</title>
      <link href="https://pytorch.org/blog/language-identification/" rel="alternate" type="text/html" title="Language Identification: Building an End-to-End AI Solution using PyTorch" />
      <published>2023-05-12T00:00:00-07:00</published>
      <updated>2023-05-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/language-identification</id>
      <content type="html" xml:base="https://pytorch.org/blog/language-identification/">&lt;p&gt;Language Identification is the process of identifying the primary language from multiple audio input samples. In natural language processing (NLP), language identification is an important problem and a challenging issue. There are many language-related tasks such as entering text on your phone, finding news articles you enjoy, or discovering answers to questions that you may have. All these tasks are powered by NLP models. To decide which model to invoke at a particular point in time, we must perform language identification.&lt;/p&gt;

&lt;p&gt;This article presents an in-depth solution and code sample for language identification using &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt;, which is a version of the popular PyTorch AI framework optimized for use on Intel® processors, and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&quot;&gt;Intel® Neural Compressor&lt;/a&gt;, which is a tool to accelerate AI inference without sacrificing accuracy.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification&quot;&gt;code sample&lt;/a&gt; demonstrates how to train a model to perform language identification using the Hugging Face SpeechBrain* toolkit and optimize it using the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit-download.html&quot;&gt;Intel® AI Analytics Toolkit (AI Kit)&lt;/a&gt;. The user can modify the code sample and identify up to 93 languages using the Common Voice dataset.&lt;/p&gt;

&lt;h2 id=&quot;proposed-methodology-for-language-identification&quot;&gt;Proposed Methodology for Language Identification&lt;/h2&gt;

&lt;p&gt;In the proposed solution, the user will use an Intel AI Analytics Toolkit container environment to train a model and perform inference leveraging Intel-optimized libraries for PyTorch. There is also an option to quantize the trained model with Intel Neural Compressor to speed up inference.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;http://commonvoice.mozilla.org/en/datasets&quot;&gt;Common Voice&lt;/a&gt; dataset is used and for this code sample, specifically, Common Voice Corpus 11.0 for Japanese and Swedish. This dataset is used to train an &lt;a href=&quot;http://arxiv.org/abs/2005.07143&quot;&gt;Emphasized Channel Attention, Propagation and Aggregation Time Delay Neural Network (ECAPA-TDNN)&lt;/a&gt;, which is implemented using the &lt;a href=&quot;http://huggingface.co/SpeechBrain&quot;&gt;Hugging Face SpeechBrain&lt;/a&gt; library. Time Delay Neural Networks (TDNNs), aka one-dimensional Convolutional Neural Networks (1D CNNs), are multilayer artificial neural network architectures to classify patterns with shift-invariance and model context at each layer of the network. ECAPA-TDNN is a new TDNN-based speaker-embedding extractor for speaker verification; it is built upon the original x-vector architecture and puts more emphasis on channel attention, propagation, and aggregation.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;After downloading the Common Voice dataset, the data is preprocessed by converting the MP3 files into WAV format to avoid information loss and separated into training, validation, and testing sets.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;http://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa&quot;&gt;pretrained VoxLingua107 &lt;/a&gt;model is retrained with the Common Voice dataset using the Hugging Face SpeechBrain library to focus on the languages of interest. &lt;a href=&quot;http://bark.phon.ioc.ee/voxlingua107/&quot;&gt;VoxLingua107&lt;/a&gt; is a speech dataset used for training spoken language recognition models that work well with real-world and varying speech data. This dataset contains data for 107 languages. By default, Japanese and Swedish are used, and more languages can be included. This model is then used for inference on the testing dataset or a user-specified dataset. Also, there is an option to utilize SpeechBrain’s Voice Activity Detection (VAD) where only the speech segments from the audio files are extracted and combined before samples are randomly selected as input into the model. This &lt;a href=&quot;http://huggingface.co/speechbrain/vad-crdnn-libriparty&quot;&gt;link&lt;/a&gt; provides all the necessary tools to perform VAD. To improve performance, the user may quantize the trained model to integer-8 (INT8) using Intel Neural Compressor to decrease latency.&lt;/p&gt;

&lt;h4 id=&quot;training&quot;&gt;Training&lt;/h4&gt;

&lt;p&gt;The copies of training scripts are added to the current working directory, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_wds_shards.py&lt;/code&gt; - for creating the &lt;a href=&quot;http://github.com/webdataset/webdataset&quot;&gt;WebDataset&lt;/a&gt; shards, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt; - to perform the actual training procedure, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_ecapa.yaml&lt;/code&gt; - to configure the training options. The script to create WebDataset shards and YAML file are patched to work with the two languages chosen for this code sample.&lt;/p&gt;

&lt;p&gt;In the data preprocessing phase, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepareAllCommonVoice.py&lt;/code&gt; script is executed to randomly select a specified number of samples to convert the input from MP3 to WAV format. Here, 80% of these samples will be used for training, 10% for validation, and 10% for testing. At least 2000 samples are recommended as the number of input samples and is the default value.&lt;/p&gt;

&lt;p&gt;In the next step, WebDataset shards are created from the training and validation datasets. This stores the audio files as tar files which allows writing purely sequential I/O pipelines for large-scale deep learning in order to achieve high I/O rates from local storage—about 3x-10x faster compared to random access.&lt;/p&gt;

&lt;p&gt;The YAML file will be modified by the user. This includes setting the value for the largest number for the WebDataset shards, output neurons to the number of languages of interest, number of epochs to train over the entire dataset, and the batch size. The batch size should be decreased if the CPU or GPU runs out of memory while running the training script.&lt;/p&gt;

&lt;p&gt;In this code sample, the training script will be executed with CPU. While running the script, “cpu” will be passed as an input parameter. The configurations defined in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_ecapa.yaml&lt;/code&gt; are also passed as parameters.&lt;/p&gt;

&lt;p&gt;The command to run the script to train the model is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python train.py train_ecapa.yaml --device &quot;cpu&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the future, the training script train.py will be designed to work for Intel® GPUs such as the Intel® Data Center GPU Flex Series, Intel® Data Center GPU Max Series, and Intel® Arc™ A-Series with updates from Intel Extension for PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification#train-the-model-with-languages&quot;&gt;Run the training script&lt;/a&gt; to learn how to train the models and execute the training script. The 4th Generation Intel® Xeon® Scalable Processor is recommended for this &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/transfer-learning.html&quot;&gt;transfer learning&lt;/a&gt; application because of its performance improvements through its Intel® Advanced Matrix Extensions (Intel® AMX) instruction set.&lt;/p&gt;

&lt;p&gt;After training, checkpoint files are available. These files are used to load the model for inference.&lt;/p&gt;

&lt;h4 id=&quot;inference&quot;&gt;Inference&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-inference-pipeline-language-identification.png&quot; alt=&quot;Inference Pipeline&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The crucial step before running inference is to patch the SpeechBrain library’s pretrained &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interfaces.py&lt;/code&gt; file so that PyTorch TorchScript* can be run to improve the runtime. TorchScript requires the output of the model to be only tensors.&lt;/p&gt;

&lt;p&gt;Users can choose to run inference using the testing set from Common Voice or their own custom data in WAV format. The following are the options the inference scripts (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inference_custom.py and inference_commonVoice.py&lt;/code&gt;) can be run with:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Input Option&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Description&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;-p
   &lt;/td&gt;
   &lt;td&gt;Specify the data path.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;-d
   &lt;/td&gt;
   &lt;td&gt;Specify the duration of wave sample. The default value is &lt;strong&gt;3&lt;/strong&gt;.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;-s
   &lt;/td&gt;
   &lt;td&gt;Specify size of sample waves, default is &lt;strong&gt;100&lt;/strong&gt;.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--vad
   &lt;/td&gt;
   &lt;td&gt;(`inference_custom.py` only) Enable VAD model to detect active speech. The VAD option will identify speech segments in the audio file and construct a new &lt;strong&gt;.wav&lt;/strong&gt; file containing only the speech segments. This improves the quality of speech data used as input into the language identification model.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--ipex
   &lt;/td&gt;
   &lt;td&gt;Run inference with optimizations from Intel Extension for PyTorch. This option will apply optimizations to the pretrained model. Using this option should result in performance improvements related to latency.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--ground_truth_compare
   &lt;/td&gt;
   &lt;td&gt;(`inference_custom.py` only) Enable comparison of prediction labels to ground truth values.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--verbose
   &lt;/td&gt;
   &lt;td&gt;Print additional debug information, like latency.
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The path to the data must be specified. By default, 100 audio samples of 3-seconds will be randomly selected from the original audio file and used as input to the language identification model.&lt;/p&gt;

&lt;p&gt;A small Convolutional Recurrent Deep Neural Network (CRDNN) pretrained on the &lt;a href=&quot;http://drive.google.com/file/d/1--cAS5ePojMwNY5fewioXAv9YlYAWzIJ/view&quot;&gt;LibriParty&lt;/a&gt; dataset is used to process audio samples and output the segments where speech activity is detected. This can be used in inference with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--vad&lt;/code&gt; option.&lt;/p&gt;

&lt;p&gt;From the figure below, the timestamps where speech will be detected is delivered from the CRDNN model, and these are used to construct a new, shorter audio file with only speech. Sampling from this new audio file will give a better prediction of the primary language spoken.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-timestamps-delivered-from-crdnn-model.png&quot; alt=&quot;Audio wave file visualization&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification#run-inference&quot;&gt;Run the inference script&lt;/a&gt; yourself. An example command of running inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python inference_custom.py -p data_custom -d 3 -s 50 --vad
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will run inference on data you provide located inside the &lt;em&gt;data_custom&lt;/em&gt; folder. This command performs inference on 50 randomly selected 3-second audio samples with voice activity detection.&lt;/p&gt;

&lt;p&gt;If you want to run the code sample for other languages, download Common Voice Corpus 11.0 datasets for other languages.&lt;/p&gt;

&lt;h2 id=&quot;optimizations-with-intel-extension-for-pytorch-and-intel-neural-compressor&quot;&gt;Optimizations with Intel Extension for PyTorch and Intel Neural Compressor&lt;/h2&gt;

&lt;h3 id=&quot;pytorch&quot;&gt;PyTorch&lt;/h3&gt;

&lt;p&gt;The Intel extension expands PyTorch with up-to-date features and optimizations for an extra performance boost on Intel hardware. Check out &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch#installation&quot;&gt;how to install Intel Extension for PyTorch&lt;/a&gt;. The extension can be loaded as a Python module or linked as a C++ library. Python users can enable it dynamically by importing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel_extension_for_pytorch&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/cpu/latest/&quot;&gt;CPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel CPUs. Source code is available at the &lt;a href=&quot;https://github.com/intel/intel-extension-for-pytorch/tree/master&quot;&gt;master branch&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/xpu/latest/&quot;&gt;GPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel GPUs. Source code is available at the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/xpu-master&quot;&gt;xpu-master branch&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To optimize the model for inference using Intel Extension for PyTorch, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--ipex&lt;/code&gt;option can be passed in. The model is optimized using the plug-in. TorchScript speeds up inference because PyTorch is run in graph mode. The command to run with this optimization is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python inference_custom.py -p data_custom -d 3 -s 50 --vad --ipex --verbose
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--verbose&lt;/code&gt; option is required to view the latency measurements.&lt;/p&gt;

&lt;p&gt;Auto-mixed precision such as bfloat16 (BF16) support will be added in a future release of the code sample.&lt;/p&gt;

&lt;h3 id=&quot;intel-neural-compressor&quot;&gt;Intel Neural Compressor&lt;/h3&gt;

&lt;p&gt;This is an open-source Python library that runs on CPUs or GPUs, which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Performs model quantization to reduce the model size and increase the speed of deep learning inference for deployment.&lt;/li&gt;
  &lt;li&gt;Automates popular methods such as quantization, compression, pruning, and knowledge distillation across multiple deep-learning frameworks.&lt;/li&gt;
  &lt;li&gt;Is part of the AI Kit&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model can be quantized from float32 (FP32) precision to integer-8 (INT8) by running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_model.py&lt;/code&gt; script while passing in the path to the model and a validation dataset. The following code can be used to load this INT8 model for inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from neural_compressor.utils.pytorch import load
model_int8 = load(&quot;./lang_id_commonvoice_model_INT8&quot;, self.language_id)
signal = self.language_id.load_audio(data_path)
prediction = self.model_int8(signal)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the original model is required when loading the quantized model. The command to quantize the trained model from FP32 to INT8 by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_model.py&lt;/code&gt; is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python quantize_model.py -p ./lang_id_commonvoice_model -datapath $COMMON_VOICE_PATH/commonVoiceData/commonVoice/dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Try out the above code sample by upgrading the hardware to a 4th Generation Intel Xeon Scalable Processor with Intel AMX and identify up to 93 different languages from Common Voice datasets.&lt;/p&gt;

&lt;p&gt;We encourage you to learn more about and incorporate Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;AI/ML Framework optimizations&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;end-to-end portfolio of tools&lt;/a&gt; into your AI workflow. Also, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI &amp;amp; ML page&lt;/a&gt; covering Intel’s AI software development resources for preparing, building, deploying, and scaling your AI solutions.&lt;/p&gt;

&lt;p&gt;For more details about the new 4th Gen Intel Xeon Scalable processors, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;Intel’s AI Solution Platform portal&lt;/a&gt; where you can learn how Intel is empowering developers to run end-to-end AI pipelines on these powerful CPUs.&lt;/p&gt;

&lt;h3 id=&quot;useful-resources&quot;&gt;Useful resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;Intel AI Developer Tools and resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI unified programming model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-tensorflow.html&quot;&gt;Official documentation - Intel® Optimization for TensorFlow*&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&quot;&gt;Official documentation - Intel® Neural Compressor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/ai-solution-brief.html&quot;&gt;Accelerate AI Workloads with Intel® AMX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;explore-more-ai-code-samples&quot;&gt;Explore more AI code samples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;Optimize PyTorch Models using Intel® Extension for PyTorch (IPEX) Quantization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPyTorch_TrainingOptimizations_AMX_BF16&quot;&gt;PyTorch Training Optimizations with Advanced Matrix Extensions Bfloat16&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Getting-Started-Samples/INC-Sample-for-Tensorflow&quot;&gt;Intel® Neural Compressor TensorFlow* Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/code-samples.html&quot; class=&quot;btn btn-lg with-right-arrow&quot; data-cta=&quot;get-started&quot;&gt;See all code samples&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Language Identification is the process of identifying the primary language from multiple audio input samples. In natural language processing (NLP), language identification is an important problem and a challenging issue. There are many language-related tasks such as entering text on your phone, finding news articles you enjoy, or discovering answers to questions that you may have. All these tasks are powered by NLP models. To decide which model to invoke at a particular point in time, we must perform language identification.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing PyTorch Docathon 2023</title>
      <link href="https://pytorch.org/blog/announcing-docathon/" rel="alternate" type="text/html" title="Announcing PyTorch Docathon 2023" />
      <published>2023-05-03T00:00:00-07:00</published>
      <updated>2023-05-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/announcing-docathon</id>
      <content type="html" xml:base="https://pytorch.org/blog/announcing-docathon/">&lt;p&gt;&lt;img src=&quot;/assets/images/docathon-cover.jpg&quot; alt=&quot;PyTorch Docathon&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are excited to announce the first ever PyTorch Docathon! The Docathon is a hackathon-style event focused on improving the documentation by enlisting the help of the community. Documentation is a crucial aspect of any technology and by improving the documentation, we can make it easier for users to get started with PyTorch, help them understand how to use its features effectively, and ultimately accelerate research to production in the field of machine learning.&lt;/p&gt;

&lt;h2 id=&quot;why-participate&quot;&gt;WHY PARTICIPATE&lt;/h2&gt;

&lt;h3 id=&quot;low-barrier-to-entry&quot;&gt;Low Barrier to Entry&lt;/h3&gt;

&lt;p&gt;Many open-source projects require extensive knowledge of the codebase and prior contributions to the project to participate in any sort of hackathon events. The Docathon, on the other hand, is designed for newcomers. We do expect familiarity with Python, basic knowledge of PyTorch, and ML. But don’t fret, there are some tasks that are related to website issues that won’t require even that.&lt;/p&gt;

&lt;h3 id=&quot;tangible-results&quot;&gt;Tangible Results&lt;/h3&gt;

&lt;p&gt;One of the best things about the Docathon is that you can see the results of your efforts in real time. Improving documentation can have a huge impact on a project’s usability and accessibility and you’ll be able to see those improvements firsthand. Plus having tangible results can be a great motivator to keep contributing.&lt;/p&gt;

&lt;h3 id=&quot;collaborative-environment&quot;&gt;Collaborative Environment&lt;/h3&gt;

&lt;p&gt;The Docathon is a collaborative event which means you’ll have the opportunity to work with other contributors and PyTorch maintainers on improving the documentation. This can be a great way to learn from others, share ideas, and build connections.&lt;/p&gt;

&lt;h3 id=&quot;learning-opportunities&quot;&gt;Learning Opportunities&lt;/h3&gt;

&lt;p&gt;Finally, even if you are not an expert in PyTorch, the Docathon can be a great learning experience. You’ll have the opportunity to explore the PyTorch modules and test some of the tutorials on your machine as well as in the CI.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;EVENT DETAILS&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;May 31&lt;/strong&gt;: Kick-off&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;May 31 - June 11&lt;/strong&gt;:  Submissions and Feedback&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 12 - June 13&lt;/strong&gt;: Final Reviews&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 15&lt;/strong&gt;: Winner Announcements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Details for the Docathon to be announced at the kick-off stream on May 31.&lt;/p&gt;

&lt;p&gt;Please register to join this year’s event: &lt;a href=&quot;https://community.linuxfoundation.org/e/mmbqqb/&quot;&gt;&lt;strong&gt;RSVP&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Image Segmentation using PyTorch</title>
      <link href="https://pytorch.org/blog/accelerated-image-seg/" rel="alternate" type="text/html" title="Accelerated Image Segmentation using PyTorch" />
      <published>2023-05-02T00:00:00-07:00</published>
      <updated>2023-05-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-image-seg</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-image-seg/">&lt;p&gt;&lt;em&gt;Using Intel® Extension for PyTorch to Boost Image Processing Performance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;PyTorch delivers great CPU performance, and it can be further accelerated with Intel® Extension for PyTorch. I trained an AI image segmentation model using PyTorch 1.13.1 (with ResNet34 + UNet architecture) to identify roads and speed limits from satellite images, all on the 4th Gen Intel® Xeon® Scalable processor.&lt;/p&gt;

&lt;p&gt;I will walk you through the steps to work with a satellite image dataset called SpaceNet5 and how I optimized the code to make deep learning workloads feasible on CPUs just by flipping a few key switches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Before we get started, some housekeeping…&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The code accompanying this article is available in the examples folder in the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/master/examples/cpu/usecase_spacenet5&quot;&gt;Intel Extension for PyTorch repository&lt;/a&gt;. I borrowed heavily from the &lt;a href=&quot;http://github.com/avanetten/cresi/&quot;&gt;City-Scale Road Extraction from Satellite Imagery (CRESI) repository&lt;/a&gt;. I adapted it for the 4th Gen Intel Xeon processors with PyTorch optimizations and &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel Extension for PyTorch&lt;/a&gt; optimizations. In particular, I was able to piece together a workflow using the &lt;a href=&quot;http://github.com/avanetten/cresi/tree/main/notebooks&quot;&gt;notebooks here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find the accompanying talk I gave &lt;a href=&quot;http://www.youtube.com/watch?v=LVZWm5GFvAw&quot;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also highly recommend these articles for a detailed explanation of how to get started with the SpaceNet5 data:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-1-imagery-and-label-preparation-598af46d485e&quot;&gt;The SpaceNet 5 Baseline — Part 1: Imagery and Label Preparation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-2-training-a-road-speed-segmentation-model-2bc93de564d7&quot;&gt;The SpaceNet 5 Baseline — Part 2: Training a Road Speed Segmentation Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/the-downlinq/the-spacenet-5-baseline-part-3-extracting-road-speed-vectors-from-satellite-imagery-5d07cd5e1d21&quot;&gt;The SpaceNet 5 Baseline — Part 3: Extracting Road Speed Vectors from Satellite Imagery&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;SpaceNet 5 Winning Model Release: End of the Road&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I referenced two Hugging Face blogs by Julien Simon; he ran his tests on the AWS instance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r7iz.metal-16xl&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://huggingface.co/blog/intel-sapphire-rapids&quot;&gt;Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://huggingface.co/blog/intel-sapphire-rapids-inference&quot;&gt;Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The potential cost savings from using a CPU instance instead of a GPU instance on the major cloud service providers (CSP) can be significant. The latest processors are still being rolled out to the CSPs, so I’m using a 4th Gen Intel Xeon processor that is hosted on the Intel® Developer Cloud (you can sign up for the Beta here: &lt;a href=&quot;http://cloud.intel.com/&quot;&gt;cloud.intel.com&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;On AWS, you can select from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r7iz.*&lt;/code&gt; EC2 instances after you &lt;a href=&quot;http://pages.awscloud.com/R7iz-Preview.html&quot;&gt;sign up for the preview here&lt;/a&gt; (Figure 1). At the time of writing, the new AI-acceleration engine, Intel® Advanced Matrix Extensions (Intel® AMX), is only available on bare metal but it should soon be enabled on the virtual machines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-4th-gen-xeon-aws-instances.png&quot; alt=&quot;List of 4th Gen Xeon  instances on AWS EC2&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;. List of 4th Gen Xeon  instances on AWS EC2 (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;On Google Cloud* Platform, you can select from the 4th Gen Xeon Scalable processors C3 VMs (Figure 2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-4th-gen-xeon-googlecloud-instances.png&quot; alt=&quot;List of 4th Gen Intel Xeon Scalable processor instances on Google Cloud Platform&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. List of 4th Gen Intel Xeon Scalable processor instances on Google Cloud Platform (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;hardware-introduction-and-optimizations&quot;&gt;Hardware Introduction and Optimizations&lt;/h2&gt;

&lt;p&gt;The 4th Gen Intel Xeon processors were released January 2023, and the bare-metal instance I am using has two sockets (each with 56 physical cores), 504 GB of memory, and Intel AMX acceleration. I installed a few key libraries in the backend to take control and monitor the sockets, memory, and cores that I am using on the CPU:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get install numactl&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libjemalloc-dev&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get install libjemalloc&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel-openmp&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda install intel-openmp&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gperftools&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda install gperftools -c conda-forge&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Both PyTorch and Intel Extension for PyTorch have helper scripts so that one does not need to explicitly use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel-openmp&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt;, but they do need to be installed in the backend. In case you want to set them up for other work, here is what I used for OpenMP* …&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export OMP_NUM_THREADS=36
export KMP_AFFINITY=granularity=fine,compact,1,0
export KMP_BLOCKTIME=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;… where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OMP_NUM_THREADS&lt;/code&gt; is the number of threads allocated to the job, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KMP_AFFINITY&lt;/code&gt; affects thread affinity settings (including packing threads close to each other, the state of pinning threads), and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KMP_BLOCKTIME&lt;/code&gt; sets the time in milliseconds that an idle thread should wait before going to sleep.&lt;/p&gt;

&lt;p&gt;Here’s what I used for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt; …&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;numactl -C 0-35 --membind=0 train.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;…where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-C&lt;/code&gt; specifies which cores to use and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--membind&lt;/code&gt; instructs the program to only use one socket (socket 0 in this case).&lt;/p&gt;

&lt;h2 id=&quot;spacenet-data&quot;&gt;SpaceNet Data&lt;/h2&gt;

&lt;p&gt;I am using a satellite image dataset from the &lt;a href=&quot;http://spacenet.ai/sn5-challenge/&quot;&gt;SpaceNet 5 Challenge&lt;/a&gt;. Different cities can be downloaded for free from an AWS S3 bucket:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws s3 ls s3://spacenet-dataset/spacenet/SN5_roads/tarballs/ --human-readable
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2019-09-03 20:59:32    5.8 GiB SN5_roads_test_public_AOI_7_Moscow.tar.gz
2019-09-24 08:43:02    3.2 GiB SN5_roads_test_public_AOI_8_Mumbai.tar.gz
2019-09-24 08:43:47    4.9 GiB SN5_roads_test_public_AOI_9_San_Juan.tar.gz
2019-09-14 13:13:26   35.0 GiB SN5_roads_train_AOI_7_Moscow.tar.gz
2019-09-14 13:13:34   18.5 GiB SN5_roads_train_AOI_8_Mumbai.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can use the following commands to download and unpack a file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws s3 cp s3://spacenet-dataset/spacenet/SN5_roads/tarballs/SN5_roads_train_AOI_7_Moscow.tar.gz .
tar -xvzf ~/spacenet5data/moscow/SN5_roads_train_AOI_7_Moscow.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dataset-preparation&quot;&gt;Dataset Preparation&lt;/h3&gt;

&lt;p&gt;I used the Moscow satellite image dataset, which consists of 1,352 images of 1,300 by 1,300 pixels with corresponding street labels in separate text files. The dataset contains both 8-band multispectral images and 3-band RGB images. Figure 3 shows four sample RGB satellite images and their corresponding generated masks. I used the &lt;a href=&quot;http://github.com/avanetten/cresi/blob/main/cresi/data_prep/speed_masks.py&quot;&gt;speed_masks.py&lt;/a&gt; script from the CRESI repository to generate the segmentation masks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f3-moscow-satellite-image-dataset.png&quot; alt=&quot;Satellite image 3-channel RGB chips from Moscow (top row) and corresponding pixel segmentation masks with varying speed limits&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;. Satellite image 3-channel RGB chips from Moscow (top row) and corresponding pixel segmentation masks with varying speed limits (bottom row) (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;There is a JSON configuration file that must be updated for all remaining components: training and validation split, training, and inference. &lt;a href=&quot;http://github.com/avanetten/cresi/blob/main/cresi/configs/sn5_baseline_aws.json.&quot;&gt;An example configuration can be found here&lt;/a&gt;. I perform an 80:20 training/validation split, making sure to point to the correct folder of satellite images and corresponding masks for training. The configuration parameters are explained in more in the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/master/examples/cpu/usecase_spacenet5&quot;&gt;notebook under examples in GitHub for Intel Extension for PyTorch here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;training-a-resnet34--unet-model&quot;&gt;Training a ResNet34 + UNet Model&lt;/h3&gt;

&lt;p&gt;I made some changes to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cresi&lt;/code&gt; code described below in order to run on a CPU and optimize the training. To run natively on a CPU, replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.model = nn.DataParallel(model).cuda()&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.model = nn.DataParallel(model)&lt;/code&gt; in the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/net/pytorch_utils/train.py&quot;&gt;train.py&lt;/a&gt; script. In the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/01_train.py&quot;&gt;01_train.py&lt;/a&gt; script, remove &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.randn(10).cuda()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To optimize training, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import intel_extension_for_pytorch as ipex&lt;/code&gt; to the import statements in the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/net/pytorch_utils/train.py&quot;&gt;train.py&lt;/a&gt; script. Just after defining the model and optimizer as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model = nn.DataParallel(model)
self.optimizer = optimizer(self.model.parameters(), lr=config.lr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipex.optimize&lt;/code&gt; line to use BF16 precision, instead of FP32: \&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model, self.optimizer = ipex.optimize(self.model, 
    optimizer=self.optimizer,dtype=torch.bfloat16)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add a line to do mixed-precision training just before running a forward pass and calculating the loss function:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.cpu.amp.autocast():
    if verbose:
        print(&quot;input.shape, target.shape:&quot;, input.shape, target.shape)
    output = self.model(input)
    meter = self.calculate_loss_single_channel(output, target, meter, training, iter_size)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that we have optimized our training code, we can move onto training our model.&lt;/p&gt;

&lt;p&gt;Like the &lt;a href=&quot;https://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;winner of the SpaceNet 5 competition&lt;/a&gt;, I trained a ResNet34 encoder + UNet decoder model. It is pretrained from ImageNet weights, and the backbone is left completely unfrozen during training. The training can be run with the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/01_train.py&quot;&gt;01_train.py&lt;/a&gt; script, but in order to control the use of hardware I used a helper script. There are actually two helper scripts: one that comes with stock PyTorch and one that comes with Intel Extension for PyTorch. They both accomplish the same thing, but the first one from stock is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.backends.xeon.run_cpu&lt;/code&gt;, and the second one from Intel Extension for PyTorch is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipexrun&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here is what I ran in the command-line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -m torch.backends.xeon.run_cpu --ninstances 1 \
  --ncores_per_instance 32 \
  --log_path /home/devcloud/spacenet5data/moscow/v10_xeon4_devcloud22.04/logs/run_cpu_logs \
  /home/devcloud/cresi/cresi/01_train.py \
  /home/devcloud/cresi/cresi/configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ipexrun --ninstances 1 \
--ncore_per_instance 32 \
/home/devcloud/cresi/cresi/01_train.py \
/home/devcloud/cresi/cresi/configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In both cases, I am asking PyTorch to run training on one socket with 32 cores. Upon running, I get a printout of what environment variables get set in the backend to understand how PyTorch is using the hardware:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INFO - Use TCMalloc memory allocator
INFO - OMP_NUM_THREADS=32
INFO - Using Intel OpenMP
INFO - KMP_AFFINITY=granularity=fine,compact,1,0
INFO - KMP_BLOCKTIME=1
INFO - LD_PRELOAD=/home/devcloud/.conda/envs/py39/lib/libiomp5.so:/home/devcloud/.conda/envs/py39/lib/libtcmalloc.so
INFO - numactl -C 0-31 -m 0 /home/devcloud/.conda/envs/py39/bin/python -u 01_train.py configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;During training, I make sure that my total loss function is decreasing (i.e., the model is converging on a solution).&lt;/p&gt;

&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;

&lt;p&gt;After training a model, we can start to make predictions from satellite images alone. In the eval.py inference script, add import intel_extension_for_pytorch as ipex to the import statements. After loading the PyTorch model, use Intel Extension for PyTorch to optimize the model for BF16 inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.load(os.path.join(path_model_weights, 
    'fold{}_best.pth'.format(fold)), 
    map_location = lambda storage, 
    loc: storage)
model.eval()
model = ipex.optimize(model, dtype = torch.bfloat16)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just prior to running prediction, add two lines for mixed precision:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.no_grad():
    with torch.cpu.amp.autocast():
        for data in pbar:
            samples = torch.autograd.Variable(data['image'], volatile=True)
            predicted = predict(model, samples, flips=self.flips)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To run inference, we can use the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/02_eval.py&quot;&gt;02_eval.py&lt;/a&gt; script. Now that we have a trained model, we can make predictions on satellite images (Figure 4). We can see that it does seem to map the roads closely to the image!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f4-moscow-satellite-image-complete.png&quot; alt=&quot;Moscow satellite image and accompanying prediction of roads&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;. Moscow satellite image and accompanying prediction of roads (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;I realize that the model I’ve trained is overfit to the Moscow image data and probably won’t generalize well to other cities. However, the &lt;a href=&quot;http://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;winning solution to this challenge&lt;/a&gt; used data from six cities (Las Vegas, Paris, Shanghai, Khartoum, Moscow, Mumbai) and performs well on new cities. In the future, one thing that would be worth testing is training on all six cities and running inference on another city to reproduce their results.&lt;/p&gt;

&lt;h2 id=&quot;note-on-post-processing&quot;&gt;Note on Post-Processing&lt;/h2&gt;

&lt;p&gt;There are further post-processing steps that can be performed to add the mask as graph features to maps. You can read more about the post-processing steps here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-3-extracting-road-speed-vectors-from-satellite-imagery-5d07cd5e1d21&quot;&gt;The SpaceNet 5 Baseline — Part 3: Extracting Road Speed Vectors from Satellite Imagery&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/avanetten/cresi/tree/main/cresi&quot;&gt;Post-processing scripts&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In summary, we:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Created 1,352 image training masks (with speed limits) to correspond to our training satellite image data (from .geojson text file labels)&lt;/li&gt;
  &lt;li&gt;Defined our configuration file for training and inference&lt;/li&gt;
  &lt;li&gt;Split up our data into training and validation sets&lt;/li&gt;
  &lt;li&gt;Optimized our code for CPU training, including using Intel Extension for PyTorch and BF16&lt;/li&gt;
  &lt;li&gt;Trained a performant ResNet34 + UNet model on a 4th Gen Intel Xeon CPU&lt;/li&gt;
  &lt;li&gt;Ran initial inference to see the prediction of a speed limit mask&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can find &lt;a href=&quot;http://edc.intel.com/content/www/us/en/products/performance/benchmarks/4th-generation-intel-xeon-scalable-processors/&quot;&gt;detailed benchmarks here for the 4th Gen Intel Xeon CPU here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Extend the optimizations on an Intel CPU by using the Intel Extension for PyTorch:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install intel-extension-for-pytorch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git clone https://github.com/intel/intel-extension-for-pytorch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://linkedin.com/in/bconsolvo&quot;&gt;Get in touch with me on LinkedIn&lt;/a&gt; if you have any more questions!&lt;/p&gt;

&lt;p&gt;More information about the Intel Extension for PyTorch &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;can be found here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;get-the-software&quot;&gt;Get the Software&lt;/h3&gt;

&lt;p&gt;I encourage you to check out Intel’s other &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Framework&lt;/a&gt;&lt;/strong&gt; optimizations and learn about the open, standards-based &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt;&lt;/strong&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel’s AI software portfolio.&lt;/p&gt;

&lt;p&gt;For more details about 4th Gen Intel Xeon Scalable processor, visit &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI Platform&lt;/a&gt;&lt;/strong&gt; where you can learn about how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-resources&quot;&gt;PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Using Intel® Extension for PyTorch to Boost Image Processing Performance</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving</title>
      <link href="https://pytorch.org/blog/introducing-hidet/" rel="alternate" type="text/html" title="Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving" />
      <published>2023-04-27T00:00:00-07:00</published>
      <updated>2023-04-27T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/introducing-hidet</id>
      <content type="html" xml:base="https://pytorch.org/blog/introducing-hidet/">&lt;p&gt;&lt;a href=&quot;https://github.com/hidet-org/hidet&quot;&gt;Hidet&lt;/a&gt; is a powerful deep learning compiler that simplifies the process of implementing high-performing deep learning operators on modern accelerators (e.g., NVIDIA GPUs). With the new feature of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; in PyTorch 2.0, integrating a novel compiler into PyTorch is easier than ever - Hidet now can be used as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; backend to accelerate PyTorch models, making it an attractive option for PyTorch users who want to improve the inference performance of their models, especially for those who also need to implement extremely optimized custom operators.&lt;/p&gt;

&lt;h2 id=&quot;using-hidet-to-compile-a-pytorch-model&quot;&gt;Using Hidet to Compile A PyTorch Model&lt;/h2&gt;

&lt;p&gt;To use Hidet in PyTorch, you need to first install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet&lt;/code&gt; package via pip:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install hidet
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hidet is integrated with PyTorch as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; backend following the &lt;a href=&quot;https://pytorch.org/docs/stable/dynamo/custom-backends.html&quot;&gt;Custom Backends tutorial&lt;/a&gt;. You can specify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet&lt;/code&gt; as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backend&lt;/code&gt; when you compile a model. (Note: requires PyTorch version 2.0+):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.compile(..., backend='hidet')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hidet converts the given PyTorch model in the torch.fx.Graph format into its internal graph representation, and conducts a series of optimizations. Hidet provides a few options to configure the optimizations. For example, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet.torch.dynamo_config.use_tensor_core(True)&lt;/code&gt; to allow Hidet to generate CUDA kernels that leverage the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tensor-cores/&quot;&gt;Tensor Cores on NVIDIA GPUs&lt;/a&gt;, and use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet.torch.dynamo_config.search_space(2)&lt;/code&gt; to allow Hidet to search for the best operator schedule specific for your hardware and input sizes. More configurations can be found in &lt;a href=&quot;https://docs.hidet.org/stable/gallery/tutorials/optimize-pytorch-model.html&quot;&gt;Hidet’s documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s a complete example of how to use Hidet to compile and optimize a pre-trained ResNet50 model from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import hidet
import torch

# Load a pre-trained ResNet50 model
x = torch.randn(1, 3, 224, 224, device='cuda').half()
model = torch.hub.load(
    'pytorch/vision:v0.6.0', 'resnet50', pretrained=True
).cuda().half().eval()

# Configure hidet to use tensor core and enable tuning
hidet.torch.dynamo_config.use_tensor_core(True)
hidet.torch.dynamo_config.search_space(2) 

# Compile the model using Hidet
model_opt = torch.compile(model, backend='hidet')

# Check correctness
torch.testing.assert_close(actual=model_opt(x), expected=model(x), rtol=1e-2, atol=1e-2)

# Benchmark
from hidet.utils import benchmark_func
print('eager: {:2f}'.format(benchmark_func(lambda: model(x))))
print('hidet: {:2f}'.format(benchmark_func(lambda: model_opt(x))))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We encourage you to try out the above script on your own NVIDIA GPU(s)! If you run this script on an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws.g5.2xlarge&lt;/code&gt; instance, you would get the result shown in the following figure. Hidet achieves the speedup because it could automatically fuse multiple operators, tune operator schedules, and use CUDA Graph to reduce framework-level overhead. More results can be found in the &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3575693.3575702&quot;&gt;ASPLOS’23 publication of Hidet&lt;/a&gt; and our &lt;a href=&quot;https://github.com/hidet-org/hidet/issues/154&quot;&gt;performance tracking&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-4-27-hidet.png&quot; alt=&quot;Eager vs Hidet latency&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;using-hidet-script-to-write-custom-operators&quot;&gt;Using Hidet Script to Write Custom Operators&lt;/h2&gt;

&lt;p&gt;Hidet Script is one approach to implement tensor operators in Python. The following example shows how to implement a naive matrix multiplication using Hidet Script and integrate it as a PyTorch operator.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import hidet


def matmul(m_size, n_size, k_size):
    from hidet.lang import f32, attr
    from hidet.lang.cuda import threadIdx, blockIdx, blockDim

    with hidet.script_module() as script_module:
        @hidet.script
        def matmul(
            a: f32[m_size, k_size],
            b: f32[k_size, n_size],
            c: f32[m_size, n_size]
        ):
            attr.cuda_grid_dim = ((m_size + 31) // 32, (n_size + 31) // 32)
            attr.cuda_block_dim = (32, 32)
            i = threadIdx.x + blockIdx.x * blockDim.x
            j = threadIdx.y + blockIdx.y * blockDim.y
            if i &amp;lt; m_size and j &amp;lt; n_size:
                c[i, j] = 0.0
                for k in range(k_size):
                    c[i, j] += a[i, k] * b[k, j]

    ir_module = script_module.ir_module()
    func = hidet.driver.build_ir_module(ir_module)
    return func


class NaiveMatmul(torch.autograd.Function):
    @staticmethod
    def forward(ctx, a, b):
        m, k = a.shape
        k, n = b.shape
        c = torch.empty([m, n], dtype=a.dtype, device=a.device)
        func = matmul(m, n, k)
        func(a, b, c)
        return c


a = torch.randn([3, 4], device='cuda')
b = torch.randn([4, 5], device='cuda')
c = NaiveMatmul.apply(a, b)
cc = torch.matmul(a, b)
torch.testing.assert_close(c, cc)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More optimizations can be applied, see the &lt;a href=&quot;https://docs.hidet.org/stable/gallery/developer-guides/hidet-script-dynamic-kernel.html&quot;&gt;example&lt;/a&gt; in our documentation to learn more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hidet Script vs. Triton&lt;/strong&gt;: Triton greatly simplifies the CUDA programming by introducing the tile-based programming model where the parallel execution unit is thread blocks instead of threads. However, this simplification also prevents the tensor program developers from manipulating the fine-grained computation and memory resources (e.g., warps, shared memory) in their preferred ways. It would be challenging to implement an optimization that requires fine-grained control of these resources using Triton if it has not been implemented by the Triton compiler itself. Hidet Script, on the other hand, simplifies tensor programming while still enabling users to implement their own optimizations with extensive flexibility. It’s worth noting that the more granular control of Hidet Script also brings added complexity compared to Triton.&lt;/p&gt;

&lt;h2 id=&quot;more-about-hidet&quot;&gt;More about Hidet&lt;/h2&gt;

&lt;p&gt;Hidet originates from a research project led by the &lt;a href=&quot;https://www.cs.toronto.edu/ecosystem/&quot;&gt;EcoSystem lab&lt;/a&gt; at the University of Toronto (UofT) and AWS. The authors propose a new way, named the task-mapping programming paradigm, to construct tensor programs. It aims to simplify the tensor programming without sacrificing any optimization opportunity. Now, Hidet is an open-source project, jointly supported by &lt;a href=&quot;https://centml.ai/&quot;&gt;CentML&lt;/a&gt; and the EcoSystem lab, that aims to provide an efficient solution to end-to-end inference on modern accelerators (e.g., NVIDIA GPUs).&lt;/p&gt;

&lt;h3 id=&quot;additional-resources&quot;&gt;Additional Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;GitHub Repository: &lt;a href=&quot;https://github.com/hidet-org/hidet&quot;&gt;https://github.com/hidet-org/hidet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hidet’s Documentation: &lt;a href=&quot;https://docs.hidet.org&quot;&gt;https://docs.hidet.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ASPLOS ’23 Publication: &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3575693.3575702&quot;&gt;https://dl.acm.org/doi/10.1145/3575693.3575702&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ASPLOS ’23 Tutorial: &lt;a href=&quot;https://centml.github.io/asplos23-tutorial/&quot;&gt;https://centml.github.io/asplos23-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;We would like to thank Jerry Park, Mark Saroufim, Jason Liang and Helen Suk for their valuable help on preparing the blog post and feedback on the text. We also would like to thank Nikita Shulga, Jason Ansel, and Dmytro Dzhulgakov for reviewing and improving our PR https://github.com/pytorch/pytorch/pull/93873 on the 3rd-party dynamo backend registration.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team Hidet</name>
        
        
      </author>

      

      

      
        <summary type="html">Hidet is a powerful deep learning compiler that simplifies the process of implementing high-performing deep learning operators on modern accelerators (e.g., NVIDIA GPUs). With the new feature of torch.compile(...) in PyTorch 2.0, integrating a novel compiler into PyTorch is easier than ever - Hidet now can be used as a torch.compile(...) backend to accelerate PyTorch models, making it an attractive option for PyTorch users who want to improve the inference performance of their models, especially for those who also need to implement extremely optimized custom operators.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Large Language Models with Accelerated Transformers</title>
      <link href="https://pytorch.org/blog/accelerating-large-language-models/" rel="alternate" type="text/html" title="Accelerating Large Language Models with Accelerated Transformers" />
      <published>2023-04-19T00:00:00-07:00</published>
      <updated>2023-04-19T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-large-language-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-large-language-models/">&lt;p&gt;&lt;strong&gt;TL;DR.&lt;/strong&gt; We show how to use Accelerated PyTorch 2.0 Transformers and the newly introduced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; method to accelerate Large Language Models on the example of &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt;, a compact open-source implementation of the GPT model from Andrej Karpathy. Using the new &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;scaled dot product attention operator&lt;/a&gt; introduced with Accelerated PT2 Transformers, we select the flash_attention custom kernel and achieve faster training time per batch (measured with Nvidia A100 GPUs), going from a ~143ms/batch baseline to ~113 ms/batch. In addition, the enhanced implementation using the SDPA operator offers better numerical stability. Finally, further optimizations are achieved using padded inputs, which when combined with flash attention lead to ~87ms/batch.&lt;/p&gt;

&lt;p&gt;Recent times have seen exponential adoption of large language models (LLMs) and Generative AI in everyday life. Tightly coupled with these ever-growing models is the ever-growing training cost - in terms of both time and hardware utilization. The PyTorch team has tackled these challenges head on with &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-2/&quot;&gt;Accelerated PyTorch 2 Transformers&lt;/a&gt; (previously known as “Better Transformer”) and JIT Compilation in &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-release/&quot;&gt;PyTorch 2.0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post, we explore training optimizations gained by utilizing custom kernel implementations of SDPA - also known as scaled dot product attention - a critical layer in transformer models. The custom kernel for SDPA replaces several discrete sequential operations with one globally optimized kernel which avoids allocating a large amount of intermediate CUDA memory. This approach offers a number of advantages, including but not limited to:  higher performance computation of SDPA by reducing memory bandwidth bottleneck, reduced memory footprint to support larger batch sizes, and finally added numerical stability by prescaling input tensors. These optimizations are demonstrated on nanoGPT, an open-source implementation of GPT from Andrej Karpathy.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Scaled dot product attention is the fundamental building block of multihead attention, as introduced in &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;“Attention is All You Need”&lt;/a&gt;, and has a wide range of applications in LLM and Generative AI models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/PyTorch_Better-Transformer_Figure-1.png&quot; alt=&quot;The Transformer model architecture&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; The Transformer model architecture based on &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;“Attention is All You Need”&lt;/a&gt;. With the new PyTorch SDPA operator, Multi-Head Attention is efficiently implemented by a linear layer for the in-projection, the SDPA operator, and a linear layer for the out-projection.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;With the new scaled_dot_product_attention operator, multihead attention can be implemented in just 3 steps: in projection with a linear layer, SDPA, and out projection with a linear layer.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# In Projection
# variable descriptions:
# q,k,v = Query, Key, Value tensors
# bsz = batch size
# num_heads = Numner of heads for Multihead Attention
# tgt_len = Target length
# src_len = Source Length
# head_dim: Head Dimension
    q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)
    q = q.view(bsz, num_heads, tgt_len, head_dim)
    k = k.view(bsz, num_heads, src_len, head_dim)
    v = v.view(bsz, num_heads, src_len, head_dim)

    # Scaled Dot Product Attention
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)

    # Out Projection
    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)
    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
    attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch 2. supports multiple different kernels optimized for specific use cases, with specific requirements. A kernel picker picks the best kernel for a particular combination of input parameters. If no optimized “custom kernel” for a particular combination of input parameters can be identified, the kernel picker selects a general kernel that can handle all input combinations.&lt;/p&gt;

&lt;p&gt;While future releases may extend this set of operators, PyTorch 2.0 launches with 3 implementations for the SDPA operator:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A generic kernel which implements the mathematical equation of SDPA in the function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sdpa_math()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;An optimized kernel based on the paper “&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;Flash Attention&lt;/a&gt;”, which supports evaluation of SDPA with 16 bit floating point data types on compute architecture SM80 (A100).&lt;/li&gt;
  &lt;li&gt;An optimized kernel based on the paper “&lt;a href=&quot;https://arxiv.org/abs/2112.0568&quot;&gt;Self-Attention Does Not Need O(n^2) Memory&lt;/a&gt;” and implemented in &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormer&lt;/a&gt;, which supports both 32 and 16 bit floating data types on a wider range of architectures (SM40 and later). This blog post refers to this kernel as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mem_efficient&lt;/code&gt; kernel.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that both optimized kernels (two and three listed above), support a key padding mask and limit the supported attention mask to causal attention. Accelerated PyTorch 2.0 Transformers today only support the causal mask when it is specified using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_causal&lt;/code&gt; boolean. When a mask is specified, the general-purpose kernel will be selected because it is too expensive to analyze the contents of a provided mask to determine if it is the causal mask. Additional explanations on the constraints for each kernel can be found in the &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-2/&quot;&gt;Accelerated PT2 Transformer blog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;enabling-accelerated-transformers-with-nanogpt&quot;&gt;Enabling Accelerated Transformers with nanoGPT&lt;/h2&gt;

&lt;p&gt;The SDPA operator being a critical component of the GPT model,  we identified the open source nanoGPT model as an excellent candidate for both demonstrating the ease of implementation and benefits of PyTorch 2.0’s Accelerated Transformers. The following demonstrates the exact process by which Accelerated Transformers was enabled on nanoGPT.&lt;/p&gt;

&lt;p&gt;This process largely revolves around replacing the existing SDPA implementation with the newly added F.scaled_dot_product_attention operator from &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/df14650f0b14b80db132b0c1797dc595fbee1054/torch/nn/functional.py#L4834&quot;&gt;functional.py&lt;/a&gt;. This process can be easily adapted to enable the operator in many other LLMs. Alternatively, users can instead choose to call F.multi_head_attention_forward() or utilize the nn.MultiHeadAttention module directly where applicable. The following code snippets are adapted from Karpathy’s nanoGPT repository.&lt;/p&gt;

&lt;h3 id=&quot;step-1-identify-the-existing-sdpa-implementation&quot;&gt;Step 1: Identify the existing SDPA implementation&lt;/h3&gt;

&lt;p&gt;In the case of nanoGPT, SDPA is implemented in the model’s &lt;a href=&quot;https://github.com/karpathy/nanoGPT/blob/master/model.py#L37&quot;&gt;CausalSelfAttention&lt;/a&gt; class. The original implementation at time of writing is adapted below for this post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/causal_attention_step_1.png&quot; alt=&quot;The original implementation at time of writing&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-2-replace-with-torchs-scaled_dot_product_attention&quot;&gt;Step 2: Replace with Torch’s &lt;em&gt;scaled_dot_product_attention&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;At this point we can note the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lines 36 - 42 define the mathematical implementation of SDPA which we are replacing&lt;/li&gt;
  &lt;li&gt;The mask applied on line 39 is no longer relevant since we are using scaled_dot_product_attention’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_causal&lt;/code&gt; flag.&lt;/li&gt;
  &lt;li&gt;The dropout layer used in line 41 is also now unnecessary.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Swapping out the SDPA implementation for torch’s scaled_dot_product_attention and removing the now redundant code yields the following implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/causal_attention_step_2.png&quot; alt=&quot;Swapping out the SDPA implementation for torch’s scaled_dot_product_attention and removing the now redundant code yields the following implementation.&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, the original mask can be passed into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn_mask&lt;/code&gt; field however due to the mentioned kernel constraints that would limit the implementation to only support the generic &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sdpa_math&lt;/code&gt; kernel.&lt;/p&gt;

&lt;h3 id=&quot;step-3-bonus-faster-matmuls-with-padding&quot;&gt;Step 3 (Bonus): Faster matmuls with padding&lt;/h3&gt;

&lt;p&gt;On top of the performance improvements from SDPA, our analysis yielded a nice ancillary win.  In Andrej’s words “The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase the vocab size from 50257 to 50304 (nearest multiple of 64).”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/tweet.png&quot; alt=&quot;Tweet by Andrej Karpathy&quot; style=&quot;max-height:800px; width:100%; max-width:600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The vocab size determines the dimensions of matmuls in the output layer of GPT, and these are so large that they were taking a &lt;em&gt;majority&lt;/em&gt; of the time for the entire training loop!  We discovered that they were achieving performance significantly below the peak throughput achievable on the A100 GPU, and guessed from &lt;a href=&quot;https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html&quot;&gt;NVIDIA’s matmul documentation&lt;/a&gt; that 64-element alignment would yield better results.  Indeed, padding these matmuls achieves nearly a 3x speedup!  The underlying cause is that unaligned memory accesses significantly reduce efficiency.  A deeper analysis can be found in &lt;a href=&quot;https://twitter.com/cHHillee/status/1630274804795445248&quot;&gt;this Twitter thread&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this optimization we were able to further reduce training time from ~113 ms (using flash attention) to ~87 ms per batch.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The figure below demonstrates the performance gained using Pytorch custom kernels. Here are the exact figures:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;baseline (nanoGPT implementation):  ~143ms&lt;/li&gt;
  &lt;li&gt;sdpa_math (generic): ~134ms (6.71% faster)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mem_efficient&lt;/code&gt; kernel: ~119ms (20.16% faster)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flash_attention&lt;/code&gt; kernel: ~113ms (26.54% faster)&lt;/li&gt;
  &lt;li&gt;flash_attention + padded vocab:  ~87ms (64.37% faster)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All code was run on an 8 x NVIDIA Corporation A100 server with 80 GB HBM [A100 SXM4 80GB], and for the purpose of this experiment dropout was set to 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/PyTorch_Better-Transformer_Chart-2.png&quot; alt=&quot;Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;enhancing-numerical-model-stability&quot;&gt;Enhancing Numerical Model Stability&lt;/h2&gt;

&lt;p&gt;In addition to being faster, PyTorch’s implementation offers increased numerical stability by avoiding loss of precision in many execution scenarios. There is a great explanation &lt;a href=&quot;https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/118&quot;&gt;here&lt;/a&gt;, but essentially the PyTorch implementation scales the Query and Key matrices &lt;em&gt;before&lt;/em&gt; multiplication, which is said to be more stable and avoid loss of precision. Because of the merged custom kernel architecture of SDPA, this scaling does not introduce additional overhead in the computation of the attention result.  In comparison, an implementation from the individual computational components would require separate pre-scaling at additional cost. For an additional explanation, see Appendix A.&lt;/p&gt;

&lt;h3 id=&quot;improved-memory-consumption&quot;&gt;Improved Memory Consumption&lt;/h3&gt;

&lt;p&gt;Yet another large advantage of using the torch SDPA kernels is the reduced memory footprint, which allows for the utilization of larger batch sizes. The following chart compares the best validation loss after one hour of training for both flash attention and the baseline implementations of causal attention. As can be seen, the maximum batch size achieved with the baseline causal attention implementation (on 8 x NVIDIA Corporation A100 server with 80 GB HBM) was 24, significantly less then the maximum achieved with flash attention, which was 39.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/chart.png&quot; alt=&quot;Using Flash Attention enables the usage of larger batch sizes&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Using Flash Attention enables the usage of larger batch sizes, allowing users to achieve lower validation loss after one hour of training (smaller is better).&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Accelerated PyTorch 2 Transformers were designed to make the training and production deployment of state-of-the-art transformer models affordable and integrated with PyTorch 2.0 model JIT compilation.  The newly introduced PyTorch SDPA operator provides improved performance for training Transformer models and is particularly valuable for the expensive Large Language Model training. In this post we demonstrate a number of optimizations on the exemplary nanoGPT model  including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Over 26% training speedup, when compared against the baseline with constant batch size&lt;/li&gt;
  &lt;li&gt;An additional speedup achieved with padded vocabulary, bringing the total optimization to approximately 64% compared to the baseline&lt;/li&gt;
  &lt;li&gt;Additional numerical stability&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;appendix-a-analyzing-attention-numeric-stability&quot;&gt;Appendix A: Analyzing Attention Numeric Stability&lt;/h2&gt;

&lt;p&gt;In this section we provide a more in depth explanation of the previously mentioned enhanced numerical stability which is gained by prescaling SDPA’s input vectors. The following is a simplified version of nanoGPT’s mathematical implementation of SDPA. The important thing to note here is that the query undergoes matrix multiplication without being scaled.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# nanoGPT implementation of SDPA
# notice q (our query vector) is not scaled !
att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
att = F.softmax(att, dim=-1)

# Dropout is set to 0, so we can safely ignore this line in the implementation# att = self.attn_dropout(att) 

y_nanogpt = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&amp;gt; (B, nh, T, hs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following is the equivalent mathematical implementation in torch’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# PyTorch implementation of SDPA
embed_size = q.size(-1)
scaling_factor = math.sqrt(math.sqrt(embed_size))
q = q / scaling_factor 	# notice q _is_ scaled here !

# same as above, but with scaling factor
att = q @ (k.transpose(-2, -1) / scaling_factor)
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
att = F.softmax(att0, dim=-1)

# Dropout is set to 0, so we can safely ignore this line in the implementation# att = self.attn_dropout(att) 

y_scale_before = att @ v
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Mathematically both approaches should be equivalent, however our experimentation shows that in practice we receive different results from each approach.&lt;/p&gt;

&lt;p&gt;Using the approach above, we verified &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_scale_before&lt;/code&gt; matches the expected output from using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention &lt;/code&gt;method while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_nanogpt&lt;/code&gt; does not.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.allclose&lt;/code&gt; method was used to test equivalence. Specifically, we showed that:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y_sdpa = torch.nn.functional._scaled_dot_product_attention(
	q,
	k,
	v,
	attn_mask=self.bias[:,:,:T,:T] != 0,
	dropout_p=0.0,
	need_attn_weights=False,
	is_causal=False,
)

torch.allclose(y_sdpa, y_nanogpt) # False, indicating fp issues
torch.allclose(y_sdpa, y_scale_before) # True, as expected
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;appendix-b-reproducing-experiment-results&quot;&gt;Appendix B: Reproducing Experiment Results&lt;/h2&gt;

&lt;p&gt;Researchers seeking to reproduce these results should start with the following commit from Andrej’s nanoGPT repository - &lt;strong&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;b3c17c6c6a363357623f223aaa4a8b1e89d0a465&lt;/span&gt;&lt;/strong&gt;. This commit was used as the baseline when measuring the per batch speed improvements. For results which include padded vocabulary optimizations (which yielded the most significant improvements to batch speed), use the following commit - &lt;strong&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;77e7e04c2657846ddf30c1ca2dd9f7cbb93ddeab&lt;/span&gt;&lt;/strong&gt;. From either checkout, selecting kernels for experimentation is made trivial with the use of the &lt;a href=&quot;https://pytorch.org/docs/stable/backends.html&quot;&gt;torch.backends&lt;/a&gt; API.&lt;/p&gt;

&lt;p&gt;The desired kernel can be selected via a context manager:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.backends.cuda.sdp_kernel (
    enable_math = False,
    enable_flash = False,
    enable_mem_efficient = True
):
    train(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Lucas Pasqualin, Driss Guessous, Christian Puhrsch, Bertrand Maher, Michael Gschwind</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR. We show how to use Accelerated PyTorch 2.0 Transformers and the newly introduced torch.compile() method to accelerate Large Language Models on the example of nanoGPT, a compact open-source implementation of the GPT model from Andrej Karpathy. Using the new scaled dot product attention operator introduced with Accelerated PT2 Transformers, we select the flash_attention custom kernel and achieve faster training time per batch (measured with Nvidia A100 GPUs), going from a ~143ms/batch baseline to ~113 ms/batch. In addition, the enhanced implementation using the SDPA operator offers better numerical stability. Finally, further optimizations are achieved using padded inputs, which when combined with flash attention lead to ~87ms/batch.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Experience the power of PyTorch 2.0 on AMD Solutions</title>
      <link href="https://pytorch.org/blog/experience-power-pytorch-2.0/" rel="alternate" type="text/html" title="Experience the power of PyTorch 2.0 on AMD Solutions" />
      <published>2023-04-15T00:00:00-07:00</published>
      <updated>2023-04-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/experience-power-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/experience-power-pytorch-2.0/">&lt;p&gt;PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework.  The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that the PyTorch 2.0 stable release includes support for AMD Instinct™ and Radeon™ GPUs that are supported by the ROCm™ software platform.&lt;/p&gt;

&lt;p&gt;With the stable PyTorch 2.0 release, PyTorch 2.0 introduces torch.compile as a beta feature underpinned by TorchInductor with support for AMD Instinct and Radeon GPUs through OpenAI Triton deep learning compiler. Through TorchInductor, developers can now generate low level kernels using Triton that are portable and performant to hand-written kernels on native hardware centric kernel programming models.&lt;/p&gt;

&lt;p&gt;OpenAI Triton is a language and compiler for blocked algorithms, which aims to provide an abstraction layer between CUDA/HIP and Torch at which developers can write efficient kernels more productively.  We have written a new backend which interfaces Triton’s custom MLIR dialects with our ROCm compiler stack.&lt;/p&gt;

&lt;p&gt;Triton can automatically optimize kernels generated by machine learning compilers such as TorchInductor for multiple AI accelerators including AMD Instinct GPU accelerator by leveraging hardware-specific features of the AMD CDNA™ GPU architecture. This makes it easy for developers and users to switch seamlessly from any HW to AMD Instinct GPU accelerators and get great out of the box performance.&lt;/p&gt;

&lt;p&gt;In addition, compilers like Triton can also enable developers to use high-level programming languages, such as Python, to write machine learning code that can be efficiently compiled and executed on specialized hardware. This can help greatly improve the productivity of machine learning developers, as they can focus on the algorithmic aspects of their models and rely on the compiler to generate efficient code.&lt;/p&gt;

&lt;p&gt;By design, PyTorch 2.0 is backward compatible to earlier PyTorch releases. This holds true for the ROCm build of PyTorch 2.0 as well. Developers using PyTorch with AMD GPUs can migrate to PyTorch 2.0 with the confidence that their existing code will continue to work without any required changes, so there is no penalty to access the improvements that come with this release. On the other hand, using PyTorch 2.0 and TorchInductor can result in significant performance improvement over the default eager-mode as shown below.&lt;/p&gt;

&lt;p&gt;The initial results using AMD Instinct MI250 GPUs already shows strong performance improvement with minimal optimization on TorchInductor compared to the default eager-mode. We see an average performance increase of up to 1.54X on 44 out of the 45 models on HuggingFace benchmarks suite with CamemBert, DistillGPT2 and T5Small being a few of the standout models with up to 1.5X or more performance improvement over eager-mode. We are looking forward to continued engagement with members of the PyTorch team at Meta to enable further optimization on ROCm software stack and the additional performance improvement for future PyTorch releases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/t-vs-eager-mode.svg&quot; alt=&quot;Image 1: AMD MI250 GPU performance improvement for TorchInductor vs eager-mode using HuggingFace&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Image 1: AMD MI250 GPU performance improvement for TorchInductor vs eager-mode using HuggingFace &lt;sup&gt;MI200-89.&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.0 follows the same set of install options as before to build and install for supporting AMD GPUs. These include an installable Python package hosted at &lt;a href=&quot;https://pytorch.org/&quot;&gt;pytorch.org&lt;/a&gt;, AMD’s public PyTorch docker image, and of course the option to build from source using the upstream PyTorch repository. As with PyTorch builds for other platforms, the specific command line to be run for pip-based install is provided by the configurator at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The GPUs supported by the ROCm software platform which forms the basis for PyTorch support on AMD GPUs are documented at &lt;a href=&quot;https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html&quot;&gt;https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 represents a major step in continuing to broaden support for ML developers by increasing performance while maintaining a simple, Pythonic interface. This performance uplift is made possible in large part by the new TorchInductor infrastructure, which in turn harnesses the Triton ML programming language and just-in-time compiler. AMD’s support for these technologies allows users to realize the full promise of the new PyTorch architecture.  Our GPU support in PyTorch 2.0 is just one manifestation of a larger vision around AI and machine learning. AI/ML plays an important role in multiple AMD product lines, including Instinct and Radeon GPUs, Alveo™ data center accelerators, and both Ryzen™ and EPYC processors. These hardware and software initiatives are all part of AMD’s Pervasive AI vision, and we look forward to addressing the many new challenges and opportunities of this dynamic space.&lt;/p&gt;

&lt;p&gt;MI200-89 – PyTorch Inductor mode HuggingFace Transformers training speedup, running the standard PyTorch 2.0 test suite, over PyTorch eager-mode comparison based on AMD internal testing on a single GCD as of 3/10/2023 using a 2P AMD EPYC™ 7763 production server with 4x AMD Instinct™ MI250 (128GB HBM2e) 560W GPUs with Infinity Fabric™ technology; host ROCm™ 5.3, guest ROCm™ 5.4.4, PyTorch 2.0.0, Triton 2.0. Server manufacturers may vary configurations, yielding different results. Performance may vary based on factors including use of latest drivers and optimizations.&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;© 2023 Advanced Micro Devices, Inc. All rights reserved. AMD, the AMD Arrow logo, AMD CDNA, AMD Instinct, EPYC, Radeon, ROCm, Ryzen, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective owners.&lt;/small&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>AMD</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework. The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that the PyTorch 2.0 stable release includes support for AMD Instinct™ and Radeon™ GPUs that are supported by the ROCm™ software platform.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Generative Diffusion Models with PyTorch 2</title>
      <link href="https://pytorch.org/blog/accelerated-generative-diffusion-models/" rel="alternate" type="text/html" title="Accelerated Generative Diffusion Models with PyTorch 2" />
      <published>2023-04-14T00:00:00-07:00</published>
      <updated>2023-04-14T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-generative-diffusion-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-generative-diffusion-models/">&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;A large part of the recent progress in Generative AI came from denoising diffusion models, which allow producing high quality images and videos from text prompts. This family includes Imagen, DALLE, Latent Diffusion, and others. However, all models in this family share a common drawback: generation is rather slow, due to the iterative nature of the sampling process by which the images are produced. This makes it important to optimize the code running inside the sampling loop.&lt;/p&gt;

&lt;p&gt;We took an open source implementation of a popular text-to-image diffusion model as a starting point and accelerated its generation using two optimizations available in PyTorch 2: compilation and fast attention implementation. Together with a few minor memory processing improvements in the code these optimizations give up to 49% inference speedup relative to the original implementation without &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt;, and 39% inference speedup relative to using the original code with xFormers (excluding the compilation time), depending on the GPU architecture and batch size. Importantly, the speedup comes without a need to install xFormers or any other extra dependencies.&lt;/p&gt;

&lt;p&gt;The table below shows the improvement in runtime between the original implementation with xFormers installed and our optimized version with PyTorch-integrated memory efficient attention (originally developed for and released in the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; library)  and PyTorch compilation. The compilation time is excluded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Runtime improvement in % compared to original+xFormers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;See the absolute runtime numbers in section “Benchmarking setup and results summary”&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;GPU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 1&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 2&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 4&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;P100 (no compilation)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-3.8
   &lt;/td&gt;
   &lt;td&gt;0.44
   &lt;/td&gt;
   &lt;td&gt;5.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;2.12
   &lt;/td&gt;
   &lt;td&gt;10.51
   &lt;/td&gt;
   &lt;td&gt;14.2
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-2.34
   &lt;/td&gt;
   &lt;td&gt;8.99
   &lt;/td&gt;
   &lt;td&gt;10.57
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;18.63
   &lt;/td&gt;
   &lt;td&gt;6.39
   &lt;/td&gt;
   &lt;td&gt;10.43
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;38.5
   &lt;/td&gt;
   &lt;td&gt;20.33
   &lt;/td&gt;
   &lt;td&gt;12.17
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;One can notice the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The improvements are significant for powerful GPUs like A100 and V100. For those GPUs the improvement is most pronounced for batch size 1&lt;/li&gt;
  &lt;li&gt;For less powerful GPUs we observe smaller speedups (or in two cases slight regressions). The batch size trend is reversed here: improvement is larger for larger batches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following sections we describe the applied optimizations and provide detailed benchmarking data, comparing the generation time with various optimization features on/off.&lt;/p&gt;

&lt;p&gt;Specifically, we benchmark 5 configurations and the plots below compare their absolute performance for different GPUs and batch sizes. For definitions of these configurations see section “Benchmarking setup and results”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models1.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 1&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models2.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 2&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models3.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 1&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizations&quot;&gt;Optimizations&lt;/h2&gt;

&lt;p&gt;Here we’ll go into more detail about the optimizations introduced into the model code. These optimizations rely on features of PyTorch 2.0 which has been released recently.&lt;/p&gt;

&lt;h3 id=&quot;optimized-attention&quot;&gt;Optimized Attention&lt;/h3&gt;

&lt;p&gt;One part of the code which we optimized is the scaled dot-product attention. Attention is known to be a heavy operation: naive implementation materializes the attention matrix, leading to time and memory complexity quadratic in sequence length. It is common for diffusion models to use attention (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CrossAttention&lt;/code&gt;) as part of Transformer blocks in multiple parts of the U-Net. Since the U-Net runs at every sampling step, this becomes a critical point to optimize. Instead of custom attention implementation one can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.MultiheadAttention,&lt;/code&gt; which in PyTorch 2 has optimized attention implementation is integrated into it. This optimization schematically boils down to the following pseudocode:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class CrossAttention(nn.Module):
    def __init__(self, ...):
        # Create matrices: Q, K, V, out_proj
        ...
    def forward(self, x, context=None, mask=None):
       # Compute out = SoftMax(Q*K/sqrt(d))V
       # Return out_proj(out)
       …
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gets replaced with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class CrossAttention(nn.Module):
    def __init__(self, ...):
        self.mha = nn.MultiheadAttention(...)
    def forward(self, x, context):
	return self.mha(x, context, context)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The optimized implementation of attention was available already in PyTorch 1.13 (see &lt;a href=&quot;https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/&quot;&gt;here&lt;/a&gt;) and widely adopted (see e.g. &lt;a href=&quot;https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2&quot;&gt;HuggingFace transformers library example&lt;/a&gt;). In particular, it integrates memory-efficient attention from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; library and flash attention from &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;. PyTorch 2.0 expands this to additional attention functions such as cross attention and custom kernels for further acceleration, making it applicable to diffusion models.&lt;/p&gt;

&lt;p&gt;Flash attention is available on GPUs with compute capability SM 7.5 or SM 8.x - for example, on T4, A10, and A100, which are included in our benchmark (you can check compute capability of each NVIDIA GPU &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot;&gt;here&lt;/a&gt;). However, in our tests on A100 the memory efficient attention performed better than flash attention for the particular case of diffusion models, due to the small number of attention heads and small batch size.  PyTorch understands this and in this case chooses memory efficient attention over flash attention when both are available (see the logic &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/d8e795ecd53670682bd3b2e5ff1f378402b147d5/aten/src/ATen/native/transformers/cuda/sdp_utils.h#L33-L71&quot;&gt;here&lt;/a&gt;). For full control over the attention backends (memory-efficient attention, flash attention, “vanilla math”, or any future ones), power users can enable and disable them manually with the help of the context manager &lt;a href=&quot;https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel&quot;&gt;torch.backends.cuda.sdp_kernel&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;compilation&quot;&gt;Compilation&lt;/h3&gt;

&lt;p&gt;Compilation is a &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/#user-experience&quot;&gt;new feature of PyTorch 2.0&lt;/a&gt;, enabling significant speedups with a very simple user experience. To invoke the default behavior, simply wrap a PyTorch module or a function into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.compile(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch compiler then turns Python code into a set of instructions which can be executed efficiently without Python overhead. The compilation happens dynamically the first time the code is executed. With the default behavior, under the hood PyTorch utilized &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/index.html&quot;&gt;TorchDynamo&lt;/a&gt; to compile the code and &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747&quot;&gt;TorchInductor&lt;/a&gt; to further optimize it. See &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamo_tutorial.html&quot;&gt;this tutorial&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Although the one-liner above is enough for compilation, certain modifications in the code can squeeze a larger speedup. In particular, one should avoid so-called graph breaks - places in the code which PyTorch can’t compile. As opposed to previous PyTorch compilation approaches (like TorchScript), PyTorch 2 compiler doesn’t break in this case. Instead it falls back on eager execution - so the code runs, but with reduced performance. We introduced a few minor changes to the model code to get rid of graph breaks. This included eliminating functions from libraries not supported by the compiler, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inspect.isfunction&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;einops.rearrange&lt;/code&gt;. See this &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&quot;&gt;doc&lt;/a&gt; to learn more about graph breaks and how to eliminate them.&lt;/p&gt;

&lt;p&gt;Theoretically, one can apply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile &lt;/code&gt;on the whole diffusion sampling loop. However, in practice it is enough to just compile the U-Net. The reason is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; doesn’t yet have a loop analyzer and would recompile the code for each iteration of the sampling loop. Moreover, compiled sampler code is likely to generate graph breaks - so one would need to adjust it if one wants to get a good performance from the compiled version.&lt;/p&gt;

&lt;p&gt;Note that compilation &lt;a href=&quot;https://github.com/openai/triton/blob/b5d32896b1f89fc44a82f8df3bb010934c53f4f5/README.md?plain=1#L66-L68&quot;&gt;requires GPU compute capability &amp;gt;= SM 7.0&lt;/a&gt; to run in non-eager mode. This covers all GPUs in our benchmarks -  T4, V100, A10, A100 - except for P100 (see the &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot;&gt;full list&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;other-optimizations&quot;&gt;Other optimizations&lt;/h3&gt;

&lt;p&gt;In addition, we have improved efficiency of GPU memory operations by eliminating some common pitfalls, e.g. creating a tensor on GPU directly rather than creating it on CPU and later moving to GPU. The places where such optimizations were necessary were determined by line-profiling and looking at CPU/GPU traces and &lt;a href=&quot;https://github.com/brendangregg/FlameGraph&quot;&gt;Flame Graphs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-setup-and-results-summary&quot;&gt;Benchmarking setup and results summary&lt;/h2&gt;

&lt;p&gt;We have two versions of code to compare: &lt;em&gt;original&lt;/em&gt; and &lt;em&gt;optimized&lt;/em&gt;. On top of this, several optimization features (xFormers, PyTorch memory efficient attention, compilation) can be turned on/off. Overall, as mentioned in the introduction, we will be benchmarking 5 configurations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Original code without xFormers&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Original code with xFormers&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with vanilla math attention backend and no compilation&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with memory-efficient attention backend and no compilation&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with memory-efficient attention backend and compilation&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As the &lt;em&gt;original version&lt;/em&gt; we took the version of the code which uses PyTorch 1.12 and a custom implementation of attention. The &lt;em&gt;optimized version&lt;/em&gt; uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.MultiheadAttention&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CrossAttention&lt;/code&gt; and PyTorch 2.0.0.dev20230111+cu117. It also has a few other minor optimizations in PyTorch-related code.&lt;/p&gt;

&lt;p&gt;The table below shows runtime of each version of the code in seconds, and the percentage improvement compared to the _original with xFormers. _The compilation time is excluded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 1. In parenthesis - relative improvement with respect to the “Original with xFormers” row&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;30.4s (-19.3%)
   &lt;/td&gt;
   &lt;td&gt;29.8s (-77.3%)
   &lt;/td&gt;
   &lt;td&gt;13.0s (-83.9%)
   &lt;/td&gt;
   &lt;td&gt;10.9s (-33.1%)
   &lt;/td&gt;
   &lt;td&gt;8.0s (-19.3%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;25.5s&lt;/strong&gt; (0.0%)
   &lt;/td&gt;
   &lt;td&gt;16.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;7.1s&lt;/strong&gt; (0.0%)
   &lt;/td&gt;
   &lt;td&gt;8.2s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;6.7s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;27.3s (-7.0%)
   &lt;/td&gt;
   &lt;td&gt;19.9s (-18.7%)
   &lt;/td&gt;
   &lt;td&gt;13.2s (-87.2%)
   &lt;/td&gt;
   &lt;td&gt;7.5s (8.7%)
   &lt;/td&gt;
   &lt;td&gt;5.7s (15.1%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;26.5s (-3.8%)
   &lt;/td&gt;
   &lt;td&gt;16.8s (0.2%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;7.1s&lt;/strong&gt; (-0.8%)
   &lt;/td&gt;
   &lt;td&gt;6.9s (16.0%)
   &lt;/td&gt;
   &lt;td&gt;5.3s (20.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;16.4s &lt;/strong&gt;(2.1%)
   &lt;/td&gt;
   &lt;td&gt;7.2s (-2.3%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;6.6s&lt;/strong&gt; (18.6%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;4.1s&lt;/strong&gt; (38.5%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 2&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;58.0s (-21.6%)
   &lt;/td&gt;
   &lt;td&gt;57.6s (-84.0%)
   &lt;/td&gt;
   &lt;td&gt;24.4s (-95.2%)
   &lt;/td&gt;
   &lt;td&gt;18.6s (-63.0%)
   &lt;/td&gt;
   &lt;td&gt;12.0s (-50.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;47.7s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;31.3s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;12.5s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;11.4s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;8.0s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;49.3s (-3.5%)
   &lt;/td&gt;
   &lt;td&gt;37.9s (-21.0%)
   &lt;/td&gt;
   &lt;td&gt;17.8s (-42.2%)
   &lt;/td&gt;
   &lt;td&gt;12.7s (-10.7%)
   &lt;/td&gt;
   &lt;td&gt;7.8s (1.8%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;47.5s &lt;/strong&gt;(0.4%)
   &lt;/td&gt;
   &lt;td&gt;31.2s (0.5%)
   &lt;/td&gt;
   &lt;td&gt;12.2s (2.6%)
   &lt;/td&gt;
   &lt;td&gt;11.5s (-0.7%)
   &lt;/td&gt;
   &lt;td&gt;7.0s (12.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;28.0s&lt;/strong&gt; (10.5%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;11.4s&lt;/strong&gt; (9.0%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;10.7s &lt;/strong&gt;(6.4%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;6.4s&lt;/strong&gt; (20.3%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 4&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;117.9s (-20.0%)
   &lt;/td&gt;
   &lt;td&gt;112.4s (-81.8%)
   &lt;/td&gt;
   &lt;td&gt;47.2s (-101.7%)
   &lt;/td&gt;
   &lt;td&gt;35.8s (-71.9%)
   &lt;/td&gt;
   &lt;td&gt;22.8s (-78.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;98.3s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;61.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;23.4s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;20.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;12.7s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;101.1s (-2.9%)
   &lt;/td&gt;
   &lt;td&gt;73.0s (-18.0%)
   &lt;/td&gt;
   &lt;td&gt;28.3s (-21.0%)
   &lt;/td&gt;
   &lt;td&gt;23.3s (-11.9%)
   &lt;/td&gt;
   &lt;td&gt;14.5s (-13.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;92.9s &lt;/strong&gt;(5.5%)
   &lt;/td&gt;
   &lt;td&gt;61.1s (1.2%)
   &lt;/td&gt;
   &lt;td&gt;23.9s (-1.9%)
   &lt;/td&gt;
   &lt;td&gt;20.8s (-0.1%)
   &lt;/td&gt;
   &lt;td&gt;12.8s (-0.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;53.1s &lt;/strong&gt;(14.2%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;20.9s&lt;/strong&gt; (10.6%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;18.6s&lt;/strong&gt; (10.4%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;11.2s&lt;/strong&gt; (12.2%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To minimize fluctuations and external influence on the performance of the benchmarked code, we ran each version of the code one after another, and then repeated this sequence 10 times: A, B, C, D, E,  A, B, … So the results of a typical run would look like the one in the picture below.. Note that one shouldn’t rely on comparison of absolute run times between different graphs, but comparison of run times_ inside_ one graph is pretty reliable, thanks to our benchmarking setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models4.png&quot; alt=&quot;Denoising diffusion model generation benchmarks&quot; style=&quot;max-height:700px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each run of text-to-image generation script produces several batches, the number of which is regulated by the CLI parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--n_iter&lt;/code&gt;. In the benchmarks we used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_iter = 2&lt;/code&gt;, but introduced an additional “warm-up” iteration, which doesn’t contribute to the run time. This was necessary for the runs with compilation, because compilation happens the first time the code runs, and so the first iteration is much longer than all subsequent. To make comparison fair, we also introduced this additional “warm-up” iteration to all other runs.&lt;/p&gt;

&lt;p&gt;The numbers in the table above are for number of iterations 2 (plus a “warm-up one”), prompt ”A photo”, seed 1, PLMS sampler, and autocast turned on.&lt;/p&gt;

&lt;p&gt;Benchmarks were done using P100, V100, A100, A10 and T4 GPUs. The T4 benchmarks were done in Google Colab Pro. The A10 benchmarks were done on g5.4xlarge AWS instances with 1 GPU.&lt;/p&gt;

&lt;h2 id=&quot;conclusions-and-next-steps&quot;&gt;Conclusions and next steps&lt;/h2&gt;

&lt;p&gt;We have shown that new features of PyTorch 2 - compiler and optimized attention implementation - give performance improvements exceeding or comparable with what previously required installation of an external dependency (xFormers). PyTorch achieved this, in particular, by integrating memory efficient attention from xFormers into its codebase. This is a significant improvement for user experience, given that xFormers, being a state-of-the-art library, in many scenarios requires custom installation process and long builds.&lt;/p&gt;

&lt;p&gt;There are a few natural directions in which this work can be continued:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The optimizations we implemented and described here are only benchmarked for text-to-image inference so far. It would be interesting to see how they affect training performance. PyTorch compilation can be directly applied to training; enabling training with PyTorch optimized attention is on the roadmap&lt;/li&gt;
  &lt;li&gt;We intentionally minimized changes to the original model code. Further profiling and optimization can probably bring more improvements&lt;/li&gt;
  &lt;li&gt;At the moment compilation is applied only to the U-Net model inside the sampler. Since there is a lot happening outside of U-Net (e.g. operations directly in the sampling loop), it would be beneficial to compile the whole sampler. However, this would require analysis of the compilation process to avoid recompilation at every sampling step&lt;/li&gt;
  &lt;li&gt;Current code only applies compilation within the PLMS sampler, but it should be trivial to extend it to other samplers&lt;/li&gt;
  &lt;li&gt;Besides text-to-image generation, diffusion models are also applied to other tasks - image-to-image and inpainting. It would be interesting to measure how their performance improves from PyTorch 2 optimizations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See if you can increase performance of open source diffusion models using the methods we described, and share the results!&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch 2.0 overview, which has a lot of information on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile:&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;https://pytorch.org/get-started/pytorch-2.0/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tutorial on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;General compilation troubleshooting: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/troubleshooting.html&quot;&gt;https://pytorch.org/docs/master/dynamo/troubleshooting.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Details on graph breaks: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&quot;&gt;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Details on guards: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/guards-overview.html&quot;&gt;https://pytorch.org/docs/master/dynamo/guards-overview.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Video deep dive on TorchDynamo &lt;a href=&quot;https://www.youtube.com/watch?v=egZB5Uxki0I&quot;&gt;https://www.youtube.com/watch?v=egZB5Uxki0I&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tutorial on optimized attention in PyTorch 1.12: &lt;a href=&quot;https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html&quot;&gt;https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank Geeta Chauhan, Natalia Gimelshein, Patrick Labatut, Bert Maher, Mark Saroufim, Michael Voznesensky and Francisco Massa for their valuable advice and early feedback on the text.&lt;/p&gt;

&lt;p&gt;Special thanks to Yudong Tao initiating the work on using PyTorch native attention in diffusion models.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Grigory Sizov, Michael Gschwind, Hamid Shojanazeri, Driss Guessous, Daniel Haziza, Christian Puhrsch</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new torch.compile() compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Celebrate PyTorch 2.0 with New Performance Features for AI Developers</title>
      <link href="https://pytorch.org/blog/celebrate-pytorch-2.0/" rel="alternate" type="text/html" title="Celebrate PyTorch 2.0 with New Performance Features for AI Developers" />
      <published>2023-04-07T00:00:00-07:00</published>
      <updated>2023-04-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/celebrate-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/celebrate-pytorch-2.0/">&lt;p&gt;Congratulations to the PyTorch Foundation for its release of &lt;strong&gt;PyTorch 2.0&lt;/strong&gt;! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;TorchInductor&lt;/li&gt;
  &lt;li&gt;GNN&lt;/li&gt;
  &lt;li&gt;INT8 Inference Optimization&lt;/li&gt;
  &lt;li&gt;oneDNN Graph API&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We at Intel are delighted to be part of the PyTorch community and appreciate the collaboration with and feedback from our colleagues at &lt;a href=&quot;http://www.meta.com/&quot;&gt;Meta&lt;/a&gt; as we co-developed these features.&lt;/p&gt;

&lt;p&gt;Let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;1-torchinductor-cpu-fp32-inference-optimized&quot;&gt;1. TorchInductor CPU FP32 Inference Optimized&lt;/h2&gt;

&lt;p&gt;As part of the PyTorch 2.0 compilation stack, TorchInductor CPU backend optimization brings notable performance improvements via graph compilation over the PyTorch eager mode.&lt;/p&gt;

&lt;p&gt;The TorchInductor CPU backend is sped up by leveraging the technologies from the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; for Conv/GEMM ops with post-op fusion and weight prepacking, and PyTorch ATen CPU kernels for memory-bound ops with explicit vectorization on top of OpenMP*-based thread parallelization.&lt;/p&gt;

&lt;p&gt;With these optimizations on top of the powerful loop fusions in TorchInductor codegen, we achieved up to a &lt;strong&gt;1.7x&lt;/strong&gt; FP32 inference performance boost over three representative deep learning benchmarks: TorchBench, HuggingFace, and timm1. Training and low-precision support are under development.&lt;/p&gt;

&lt;h3 id=&quot;see-the-improvements&quot;&gt;See the Improvements&lt;/h3&gt;

&lt;p&gt;The performance improvements on various backends are tracked on this &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/93531#issuecomment-1457373890&quot;&gt;TouchInductor CPU Performance Dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;improve-graph-neural-network-gnn-in-pyg-for-inference-and-training-performance-on-cpu&quot;&gt;Improve Graph Neural Network (GNN) in PyG for Inference and Training Performance on CPU&lt;/h2&gt;

&lt;p&gt;GNN is a powerful tool to analyze graph structure data. This feature is designed to improve GNN inference and training performance on Intel® CPUs, including the new 4th Gen Intel® Xeon® Scalable processors.&lt;/p&gt;

&lt;p&gt;PyTorch Geometric (PyG) is a very popular library built upon PyTorch to perform GNN workflows. Currently on CPU, GNN models of PyG run slowly due to the lack of GNN-related sparse matrix multiplication operations (i.e., SpMM_reduce) and the lack of several critical kernel-level optimizations (scatter/gather, etc.) tuned for GNN compute.&lt;/p&gt;

&lt;p&gt;To address this, optimizations are provided for message passing between adjacent neural network nodes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;scatter_reduce:&lt;/strong&gt; performance hotspot in message-passing when the edge index is stored in coordinate format (COO).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;gather:&lt;/strong&gt; backward computation of scatter_reduce, specially tuned for the GNN compute when the index is an expanded tensor.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;torch.sparse.mm with reduce flag:&lt;/strong&gt; performance hotspot in message-passing when the edge index is stored in compressed sparse row (CSR). Supported reduce flag for: sum, mean, amax, amin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;End-to-end performance benchmark results for both inference and training on 3rd Gen Intel® Xeon® Scalable processors 8380 platform and on 4th Gen 8480+ platform are discussed in &lt;a href=&quot;http://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;Accelerating PyG on Intel CPUs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;optimize-int8-inference-with-unified-quantization-backend-for-x86-cpu-platforms&quot;&gt;Optimize int8 Inference with Unified Quantization Backend for x86 CPU Platforms&lt;/h2&gt;

&lt;p&gt;The new X86 quantization backend is a combination of &lt;a href=&quot;http://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt; (Facebook General Matrix-Matrix Multiplication) and &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneAPI Deep Neural Network Library (oneDNN&lt;/a&gt;) backends and replaces FBGEMM as the default quantization backend for x86 platforms. The result: better end-to-end int8 inference performance than FBGEMM.&lt;/p&gt;

&lt;p&gt;Users access the x86 quantization backend by default for x86 platforms, and the selection between different kernels is automatically done behind the scenes. The rules of selection are based on prior performance testing data done by Intel during feature development. Thus, the x86 backend replaces FBGEMM and may offer better performance, depending on the use case.&lt;/p&gt;

&lt;p&gt;The selection rules are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On platforms without VNNI (e.g., Intel® Core™ i7 processors), FBGEMM is always used.&lt;/li&gt;
  &lt;li&gt;On platforms with VNNI (e.g., 2nd-4th Gen Intel® Xeon® Scalable processors and future platforms):
    &lt;ul&gt;
      &lt;li&gt;For linear, FBGEMM is always used.&lt;/li&gt;
      &lt;li&gt;For convolution layers, FBGEMM is used for depth-wise convolution whose layers &amp;gt; 100; otherwise, oneDNN is used.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that as the kernels continue to evolve.&lt;/p&gt;

&lt;p&gt;The selection rules above are subject to change to achieve better performance. Performance metrics for through-put speed-up ratios of unified x86 backend vs. pure FBGEMM are discussed in &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;[RFC] Unified quantization backend for x86 CPU platforms #83888&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;leverage-onednn-graph-api-to-accelerate-inference-on-cpu&quot;&gt;Leverage oneDNN Graph API to Accelerate Inference on CPU&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on Intel® AI hardware. It automatically identifies the graph partitions to be accelerated via fusion. The &lt;a href=&quot;http://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul, and their neighbor operations for both inference and training use cases.&lt;/p&gt;

&lt;p&gt;Currently, BFloat16 and Float32 datatypes are supported and only inference workloads can be optimized.  BF16 is only optimized on machines with Intel® Advanced Vector Extensions 512 (Intel® AVX-512) BF16 support.&lt;/p&gt;

&lt;p&gt;Few or no modifications are needed in PyTorch to support newer oneDNN Graph fusions/optimized kernels. To use oneDNN Graph, users can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Either use the API &lt;em&gt;torch.jit.enable_onednn_fusion(True)&lt;/em&gt; before JIT tracing a model, OR …&lt;/li&gt;
  &lt;li&gt;Use its context manager, viz. &lt;em&gt;with torch.jit.fuser(“fuser3”).&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;For accelerating &lt;a href=&quot;http://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16 inference&lt;/a&gt;, we rely on eager-mode AMP (Automatic Mixed Precision) support in PyTorch and disable JIT mode’s AMP.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;PyTorch performance tuning guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;h3 id=&quot;get-the-software&quot;&gt;Get the Software&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://pytorch.org/get-started/locally/&quot;&gt;Try out PyTorch 2.0&lt;/a&gt; and realize the performance benefits for yourself from these Intel-contributed features.&lt;/p&gt;

&lt;p&gt;We encourage you to check out Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Framework&lt;/a&gt; optimizations and learn about the open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel’s AI software portfolio.&lt;/p&gt;

&lt;p&gt;For more details about 4th Gen Intel Xeon Scalable processor, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI Platform&lt;/a&gt; where you can learn about how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-resources&quot;&gt;PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Congratulations to the PyTorch Foundation for its release of PyTorch 2.0! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Straggler Mitigation On PyTorch DDP By Hierarchical SGD</title>
      <link href="https://pytorch.org/blog/straggler-mitigation/" rel="alternate" type="text/html" title="Straggler Mitigation On PyTorch DDP By Hierarchical SGD" />
      <published>2023-04-07T00:00:00-07:00</published>
      <updated>2023-04-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/straggler-mitigation</id>
      <content type="html" xml:base="https://pytorch.org/blog/straggler-mitigation/">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/ddp.html&quot;&gt;PyTorch DDP&lt;/a&gt; has been widely adopted across the industry for distributed training, which by default runs synchronous SGD to synchronize gradients across model replicas at every step. The performance of this technique is critical for fast iteration during model exploration as well as resource and cost saving. The performance is critical for fast iteration and cost saving of model development and exploration. To resolve a ubiquitous performance bottleneck introduced by slow nodes in large-scale training, Cruise and Meta co-developed a solution based on the &lt;a href=&quot;https://arxiv.org/abs/2007.13819&quot;&gt;Hierarchical SGD&lt;/a&gt; algorithm to significantly accelerate training in the presence of these stragglers.&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-straggler-mitigation&quot;&gt;The Need For Straggler Mitigation&lt;/h2&gt;

&lt;p&gt;In DDP setup, a straggler problem can occur when one or more processes run much slower (“stragglers”) than other processes. When this happens, all the processes have to wait for the stragglers before synchronizing gradients and completing the communication, which essentially bottlenecks distributed performance to the slowest worker.As a result, even for the cases of training relatively small models, the communication cost can still be a major performance bottleneck.&lt;/p&gt;

&lt;h3 id=&quot;potential-causes-of-stragglers&quot;&gt;Potential Causes of Stragglers&lt;/h3&gt;

&lt;p&gt;Severe straggler issues are usually caused by workload imbalance before synchronization, and many factors can contribute to this imbalance. For instance, some data loader workers in the distributed environment can become stragglers, because some input examples can be outliers in terms of the data size, or the data transfer of some examples can be drastically slowed down due to unstable network I/O, or the on-the-fly data transformation costs can have a high variance.&lt;/p&gt;

&lt;p&gt;Besides data loading, other phases before gradient synchronization can also cause stragglers, such as unbalanced workloads of embedding table lookup during the forward pass in recommendation systems.&lt;/p&gt;

&lt;h3 id=&quot;the-appearance-of-stragglers&quot;&gt;The Appearance of Stragglers&lt;/h3&gt;

&lt;p&gt;If we profile DDP training jobs that have stragglers, we can find that some processes may have much higher gradient synchronization costs (a.k.a., allreducing gradients) than other processes at a certain step. As a result, the distributed performance can be dominated by the communication cost even if the model size is very small. In this case, some processes run faster than the straggler(s) at a step, and hence they have to wait for the stragglers and spend a much longer time on allreduce.&lt;/p&gt;

&lt;p&gt;The below shows screenshots of two trace files output by PyTorch profiler in a use case. Each screenshot profiles 3 steps.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first screenshot shows that a process has a very high allreduce cost in both the first and the third steps, because this process reaches the synchronization phase earlier than the straggler(s), and it spends more time on waiting. On the other hand, the allreduce cost is relatively small in the second step, this suggests that 1) there is no straggler at this step; or 2) this process is the straggler among all the processes, so it does not need to wait for any other process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-1.png&quot; alt=&quot;chart showing allreduce cost&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Both the 1st and the 3rd Steps Are Slowed Down by Stragglers&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The second screenshot shows a normal case without stragglers. In this case, all the gradient synchronizations are relatively short.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-2.png&quot; alt=&quot;chart showing normal case without stragglers&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Normal Case Without Stragglers&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;hierarchical-sgd-in-pytorch&quot;&gt;Hierarchical SGD in PyTorch&lt;/h2&gt;

&lt;p&gt;Recently hierarchical SGD has been proposed to optimize the communication costs by mainly reducing the total amount of data transfer in large-scale distributed training, and multiple convergence analyses have been provided (&lt;a href=&quot;https://arxiv.org/pdf/2010.12998.pdf&quot;&gt;example&lt;/a&gt;). As a main novelty of this post, at Cruise we could leverage hierarchical SGD to mitigate stragglers, which may also occur on training relatively small models. Our implementation has been upstreamed by Cruise to PyTorch in early 2022.&lt;/p&gt;

&lt;h3 id=&quot;how-does-hierarchical-sgd-work&quot;&gt;How Does Hierarchical SGD Work?&lt;/h3&gt;

&lt;p&gt;As the name implies, hierarchical SGD organizes all the processes into groups at different levels as a hierarchy, and runs synchronization by following the rules below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the groups at the same level have the same number of processes, and the processes in these groups synchronize at the same frequency concurrently, where the synchronization period is pre-defined by the user.&lt;/li&gt;
  &lt;li&gt;The higher level a group is, the larger synchronization period is used, as the synchronization becomes more expensive.&lt;/li&gt;
  &lt;li&gt;When multiple overlapping groups are supposed to synchronize according to their periods, to reduce redundant synchronization and avoid data race across groups, only the highest-level group runs synchronization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following figure illustrates an example of 4-level hierarchy SGD among 16 processes on 8 machines, each of which has 2 GPUs:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Level 1:&lt;/strong&gt; Each process runs mini-batch SGD locally;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 2:&lt;/strong&gt; Each 4-process group across 2 machines runs synchronization every 2 steps;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 3:&lt;/strong&gt; Each 8-process group across 4 machines runs synchronization every 4 steps;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 4:&lt;/strong&gt; The global process group of all 16 processes over 8 machines runs synchronization every 8 steps.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Particularly, when the step number can be divided by 8, only the synchronization at 3) is executed, and when the step number can be divided by 4 but not 8, only the synchronization at 2) is executed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-3.png&quot; alt=&quot;An example of 4-level hierarchy SGD among 16 processes on 8 machines, each of which has 2 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Intuitively, hierarchical SGD can be viewed as an extension of &lt;a href=&quot;https://core.ac.uk/download/pdf/211998087.pdf&quot;&gt;local SGD&lt;/a&gt;, which only has a two-level hierarchy – every process runs mini-batch SGD locally and then synchronizes globally at a certain frequency. This can also help explain that, just like local SGD, hierarchical SGD synchronizes model parameters instead of gradients. Otherwise the gradient descent will be mathematically incorrect when the frequency is greater than 1.&lt;/p&gt;

&lt;h3 id=&quot;why-can-hierarchical-sgd-mitigate-stragglers&quot;&gt;Why Can Hierarchical SGD Mitigate Stragglers?&lt;/h3&gt;

&lt;p&gt;The key insight here is that, when there is a random straggler, it only directly slows down a relatively small group of processes instead of all the processes. Next time another random straggler is very likely to slow down a different small group, and hence a hierarchy can help smooth out the straggler effect.&lt;/p&gt;

&lt;p&gt;The example below assumes that there is a random straggler among totally 8 processes at every step. After 4 steps, vanilla DDP that runs synchronous SGD will be slowed down by straggler 4 times, because it runs global synchronization at every step. In contrast, hierarchical SGD runs synchronization with the groups of 4 processes after the first two steps, and then a global synchronization after another two steps. We can see that both the first two and the last two stragglers have a large overlap, and hence the performance loss can be mitigated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-4.png&quot; alt=&quot;flow diagram&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Essentially, the mitigation effect of this hierarchical SGD example actually is between local SGD at a frequency of every 2 steps and every 4 steps. The main advantage of hierarchical SGD over local SGD is a better convergence efficiency of the same global synchronization frequency, because hierarchical SGD allows more low-level synchronization. Moreover, it is possible for hierarchical SGD to provide a global synchronization frequency lower than local SGD with model parity, leading to a higher training performance, especially in a large-scale distributed training.&lt;/p&gt;

&lt;h3 id=&quot;ease-of-use&quot;&gt;Ease of Use&lt;/h3&gt;

&lt;p&gt;Straggler mitigation is not a novel study in distributed training. Multiple approaches have been proposed, such as &lt;a href=&quot;https://arxiv.org/pdf/1705.09056.pdf&quot;&gt;gossip SGD&lt;/a&gt;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf&quot;&gt;data encoding&lt;/a&gt;, &lt;a href=&quot;http://proceedings.mlr.press/v70/tandon17a/tandon17a.pdf&quot;&gt;gradient coding&lt;/a&gt;, as well as some particularly designed for parameter-server architecture, including &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45187.pdf&quot;&gt;backup workers&lt;/a&gt; and &lt;a href=&quot;http://www.cs.cmu.edu/~seunghak/SSPTable_NIPS2013.pdf&quot;&gt;stale synchronous parallel&lt;/a&gt;. However, to the best of our knowledge, before this effort we have not found a good open-source PyTorch implementation of straggler mitigation that can work like a plugin to our training system at Cruise. In contrast, our implementation only requires the minimal changes – no need to modify the existing code or tune any existing hyperparameters. This is a very appealing advantage for industry users.&lt;/p&gt;

&lt;p&gt;As the code example below shows, only a few lines need to be added to the setup of DDP model, and the training loop code can keep untouched. As explained previously, hierarchical SGD is an extended form of local SGD, so the enablement can be quite similar to local SGD (see PyTorch docs of &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer&quot;&gt;PostLocalSGDOptimizer&lt;/a&gt;):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Register a post-local SGD communication hook to run a warmup stage of fully synchronous SGD and defer hierarchical SGD.&lt;/li&gt;
  &lt;li&gt;Create a post-local SGD optimizer that wraps an existing local optimizer and a hierarchical SGD configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch.distributed.algorithms.model_averaging.hierarchical_model_averager as hierarchicalSGD
from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (
    PostLocalSGDState,
    post_localSGD_hook,
)
from torch.distributed.optim import PostLocalSGDOptimizer

ddp_model = nn.parallel.DistributedDataParallel(
    module=model,
    device_ids=[rank],
)

# Register a post-local SGD communication hook for the warmup.
subgroup, _ = torch.distributed.new_subgroups()
state = PostLocalSGDState(subgroup=subgroup, start_localSGD_iter=1_000)
ddp_model.register_comm_hook(state, post_localSGD_hook)

# Wraps the existing (local) optimizer to run hierarchical model averaging.
optim = PostLocalSGDOptimizer(
  optim=optim,
  averager=hierarchicalSGD.HierarchicalModelAverager(
    # The config runs a 4-level hierarchy SGD among 128 processes:
    # 1) Each process runs mini-batch SGD locally;
    # 2) Each 8-process group synchronize every 2 steps;
    # 3) Each 32-process group synchronize every 4 steps;
    # 4) All 128 processes synchronize every 8 steps.
    period_group_size_dict=OrderedDict([(2, 8), (4, 32), (8, 128)]),
    # Do not run hierarchical SGD until 1K steps for model parity.
    warmup_steps=1_000)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;algorithm-hyperparameters&quot;&gt;Algorithm Hyperparameters&lt;/h3&gt;

&lt;p&gt;Hierarchical SGD has two major hyperparameters: &lt;em&gt;period_group_size_dict&lt;/em&gt; and &lt;em&gt;warmup_steps&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;period_group_size_dict&lt;/strong&gt; is an ordered dictionary mapping from synchronization period to process group size, used for initializing process groups of different sizes in a hierarchy to synchronize parameters concurrently. A larger group is expected to use a larger synchronization period.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;warmup_steps&lt;/strong&gt; specifies a number of steps as the warmup stage to run synchronous SGD before hierarchical SGD. Similar to &lt;a href=&quot;https://arxiv.org/pdf/1808.07217.pdf&quot;&gt;post-local SGD&lt;/a&gt; algorithm, a warmup stage is usually recommended to achieve a higher accuracy. The value should be the same as &lt;em&gt;start_localSGD_iter&lt;/em&gt; arg used in &lt;em&gt;PostLocalSGDState&lt;/em&gt; when post_localSGD_hook is registered. Typically the warmup stage should at least cover the beginning of training when the loss is decreased drastically.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A subtle difference between the PyTorch implementation and the initial design proposed by relevant papers is that, after the warmup stage, by default the processes within each host still run intra-host gradient synchronization at every step. This is because that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The intra-host communication is relatively cheap, and it can usually significantly accelerate the convergence;&lt;/li&gt;
  &lt;li&gt;The intra-host group (of size 4 or 8 for most industry users) can usually be a good choice of the smallest group of processes that synchronize most frequently in hierarchical SGD. If the synchronization period is 1, then gradient synchronization is faster than model parameter synchronization (a.k.a., model averaging), because DDP automatically overlaps gradient synchronization and the backward pass.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Such intra-host gradient synchronization can be disabled by unsetting &lt;em&gt;post_local_gradient_allreduce&lt;/em&gt; arg in &lt;em&gt;PostLocalSGDState&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;demonstration&quot;&gt;Demonstration&lt;/h2&gt;

&lt;p&gt;Now we demonstrate that hierarchical SGD can accelerate distributed training by mitigating stragglers.&lt;/p&gt;

&lt;h3 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h3&gt;

&lt;p&gt;We compared the performance of hierarchical SGD against local SGD and synchronous SGD on &lt;a href=&quot;https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html&quot;&gt;ResNet18&lt;/a&gt; (model size: 45MB). Since the model is so small, the training is not bottlenecked by data transfer cost during synchronization. To avoid the noises incurred by data loading from remote storage, the input data was randomly simulated from memory. We varied the number of GPUs used by training from 64 to 256. The batch size per worker is 32, and the number of iterations of training is 1,000. Since we don’t evaluate convergence efficiency in this set of experiments, warmup is not enabled.&lt;/p&gt;

&lt;p&gt;We also emulated stragglers at a rate of 1% on 128 and 256 GPUs, and 2% on 64 GPUs, to make sure at least one stragglers at every step on average. These stragglers randomly appear on different CUDA devices. Each straggler stalls for 1 second besides the normal per-step training time (~55ms in our setup). This can be perceived as a practical scenario where 1% or 2% of input data are outliers in terms of the data pre-processing cost (I/O and/or data transformation on the fly) during training, and such cost is 20X+ larger than the average.&lt;/p&gt;

&lt;p&gt;The code snippet below shows how a straggler can be emulated in the training loop. We applied it to a ResNet model, and it can be easily applied to the other models as well.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     loss = loss_fn(y_pred, y)
     # Emulate a straggler that lags for 1 second at a rate of 1%.
     if random.randint(1, 100) == 1:
         time.sleep(1)
     loss.backward()
     optimizer.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The experiments are conducted on us-central1 GCP cluster. Each machine has 4 NVIDIA Tesla T4 GPUs with 16 GB memory per GPU, connected through a 32 Gbit/s ethernet network. Each instance also features 96 vCPUs, 360 GB RAM.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot; style=&quot;max-width: 450px;&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Architecture
   &lt;/td&gt;
   &lt;td&gt;ResNet18 (45MB)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Workers
   &lt;/td&gt;
   &lt;td&gt;64, 128, 256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Backend
   &lt;/td&gt;
   &lt;td&gt;NCCL
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;Tesla T4, 16 GB memory
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Batch size
   &lt;/td&gt;
   &lt;td&gt;32 x ## of workers
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Straggler Duration
   &lt;/td&gt;
   &lt;td&gt;1 sec
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Straggler Rate
   &lt;/td&gt;
   &lt;td&gt;1% on 128 and 256 GPUs, 2% on 64 GPUs
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We used multiple configurations for both local SGD and hierarchical SGD. Local SGD runs global synchronization every 2, 4, and 8 steps, respectively.&lt;/p&gt;

&lt;p&gt;We ran hierarchical SGD with the following configurations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;On 64 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 8-process group, 32-process, and the global 64-process group synchronizes every 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-32,8-64&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 32-process group and the global 64-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-32,8-64&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;On 128 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 8-process group, 32-process group, and the global 128-process group synchronizes every 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-32,8-128&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 32-process group and the global 128-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-32,8-128&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;On 256 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 4-process group, 16-process group, 64-process group, and the global 256-process group synchronizes every 1, 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 1-4,2-16,4-64,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 8-process group, 64-process group, and the global 256-process group synchronizes every 2, 4, and 8 steps. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-64,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 16-process group and the global 256-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-16,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h3&gt;

&lt;p&gt;The figures below show the speedups of different communication schemes against the baseline of synchronous SGD, with the emulated stragglers. We can make the following observations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As expected, we can see that both hierarchical SGD and local SGD can achieve a higher speedup with a lower synchronization frequency.&lt;/li&gt;
  &lt;li&gt;The speedups of the hierarchical SGD schemes are &lt;strong&gt;2.08X-2.45X&lt;/strong&gt; on 64 GPUs, &lt;strong&gt;2.57X-2.68X&lt;/strong&gt; on 128 GPUs, and &lt;strong&gt;2.63X-3.25X&lt;/strong&gt; on 256 GPUs, respectively. This shows that hierarchical SGD can significantly mitigate stragglers, and such mitigation can be more effective at a larger scale.&lt;/li&gt;
  &lt;li&gt;The performance of local SGD with the synchronization period of 2 steps and 8 steps can be perceived as the lower bound and upper bound of the experimented hierarchical SGD schemes, respectively. This is because the hierarchical SGD schemes synchronize less frequently than every 2 steps globally, but their low-level synchronization at small groups are the extra overheads in comparison with the global synchronization every 8 steps.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Overall, hierarchical SGD can provide a finer-grained trade-off between communication cost and model quality than local SGD. Therefore, when local SGD at a relatively large synchronization period like 8 or 4 cannot give a satisfactory convergence efficiency, hierarchical SGD can have a much better chance to achieve both a good speedup and a model parity.&lt;/p&gt;

&lt;p&gt;Since only simulated data is used in the experiments, we did not demonstrate the model parity here, which in practice can be achieved in two ways:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Tuning the hyperparameters including both hierarchy and warmup steps;&lt;/li&gt;
  &lt;li&gt;For some cases, hierarchical SGD could lead to a slightly lower quality than the original model for the same number of training steps (i.e., lower convergence rate), but with a speedup like 2X+ per training step, it is still possible to achieve model parity with more steps but still less total training time.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-5.png&quot; alt=&quot;Speedups on 64 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-6.png&quot; alt=&quot;Speedups on 128 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-7.png&quot; alt=&quot;Speedups on 256 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;Before applying hierarchical SGD to straggler mitigation, the user should be aware of a few limitations of this approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;This approach can only mitigate non-persistent stragglers, which occur to different workers at different times. However, for the case of persistent stragglers, which can be caused by hardware degradation or a network issue on a specific host, these stragglers will slow down the same low-level subgroup at every time, leading to nearly no straggler mitigation.&lt;/li&gt;
  &lt;li&gt;This approach can only mitigate low-frequency stragglers. E.g., if 30% workers can randomly become stragglers at every step, then most low-level synchronizations will still be slowed down by stragglers. As a result, hierarchical SGD may not show an obvious performance advantage over synchronous SGD.&lt;/li&gt;
  &lt;li&gt;Since hierarchical SGD applies model averaging that does not overlap with backward like gradient averaging used by vanilla DDP, its performance gain of straggler mitigation must outweigh the performance loss of no overlap between communication and backward pass. Therefore, if stragglers only slow down training by less than 10%, hierarchical SGD may not be able to bring much speedup. This limitation can be addressed by &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/release/1.13/torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py&quot;&gt;overlapping optimizer step and backward pass&lt;/a&gt; in the future.&lt;/li&gt;
  &lt;li&gt;Since hierarchical SGD is less well-studied than local SGD, there is no guarantee that hierarchical SGD with a finer-grained synchronization granularity can converge faster than certain advanced forms of local SGD, such as &lt;a href=&quot;https://openreview.net/pdf?id=SkxJ8REYPH&quot;&gt;SlowMo&lt;/a&gt;, which can improve convergence efficiency with slow momentum. However, to the best of our knowledge, these advanced algorithms cannot be natively supported as a PyTorch DDP plugin like hierarchical SGD yet.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank Cruise teammates &lt;strong&gt;Bo Tian&lt;/strong&gt;, &lt;strong&gt;Sergei Vorobev&lt;/strong&gt;, &lt;strong&gt;Eugene Selivonchyk, Tsugn-Hsien Lee&lt;/strong&gt;, &lt;strong&gt;Dan Ring&lt;/strong&gt;, &lt;strong&gt;Ian Ackerman&lt;/strong&gt;, &lt;strong&gt;Lei Chen&lt;/strong&gt;, &lt;strong&gt;Maegan Chew&lt;/strong&gt;, &lt;strong&gt;Viet Anh To&lt;/strong&gt;, &lt;strong&gt;Xiaohui Long&lt;/strong&gt;, &lt;strong&gt;Zeyu Chen&lt;/strong&gt;, &lt;strong&gt;Alexander Sidorov&lt;/strong&gt;, &lt;strong&gt;Igor Tsvetkov&lt;/strong&gt;, &lt;strong&gt;Xin Hu&lt;/strong&gt;, &lt;strong&gt;Manav Kataria&lt;/strong&gt;, &lt;strong&gt;Marina Rubtsova&lt;/strong&gt;, and &lt;strong&gt;Mohamed Fawzy&lt;/strong&gt;, as well as Meta teammates &lt;strong&gt;Shen Li, Yanli Zhao, Suraj Subramanian, Hamid Shojanzeri, Anjali Sridhar&lt;/strong&gt; and &lt;strong&gt;Bernard Nguyen&lt;/strong&gt; for the support.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yi Wang (Cruise AI), Rohan Varma (Meta AI)</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch DDP has been widely adopted across the industry for distributed training, which by default runs synchronous SGD to synchronize gradients across model replicas at every step. The performance of this technique is critical for fast iteration during model exploration as well as resource and cost saving. The performance is critical for fast iteration and cost saving of model development and exploration. To resolve a ubiquitous performance bottleneck introduced by slow nodes in large-scale training, Cruise and Meta co-developed a solution based on the Hierarchical SGD algorithm to significantly accelerate training in the presence of these stragglers.</summary>
      

      
      
    </entry>
  
</feed>


