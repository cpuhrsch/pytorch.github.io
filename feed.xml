<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-10-14T18:03:36-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Flash-Decoding for long-context inference</title>
      <link href="https://pytorch.org/blog/flash-decoding/" rel="alternate" type="text/html" title="Flash-Decoding for long-context inference" />
      <published>2023-10-13T00:00:00-07:00</published>
      <updated>2023-10-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/flash-decoding</id>
      <content type="html" xml:base="https://pytorch.org/blog/flash-decoding/">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Large language models (LLM) such as ChatGPT or Llama have received unprecedented attention lately. However, they remain massively expensive to run. Even though generating a single response can cost about $0.01 (a few seconds of an 8xA100 instance on AWS), the costs quickly add up when scaling to billions of users, who could have multiple daily interactions with such LLMs. Some use cases are more expensive, like code auto-completion, because it runs whenever a new character is typed. As LLM applications multiply, even small efficiency gains to the generation time can have a massive impact.&lt;/p&gt;

&lt;p&gt;LLM inference (or “decoding”) is an iterative process: tokens are generated one at a time. Generating full sentences of N tokens requires N forward passes through the model. Fortunately, it is possible to cache previously calculated tokens: this means that a single generation step does not depend on the context length, except for a single operation, the attention. This operation does not scale well with context length.&lt;/p&gt;

&lt;p&gt;There are a number of important emerging use cases of LLMs that utilize a long context. With a longer context, LLMs can reason about longer documents, either to summarize or answer questions about them, they can keep track of longer conversations, or even process entire codebases before writing code. As an example, most LLMs had a context length of up to 2k in 2022 (GPT-3), but we now have open-source LLMs scaling up to 32k (&lt;a href=&quot;https://together.ai/blog/llama-2-7b-32k&quot;&gt;Llama-2-32k&lt;/a&gt;), or even 100k more recently (&lt;a href=&quot;https://about.fb.com/news/2023/08/code-llama-ai-for-coding/&quot;&gt;CodeLlama&lt;/a&gt;). In this setting, attention takes a significant fraction of time during inference.&lt;/p&gt;

&lt;p&gt;When scaling on the batch size dimension, the attention can also become a bottleneck even with relatively small contexts. This is because the amount of memory to read scales with the batch dimension, whereas it only depends on the model size for the rest of the model.&lt;/p&gt;

&lt;p&gt;We present a technique, Flash-Decoding, that significantly speeds up attention during inference, bringing up to 8x faster generation for very long sequences. The main idea is to load the keys and values in parallel as fast as possible, then separately rescale and combine the results to maintain the right attention outputs.&lt;/p&gt;

&lt;h2 id=&quot;multi-head-attention-for-decoding&quot;&gt;Multi-head attention for decoding&lt;/h2&gt;

&lt;p&gt;During decoding, every new token that is generated needs to attend to all previous tokens, to compute:&lt;/p&gt;

&lt;p&gt;softmax(queries @ keys.transpose) @ values&lt;/p&gt;

&lt;p&gt;This operation has been optimized with FlashAttention (v1 and v2 recently) in the training case, where the bottleneck is the memory bandwidth to read and write the intermediate results (e.g. Q @ K^T). However, these optimizations don’t apply directly to the inference case, because the bottlenecks are different. For training, FlashAttention parallelizes across the batch size and query length dimensions. During inference, the query length is typically 1: this means that if the batch size is smaller than the number of streaming multiprocessors (SMs) on the GPU (108 for an A100), the operation will only use a small part of the GPU! This is especially the case when using long contexts, because it requires smaller batch sizes to fit in GPU memory. With a batch size of 1, FlashAttention will use less than 1% of the GPU!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Inference_regular_attn.gif&quot; alt=&quot;FlashAttention&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;FlashAttention parallelizes across blocks of queries and batch size only, and does not manage to occupy the entire GPU during decoding&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The attention can also be done using matrix multiplication primitives - without using FlashAttention. In this case, the operation occupies the GPU entirely, but launches many kernels that write and read intermediate results, which is not optimal.&lt;/p&gt;

&lt;h2 id=&quot;a-faster-attention-for-decoding-flash-decoding&quot;&gt;A faster attention for decoding: Flash-Decoding&lt;/h2&gt;

&lt;p&gt;Our new approach Flash-Decoding is based on FlashAttention, and adds a new parallelization dimension: the keys/values sequence length. It combines the benefits of the 2 approaches from above. Like FlashAttention, it stores very little extra data to global memory, however it fully utilizes the GPU even when the batch size is small, as long as the context length is large enough.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inference_splitkv.gif&quot; alt=&quot;Flash-Decoding&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Flash-Decoding also parallelizes across keys and values, at the cost of a small final reduction step&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Flash-Decoding works in 3 steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, we split the keys/values in smaller chunks.&lt;/li&gt;
  &lt;li&gt;We compute the attention of the query with each of these splits in parallel using FlashAttention. We also write 1 extra scalar per row and per split: the log-sum-exp of the attention values.&lt;/li&gt;
  &lt;li&gt;Finally, we compute the actual output by reducing over all the splits, using the log-sum-exp to scale the contribution of each split.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All of this is possible because the attention/softmax can be calculated iteratively. In Flash-Decoding, it is used at 2 levels: within splits (like FlashAttention), and across splits to perform the final reduction.&lt;/p&gt;

&lt;p&gt;In practice, step (1) does not involve any GPU operation, as the key/value chunks are views of the full key/value tensors. We then have 2 separate kernels to perform respectively (2) and (3).&lt;/p&gt;

&lt;h2 id=&quot;benchmarks-on-codellama-34b&quot;&gt;Benchmarks on CodeLlama 34B&lt;/h2&gt;

&lt;p&gt;To validate this approach, we benchmark the decoding throughput of the CodeLLaMa-34b. This model has the same architecture as Llama 2, and more generally results should generalize across many LLMs. We measure the decoding speed in tok/s at various sequence lengths, from 512 to 64k, and compare multiple ways of calculating the attention:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pytorch: Running the attention using pure PyTorch primitives (without using FlashAttention)&lt;/li&gt;
  &lt;li&gt;FlashAttention v2&lt;/li&gt;
  &lt;li&gt;FasterTransformer: Uses the FasterTransformer attention kernel&lt;/li&gt;
  &lt;li&gt;Flash-Decoding&lt;/li&gt;
  &lt;li&gt;And an upper bound calculated as the time it takes to read from memory the entire model along with the KV-cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Flash-Decoding unlocks up to 8x speedups in decoding speed for very large sequences, and scales much better than alternative approaches.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/decoding_codellama34b.png&quot; alt=&quot;CodeLlama&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All approaches perform similarly for small prompts, but scale poorly as the sequence length increases from 512 to 64k, except Flash-Decoding. In this regime (batch size 1) with Flash-Decoding, scaling the sequence length has little impact on generation speed&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;component-level-micro-benchmarks&quot;&gt;Component-level micro-benchmarks&lt;/h2&gt;

&lt;p&gt;We also micro-benchmark the scaled multi-head attention for various sequence lengths and batch sizes on A100 with inputs in f16. We set the batch size to 1, and use 16 query heads of dimension 128, for 2 key/value heads (grouped-query attention), which matches the dimensions used in CodeLLaMa-34b when running on 4 GPUs.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Setting \ Algorithm&lt;/td&gt;
      &lt;td&gt;PyTorch Eager&lt;/td&gt;
      &lt;td&gt;Flash-Attention v2.0.9&lt;/td&gt;
      &lt;td&gt;Flash-Decoding&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=256, seqlen=256&lt;/td&gt;
      &lt;td&gt;3058.6&lt;/td&gt;
      &lt;td&gt;390.5&lt;/td&gt;
      &lt;td&gt;63.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=128, seqlen=512&lt;/td&gt;
      &lt;td&gt;3151.4&lt;/td&gt;
      &lt;td&gt;366.3&lt;/td&gt;
      &lt;td&gt;67.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=64, seqlen=1024&lt;/td&gt;
      &lt;td&gt;3160.4&lt;/td&gt;
      &lt;td&gt;364.8&lt;/td&gt;
      &lt;td&gt;77.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=32, seqlen=2048&lt;/td&gt;
      &lt;td&gt;3158.3&lt;/td&gt;
      &lt;td&gt;352&lt;/td&gt;
      &lt;td&gt;58.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=16, seqlen=4096&lt;/td&gt;
      &lt;td&gt;3157&lt;/td&gt;
      &lt;td&gt;401.7&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=8, seqlen=8192&lt;/td&gt;
      &lt;td&gt;3173.1&lt;/td&gt;
      &lt;td&gt;529.2&lt;/td&gt;
      &lt;td&gt;56.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=4, seqlen=16384&lt;/td&gt;
      &lt;td&gt;3223&lt;/td&gt;
      &lt;td&gt;582.7&lt;/td&gt;
      &lt;td&gt;58.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=2, seqlen=32768&lt;/td&gt;
      &lt;td&gt;3224.1&lt;/td&gt;
      &lt;td&gt;1156.1&lt;/td&gt;
      &lt;td&gt;60.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=1, seqlen=65536&lt;/td&gt;
      &lt;td&gt;1335.6&lt;/td&gt;
      &lt;td&gt;2300.6&lt;/td&gt;
      &lt;td&gt;64.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=1, seqlen=131072&lt;/td&gt;
      &lt;td&gt;2664&lt;/td&gt;
      &lt;td&gt;4592.2&lt;/td&gt;
      &lt;td&gt;106.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Micro-benchmark of the multi-head attention, run-time in us. Flash-Decoding achieves almost constant run-time as the sequence length scales to up to 64k.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The up to 8x speedup end-to-end measured earlier is made possible because the attention itself is up to 50x faster than FlashAttention. Up until sequence length 32k, the attention time is roughly constant, because Flash-Decoding manages to fully utilize the GPU.&lt;/p&gt;

&lt;h2 id=&quot;using-flash-decoding&quot;&gt;Using Flash-Decoding&lt;/h2&gt;

&lt;p&gt;Flash-decoding is available:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention/tree/main&quot;&gt;FlashAttention&lt;/a&gt; package, starting at version 2.2&lt;/li&gt;
  &lt;li&gt;Through &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; starting at version 0.0.22 through `xformers.ops.memory_efficient_attention`. The dispatcher will automatically use either the Flash-Decoding or FlashAttention approaches depending on the problem size. When these approaches are not supported, it can dispatch to an efficient triton kernel that implements the Flash-Decoding algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A full example of decoding with LLaMa v2 / CodeLLaMa is available in the FlashAttention repo &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention/tree/main/examples/inference&quot;&gt;here&lt;/a&gt; and in the xFormers &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;repo&lt;/a&gt; here. We also provide a &lt;a href=&quot;https://github.com/facebookresearch/xformers/tree/main/examples/llama_inference&quot;&gt;minimal example&lt;/a&gt; of an efficient decoding code for LLaMa v1/v2 models, meant to be fast, easy to read, educational and hackable.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Thanks to Erich Elsen, Ashish Vaswani, and Michaël Benesty for suggesting this idea of splitting the KVcache loading. We want to thank Jeremy Reizenstein, Patrick Labatut and Andrew Tulloch for the valuable discussions. We also want to thank Geeta Chauhan and Gregory Chanan for helping with the writing and more broadly contributing to getting this published on the PyTorch blog.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Tri Dao, Daniel Haziza, Francisco Massa, Grigory Sizov</name>
        
        
      </author>

      

      

      
        <summary type="html">Motivation</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ML Model Server Resource Saving - Transition From High-Cost GPUs to Intel CPUs and oneAPI powered Software with performance</title>
      <link href="https://pytorch.org/blog/ml-model-server-resource-saving/" rel="alternate" type="text/html" title="ML Model Server Resource Saving - Transition From High-Cost GPUs to Intel CPUs and oneAPI powered Software with performance" />
      <published>2023-10-11T00:00:00-07:00</published>
      <updated>2023-10-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/ml-model-server-resource-saving</id>
      <content type="html" xml:base="https://pytorch.org/blog/ml-model-server-resource-saving/">&lt;p&gt;Reviewers: &lt;a href=&quot;https://www.linkedin.com/in/yunsang-ju/&quot;&gt;Yunsang Ju&lt;/a&gt;(Naver GplaceAI Leader), Min Jean Cho(Intel), Jing Xu(Intel), Mark Saroufim(Meta)&lt;/p&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;Here, We will be sharing our experience in moving AI workloads from our GPU servers to our Intel CPU servers without any performance or quality degradation, and &lt;strong&gt;saving annual costs of approximately 340 thousand U.S. Dollar&lt;/strong&gt; (refer to the &lt;strong&gt;Conclusion&lt;/strong&gt;) in the process.&lt;/p&gt;

&lt;p&gt;We aim to provide value to our consumers by serving various AI models that enhance the Online to Offline (O2O) experience. With the ongoing growth in the demand for new models and the limited nature of high-cost resource GPUs, we needed to transition relatively lightweight AI models from GPU servers to Intel CPU servers for reducing resource consumption. In the same setting, however, the CPU server had issues where performance of rps, inference time, etc. was reduced by tens of times. We applied various engineering techniques and lightweighted the model to solve this problem, and we were able to successfully transition to the Intel CPU servers with the same performance or better performance as the GPU servers with just a three-fold scale out.&lt;/p&gt;

&lt;p&gt;For a more detailed introduction about our team, please refer to the &lt;a href=&quot;https://medium.com/naver-place-dev/introduction-to-naver-place-ai-development-team-a8b0630e3b23&quot;&gt;Introduction to NAVER Place AI Development Team&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ll mention it again in the middle, but I’ve received a lot of help from &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;Grokking Pytorch Intel CPU Performance From First Principles&lt;/a&gt; written by Intel and PyTorch in the overall work.&lt;/p&gt;

&lt;h2 id=&quot;problem-definition&quot;&gt;Problem Definition&lt;/h2&gt;

&lt;h3 id=&quot;1-service-architecture&quot;&gt;1: Service Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg1.jpg&quot; alt=&quot;Simplified service architecture&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simplified service architecture (Image Source: NAVER GplaceAI)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To facilitate understanding, a brief introduction to our service architecture will be provided. CPU intensive tasks such as preprocessing input to tensor format (then forwarded to the model) and post processing inference results to human readable output (e.g. natural language and image formats) are performed on the App Server(FastAPI) The Model Server(TorchServe) exclusively handles inference operations. For stable operation of the service, the following actions need to be performed with sufficient throughput and low latency.&lt;/p&gt;

&lt;p&gt;The specific processing sequence is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The client submits a request to the app server via the Traefik gateway.&lt;/li&gt;
  &lt;li&gt;The app server pre-processes the input by performing actions such as resizing and transforming, and converting it into a Torch tensor before then requesting the model server.&lt;/li&gt;
  &lt;li&gt;The model server performs inference and returns the feature to the app server&lt;/li&gt;
  &lt;li&gt;The app server converts the feature into a format understandable by humans through post-processing and returns it to the client&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-throughput-and-latency-measurement&quot;&gt;2:  Throughput and Latency Measurement&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg1-1.jpg&quot; alt=&quot;Comparison of Image Scoring Models&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparison of Image Scoring Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With all other conditions remaining the same, deploying on a threefold increase CPU server pod, yet, notably, the RPS (requests per second) and response time deteriorated by more than tenfold. While it was not surprising that CPU inference performance is inferior to GPUs, the challenging situation was evident. Given the goal of maintaining performance within limited resources, achieving an approximate &lt;strong&gt;10 to 20 times performance improvement&lt;/strong&gt; was necessary Barring any additional scaling.&lt;/p&gt;

&lt;h3 id=&quot;3-challenges-from-a-throughput-perspective&quot;&gt;3: Challenges From a Throughput Perspective&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                        37     0(0.00%) |   9031    4043   28985   8200 |    1.00        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                        37     0(0.00%) |   9031    4043   28985   8200 |    1.00        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One of the first steps TorchServer framework users might take in order to improve throughput is to increase the number of workers in TorchServe. This approach is effective on GPU servers Because of parallel workload processing, excluding the linear memory usage increase as workers scale. However, we were experiencing worse performance when increasing the number of workers. Identifying the cause of performance degradation on CPU servers required further investigation.&lt;/p&gt;

&lt;h3 id=&quot;4challenges-from-a-latency-perspective&quot;&gt;4: Challenges From a Latency Perspective&lt;/h3&gt;

&lt;p&gt;Our primary concern was latency. Throughput improvement is normally achievable when a system’s implementation is faithful to scale-out principles, except for perhaps very rare worst-case scenarios. However, in the case of the Image Scoring model example, even performing a single inference took more than 1 second, and as the request volume increased, latency increased to as much as 4 seconds. It was a situation where the timeout criteria to satisfy the client could not be met even with a single inference.&lt;/p&gt;

&lt;h2 id=&quot;proposed-solutions&quot;&gt;Proposed Solutions&lt;/h2&gt;

&lt;p&gt;Improvements were needed from both an ML and an engineering perspective. It was essential to fundamentally reduce the inference time on the CPU and to identify the causes of performance degradation when applying config that generally enhances performance, in order to find the optimal configuration values. To accomplish this, collaboration was established with MLE professionals to concurrently execute tasks encompassing ‘model lightweighting without compromising performance’, and ‘Identify optimal configurations for achieving peak performance’. Using the aforementioned approaches we were able to effectively transition workload handling to our CPU servers.&lt;/p&gt;

&lt;h3 id=&quot;1-resolving-low-rps-from-an-engineering-perspective&quot;&gt;1: Resolving Low RPS from an Engineering Perspective&lt;/h3&gt;

&lt;p&gt;First, the reason for performance degradation even after increasing the worker number was the front-end bound caused by logical threads in GEMM operations. Generally, when increasing the number of workers, the expected improvement effect is the increase in parallelism. Conversely, if performance decreases, one can infer the corresponding trade-off effect.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg2.jpg&quot; alt=&quot;CPU + GPU&quot; style=&quot;width:100%; max-width: 420px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image Source: &lt;a href=&quot;https://blogs.nvidia.com/blog/2018/06/11/what-is-a-virtual-gpu/&quot;&gt;Nvidia&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As many are aware, the reason model inference performance on CPUs is inferior to GPUs lies in the difference in hardware design, particularly in terms of multi-threading capabilities. Diving deeper, model inference is fundamentally a repetition of &lt;strong&gt;GEMM (General Matrix Multiply)&lt;/strong&gt; operations, and these GEMM operations are executed independently in &lt;strong&gt;“fused-multiply-add” (FMA)&lt;/strong&gt; or &lt;strong&gt;“dot-product” (DP)&lt;/strong&gt; execution units. If the GEMM operation becomes a bottleneck on the CPU, increasing parallelism might actually result in decreased performance. While researching the problem we found relevant information within the &lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/advanced/cpu_affinity.html#binding-processes-to-physical-cores&quot;&gt;PyTorch documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;While two logical threads run GEMM at the same time, they will be sharing the same core resources causing front-end bound&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This information highlighted that logical threads could cause a bottleneck in CPU GEMM operations, which helped us intuitively understand why performance decreased when increasing the worker num. This is because the default value of the torch thread corresponds to the physical core value of the CPU.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@test-pod:/# lscpu
  …
Thread(s) per core: 2
Core(s) per socket: 12
  …
root@test-pod:/# python
&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; print(torch.get_num_threads())
24
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the worker_num increases, the total thread count increases by the product of the physical core * worker number. Consequently, logical threads are utilized. In order to improve performance, the total number of threads per worker was adjusted to align with the physical core count. Below, it can be observed that the metric RPS &lt;strong&gt;increased approximately threefold&lt;/strong&gt; to 6.3(from the previous value of 2.1) when the worker_num was increased to 4 and the total thread count was aligned with the number of physical cores.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                       265     0(0.00%) |   3154    1885    4008   3200 |    6.30        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       265     0(0.00%) |   3154    1885    4008   3200 |    6.30        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 1&lt;/strong&gt;: Our team is Using Kubernetes to maintain our deployments. So we are adjusting the which required us to adjust according to the CPU resource limit of the pod, rather than the physical core count of the node that can be checked using the lscpu command. (Setting the torch thread of each worker to 8/4 = 2, or 24/4 = 6 resulted in performance degradation.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 2&lt;/strong&gt;: Since torch thread settings for each worker &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.set_num_threads.html&quot;&gt;can only be configured as integers&lt;/a&gt;, it’s advisable to set the CPU limit divisible by the worker_num in order to adequately utilize CPU usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg3.jpg&quot; alt=&quot;example&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ex) core=8, In the case of worker_num=3: int(8/worker_num) = 2, 2*worker_num/8 = 75%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg4.jpg&quot; alt=&quot;example&quot; style=&quot;width:100%; margin-top: 30px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ex) core=8, In the case of worker_num=4: int(8/worker_num) = 2, 2*worker_num/8 = 100%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We also analyzed the model containers to see why we got a mere threefold improvement in performance despite a four times increase in the number of workers. Various resources were monitored, and among them, the core utilization rate was identified as the underlying cause.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg5.jpg&quot; alt=&quot;threads&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even when the total thread count was adjusted to match the CPU(2nd Generation, Intel(R) Xeon(R) Silver 4214) limit(8 core), there were instances where computations were executed from logical thread to logical core. Due to the presence of 24 physical cores, the cores numbered 25 to 48 are classified as logical cores. The possibility of confining thread execution solely within physical cores seemed to offer the potential for further performance enhancement. The reference to this solution could be found within the source document mentioned in the PyTorch-geometric article that warned about CPU GEMM bottlenecks.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reference Documentation: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;Grokking Pytorch Intel CPU Performance From First Principles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As per the instructions in the document, Intel provides Intel® Extension for PyTorch where we can simply pin cores to specific sockets. The application method is also made very simple, by adding the following settings to the &lt;strong&gt;torchserve config.properties&lt;/strong&gt; file.(used intel_extension_for_pytorch==1.13.0)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ipex_enable=true
CPU_launcher_enable=true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg6.jpg&quot; alt=&quot;two-socket configuration&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image Source: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Beyond the removal of logical threads through socket pinning, there is an additional effect of eliminating UPI cache hit overhead. Since the CPU comprises more than one socket when threads scheduled on socket 1 are rescheduled on socket 2, cache hits occur in cases of accessing the cache of socket 1 via Intel Ultra Path Interconnect (UPI). At this point, UPI access to the local cache becomes more than twice as slow as local cache access, resulting in more bottlenecks. With threads being pinned to socket units by oneAPI powered Intel® Extension for PyTorch, We observed rps handling increase of up to &lt;strong&gt;four times than when the bottleneck existed&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                       131     0(0.00%) |   3456    1412    6813   3100 |    7.90        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       131     0(0.00%) |   3456    1412    6813   3100 |    7.90        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 1&lt;/strong&gt;: Intel® Extension for PyTorch is specialized in neural network (referred to as “nn” hereafter) inference optimization, so the performance improvement from additional techniques outside nn might be minimal. Indeed, in the instance of the image scoring system highlighted as an example, where svr (support vector regression) is applied post-inference, the performance enhancement was confined to a 4-fold increase. However, for a purely nn inference model such as the food recognition model, &lt;strong&gt;a&lt;/strong&gt; &lt;strong&gt;performance boost of 7-fold (2.5rps -&amp;gt; 17.5rps)&lt;/strong&gt; was detected.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/food-classification                                                 446     0(0.00%) |   1113     249    1804   1200 |   17.50        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       446     0(0.00%) |   1113     249    1804   1200 |   17.50        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 2&lt;/strong&gt;: Applying Intel® Extension for PyTorch requires &lt;strong&gt;torchserve version 0.6.1 or higher&lt;/strong&gt;. Since our team was using version 0.6.0, there was an issue where socket pinning was not functioning correctly. Currently, we have made modifications to the guide document, specifying the required version.&lt;/p&gt;

&lt;p&gt;Within &lt;a href=&quot;https://github.com/pytorch/serve/blob/4236a86dc0a018198ecd3fe261e835b416df739e/frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java&quot;&gt;WorkerLifeCycle.java&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;,&lt;/span&gt; multi-worker pinning is not supported in 0.6.0 and below (ninstance is hardcoded to 1)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// 0.6.0 version

public ArrayList&amp;lt;String&amp;gt; launcherArgsToList() {
   ArrayList&amp;lt;String&amp;gt; arrlist = new ArrayList&amp;lt;String&amp;gt;();
   arrlist.add(&quot;-m&quot;);
   arrlist.add(&quot;intel_extension_for_pytorch.cpu.launch&quot;);
   arrlist.add(&quot; — ninstance&quot;);
   arrlist.add(&quot;1&quot;);
   if (launcherArgs != null &amp;amp;&amp;amp; launcherArgs.length() &amp;gt; 1) {
     String[] argarray = launcherArgs.split(&quot; &quot;);
     for (int i = 0; i &amp;lt; argarray.length; i++) {
       arrlist.add(argarray[i]);
     }
   }
   return arrlist;
 }
// master version

if (this.numWorker &amp;gt; 1) {
   argl.add(&quot; — ninstances&quot;);
   argl.add(String.valueOf(this.numWorker));
   argl.add(&quot; — instance_idx&quot;);
   argl.add(String.valueOf(this.currNumRunningWorkers));
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-addressing-slow-latency-through-model-lightweighting&quot;&gt;2: Addressing Slow Latency Through Model Lightweighting&lt;/h3&gt;

&lt;p&gt;We also streamlined our model using &lt;strong&gt;Knowledge Distillation&lt;/strong&gt; (commonly abbreviated as KD) to further reduce latency. As is widely known, kd is a technique where knowledge from a larger network (Teacher network) is conveyed to a smaller, lightweight network (Student network) which is less resource intensive and can be more readily deployed. For more detailed information, please refer to the paper where this concept was initially introduced, titled &lt;span style=&quot;text-decoration:underline;&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg7.jpg&quot; alt=&quot;neural networks&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a variety of KD techniques available and because we were primarily focused on &lt;strong&gt;accuracy loss minimization&lt;/strong&gt;, we adopted the approach from the paper &lt;a href=&quot;https://arxiv.org/pdf/2205.10536.pdf&quot;&gt;Knowledge Distillation from A Stronger Teacher&lt;/a&gt;, which was published in the year 2022. The concept is straightforward. Unlike the conventional method of distillation that utilizes only the model’s prop values, the chosen approach involves having the student network learn the correlations between classes in the teacher network. When put into actual application, We observed effective model weight reduction to observe the effective reduction in the model’s weight while mainting high accuracy. The following are the outcomes of our experimentation with the mentioned knowledge distillation technique on several candidate student models, where selections were made based on the maintained level of accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg8.jpg&quot; alt=&quot;table of services&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the image scoring system, additional measures were taken to reduce the input size. Considering that the prior use of CPU-based ML technique SVR (Support Vector Regression) was used (2-stage: CNN + SVR), even when this was streamlined into a 1-stage model, significant speed advantages were not observed in CPU inference. In order for streamlining to have significance, the input size of the student model during inference needed further reduction. Consequently, experiments were conducted with the size reduced from 384&lt;em&gt;384 to 224&lt;/em&gt;224.&lt;/p&gt;

&lt;p&gt;Further simplifying transformations, the 2-stage (CNN + SVR) approach was unified into a 1-stage model with a larger ConvNext, and then kd was applied using the lightweight EfficientNet to resolve the accuracy trade-off. During the experiments, we encountered a problem where changing Img_resize to 224 led to a performance drop from 0.4007 to 0.4296 in terms of MAE. Due to the reduction in input size, various preprocessing techniques applied to the original training images (such as Affine, RandomRotate90, Blur, OneOf [GridDistortion, OpticalDistortion, ElasticTransform], VerticalFlip) had a counterproductive effect. By adopting these measures, effective training of the student was achieved, and the &lt;strong&gt;MAE value improved by 25% compared to the previous one (.518 to .3876)&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;validation&quot;&gt;Validation&lt;/h2&gt;

&lt;h3 id=&quot;1-final-performance-measurement&quot;&gt;1: Final Performance Measurement&lt;/h3&gt;

&lt;p&gt;The following shows the final performance improvements using CPU servers, on the three models mentioned throughout this article.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Food photo classifier (pod 3): 2.5rps -&amp;gt; 84 rps

 Type Name                                                                           # reqs # fails | Avg Min Max Med | req/s failures/s
 --------|----------------------------------------------------------------------------|------|------------|-------|------|-------|-------|--------|--------- 
POST /predictions/food-classification 2341 0(0.00%) | 208 130 508 200 | 84.50 0.00 
--------|----------------------------------------------------------------------------|--------|-------------|------|-------|--------|------|--------|----------
         Aggregated                                                                      2341     0(0.00%) |    208     130     508    200 |   84.50        0.00

# Image scoring (pod 3): 2.1rps -&amp;gt; 62rps
 Type Name                                                                               #reqs #fails | Avg Min Max Median | req/s failures/s
 --------|---------------------------------------------------------------------------------|--------|-------------|--------|-------|--------|---------|--------|--------- 
  POST /predictions/image-scoring 1298 0 (0.00%) | 323 99 607 370 | 61.90 0.00 
--------|---------------------------------------------------------------------------------|--------|-------------|--------|------|--------|---------|--------|----------
          Aggregated                                                                          1298     0(0.00%)  |     323      99     607     370  |   61.90        0.00

# receipt classifier(pod 3) : 20rps -&amp;gt; 111.8rps
Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/receipt-classification                                             4024     0(0.00%) |    266     133    2211    200 |   111.8        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                      4020     0(0.00%) |    266     133    2211    200 |   111.8        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-traffic-mirroring&quot;&gt;2:  Traffic Mirroring&lt;/h3&gt;

&lt;p&gt;As previously mentioned, our team’s service architecture employs the tool “traefik” as a gateway in front of the app server, as briefly introduced at the beginning of the article. For final validation, the mirroring feature of this traefik gateway was utilized to mirror traffic from production to staging for a month of validation before applying it to production, which is now operational.&lt;/p&gt;

&lt;p&gt;Details regarding mirroring are beyond the scope of this topic and hence omitted. For those interested, kindly refer to the document at &lt;a href=&quot;https://doc.traefik.io/traefik/routing/services/#mirroring-service&quot;&gt;https://doc.traefik.io/traefik/routing/services/#mirroring-service&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;This concludes the discussion about transitioning from a GPU model server to a CPU server while maintaining service quality. Through this effort, our team &lt;strong&gt;was able to save 15 GPUs each in South Korea and Japan&lt;/strong&gt;, resulting in an &lt;strong&gt;annual cost savings of approximately 340 thousand U.S. Dollar&lt;/strong&gt;. Although we directly purchase and use GPUs within NAVER, we calculated a rough cost reduction &lt;a href=&quot;https://aws.amazon.com/ko/ec2/instance-types/g4/&quot;&gt;based on AWS EC2 instances&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt; &lt;/span&gt;that stably support T4 GPUs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg9.jpg&quot; alt=&quot;instance sizes&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Calculation: 1.306 (1-year reserved instance effective hourly cost) * 24 (hours) * 365 (days) * 15 (number of GPUs) * 2 (KR + JP)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These secured GPUs will be harnessed to further advance and enhance our team’s AI services, delivering exceptional service experiences. We sincerely appreciate your encouragement and anticipation.:)&lt;/p&gt;

&lt;h2 id=&quot;explore-more&quot;&gt;Explore More&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/ecosystem/pytorch-foundation.html&quot;&gt;https://www.intel.com/content/www/us/en/developer/ecosystem/pytorch-foundation.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/advanced/cpu_affinity.html#binding-processes-to-physical-cores&quot;&gt;https://pytorch-geometric.readthedocs.io/en/latest/advanced/CPU_affinity.html#binding-processes-to-physical-cores&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.10536.pdf&quot;&gt;https://arxiv.org/pdf/2205.10536.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Sangjune Park(Naver GplaceAI MLOps), Jooyoung Lee(Naver GplaceAI MLE), Junho Min(Naver GplaceAI MLE)</name>
        
        
      </author>

      

      

      
        <summary type="html">Reviewers: Yunsang Ju(Naver GplaceAI Leader), Min Jean Cho(Intel), Jing Xu(Intel), Mark Saroufim(Meta)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Real-time Audio-visual Speech Recognition</title>
      <link href="https://pytorch.org/blog/real-time-speech-rec/" rel="alternate" type="text/html" title="Real-time Audio-visual Speech Recognition" />
      <published>2023-10-10T00:00:00-07:00</published>
      <updated>2023-10-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/real-time-speech-rec</id>
      <content type="html" xml:base="https://pytorch.org/blog/real-time-speech-rec/">&lt;p&gt;Audio-Visual Speech Recognition (AV-ASR, or AVSR) is the task of transcribing text from audio and visual streams, which has recently attracted a lot of research attention due to its robustness to noise. The vast majority of work to date has focused on developing AV-ASR models for non-streaming recognition; studies on streaming AV-ASR are very limited.&lt;/p&gt;

&lt;p&gt;We have developed a compact real-time speech recognition system based on TorchAudio, a library for audio and signal processing with &lt;a href=&quot;http://pytorch.org&quot;&gt;PyTorch&lt;/a&gt;. It can run locally on a laptop with high accuracy without accessing the cloud. Today, we are releasing &lt;a href=&quot;https://github.com/pytorch/audio/tree/main/examples/avsr&quot;&gt;the real-time AV-ASR recipe&lt;/a&gt; under a permissive open license (BSD-2-Clause license), enabling a broad set of applications and fostering further research on audio-visual models for speech recognition.&lt;/p&gt;

&lt;p&gt;This work is part of our approach to &lt;a href=&quot;https://arxiv.org/abs/2303.14307&quot;&gt;AV-ASR research&lt;/a&gt;. A promising aspect of this approach is its ability to automatically annotate large-scale audio-visual datasets, which enables the training of more accurate and robust speech recognition systems. Furthermore, this technology has the potential to run on smart devices since it achieves the latency and memory efficiency that such devices require for inference.&lt;/p&gt;

&lt;p&gt;In the future, speech recognition systems are expected to power applications in numerous domains. One of the primary applications of AV-ASR is to enhance the performance of ASR in noisy environments. Since visual streams are not affected by acoustic noise, integrating them into an audio-visual speech recognition model can compensate for the performance drop of ASR models. Our AV-ASR system has the potential to serve multiple purposes beyond speech recognition, such as text summarization, translation and even text-to-speech conversion. Moreover, the exclusive use of VSR can be useful in certain scenarios, e.g. where speaking is not allowed, in meetings, and where privacy in public conversations is desired.&lt;/p&gt;

&lt;h1 id=&quot;av-asr&quot;&gt;AV-ASR&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/real-time-speech-rec/pipeline.jpg&quot; alt=&quot;Fig. 1 The pipeline for audio-visual speech recognition system&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: The pipeline for audio-visual speech recognition system&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Our real-time AV-ASR system is presented in Fig. 1. It consists of three components, a data collection module, a pre-processing module and an end-to-end model. The data collection module comprises hardware devices, such as a microphone and camera. Its role is to collect information from the real world. Once the information is collected, the pre-processing module location and crop out face. Next, we feed the raw audio stream and the pre-processed video stream into our end-to-end model for inference.&lt;/p&gt;

&lt;h2 id=&quot;data-collection&quot;&gt;Data collection&lt;/h2&gt;

&lt;p&gt;We use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.io.StreamReader&lt;/code&gt; to capture audio/video from streaming device input, e.g. microphone and camera on laptop. Once the raw video and audio streams are collected, the pre-processing module locates and crops faces. It should be noted that data is immediately deleted during the streaming process.&lt;/p&gt;

&lt;h2 id=&quot;pre-processing&quot;&gt;Pre-processing&lt;/h2&gt;

&lt;p&gt;Before feeding the raw stream into our model, each video sequence has to undergo a specific pre-processing procedure. This involves three critical steps. The first step is to perform face detection. Following that, each individual frame is aligned to a referenced frame, commonly known as the mean face, in order to normalize rotation and size differences across frames. The final step in the pre-processing module is to crop the face region from the aligned face image. We would like to clearly note that our model is fed with raw audio waveforms and pixels of the face, without any further preprocessing like face parsing or landmark detection. An example of the pre-processing procedure is illustrated in Table 1.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/original.gif&quot; alt=&quot;Original image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img src=&quot;/assets/images/real-time-speech-rec/detected.gif&quot; alt=&quot;Detected image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/transformed.gif&quot; alt=&quot;Transformed image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/cropped.gif&quot; alt=&quot;Cropped image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
    0. Original
   &lt;/td&gt;
   &lt;td&gt;
1. Detection
   &lt;/td&gt;
   &lt;td&gt;
2. Alignment
   &lt;/td&gt;
   &lt;td&gt;
3. Crop
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: Preprocessing pipeline.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/real-time-speech-rec/model.jpg&quot; alt=&quot;Fig. 2 The architecture for the audio-visual speech recognition system.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: The architecture for the audio-visual speech recognition system&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;We consider two configurations: Small with 12 Emformer blocks and Large with 28, with 34.9M and 383.3M parameters, respectively. Each AV-ASR model composes front-end encoders, a fusion module, an Emformer encoder, and a transducer model. To be specific, we use convolutional frontends to extract features from raw audio waveforms and facial images. The features are concatenated to form 1024-d features, which are then passed through a two-layer multi-layer perceptron and an Emformer transducer model. The entire network is trained using RNN-T loss. The architecture of the proposed AV-ASR model is illustrated in Fig. 2.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Datasets.&lt;/strong&gt; We follow &lt;a href=&quot;https://arxiv.org/abs/2303.14307&quot;&gt;Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels&lt;/a&gt; to use publicly available audio-visual datasets including &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/lip_reading/&quot;&gt;LRS3&lt;/a&gt;, &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html&quot;&gt;VoxCeleb2&lt;/a&gt; and &lt;a href=&quot;https://looking-to-listen.github.io/avspeech/&quot;&gt;AVSpeech&lt;/a&gt; for training. We do not use mouth ROIs or facial landmarks or attributes during both training and testing stages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparisons with the state-of-the-art.&lt;/strong&gt; Non-streaming evaluation results on LRS3 are presented in Table 2. Our audio-visual model with an algorithmic latency of 800 ms (160ms+1280msx0.5) yields a WER of 1.3%, which is on par with those achieved by state-of-the-art offline models such as AV-HuBERT, RAVEn, and Auto-AVSR.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Method&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Total Hours&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;WER (%)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;ViT3D-CM
   &lt;/td&gt;
   &lt;td&gt;90, 000
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AV-HuBERT
   &lt;/td&gt;
   &lt;td&gt;1, 759
   &lt;/td&gt;
   &lt;td&gt;1.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;RAVEn
   &lt;/td&gt;
   &lt;td&gt;1, 759
   &lt;/td&gt;
   &lt;td&gt;1.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AutoAVSR
   &lt;/td&gt;
   &lt;td&gt;3, 448
   &lt;/td&gt;
   &lt;td&gt;0.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Ours
   &lt;/td&gt;
   &lt;td&gt;3, 068
   &lt;/td&gt;
   &lt;td&gt;1.3
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 2&lt;/strong&gt;: Non-streaming evaluation results for audio-visual models on the LRS3 dataset.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Noisy experiments.&lt;/strong&gt; During training, 16 different noise types are randomly injected to audio waveforms, including 13 types from &lt;a href=&quot;https://zenodo.org/record/1227121&quot;&gt;Demand&lt;/a&gt; database, ‘DLIVING’,’DKITCHEN’, ‘OMEETING’, ‘OOFFICE’, ‘PCAFETER’, ‘PRESTO’, ‘PSTATION’, ‘STRAFFIC’,  ‘SPSQUARE’, ‘SCAFE’, ‘TMETRO’, ‘TBUS’ and ‘TCAR’, two more types of noise from &lt;a href=&quot;https://arxiv.org/abs/1804.03209&quot;&gt;speech commands&lt;/a&gt; database, white and pink and one more type of noise from &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/0167639393900953&quot;&gt;NOISEX-92&lt;/a&gt; database, babble noise. SNR levels in the range of [clean, 7.5dB, 2.5dB, -2.5dB, -7.5dB] are selected from with a uniform distribution. Results of ASR and AV-ASR models, when tested with babble noise, are shown in Table 3. With increasing noise level, the performance advantage of our audio-visual model over our audio-only model grows, indicating that incorporating visual data improves noise robustness.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Type&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;∞&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;10dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;5dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;-5dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;-10dB&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;A
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;1.8
   &lt;/td&gt;
   &lt;td&gt;3.2
   &lt;/td&gt;
   &lt;td&gt;10.9
   &lt;/td&gt;
   &lt;td&gt;27.9
   &lt;/td&gt;
   &lt;td&gt;55.5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;A+V
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;1.7
   &lt;/td&gt;
   &lt;td&gt;2.1
   &lt;/td&gt;
   &lt;td&gt;6.2
   &lt;/td&gt;
   &lt;td&gt;11.7
   &lt;/td&gt;
   &lt;td&gt;27.6
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 3&lt;/strong&gt;: Streaming evaluation WER (%) results at various signal-to-noise ratios for our audio-only (A) and audio-visual (A+V) models on the LRS3 dataset under 0.80-second latency constraints.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-time factor&lt;/strong&gt;. The real-time factor (RTF) is an important measure of a system’s ability to process real-time tasks efficiently. An RTF value of less than 1 indicates that the system meets real-time requirements. We measure RTF using a laptop with an Intel® Core™ i7-12700 CPU running at 2.70 GHz and an NVIDIA 3070 GeForce RTX 3070 Ti GPU. To the best of our knowledge, this is the first AV-ASR model that reports RTFs on the LRS3 benchmark. The Small model achieves a WER of 2.6% and an RTF of 0.87 on CPU (Table 4), demonstrating its potential for real-time on-device inference applications.&lt;/p&gt;

&lt;table class=&quot;table table-bordered text-center&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Device&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Streaming WER [%]&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;RTF&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Large
   &lt;/td&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;0.35
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Small
   &lt;/td&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;2.6
   &lt;/td&gt;
   &lt;td&gt;0.33
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPU
   &lt;/td&gt;
   &lt;td&gt;0.87
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 4&lt;/strong&gt;: Impact of AV-ASR model size and device on WER and RTF. Note that the RTF calculation includes the pre-processing step wherein the Ultra-Lightweight Face Detection Slim 320 model is used to generate face bounding boxes.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Learn more about the system from the published works below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shi, Yangyang, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Mike Seltzer. “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition.” In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6783-6787. IEEE, 2021.&lt;/li&gt;
  &lt;li&gt;Ma, Pingchuan, Alexandros Haliassos, Adriana Fernandez-Lopez, Honglie Chen, Stavros Petridis, and Maja Pantic. “Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels.” In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. IEEE, 2023.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">Audio-Visual Speech Recognition (AV-ASR, or AVSR) is the task of transcribing text from audio and visual streams, which has recently attracted a lot of research attention due to its robustness to noise. The vast majority of work to date has focused on developing AV-ASR models for non-streaming recognition; studies on streaming AV-ASR are very limited.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">High performance Llama 2 deployments with AWS Inferentia2 using TorchServe</title>
      <link href="https://pytorch.org/blog/high-performance-llama/" rel="alternate" type="text/html" title="High performance Llama 2 deployments with AWS Inferentia2 using TorchServe" />
      <published>2023-10-04T00:00:00-07:00</published>
      <updated>2023-10-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/high-performance-llama</id>
      <content type="html" xml:base="https://pytorch.org/blog/high-performance-llama/">&lt;p&gt;Recently, &lt;a href=&quot;https://ai.meta.com/llama/&quot;&gt;Llama 2&lt;/a&gt; was released and has attracted a lot of interest from the machine learning community. &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/inf2/&quot;&gt;Amazon EC2 Inf2 instances&lt;/a&gt;, powered by &lt;a href=&quot;https://aws.amazon.com/machine-learning/inferentia/&quot;&gt;AWS Inferentia2&lt;/a&gt;, now support training and inference of Llama 2 models. In this post, we show low-latency and cost-effective inference of Llama-2 models on Amazon EC2 Inf2 instances using the latest &lt;a href=&quot;https://aws.amazon.com/machine-learning/neuron/&quot;&gt;AWS Neuron SDK&lt;/a&gt; release.  We first introduce how to create, compile and deploy the Llama-2 model and explain the optimization techniques introduced by AWS Neuron SDK to achieve high performance at low cost. We then present our benchmarking results. Lastly, we show how the Llama-2 model can be deployed through Amazon SageMaker using TorchServe on an Inf2 instance. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/software_stack_inf2.jpg&quot; alt=&quot;Llama 2 is an auto-regressive language model that uses an optimized transformer architecture&quot; style=&quot;width:100%; max-width: 420px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-llama-2&quot;&gt;What is Llama 2&lt;/h2&gt;

&lt;p&gt;Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. Llama 2 is intended for commercial and research use in English. It comes in multiple sizes—7 billion, 13 billion, and 70 billion parameters—as well as pre-trained and fine-tuned variations. According to Meta, the tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. Llama 2 was pre-trained on 2 trillion tokens of data from publicly available sources. The tuned models are intended for assistant-like chat, whereas pre-trained models can be adapted for a variety of natural language generation tasks. Regardless of which version of the model a developer uses, the &lt;a href=&quot;https://ai.meta.com/llama/responsible-use-guide/&quot;&gt;responsible use guide from Meta &lt;/a&gt;can assist in guiding additional fine-tuning that may be necessary to customize and optimize the models with appropriate safety mitigations.&lt;/p&gt;

&lt;h2 id=&quot;amazon-ec2-inf2-instances-overview&quot;&gt;Amazon EC2 Inf2 instances Overview&lt;/h2&gt;

&lt;p&gt;Amazon EC2 Inf2 instances, featuring Inferentia2, provide 3x higher compute, 4x more accelerator memory, resulting in up to 4x higher throughput, and up to 10x lower latency, compared to the first generation Inf1 instances.&lt;/p&gt;

&lt;p&gt;Large language model (LLM) inference is a memory bound workload, performance scales up with more accelerator memory bandwidth. Inf2 instances are the only inference optimized instances in Amazon EC2 to provide high speed accelerator interconnect (NeuronLink) enabling high performance large LLM model deployments with cost effective distributed inference. You can now efficiently and cost-effectively deploy billion-scale LLMs across multiple accelerators on Inf2 instances.&lt;/p&gt;

&lt;p&gt;Inferentia2 supports FP32, TF32, BF16, FP16, UINT8, and the new configurable FP8 (cFP8) data type. AWS Neuron can take high-precision FP32 and FP16 models and autocast them to lower-precision data types while optimizing accuracy and performance. Autocasting reduces time to market by removing the need for lower-precision retraining and enabling higher-performance inference with smaller data types.&lt;/p&gt;

&lt;p&gt;To make it flexible and extendable to deploy constantly evolving deep learning models, Inf2 instances have hardware optimizations and software support for dynamic input shapes as well as custom operators written in C++ through the standard PyTorch custom operator programming interfaces.&lt;/p&gt;

&lt;h2 id=&quot;transformers-neuron-transformers-neuronx&quot;&gt;Transformers Neuron (transformers-neuronx)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx&quot;&gt;Transformers Neuron&lt;/a&gt; is a software package that enables PyTorch users to deploy performance optimized LLM inference. It has an optimized version of transformer models implemented with XLA high level operators (HLO), which enables sharding tensors across multiple NeuronCores, a.k.a. tensor parallelism, and performance optimizations such as parallel context encoding and KV caching for Neuron hardware. The Llama 2 source code in XLA HLOs can be found &lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx/blob/main/src/transformers_neuronx/llama/model.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Llama 2 is supported in Transformers Neuron through the &lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx/blob/33fa412447a4028edb252fd06aae9ed93086a450/src/transformers_neuronx/llama/model.py#L29&quot;&gt;LlamaForSampling&lt;/a&gt; class. Transformers Neuron provides a seamless user experience with Hugging Face models to provide optimized inference on Inf2 instances. More details can be found from the &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html#transformers-neuronx-developer-guide&quot;&gt;Transforms Neuron Developer Guide&lt;/a&gt;. In the following section, we will explain how to deploy the Llama-2 13B model using Transformers Neuron. And, this example also applies to other Llama-based models.&lt;/p&gt;

&lt;h2 id=&quot;llama-2-model-inference-with-transformers-neuron&quot;&gt;Llama 2 model inference with Transformers Neuron&lt;/h2&gt;

&lt;h3 id=&quot;create-model-compile-and-deploy&quot;&gt;Create model, compile and deploy&lt;/h3&gt;

&lt;p&gt;We have three simple steps here to create, compile and deploy the model on Inf2 instances.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a CPU model, use this &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/Readme.md?plain=1#L71&quot;&gt;script&lt;/a&gt; or the following code snippet to serialize and save checkpoints in a local directory.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from transformers import AutoModelForCausalLM
from transformers_neuronx.module import save_pretrained_split
model_cpu = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-13b-hf&quot;, low_cpu_mem_usage=True)
model_dir = &quot;./llama-2-13b-split&quot;
save_pretrained_split(model_cpu, model_dir)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Load and compile model from the local directory that you saved serialized checkpoints using the following.
To load the Llama 2 model, we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LlamaForSampling&lt;/code&gt; from Transformers Neuron. Note that the environment variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURON_RT_NUM_CORES&lt;/code&gt; specifies the number of NeuronCores to be used at runtime and it should match the tensor parallelism (TP) degree specified for the model. Also, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURON_CC_FLAGS&lt;/code&gt; enables compiler optimization on decoder-only LLM models.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from transformers_neuronx.llama.model import LlamaForSampling
os.environ['NEURON_RT_NUM_CORES'] = '24'
os.environ['NEURON_CC_FLAGS'] = '--model-type=transformer'
model = LlamaForSampling.from_pretrained(
        model_dir,
        batch_size=1,
        tp_degree=24,
        amp='bf16',
        n_positions=16,
        context_length_estimate=[8]
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p style=&quot;padding-left:6.25rem&quot;&gt;Now let's compile the model and load model weights into device memory with a one liner API.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.to_neuron()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Finally let’s run the inference on the compiled model. Note that both input and output of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sample&lt;/code&gt; function are a sequence of tokens.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;inputs = torch.tensor([[1, 16644, 31844, 312, 31876, 31836, 260, 3067, 2228, 31844]])
seq_len = 16
outputs = model.sample(inputs, seq_len, top_k=1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;inference-optimizations-in-transformers-neuron&quot;&gt;Inference optimizations in Transformers Neuron&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Tensor parallelism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_tp.jpg&quot; alt=&quot;Latency with different TP degrees&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Transformer Neuron implements parallel tensor operations across multiple NeuronCores. We denote the number of cores to be used for inference as TP degree. Larger TP degree provides higher memory bandwidth, leading to lower latency, as LLM token generation is a memory-IO bound workload. With increasing the TP degree, the inference latency has decreased significantly, our results shows, ~4x overall speed up with increased TP degrees from 2 to 24. For the Llama-2 7B model, latency decreases from 30.1 ms/token with 2 cores to 7.9 ms/token with 24 cores; similarly for the Llama-2 13B model, it goes down from 57.3 ms/token  to 11.1 ms/token.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parallel context encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the transformer architecture, tokens are produced in a sequential procedure called autoregressive sampling while input prompt tokens can be processed in parallel with parallel context encoding. This can significantly reduce the latency for input prompt context encoding before token generation through autoregressive sampling. By default, the parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;context_length_estimate&lt;/code&gt; would be set as a list of power-of-2 numbers which aims to cover a wide variety of context lengths. Depending on the use case, it can be set to custom numbers. This can be done when creating the Llama 2 model using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LlamaForSampling.from_pretrained&lt;/code&gt;. We characterize the impact of input token length on end-to-end (E2E) latency. As shown in the figure, latency for text generation with the Llama-2 7B model only slightly increases with bigger input prompts, thanks to parallel context encoding.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_input_token_length.jpg&quot; alt=&quot;E2E latency&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KV caching&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Self-attention block performs the self-attention operation with KV vectors. And, KV vectors are calculated using token embeddings and weights of KV and thus associated with tokens. In naive implementations, for each generated token, the entire KV cache is recalculated, but this reduces performance. Therefore Transformers Neuron library is reusing previously calculated KV vectors to avoid unnecessary computation, also known as KV caching, to reduce latency in the autoregressive sampling phase. &lt;/p&gt;

&lt;h3 id=&quot;benchmarking-results&quot;&gt;Benchmarking results&lt;/h3&gt;

&lt;p&gt;We benchmarked the latency and cost for both Llama-2 7B and 13B models under different conditions, i.e., number of output tokens, instance types. Unless specified, we use data type ‘bf16’ and batch size of 1 as this is a common configuration for real-time applications like chatbot and code assistant.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following graphs shows the per token latency on inf2.48xlarge instance with TP degree 24. Here, the latency per output token is calculated as the end-to-end latency divided by the number of output tokens. Our experiments show Llama-2 7B end-to-end latency to generate 256 tokens is 2x faster compared to other comparable inference-optimized EC2 instances. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_output_token_length.png&quot; alt=&quot;Latency on inf2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We now show the number of tokens generated per second for the Llama-2 7B and 13B models that can be delivered by the inf2.48xlarge instance. With TP degree 24, fully utilizing all the 24 NeuronCores, we can achieve 130 tokens/sec and 90 tokens/sec for the Llama-2 7B and 13B models, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/throughput_vs_output_token_length.jpg&quot; alt=&quot;E2E throughput&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For latency-first applications, we show the cost of hosting Llama-2 models on the inf2.48xlarge instance, &lt;strong&gt;$&lt;/strong&gt;0.011 per 1000 tokens and &lt;strong&gt;$&lt;/strong&gt;0.016 per 1000 tokens for the 7B and 13B models, respectively, which achieve 3x cost saving over other comparable inference-optimized EC2 instances. Note that we report the cost based on &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/inf2/&quot;&gt;3-year reserved instance price&lt;/a&gt; which is what customers use for large production deployments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/cost_vs_output_token_length_7b_13b.jpg&quot; alt=&quot;Cost on inf2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also compare the cost of hosting the Llama-2 7B model on inf2.xlarge and inf2.48xlarge instances. We can see that inf2.xlarge is more than 4x cheaper than inf2.48xlarge but at the expense of longer latency due to smaller TP degree. For example, it takes 7.9 ms for the model to generate 256 output tokens with 256 input tokens on inf2.48xlarge but 30.1 ms on Inf2.xlarge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/cost_vs_output_token_length_xl_48xl.jpg&quot; alt=&quot;Cost on Llama&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;serving-llama2-with-torchserve-on-ec2-inf2-instance&quot;&gt;Serving Llama2 with TorchServe on EC2 Inf2 instance&lt;/h2&gt;

&lt;p&gt;Now, we move on to model deployment. In this section, we show you how to deploy the &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-2-13b-hf&quot;&gt;Llama-2 13B model&lt;/a&gt; through SageMaker using TorchServe, which is the recommended model server for PyTorch, preinstalled in the AWS PyTorch Deep Learning Containers (DLC).&lt;/p&gt;

&lt;p&gt;This section describes the preparation work needed for using TorchServe, particularly, how to configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_config.yaml&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inf2_handler.py&lt;/code&gt; as well as how to generate model artifacts and pre-compile the model for use in later model deployment. Preparing the model artifacts ahead-of-time avoids model compilation during model deployment and thus reduces the model loading time.&lt;/p&gt;

&lt;h3 id=&quot;model-configurationmodel-configyaml&quot;&gt;Model configuration &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/model-config.yaml&quot;&gt;model-config.yaml&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The parameters defined in section &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handler&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;micro_batching&lt;/code&gt; are used in customer handler &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/inf2_handler.py&quot;&gt;inf2_handler.py&lt;/a&gt;. More details about model_config.yaml are &lt;a href=&quot;https://github.com/pytorch/serve/blob/2bf505bae3046b0f7d0900727ec36e611bb5dca3/docs/configuration.md?plain=1#L267&quot;&gt;here&lt;/a&gt;. TorchServe micro-batching is a mechanism to pre-process and post-process a batch of inference requests in parallel. It is able to achieve higher throughput by better utilizing the available accelerator when the backend is steadily fed with incoming data, see &lt;a href=&quot;https://github.com/pytorch/serve/tree/master/examples/micro_batching&quot;&gt;here&lt;/a&gt; for more details. For model inference on Inf2, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;micro_batch_size, amp, tp_degree and max_length&lt;/code&gt; specify the batch size, data type, tensor parallelism degree and max sequence length, respectively.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TorchServe Frontend Parameters
minWorkers: 1
maxWorkers: 1
maxBatchDelay: 100
responseTimeout: 10800
batchSize: 16

# TorchServe Backend Custom Handler Parameters
handler:
    model_checkpoint_dir: &quot;llama-2-13b-split&quot;
    amp: &quot;bf16&quot;
    tp_degree: 12
    max_length: 100

micro_batching:
    # Used by batch_size in function LlamaForSampling.from_pretrained
    micro_batch_size: 1  
    parallelism:
        preprocess: 2
        inference: 1
        postprocess: 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;custom-handlerinf2_handlerpy&quot;&gt;Custom handler &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/inf2_handler.py&quot;&gt;inf2_handler.py&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Custom handler in Torchserve is a simple Python script that lets you define the model initialization, preprocessing, inference and post-processing logic as functions. Here, we create our Inf2 custom handler.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/inf2_handler.py#L33&quot;&gt;initialize&lt;/a&gt; function is used to load the model. Here, Neuron SDK will compile the model for the first time and save the precompiled model in the directory as enabled by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURONX_CACHE&lt;/code&gt; in the directory specified by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURONX_DUMP_TO&lt;/code&gt;. After the first time, subsequent runs will check if there are already pre-compiled model artifacts. If so, it will skip model compilation.
Once the model is loaded, we initiate warm-up inference requests so that the compiled version is cached. When the &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html&quot;&gt;neuron persistent cache &lt;/a&gt;is utilized, it can significantly reduce the model loading latency, ensuring that the subsequent inference runs swiftly.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;os.environ[&quot;NEURONX_CACHE&quot;] = &quot;on&quot;
os.environ[&quot;NEURONX_DUMP_TO&quot;] = f&quot;{model_dir}/neuron_cache&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p style=&quot;padding-left:6.25rem&quot;&gt;TorchServe `TextIteratorStreamerBatch` extends Hugging Face transformers `BaseStreamer` to support response streaming when `batchSize` is larger than 1. &lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.output_streamer = TextIteratorStreamerBatch(
    self.tokenizer,
    batch_size=self.handle.micro_batch_size,
    skip_special_tokens=True,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/inf2_handler.py#L124&quot;&gt;inference&lt;/a&gt; function calls send_intermediate_predict_response to send the streaming response.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for new_text in self.output_streamer:
    logger.debug(&quot;send response stream&quot;)
    send_intermediate_predict_response(
        new_text[: len(micro_batch_req_id_map)],
        micro_batch_req_id_map,
        &quot;Intermediate Prediction success&quot;,
        200,
        self.context,
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;package-model-artifacts&quot;&gt;Package model artifacts&lt;/h3&gt;

&lt;p&gt;Package all the model artifacts into a folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1&lt;/code&gt; using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch-model-archiver&lt;/code&gt;. &lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch-model-archiver --model-name llama-2-13b-neuronx-b1 --version 1.0 --handler inf2_handler.py -r requirements.txt --config-file model-config.yaml --archive-format no-archive
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;serve-the-model&quot;&gt;Serve the model&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export TS_INSTALL_PY_DEP_PER_MODEL=&quot;true&quot;
torchserve --ncs --start --model-store model_store --models llama-2-13b-neuronx-b1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the log shows “&lt;strong&gt;WORKER_MODEL_LOADED&lt;/strong&gt;”, the pre-compiled model should be saved in the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1/neuron_cache&lt;/code&gt;, which is tightly coupled with Neuron SDK version. Then, upload the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1&lt;/code&gt; to your S3 bucket for later use in the product deployment. The Llama-2 13B model artifacts in this blog can be found &lt;a href=&quot;https://torchserve.s3.amazonaws.com/mar_files/sm-neuronx/llama-2-13b-neuronx-b1/&quot;&gt;here&lt;/a&gt;, which is associated with Neuron SDK 2.13.2, in the TorchServe model zoo.&lt;/p&gt;

&lt;h2 id=&quot;deploy-llama-2-13b-model-on-sagemakerinf2-instance-using-torchserve&quot;&gt;Deploy Llama-2 13B model on SageMaker Inf2 instance using TorchServe &lt;/h2&gt;

&lt;p&gt;In this section, we deploy the Llama-2 13B model using a &lt;a href=&quot;https://github.com/aws/deep-learning-containers/blob/master/available_images.md#neuron-containers&quot;&gt;PyTorch Neuronx container&lt;/a&gt; on a SageMaker endpoint with an ml.inf2.24xlarge hosting instance, which has 6 Inferentia2 accelerators corresponding to our model configuration &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_config.yaml&lt;/code&gt; handler’s setting - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tp_degree: 12&lt;/code&gt;. Given that we have packaged all the model artifacts into a folder using &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/model-archiver/README.md&quot;&gt;torch-model-archiver&lt;/a&gt; and uploaded to S3 bucket, we will now use the SageMaker Python SDK to create a SageMaker model and deploy it to a SageMaker real-time endpoint using the deploy &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html&quot;&gt;uncompressed model method&lt;/a&gt;. Speed is the key benefit to deploying in this manner with SageMaker and you get a fully functional production ready endpoint complete with a secure RESTful endpoint without any effort spent on infrastructure. There are 3 steps to deploying the model and running inference on SageMaker. The notebook example can be found &lt;a href=&quot;https://github.com/aws/amazon-sagemaker-examples-community/blob/main/torchserve/inf2/llama2/llama-2-13b.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a SageMaker model&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from datetime import datetime

instance_type = &quot;ml.inf2.24xlarge&quot;
endpoint_name = sagemaker.utils.name_from_base(&quot;ts-inf2-llama2-13b-b1&quot;)

model = Model(
    name=&quot;torchserve-inf2-llama2-13b&quot; + datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;),
    # Enable SageMaker uncompressed model artifacts
    model_data={
        &quot;S3DataSource&quot;: {
                &quot;S3Uri&quot;: s3_uri,
                &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
                &quot;CompressionType&quot;: &quot;None&quot;,
        }
    },
    image_uri=container,
    role=role,
    sagemaker_session=sess,
    env={&quot;TS_INSTALL_PY_DEP_PER_MODEL&quot;: &quot;true&quot;},
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Deploy a SageMaker model&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.deploy(
    initial_instance_count=1,
    instance_type=instance_type,
    endpoint_name=endpoint_name,
    volume_size=512, # increase the size to store large model
    model_data_download_timeout=3600, # increase the timeout to download large model
    container_startup_health_check_timeout=600, # increase the timeout to load large model
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Run streaming response inference on SageMaker
When the endpoint is in service, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invoke_endpoint_with_response_stream&lt;/code&gt; API call to invoke the model. This feature enables the return of each generated token to the user, enhancing the user experience. It’s especially beneficial when generating an entire sequence is time-consuming.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import json

body = &quot;Today the weather is really nice and I am planning on&quot;.encode('utf-8')
resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=body, ContentType=&quot;application/json&quot;)
event_stream = resp['Body']
parser = Parser()
for event in event_stream:
    parser.write(event['PayloadPart']['Bytes'])
    for line in parser.scan_lines():
        print(line.decode(&quot;utf-8&quot;), end=' ')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;sample-inference&quot;&gt;Sample inference:&lt;/h3&gt;

&lt;p&gt;Input&lt;/p&gt;

&lt;p&gt;“Today the weather is really nice and I am planning on”&lt;/p&gt;

&lt;p&gt;Output&lt;/p&gt;

&lt;p&gt;“Today the weather is really nice and I am planning on going to the beach. I am going to take my camera and take some pictures of the beach. I am going to take pictures of the sand, the water, and the people. I am also going to take pictures of the sunset. I am really excited to go to the beach and take pictures.&lt;/p&gt;

&lt;p&gt;The beach is a great place to take pictures. The sand, the water, and the people are all great subjects for pictures. The sunset is also a great subject for pictures.”&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we showcased how to run Llama 2 model inference using Transformers Neuron and deploy Llama 2 model serving using TorchServe through Amazon SageMaker on an EC2 Inf2 instance. We demonstrated the benefits of using Inferentia2—low latency and low cost—enabled by optimizations in AWS Neuron SDK including tensor parallelism, parallel context encoding and KV caching, particularly for LLM inference. To stay up to date, please follow &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html&quot;&gt;AWS Neuron’s latest release&lt;/a&gt; for new features.&lt;/p&gt;

&lt;p&gt;Get started today with Llama 2 examples on &lt;a href=&quot;https://github.com/aws-neuron/aws-neuron-samples/blob/master/torch-neuronx/transformers-neuronx/inference/meta-llama-2-13b-sampling.ipynb&quot;&gt;EC2&lt;/a&gt; and through &lt;a href=&quot;https://github.com/aws/amazon-sagemaker-examples-community/blob/main/torchserve/inf2/llama2/llama-2-13b.ipynb&quot;&gt;SageMaker&lt;/a&gt; and stay tuned for how to optimize Llama 70B on Inf2!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Mike Zhang, Li Ning, Sergey Ivanov, Naman Nandan, Hamid Shojanazeri, Geeta Chauhan, Abhi Shivaditya, Michael Nguyen, Pinak Panigrahi</name>
        
        
      </author>

      

      

      
        <summary type="html">Recently, Llama 2 was released and has attracted a lot of interest from the machine learning community. Amazon EC2 Inf2 instances, powered by AWS Inferentia2, now support training and inference of Llama 2 models. In this post, we show low-latency and cost-effective inference of Llama-2 models on Amazon EC2 Inf2 instances using the latest AWS Neuron SDK release.  We first introduce how to create, compile and deploy the Llama-2 model and explain the optimization techniques introduced by AWS Neuron SDK to achieve high performance at low cost. We then present our benchmarking results. Lastly, we show how the Llama-2 model can be deployed through Amazon SageMaker using TorchServe on an Inf2 instance. </summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">New Library Updates in PyTorch 2.1</title>
      <link href="https://pytorch.org/blog/new-library-updates/" rel="alternate" type="text/html" title="New Library Updates in PyTorch 2.1" />
      <published>2023-10-04T00:00:00-07:00</published>
      <updated>2023-10-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/new-library-updates</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-library-updates/">&lt;h2 id=&quot;summary&quot;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 2.1 release. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch. &lt;/p&gt;

&lt;p&gt;Along with 2.1, we are also releasing a series of beta updates to the PyTorch domain libraries including TorchAudio and TorchVision. Please find the list of the latest stable versions and updates below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Latest Stable Library Versions&lt;/th&gt;
      &lt;th&gt;(&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;Full List&lt;/a&gt;)*&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;TorchArrow 0.1.0&lt;/td&gt;
      &lt;td&gt;TorchRec 0.5.0&lt;/td&gt;
      &lt;td&gt;TorchVision 0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TorchAudio 2.1&lt;/td&gt;
      &lt;td&gt;TorchServe 0.8.2&lt;/td&gt;
      &lt;td&gt;TorchX 0.5.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TorchData 0.7.0&lt;/td&gt;
      &lt;td&gt;TorchText 0.16.0&lt;/td&gt;
      &lt;td&gt;PyTorch on XLA Devices 1.14&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;*To see &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;prior versions&lt;/a&gt; or (unstable) nightlies, click on versions in the top left menu above ‘Search Docs’.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio&quot;&gt;&lt;strong&gt;TorchAudio&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;TorchAudio v2.1 introduces the following new features and backward-incompatible changes:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] A new API to apply filter, effects and codec&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;`torchaudio.io.AudioEffector` can apply filters, effects and encodings to waveforms in online/offline fashion. You can use it as a form of augmentation.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/effector_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/effector_tutorial.html&lt;/a&gt; for the usage and examples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] Tools for Forced alignment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;New functions and a pre-trained model for forced alignment were added. `torchaudio.functional.forced_align` computes alignment from an emission and `torchaudio.pipelines.MMS_FA` provides access to the model trained for multilingual forced alignment in &lt;a href=&quot;https://ai.meta.com/blog/multilingual-model-speech-recognition/&quot;&gt;MMS: Scaling Speech Technology to 1000+ languages&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/ctc_forced_alignment_api_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/ctc_forced_alignment_api_tutorial.html&lt;/a&gt; for the usage of `forced_align` function, and &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/forced_alignment_for_multilingual_data_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/forced_alignment_for_multilingual_data_tutorial.html&lt;/a&gt; for how one can use `MMS_FA` to align transcript in multiple languages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] TorchAudio-Squim : Models for reference-free speech assessment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Model architectures and pre-trained models from the paper &lt;a href=&quot;https://arxiv.org/abs/2304.01448&quot;&gt;TorchAudio-Sequim: Reference-less Speech Quality and Intelligibility measures in TorchAudio&lt;/a&gt; were added.&lt;/p&gt;

&lt;p&gt;You can use the pre-trained models `torchaudio.pipelines.SQUIM_SUBJECTIVE` and `torchaudio.pipelines.SQUIM_OBJECTIVE`. They can estimate the various speech quality and intelligibility metrics (e.g. STOI, wideband PESQ, Si-SDR, and MOS). This is helpful when evaluating the quality of speech generation models, such as Text-to-Speech (TTS).&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/squim_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/squim_tutorial.html&lt;/a&gt; for the details.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] CUDA-based CTC decoder&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;`torchaudio.models.decoder.CUCTCDecoder` performs CTC beam search in CUDA devices. The beam search is fast. It eliminates the need to move data from CUDA device to CPU when performing automatic speech recognition. With PyTorch’s CUDA support, it is now possible to perform the entire speech recognition pipeline in CUDA.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/asr_inference_with_cuda_ctc_decoder_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/asr_inference_with_cuda_ctc_decoder_tutorial.html&lt;/a&gt; for the detail.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] Utilities for AI music generation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are working to add utilities that are relevant to music AI. Since the last release, the following APIs were added to the prototype.&lt;/p&gt;

&lt;p&gt;Please refer to respective documentation for the usage.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/main/generated/torchaudio.prototype.functional.chroma_filterbank.html&quot;&gt;torchaudio.prototype.chroma_filterbank&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/main/generated/torchaudio.prototype.transforms.ChromaScale.html&quot;&gt;torchaudio.prototype.transforms.ChromaScale&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/main/generated/torchaudio.prototype.transforms.ChromaSpectrogram.html&quot;&gt;torchaudio.prototype.transforms.ChromaSpectrogram&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/main/generated/torchaudio.prototype.pipelines.VGGISH.html&quot;&gt;torchaudio.prototype.pipelines.VGGISH&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;New recipes for training models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recipes for Audio-visual ASR, multi-channel DNN beamforming and TCPGen context-biasing were added.&lt;/p&gt;

&lt;p&gt;Please refer to the recipes&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/audio/tree/release/2.1/examples/avsr&quot;&gt;https://github.com/pytorch/audio/tree/release/2.1/examples/avsr&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/audio/tree/release/2.1/examples/dnn_beamformer&quot;&gt;https://github.com/pytorch/audio/tree/release/2.1/examples/dnn_beamformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/audio/tree/release/2.1/examples/asr/librispeech_conformer_rnnt_biasing&quot;&gt;https://github.com/pytorch/audio/tree/release/2.1/examples/asr/librispeech_conformer_rnnt_biasing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Update to FFmpeg support&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The version of supported FFmpeg libraries was updated. TorchAudio v2.1 works with FFmpeg 6, 5 and 4.4. The support for 4.3, 4.2 and 4.1 are dropped.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/installation.html#optional-dependencies&quot;&gt;https://pytorch.org/audio/2.1/installation.html#optional-dependencies&lt;/a&gt; for the detail of the new FFmpeg integration mechanism.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update to libsox integration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TorchAudio now depends on libsox installed separately from torchaudio. Sox I/O backend no longer supports file-like objects. (This is supported by FFmpeg backend and soundfile.)&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/installation.html#optional-dependencies&quot;&gt;https://pytorch.org/audio/2.1/installation.html#optional-dependencies&lt;/a&gt; for the details.&lt;/p&gt;

&lt;h2 id=&quot;torchrl&quot;&gt;TorchRL&lt;/h2&gt;

&lt;p&gt;Our RLHF components make it easy to build an RLHF training loop with limited RL knowledge. TensorDict enables an easy interaction between datasets (eg, HF datasets) and RL models. The new algorithms we provide deliver a wide range of solutions for offline RL training, which is more data efficient.&lt;/p&gt;

&lt;p&gt;Through RoboHive and IsaacGym, TorchRL now provides a built-in interface with hardware (robots), tying training at scale with policy deployment on device. Thanks to SMAC, VMAS, and PettingZoo and related MARL-oriented losses, TorchRL is now fully capable of training complex policies in multi-agent settings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;[BETA] We integrate some RLHF components and examples: we provide building blocks for data formatting in RL frameworks, reward model design, specific transforms that enable efficient learning (eg. KL correction) and training scripts&lt;/li&gt;
  &lt;li&gt;[Stable] New algorithms include Decision transformers, CQL, multi-agent losses such as MAPPO and QMixer.&lt;strong&gt;New features&lt;/strong&gt;- [Stable] New transforms such as Visual Cortex 1 (VC1), a foundational model for RL. &lt;/li&gt;
  &lt;li&gt;We widened the panel of library covered by TorchRL: 
    &lt;ul&gt;
      &lt;li&gt;[Beta] IsaacGym, a powerful GPU-based simulator that allows interaction and rendering of thousands of vectorized environments by NVIDIA.&lt;/li&gt;
      &lt;li&gt;[Stable] PettingZoo, a multi-agent library by the Farama Foundation.&lt;/li&gt;
      &lt;li&gt;[Stable] SMAC-v2, the new Starcraft Multi-agent simulator&lt;/li&gt;
      &lt;li&gt;[Stable] RoboHive, a collection of environments/tasks simulated with the MuJoCo physics engine.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Performance improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We provide faster data collection through refactoring and integration of SB3 and Gym asynchronous environments execution. We also made our value functions faster to execute.&lt;/p&gt;

&lt;h2 id=&quot;torchrec&quot;&gt;TorchRec&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] Zero Collision / Managed Collision Embedding Bags&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A common constraint in Recommender Systems is the sparse id input range is larger than the number of embeddings the model can learn for a given parameter size.   To resolve this issue, the conventional solution is to hash sparse ids into the same size range as the embedding table.  This will ultimately lead to hash collisions, with multiple sparse ids sharing the same embedding space.   We have developed a performant alternative algorithm that attempts to address this problem by tracking the &lt;em&gt;N&lt;/em&gt; most common sparse ids and ensuring that they have a unique embedding representation. The module is defined &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/b992eebd80e8ccfc3b96a7fd39cb072c17e8907d/torchrec/modules/mc_embedding_modules.py#L26&quot;&gt;here&lt;/a&gt; and an example can be found &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/b992eebd80e8ccfc3b96a7fd39cb072c17e8907d/torchrec/modules/mc_embedding_modules.py#L26&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] UVM Caching - Prefetch Training Pipeline&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For tables where on-device memory is insufficient to hold the entire embedding table, it is common to leverage a caching architecture where part of the embedding table is cached on device and the full embedding table is on host memory (typically DDR SDRAM).   However, in practice, caching misses are common, and hurt performance due to relatively high latency of going to host memory.   Building on TorchRec’s existing data pipelining, we developed a new &lt;a href=&quot;https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.train_pipeline.PrefetchPipelinedForward&quot;&gt;&lt;em&gt;Prefetch Training Pipeline&lt;/em&gt;&lt;/a&gt; to avoid these cache misses by prefetching the relevant embeddings for upcoming batch from host memory, effectively eliminating cache misses in the forward path.&lt;/p&gt;

&lt;h2 id=&quot;torchvision&quot;&gt;TorchVision &lt;/h2&gt;
&lt;h3 id=&quot;transforms-and-augmentations&quot;&gt;&lt;strong&gt;Transforms and augmentations&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Major speedups&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The new transforms in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.transforms.v2&lt;/code&gt; are now&lt;a href=&quot;https://github.com/pytorch/vision/issues/7497#issuecomment-1557478635&quot;&gt; 10%-40% faster&lt;/a&gt; than before! This is mostly achieved thanks to 2X-4X improvements made to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v2.Resize()&lt;/code&gt;, which now supports native &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8&lt;/code&gt; tensors for Bilinear and Bicubic mode. Output results are also now closer to PIL’s! Check out our&lt;a href=&quot;https://pytorch.org/vision/stable/transforms.html#performance-considerations&quot;&gt; performance recommendations&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;p&gt;Additionally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision&lt;/code&gt; now ships with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libjpeg-turbo&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libjpeg&lt;/code&gt;, which should significantly speed-up the jpeg decoding utilities (&lt;a href=&quot;https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_image&lt;/code&gt;&lt;/a&gt;,&lt;a href=&quot;https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.decode_jpeg&quot;&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;decode_jpeg&lt;/code&gt;&lt;/a&gt;), and avoid compatibility issues with PIL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CutMix and MixUp&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Long-awaited support for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CutMix&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MixUp&lt;/code&gt; augmentations is now here! Check&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py&quot;&gt; our tutorial&lt;/a&gt; to learn how to use them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Towards stable V2 transforms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the&lt;a href=&quot;https://github.com/pytorch/vision/releases/tag/v0.15.1&quot;&gt; previous release 0.15&lt;/a&gt; we BETA-released a new set of transforms in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.transforms.v2&lt;/code&gt; with native support for tasks like segmentation, detection, or videos. We have now stabilized the design decisions of these transforms and made further improvements in terms of speedups, usability, new transforms support, etc.&lt;/p&gt;

&lt;p&gt;We’re keeping the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.transforms.v2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.tv_tensors&lt;/code&gt; namespaces as BETA until 0.17 out of precaution, but we do not expect disruptive API changes in the future.&lt;/p&gt;

&lt;p&gt;Whether you’re new to Torchvision transforms, or you’re already experienced with them, we encourage you to start with&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py&quot;&gt; Getting started with transforms v2&lt;/a&gt; in order to learn more about what can be done with the new v2 transforms.&lt;/p&gt;

&lt;p&gt;Browse our&lt;a href=&quot;https://pytorch.org/vision/stable/transforms.html#&quot;&gt; main docs&lt;/a&gt; for general information and performance tips. The available transforms and functionals are listed in the&lt;a href=&quot;https://pytorch.org/vision/stable/transforms.html#v2-api-ref&quot;&gt; API reference&lt;/a&gt;. Additional information and tutorials can also be found in our&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/index.html#gallery&quot;&gt; example gallery&lt;/a&gt;, e.g.&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_e2e.html#sphx-glr-auto-examples-transforms-plot-transforms-e2e-py&quot;&gt; Transforms v2: End-to-end object detection/segmentation example&lt;/a&gt; or&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_custom_transforms.html#sphx-glr-auto-examples-transforms-plot-custom-transforms-py&quot;&gt; How to write your own v2 transforms&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-mps-support&quot;&gt;[BETA] MPS support&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nms&lt;/code&gt; and roi-align kernels (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roi_align&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roi_pool&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps_roi_align&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps_roi_pool&lt;/code&gt;) now support MPS. Thanks to&lt;a href=&quot;https://github.com/qqaatw&quot;&gt; Li-Huai (Allan) Lin&lt;/a&gt; for this contribution!&lt;/p&gt;

&lt;h2 id=&quot;torchx&quot;&gt;TorchX&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Schedulers&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;[Prototype] Kubernetes MCAD Scheduler: Integration for easily scheduling jobs on Multi-Cluster-Application-Dispatcher (MCAD)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AWS Batch &lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Add privileged option to enable running containers on EFA enabled instances with elevated networking permissions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;torchx-tracker&quot;&gt;&lt;strong&gt;TorchX Tracker&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;[Prototype] MLFlow backend for TorchX Tracker: in addition to &lt;em&gt;fsspec&lt;/em&gt; based tracker, TorchX can use MLFlow instance to track metadata/experiments &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;dist.spmd&lt;/em&gt; component to support Single-Process-Multiple-Data style applications&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Workspace&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Add ability to access image and workspace path from Dockerfile while building docker workspace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Release includes number of other bugfixes.&lt;/p&gt;

&lt;p&gt;To learn more about Torchx visit &lt;a href=&quot;https://pytorch.org/torchx/latest/&quot;&gt;https://pytorch.org/torchx/latest/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;torchtext-and-torchdata&quot;&gt;TorchText and TorchData&lt;/h2&gt;

&lt;p&gt;As of September 2023 we have paused active development of TorchText and TorchData as we re-evaluate how we want to serve the needs of the community in this space.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing</title>
      <link href="https://pytorch.org/blog/pytorch-2-1/" rel="alternate" type="text/html" title="PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing" />
      <published>2023-10-04T00:00:00-07:00</published>
      <updated>2023-10-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-1</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-1/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.1 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.1.0&quot;&gt;release note&lt;/a&gt;)! PyTorch 2.1 offers automatic dynamic shape support in &lt;em&gt;torch.compile&lt;/em&gt;, &lt;em&gt;torch.distributed.checkpoint&lt;/em&gt; for saving/loading distributed training jobs on multiple ranks in parallel, and &lt;em&gt;torch.compile&lt;/em&gt; support for the NumPy API.&lt;/p&gt;

&lt;p&gt;In addition, this release offers numerous performance improvements (e.g. CPU inductor improvements, AVX512 support, scaled-dot-product-attention support) as well as a prototype release of &lt;em&gt;torch.export&lt;/em&gt;, a sound full-graph capture mechanism, and &lt;em&gt;torch.export&lt;/em&gt;-based quantization.&lt;/p&gt;

&lt;p&gt;Along with 2.1, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog. &lt;/p&gt;

&lt;p&gt;This release is composed of 6,682 commits and 784 contributors since 2.0. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.1.  More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;Summary: &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; now includes automatic support for detecting and minimizing recompilations due to tensor shape changes using &lt;em&gt;automatic dynamic shapes.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.distributed.checkpoint&lt;/em&gt; enables saving and loading models from multiple ranks in parallel, as well as resharding due to changes in cluster topology.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; can now compile NumPy operations via translating them into PyTorch-equivalent operations.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; now includes improved support for Python 3.11.&lt;/li&gt;
  &lt;li&gt;New CPU performance features include inductor improvements (e.g. bfloat16 support and dynamic shapes), AVX512 kernel support, and scaled-dot-product-attention kernels.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.export&lt;/em&gt;, a sound full-graph capture mechanism is introduced as a prototype feature, as well as &lt;em&gt;torch.export&lt;/em&gt;-based quantization.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.sparse&lt;/em&gt; now includes prototype support for semi-structured (2:4) sparsity on NVIDIA® GPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Stable&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Beta&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Prototype&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Automatic Dynamic Shapes&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.export()&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;AVX512 kernel support&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.distributed.checkpoint&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;Torch.export-based Quantization&lt;/td&gt;
      &lt;td&gt;CPU optimizations for scaled-dot-product-attention (SPDA)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.compile&lt;/em&gt; + NumPy&lt;/td&gt;
      &lt;td&gt;semi-structed (2:4) sparsity&lt;/td&gt;
      &lt;td&gt;CPU optimizations for bfloat16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.compile&lt;/em&gt; + Python 3.11&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;cpp_wrapper&lt;/em&gt; for torchinductor&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.compile + autograd.Function&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;third-party device integration: &lt;em&gt;PrivateUse1&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public 2.1, 2.0, and 1.13 feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;&lt;strong&gt;Beta Features&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;(Beta) Automatic Dynamic Shapes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dynamic shapes is functionality built into &lt;em&gt;torch.compile&lt;/em&gt; that can minimize recompilations by tracking and generating code based on the symbolic shape of a tensor rather than the static shape (e.g. &lt;em&gt;[B, 128, 4]&lt;/em&gt; rather than &lt;em&gt;[64, 128, 4]&lt;/em&gt;). This allows &lt;em&gt;torch.compile&lt;/em&gt; to generate a single kernel that can work for many sizes, at only a modest cost to efficiency. Dynamic shapes has been greatly stabilized in PyTorch 2.1, and is now automatically enabled if &lt;em&gt;torch.compile&lt;/em&gt; notices recompilation due to varying input shapes. You can disable automatic dynamic by passing &lt;em&gt;dynamic=False&lt;/em&gt; to torch.compile, or by setting &lt;em&gt;torch._dynamo.config.automatic_dynamic_shapes = False&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In PyTorch 2.1, we have shown good performance with dynamic shapes enabled on a variety of model types, including large language models, on both CUDA and CPU.&lt;/p&gt;

&lt;p&gt;For more information on dynamic shapes, see &lt;a href=&quot;https://pytorch.org/docs/2.1/torch.compiler_dynamic_shapes.html&quot;&gt;this documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] &lt;em&gt;torch.distributed.checkpoint&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.distributed.checkpoint&lt;/em&gt; enables saving and loading models from multiple ranks in parallel. In addition, checkpointing automatically handles fully-qualified-name (FQN) mappings across models and optimizers, enabling load-time resharding across differing cluster topologies.&lt;/p&gt;

&lt;p&gt;For more information, see &lt;em&gt;torch.distributed.checkpoint&lt;/em&gt; &lt;a href=&quot;https://pytorch.org/docs/2.1/distributed.checkpoint.html&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] &lt;em&gt;torch.compile&lt;/em&gt; + &lt;em&gt;NumPy&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.compile&lt;/em&gt; now understands how to compile NumPy operations via translating them into PyTorch-equivalent operations.  Because this integration operates in a device-agnostic manner, you can now GPU-accelerate NumPy programs – or even mixed NumPy/PyTorch programs – just by using &lt;em&gt;torch.compile&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Please see &lt;a href=&quot;https://pytorch.org/docs/2.1/torch.compiler_faq.html#does-numpy-work-with-torch-compile&quot;&gt;this section&lt;/a&gt; in the &lt;em&gt;torch.compile&lt;/em&gt; FAQ for more information about &lt;em&gt;torch.compile + NumPy interaction&lt;/em&gt;, and follow the &lt;a href=&quot;https://pytorch.org/blog/&quot;&gt;PyTorch Blog&lt;/a&gt; for a forthcoming blog about this feature.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] &lt;em&gt;torch.compile&lt;/em&gt; + Python 3.11&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.compile&lt;/em&gt; previously only supported Python versions 3.8-3.10. Users can now optimize models with &lt;em&gt;torch.compile&lt;/em&gt; in Python 3.11.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] &lt;em&gt;torch.compile&lt;/em&gt; + &lt;em&gt;autograd.Function&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.compile&lt;/em&gt; can now trace and optimize the backward function of user-defined &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#function&quot;&gt;autograd Functions&lt;/a&gt;, which unlocks training optimizations for models that make heavier use of extensions mechanisms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] Improved third-party device support: &lt;em&gt;PrivateUse1&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Third-party device types can now be registered to PyTorch using the privateuse1 dispatch key.  This allows device extensions to register new kernels to PyTorch and to associate them with the new key, allowing user code to work equivalently to built-in device types.  For example, to register &lt;em&gt;“my_hardware_device&lt;/em&gt;”, one can do the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.rename_privateuse1_backend(&quot;my_hardware_device&quot;)
torch.utils.generate_methods_for_privateuse1_backend()
x = torch.randn((2, 3), device='my_hardware_device')
y = x + x # run add kernel on 'my_hardware_device'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To validate this feature, the OSS team from &lt;em&gt;Ascend NPU&lt;/em&gt; has successfully integrated &lt;a href=&quot;https://github.com/Ascend/pytorch&quot;&gt;&lt;strong&gt;torch_npu&lt;/strong&gt;&lt;/a&gt; into pytorch as a plug-in through the &lt;em&gt;PrivateUse1&lt;/em&gt; functionality.&lt;/p&gt;

&lt;p&gt;For more information, please see the PrivateUse1 tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/privateuseone.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;&lt;strong&gt;Prototype Features&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] &lt;em&gt;torch.export()&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.export()&lt;/em&gt; provides a sound tracing mechanism to capture a full graph from a PyTorch program based on new technologies provided by PT2.0.&lt;/p&gt;

&lt;p&gt;Users can extract a clean representation (Export IR) of a PyTorch program in the form of a dataflow graph, consisting of mostly straight-line calls to PyTorch operators. Export IR can then be transformed, serialized, saved to file, transferred, loaded back for execution in an environment with or without Python.&lt;/p&gt;

&lt;p&gt;For more information, please see the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] &lt;em&gt;torch.export&lt;/em&gt;-based Quantization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.ao.quantization&lt;/em&gt; now supports quantization on PyTorch 2 &lt;em&gt;torch.export&lt;/em&gt;-based flows.  This includes support for built-in &lt;em&gt;XNNPACK&lt;/em&gt; and &lt;em&gt;X64Inductor&lt;/em&gt; &lt;em&gt;Quantizer&lt;/em&gt;, as well as the ability to specify one’s own &lt;em&gt;Quantizer&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For an explanation on post-training static quantization with torch.export, see &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html&quot;&gt;this tutorial&lt;/a&gt;, for quantization-aware training for static quantization with torch.export, see &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For an explanation on how to write one’s own Quantizer, see &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quantizer.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] semi-structured (2:4) sparsity for NVIDIA® GPUs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.sparse&lt;/em&gt; now supports creating and accelerating compute over semi-structured sparse (2:4) tensors.  For more information on the format, see &lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/&quot;&gt;this&lt;/a&gt; blog from NVIDIA.A minimal example introducing semi-structured sparsity is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch.sparse import to_sparse_semi_structured
 
x = torch.rand(64, 64).half().cuda()
mask = torch.tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()
linear = nn.Linear(64, 64).half().cuda()

linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))
linear(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To learn more, please see the &lt;a href=&quot;https://pytorch.org/docs/2.1/sparse.html#sparse-semi-structured-tensors&quot;&gt;documentation&lt;/a&gt; and accompanying &lt;a href=&quot;https://pytorch.org/tutorials/prototype/semi_structured_sparse.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] &lt;em&gt;cpp_wrapper&lt;/em&gt; for &lt;em&gt;torchinductor&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;cpp_wrapper&lt;/em&gt; can reduce the Python overhead for invoking kernels in torchinductor by generating the kernel wrapper code in C++. This feature is still in the prototype phase; it does not support all programs that successfully compile in PT2 today. Please file issues if you discover limitations for your use case to help us prioritize.&lt;/p&gt;

&lt;p&gt;The API to turn this feature on is:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torch._inductor.config as config
config.cpp_wrapper = True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more information, please see the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;AVX512 kernel support&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In PyTorch 2.0, AVX2 kernels would be used even if the CPU supported AVX512 instructions.  Now, PyTorch defaults to using AVX512 CPU kernels if the CPU supports those instructions, equivalent to setting &lt;em&gt;ATEN_CPU_CAPABILITY=avx512&lt;/em&gt; in previous releases.  The previous behavior can be enabled by setting &lt;em&gt;ATEN_CPU_CAPABILITY=avx2.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CPU optimizations for scaled-dot-product-attention (SDPA)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Previous versions of PyTorch provided optimized CUDA implementations for transformer primitives via &lt;em&gt;torch.nn.functiona.scaled_dot_product_attention&lt;/em&gt;.  PyTorch 2.1 includes optimized FlashAttention-based CPU routines.&lt;/p&gt;

&lt;p&gt;See the documentation &lt;a href=&quot;https://pytorch.org/docs/2.1/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CPU optimizations for bfloat16&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.1 includes CPU optimizations for bfloat16, including improved vectorization support and &lt;em&gt;torchinductor&lt;/em&gt; codegen.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.1 (release note)! PyTorch 2.1 offers automatic dynamic shape support in torch.compile, torch.distributed.checkpoint for saving/loading distributed training jobs on multiple ranks in parallel, and torch.compile support for the NumPy API.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">How to Build an Interactive Chat-Generation Model using DialoGPT and PyTorch</title>
      <link href="https://pytorch.org/blog/interactive-chat-gen-model/" rel="alternate" type="text/html" title="How to Build an Interactive Chat-Generation Model using DialoGPT and PyTorch" />
      <published>2023-10-03T00:00:00-07:00</published>
      <updated>2023-10-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/interactive-chat-gen-model</id>
      <content type="html" xml:base="https://pytorch.org/blog/interactive-chat-gen-model/">&lt;p&gt;The focus on interactive chat-generation (or conversational response-generation) models has greatly increased in the past several months. Conversational response-generation models such as ChatGPT and Google Bard have taken the AI world by storm. The purpose of interactive chat generation is to answer various questions posed by humans, and these AI based models use natural language processing (NLP) to generate conversations almost indistinguishable from those generated by humans.&lt;/p&gt;

&lt;p&gt;This article showcases a &lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/blob/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Interactive_Chat_Quantization/IntelPytorch_Interactive_Chat_Quantization.ipynb&quot;&gt;code sample&lt;/a&gt; on how to create interactive chats based on a pre-trained DialoGPT model from Hugging Face with the addition of the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; to perform dynamic quantization on the model.&lt;/p&gt;

&lt;h2 id=&quot;get-started&quot;&gt;Get Started&lt;/h2&gt;

&lt;h3 id=&quot;why-dialogpt&quot;&gt;Why DialoGPT?&lt;/h3&gt;

&lt;p&gt;DialoGPT (&lt;strong&gt;Dialo&lt;/strong&gt;gue &lt;strong&gt;G&lt;/strong&gt;enerative &lt;strong&gt;P&lt;/strong&gt;re-trained &lt;strong&gt;T&lt;/strong&gt;ransformer) is a large-scale, pre-trained dialogue-response-generation model trained on 147M conversation-like exchanges pulled out from Reddit comment chains and discussion threads. &lt;a href=&quot;http://github.com/microsoft/DialoGPT&quot;&gt;DialoGPT&lt;/a&gt; was proposed by Microsoft in 2019. The main goal was to create open-domain chatbots capable of producing natural responses to a variety of conversational topics. The conversational response-generation systems that leverage DialoGPT generate more applicable, resourceful, diverse, and context-specific replies.&lt;/p&gt;

&lt;h3 id=&quot;dialogpt-architecture&quot;&gt;DialoGPT Architecture&lt;/h3&gt;

&lt;p&gt;DialoGPT architecture is based on the GPT-2 model. It is formulated as an autoregressive language model and uses a multi-layer transformer as the model architecture. GPT-2 was proposed by OpenAI. GPT-2 models are trained on general text data whereas DialoGPT is trained on Reddit discussion threads.&lt;/p&gt;

&lt;p&gt;Let’s look at the GPT-2 architecture. There are two types of blocks in general transformer architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Encoder - contains self-attention layer and feed-forward neural network&lt;/li&gt;
  &lt;li&gt;Decoder - similar to encoder, but the self-attention layer is masked&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The self-attention layer allows a position to peak at tokens to the right of the current word (the successive words in text), whereas masked self-attention layer prevents that from happening.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-self-attention-vs-masked.png&quot; alt=&quot;self-attention layer vs masked self-attention layer&quot; style=&quot;width:100%; max-width: 845px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GPT-2 is built using transformer decoder blocks. This means that the following layers are used in the architecture:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Embedding Layer – responsible for converting input text into embeddings (each word is converted to a fixed-length vector representation)&lt;/li&gt;
  &lt;li&gt;Transformer Decoder – includes multiple decoder blocks with masked self-attention and feed forward neural network layers&lt;/li&gt;
  &lt;li&gt;Output Layer – responsible for converting embeddings obtained from the decoder into words&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;GPT-2 architecture (and DialoGPT architecture) is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-dialogpt-article.png&quot; alt=&quot;GPT-2 architecture&quot; style=&quot;width:100%; max-width: 651px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the model is based on transformers architecture, it has the issue of repetition and copying the inputs. To avoid repetition, we can use Top-K sampling and Top-p sampling.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Top-K sampling - filters the K most likely next words and redistributes the probability mass among only those K next words.&lt;/li&gt;
  &lt;li&gt;Top-p sampling - rather than selecting only the most likely K words, selects the smallest possible set of words whose cumulative probability exceeds the probability p.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The probability mass is then redistributed among the words in the set. As a result, the size of the set of words can be dynamically increased and decreased based on the probability distribution of the next word.&lt;/p&gt;

&lt;h3 id=&quot;quantization-using-intel-extension-for-pytorch&quot;&gt;Quantization using Intel® Extension for PyTorch&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;What is Quantization?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Quantization is a systematic reduction of the precision of all or several layers within the model. This means a higher-precision type, such as the single-precision floating-point (FP32) mostly used in deep learning, is converted into a lower-precision type such as FP16 (16 bits) or INT8 (8 bits).&lt;/p&gt;

&lt;p&gt;This helps in achieving,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;lower memory bandwidth&lt;/li&gt;
  &lt;li&gt;lower storage&lt;/li&gt;
  &lt;li&gt;higher performance with minimum-to-zero accuracy loss&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantization is especially important with large models such as those based on the Transformer architecture like BERT or GPT.&lt;/p&gt;

&lt;p&gt;There are two types of quantization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Static – Static quantization quantizes the weights and activations of the model. This quantization is used when both memory bandwidth and compute savings are important.&lt;/li&gt;
  &lt;li&gt;Dynamic – In dynamic quantization, the weights are quantized ahead of time, but the activations are dynamically quantized during inference.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Intel Extension for PyTorch:&lt;/strong&gt; The Intel Extension extends PyTorch with up-to-date features and optimizations for an extra performance boost on Intel® hardware. Learn how to &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch#installation&quot;&gt;install it standalone&lt;/a&gt; or get it a part of the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html&quot;&gt;Intel® AI Analytics Toolkit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The extension can be loaded as a Python* module or linked as a C++ library. Python users can enable it dynamically by importing intel_extension_for_pytorch.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/cpu/latest/&quot;&gt;CPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel CPUs. Source code is available at the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/master&quot;&gt;master branch&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;This &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/xpu/latest/&quot;&gt;GPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel GPUs. Source code is available at the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/xpu-master&quot;&gt;xpu-master branch&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How to perform dynamic quantization using Intel Extension for PyTorch?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here are the steps to quantize the existing FP32 model to INT8 model using dynamic quantization:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prepare quantization configuration - We can use default dynamic quantization configuration with &lt;strong&gt;ipex.quantization.default_dynamic_qconfig&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Prepare the FP32 model by using the** ipex.quantization.prepare **method (provide the input parameters such as FP32 model to quantize, the prepared configuration, example inputs and information if the quantization should be in place).&lt;/li&gt;
  &lt;li&gt;Convert the model from FP32 to INT8 - Use &lt;strong&gt;ipex.quantization.convert&lt;/strong&gt; method for conversion. The input model will be the model prepared in step 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We also encourage you to check out the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&quot;&gt;Intel® Neural Compressor&lt;/a&gt; tool that automates popular model-compression technologies such as quantization, pruning, and knowledge distillation across multiple &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;deep learning frameworks&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;code-sample&quot;&gt;Code Sample&lt;/h2&gt;

&lt;p&gt;The following steps are implemented in the &lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/blob/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Interactive_Chat_Quantization/IntelPytorch_Interactive_Chat_Quantization.ipynb&quot;&gt;code sample&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Load model and tokenizer:&lt;/strong&gt; &lt;a href=&quot;http://huggingface.co/docs/transformers/index&quot;&gt;Transformers library&lt;/a&gt; (check out &lt;a href=&quot;http://github.com/intel/intel-extension-for-transformers&quot;&gt;Intel® Extension for Transformers&lt;/a&gt;) and &lt;a href=&quot;http://huggingface.co/docs/transformers/model_doc/auto&quot;&gt;Auto Classes available in the Hugging Face Main Classes&lt;/a&gt; are used in this step. These allow us to automatically find the relevant model by the given name. It also allows to easily change the model without major changes in the code on the developer’s side as shown below:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;The model parameter is specified as an input for the tokenizer, and model initialization is just the path to the pre-trained DialoGPT model. In this sample, we are using ‘microsoft/DialoGPT-large.’ If you have limited resources, you can use ‘microsoft/DialoGPT-medium’ or ‘microsoft/DialoGPT-small’ models and receive comparable results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Perform dynamic quantization of the model:&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;Create the configuration using the default dynamic quantization configuration from Intel Extension for PyTorch library.&lt;/li&gt;
      &lt;li&gt;Prepare the model.&lt;/li&gt;
      &lt;li&gt;Convert the model from FP32 to INT8. &lt;br /&gt;
The steps are explained in detail in the above section.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Response generation:&lt;/strong&gt; The first step in response generation is to encode the input sentence as shown in the code below:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;new_input_ids = tokenizer.encode(input(&quot;&amp;gt;&amp;gt; You:&quot;) + tokenizer.eos_token, return_tensors='pt')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;In this sample, we want our model to save history, so we are adding input sentences in the form of tokens to the chat history:&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_round &amp;gt; 0 else new_input_ids
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;The text generation can be done by the model.generate function, where we can specify all important parameters like saved chat history, length of the response in tokens, and usage of both Top-K and Top-p sampling.&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chat_history_ids = model.generate(bot_input_ids, do_sample=True, max_length=2000, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;The last step is to decode and print the response:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Preparation for interactive conversation:&lt;/strong&gt; After response generation, the last step is to add interaction. This can be done by using a simple for loop. Based on the initialized tokenizer, model, and empty chat history, responses are generated for a number of rounds:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for chat_round in range(n):
chat_history_ids = generate_response(
tokenizer,
model,
chat_round,
chat_history_ids
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;An example of interactive chat generation will look like the one shown in the picture below.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f3-dialogpt-interaction.png&quot; alt=&quot;An example of interactive chat generation&quot; style=&quot;width:100%; max-width: 981px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Get started with interactive chat-generation models using Intel Extension for PyTorch and DialoGPT. Download and try the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html&quot;&gt;Intel AI Analytics Toolkit&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;Intel Extension for PyTorch&lt;/a&gt; for yourself to build various end-to-end AI applications.&lt;/p&gt;

&lt;p&gt;We encourage you to also check out and incorporate Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;AI/ML Framework optimizations&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;end-to-end portfolio of tools&lt;/a&gt; into your AI workflow and learn about the unified, open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; programming model that forms the foundation of Intel’s &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI Software Portfolio&lt;/a&gt; to help you prepare, build, deploy, and scale your AI solutions.&lt;/p&gt;

&lt;p&gt;For more details about the new 4th Gen Intel® Xeon® Scalable processors, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;Intel’s AI Solution Platform portal&lt;/a&gt; where you can learn how Intel is empowering developers to run end-to-end AI pipelines on these powerful CPUs.&lt;/p&gt;

&lt;h3 id=&quot;useful-resources&quot;&gt;Useful resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;Intel AI Developer Tools and resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI unified programming model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;Official documentation - PyTorch Optimizations from Intel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/&quot;&gt;Intel® Extension for PyTorch - Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;explore-more-ai-code-samples&quot;&gt;Explore more AI code samples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification&quot;&gt;Language Identification: Building an End-to-End AI Solution using PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;Optimize PyTorch Models using Intel® Extension for PyTorch (IPEX) Quantization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPyTorch_TrainingOptimizations_AMX_BF16&quot;&gt;PyTorch Training Optimizations with Advanced Matrix Extensions Bfloat16&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/code-samples.html&quot;&gt;See all code samples&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">The focus on interactive chat-generation (or conversational response-generation) models has greatly increased in the past several months. Conversational response-generation models such as ChatGPT and Google Bard have taken the AI world by storm. The purpose of interactive chat generation is to answer various questions posed by humans, and these AI based models use natural language processing (NLP) to generate conversations almost indistinguishable from those generated by humans.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing PyTorch Docathon H2 2023</title>
      <link href="https://pytorch.org/blog/announcing-docathon-h2-2023/" rel="alternate" type="text/html" title="Announcing PyTorch Docathon H2 2023" />
      <published>2023-10-02T00:00:00-07:00</published>
      <updated>2023-10-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/announcing-docathon-h2-2023</id>
      <content type="html" xml:base="https://pytorch.org/blog/announcing-docathon-h2-2023/">&lt;p&gt;We are excited to announce that we will be holding a Docathon for PyTorch on November 1, 2023! This event is an opportunity for our community to come together and improve the quality of our documentation.&lt;/p&gt;

&lt;p&gt;During the Docathon, we will focus on updating and improving existing content, as well as adding new tutorials and docstrings. We encourage all members of the community to participate and contribute their expertise to make our documentation even better. This is a great opportunity to learn and collaborate together.&lt;/p&gt;

&lt;p&gt;Check out our previous docathon success story &lt;a href=&quot;https://pytorch.org/blog/docathon-h1-2023-wrap-up/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-participate&quot;&gt;Why Participate&lt;/h2&gt;

&lt;p&gt;One of the best things about the Docathon is that you can make a tangible, positive impact on the quality of documentation in real time. This collaborative event brings together diverse team members from various companies, backgrounds, and roles, united to work towards a common goal. This event not only fosters team building and knowledge sharing but also presents an opportunity for individuals to acquire new skills, such as writing, editing, and utilizing documentation tools. Participating in a docathon can be particularly beneficial for team members who may lack experience in these areas.&lt;/p&gt;

&lt;p&gt;And of course all participants will be recognized for their contributions. Top participants will receive special awards.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;Event Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Nov 1: Kick-off&lt;/li&gt;
  &lt;li&gt;Nov 1- Nov 12:  Submissions and Feedback&lt;/li&gt;
  &lt;li&gt;Nov 13 - Nov 15: Final Reviews&lt;/li&gt;
  &lt;li&gt;Nov 15: Winner Announcements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Details for the Docathon to be announced at the kick-off call on November 1.&lt;/p&gt;

&lt;p&gt;To participate in the Docathon and receive updates about the event, register here: &lt;a href=&quot;https://community.linuxfoundation.org/events/details/lfhq-pytorch-foundation-presents-fall-pytorch-docathon-nov-1st-rsvp/&quot;&gt;RSVP&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are excited to see the improvements that will come out of this Docathon, and we look forward to your participation!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce that we will be holding a Docathon for PyTorch on November 1, 2023! This event is an opportunity for our community to come together and improve the quality of our documentation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond</title>
      <link href="https://pytorch.org/blog/inside-the-matrix/" rel="alternate" type="text/html" title="Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond" />
      <published>2023-09-25T00:00:00-07:00</published>
      <updated>2023-09-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/inside-the-matrix</id>
      <content type="html" xml:base="https://pytorch.org/blog/inside-the-matrix/">&lt;p&gt;&lt;em&gt;Use 3D to visualize matrix multiplication expressions, attention heads with real weights, and more.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Matrix multiplications (matmuls) are the building blocks of today’s ML models. This note presents &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;mm&lt;/a&gt;, a visualization tool for matmuls and compositions of matmuls.&lt;/p&gt;

&lt;p&gt;Because mm uses all three spatial dimensions, it helps build intuition and spark ideas with less cognitive overhead than the usual squares-on-paper idioms, especially (though not only) for visual/spatial thinkers.&lt;/p&gt;

&lt;p&gt;And with three dimensions available for &lt;em&gt;composing&lt;/em&gt; matmuls, along with the ability to load trained weights, we can visualize big, compound expressions like attention heads and observe how they actually behave, using im.&lt;/p&gt;

&lt;p&gt;mm is fully interactive, runs &lt;a href=&quot;https://bhosmer.github.io/mm/&quot;&gt;in the browser&lt;/a&gt; or &lt;a href=&quot;https://colab.research.google.com/drive/1wZIoU20eRWKtRNCW7e5Iugm3MhfaE1f7&quot;&gt;notebook iframes&lt;/a&gt; and keeps its complete state in the URL, so links are shareable sessions (the screenshots and videos in this note all have links that open the visualizations in the tool). This &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;reference guide&lt;/a&gt; describes all of the available functionality.&lt;/p&gt;

&lt;p&gt;We’ll first introduce the visualization approach, build intuition by visualizing some simple matmuls and expressions, then dive into some more extended examples:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Pitch&lt;/strong&gt; - why is this way of visualizing better?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Warmup - animations&lt;/strong&gt; - watching the canonical matmul decompositions in action&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Warmup - expressions&lt;/strong&gt; - a quick tour of some fundamental expression building blocks&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inside an attention head&lt;/strong&gt; - an in-depth look at the structure, values and computation behavior of a couple of attention heads from GPT2 via &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;NanoGPT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Parallelizing attention&lt;/strong&gt; - visualizing attention head parallelization with examples from the recent &lt;a href=&quot;https://arxiv.org/pdf/2305.19370.pdf&quot;&gt;Blockwise Parallel Transformer&lt;/a&gt; paper&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sizes in an attention layer&lt;/strong&gt; - what do the MHA and FFA halves of an attention layer look like together, when we visualize a whole layer as a single structure? How does the picture change during autoregressive decoding?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt; - a visual explanation of this elaboration of the attention head architecture&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Wrapup&lt;/strong&gt; - next steps and call for feedback&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-pitch&quot;&gt;1 Pitch&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;mm&lt;/a&gt;’s visualization approach is based on the premise that &lt;em&gt;matrix multiplication is fundamentally a three-dimensional operation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In other words this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/matmul3.jpg&quot; alt=&quot;matrix multiplication is fundamentally a three-dimensional operation&quot; style=&quot;width:100%; max-width: 478px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;is a sheet of paper trying to be this (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A20%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22none%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A3%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22closed%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A0.8227%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-48.763575165818956%2C%22y%22%3A43.72517618222101%2C%22z%22%3A33.70077275818966%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/initial.jpg&quot; alt=&quot;wrap the matmul around a cube&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we wrap the matmul around a cube this way, the correct relationships between argument shapes, result shape and shared dimensions all fall into place.&lt;/p&gt;

&lt;p&gt;Now the computation makes &lt;em&gt;geometric sense&lt;/em&gt;: each location &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i, j&lt;/code&gt; in the result matrix anchors a vector running along the depth dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; in the cube’s interior, where the horizontal plane extending from row &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; and a vertical plane extending from column &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; intersect. Along this vector, pairs of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(i, k)&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(k, j)&lt;/code&gt; elements from the left and right arguments meet and are multiplied, and the resulting products are summed along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; and the result is deposited in location &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i, j&lt;/code&gt; of the result.&lt;/p&gt;

&lt;p&gt;(Jumping ahead momentarily, &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A48%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22dotprod%20(row%20major)%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;here’s an animation&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;This is the &lt;em&gt;intuitive&lt;/em&gt; meaning of matrix multiplication:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;project&lt;/strong&gt; two orthogonal matrices into the interior of a cube&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;multiply&lt;/strong&gt; the pair of values at each intersection, forming a grid of products&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;sum&lt;/strong&gt; along the third orthogonal dimension to produce a result matrix.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For orientation, the tool displays an arrow in the cube’s interior that points towards the result matrix, with a blue vane coming from the left argument and a &lt;strong&gt;r&lt;/strong&gt;ed vane coming from the &lt;strong&gt;r&lt;/strong&gt;ight argument. The tool also displays white guidelines to indicate the row axis of each matrix, though they’re faint in this screenshot.&lt;/p&gt;

&lt;p&gt;The layout constraints are straightforward:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;left argument and result must be adjoined along their shared &lt;strong&gt;height&lt;/strong&gt; (i) dimension&lt;/li&gt;
  &lt;li&gt;right argument and result must be adjoined along their shared &lt;strong&gt;width&lt;/strong&gt; (j) dimension&lt;/li&gt;
  &lt;li&gt;left and right arguments must be adjoined along their shared (left width/right height) dimension, which becomes the matmul’s &lt;strong&gt;depth&lt;/strong&gt; (k) dimension&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This geometry gives us a solid foundation for visualizing all the standard matmul decompositions, and an intuitive basis for exploring nontrivially complex &lt;em&gt;compositions&lt;/em&gt; of matmuls, as we’ll see below.&lt;/p&gt;

&lt;h2 id=&quot;2-warmup---animations&quot;&gt;2 Warmup - animations&lt;/h2&gt;

&lt;p&gt;Before diving into some more complex examples, we’ll run through a few intuition builders to get a feel for how things look and feel in this style of visualization.&lt;/p&gt;

&lt;h3 id=&quot;2a-dot-product&quot;&gt;2a Dot product&lt;/h3&gt;

&lt;p&gt;First, the canonical algorithm - computing each result element by taking the dot product of the corresponding left row and right column. What we see in the animation is the sweep of multiplied value vectors through the cube’s interior, each delivering a summed result at the corresponding position.&lt;/p&gt;

&lt;p&gt;Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; has blocks of rows filled with 1 (blue) or -1 (red); &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; has column blocks filled similarly. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; is 24 here, so the result matrix (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L @ R&lt;/code&gt;) has blue values of 24 and red values of -24 (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A48%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22dotprod%20(row%20major)%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt; - long click or control-click to inspect values):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/dotprod1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h3 id=&quot;2b-matrix-vector-products&quot;&gt;2b Matrix-vector products&lt;/h3&gt;

&lt;p&gt;A matmul decomposed into matrix-vector products looks like a vertical plane (a product of the left argument with each column of the right argument) painting columns onto the result as it sweeps horizontally through the cube’s interior (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A12%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22mvprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/mvprod1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;Observing the intermediate values of a decomposition can be quite interesting, even in simple examples.&lt;/p&gt;

&lt;p&gt;For instance, note the prominent vertical patterns in the intermediate matrix-vector products when we use randomly initialized arguments- reflecting the fact that each intermediate is a column-scaled replica of the left argument (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(i%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(j%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A6%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22mvprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22open%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22local%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.14559417241423%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768186%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/mvprod2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h3 id=&quot;2c-vector-matrix-products&quot;&gt;2c Vector-matrix products&lt;/h3&gt;

&lt;p&gt;A matmul decomposed into vector-matrix products looks like a horizontal plane painting rows onto the result as it descends through the cube’s interior (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A12%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vmprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vmprod_check.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;Switching to randomly initialized arguments, we see patterns analogous to those we saw with matrix-vector products - only this time the patterns are horizontal, corresponding to the fact that each intermediate vector-matrix product is a row-scaled replica of the right argument.&lt;/p&gt;

&lt;p&gt;When thinking about how matmuls express the rank and structure of their arguments, it’s useful to envision both of these patterns happening simultaneously in the computation (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(i%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(j%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A6%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vmprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22open%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22local%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.14559417241423%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768186%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vmprod3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;Here’s one more intuition builder using vector-matrix products, showing how the identity matrix functions exactly like a mirror set at a 45deg angle to both its counterargument and the result (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A24%2C%22init%22%3A%22eye%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(i%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22row%20major%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(j%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A12%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vmprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22open%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22local%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-50.560896320538845%2C%22y%22%3A45.336792719337595%2C%22z%22%3A34.94291121097398%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vmprod_id.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h3 id=&quot;2d-summed-outer-products&quot;&gt;2d Summed outer products&lt;/h3&gt;

&lt;p&gt;The third planar decomposition is along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; axis, computing the matmul result by a pointwise summation of vector outer products. Here we see the plane of outer products sweeping the cube “from back to front”, accumulating into the result (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A12%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vvprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vvprod_check.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;Using randomly initialized matrices with this decomposition, we can see not just values but &lt;em&gt;rank&lt;/em&gt; accumulate in the result, as each rank-1 outer product is added to it.&lt;/p&gt;

&lt;p&gt;Among other things this builds intuition for why “low-rank factorization” - i.e. approximating a matrix by constructing a matmul whose arguments are small in the depth dimension - works best when the matrix being approximated is low rank. &lt;a href=&quot;https://arxiv.org/pdf/2106.09685.pdf&quot;&gt;LoRA&lt;/a&gt; in a later section (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(i%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22closed%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(j%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22closed%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A6%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vvprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22open%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22local%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.14559417241423%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768186%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vvprod_random_fast.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h2 id=&quot;3-warmup---expressions&quot;&gt;3 Warmup - expressions&lt;/h2&gt;

&lt;p&gt;How can we extend this visualization approach to &lt;em&gt;compositions&lt;/em&gt; of matmuls? Our examples so far have all visualized a single matmul &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L @ R&lt;/code&gt; of some matrices &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; - what about when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; and/or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; are themselves matmuls, and so on transitively?&lt;/p&gt;

&lt;p&gt;It turns out we can extend the approach nicely to compound expressions. The key rules are simple: the subexpression (child) matmul is another cube, subject to the same layout constraints as the parent, and the result face of the child is &lt;em&gt;simultaneously&lt;/em&gt; the corresponding argument face of the parent, like a covalently shared electron.&lt;/p&gt;

&lt;p&gt;Within these constraints, we’re free to arrange the faces of a child matmul however we like. Here we use the tool’s default scheme, which generates alternating convex and concave cubes - this layout works well in practice to maximize use of space and minimize occlusion. (Layouts are completely customizable, however - see the &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;reference&lt;/a&gt; for details.)&lt;/p&gt;

&lt;p&gt;In this section we’ll visualize some of the key building blocks we find in ML models, to gain fluency in the visual idiom and to see what intuitions even simple examples can give us.&lt;/p&gt;

&lt;h3 id=&quot;3a-left-associative-expressions&quot;&gt;3a Left-associative expressions&lt;/h3&gt;

&lt;p&gt;We’ll look at two expressions of the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(A @ B) @ C&lt;/code&gt;, each with its own distinctive shape and character. (Note: mm adheres to the convention that matrix multiplication is left-associative and writes this simply as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A @ B @ C&lt;/code&gt;.)&lt;/p&gt;

&lt;p&gt;First we’ll give &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A @ B @ C&lt;/code&gt; the characteristic FFN shape, in which the “hidden dimension” is wider than the “input” or “output” dimensions. (Concretely in the context of this example, this means that the width of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is greater than the widths of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt;.)&lt;/p&gt;

&lt;p&gt;As in the single matmul examples, the floating arrows point towards the result matrix, blue vane coming from the left argument and red vane from right argument (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20C&amp;amp;1=A%20%40%20B%20%40%20C&amp;amp;2=none&amp;amp;12=closed&amp;amp;64=true&amp;amp;3.1=A%20%40%20B&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.12=open&amp;amp;3.2=none&amp;amp;13.1=A&amp;amp;13.4=false&amp;amp;13.5=64&amp;amp;13.6=32&amp;amp;13.7=expr&amp;amp;13.8=&amp;amp;13.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.12=open&amp;amp;14.1=B&amp;amp;14.4=false&amp;amp;14.5=32&amp;amp;14.6=96&amp;amp;14.7=row%20major&amp;amp;14.8=&amp;amp;14.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;14.9=-1&amp;amp;14.10=1&amp;amp;14.11=0&amp;amp;14.12=open&amp;amp;15.16=inherit&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.23=1&amp;amp;24.1=C&amp;amp;24.4=false&amp;amp;24.5=96&amp;amp;24.6=32&amp;amp;24.7=col%20major&amp;amp;24.8=&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.12=open&amp;amp;25.26=none&amp;amp;25.27=12&amp;amp;25.28=false&amp;amp;25.16=none&amp;amp;25.29=0&amp;amp;25.12=closed&amp;amp;30.31=1&amp;amp;30.32=1&amp;amp;30.23=1&amp;amp;33.34=blocks&amp;amp;33.35=5&amp;amp;33.36=0&amp;amp;33.37=1&amp;amp;33.38=0&amp;amp;33.18=negative&amp;amp;33.19=left&amp;amp;33.20=top&amp;amp;33.21=front&amp;amp;33.12=closed&amp;amp;39.40=6&amp;amp;39.41=true&amp;amp;39.42=2&amp;amp;39.43=1&amp;amp;39.44=0.5&amp;amp;39.45=0.5&amp;amp;39.46=10&amp;amp;39.47=false&amp;amp;39.48=false&amp;amp;39.12=open&amp;amp;49.50=semilocal&amp;amp;49.51=0.2&amp;amp;49.52=0.4&amp;amp;49.53=0.6&amp;amp;49.54=1.25&amp;amp;49.55=0.77&amp;amp;49.56=0.74&amp;amp;49.57=0.04&amp;amp;49.12=open&amp;amp;58.8=&amp;amp;59.60=-102.42301073851515&amp;amp;59.61=96.27580041479706&amp;amp;59.62=112.34410815468306&amp;amp;63.60=-4.617417891034972&amp;amp;63.61=-3.695553245058398&amp;amp;63.62=-1.8863985145585351&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;left.left=13&amp;amp;left.right=14&amp;amp;left.anim=15&amp;amp;alg=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.block=22&amp;amp;k%20blocks=23&amp;amp;right=24&amp;amp;anim=25&amp;amp;fuse=26&amp;amp;speed=27&amp;amp;hide%20inputs=28&amp;amp;spin=29&amp;amp;block=30&amp;amp;i%20blocks=31&amp;amp;j%20blocks=32&amp;amp;layout=33&amp;amp;scheme=34&amp;amp;gap=35&amp;amp;scatter=36&amp;amp;molecule=37&amp;amp;blast=38&amp;amp;deco=39&amp;amp;legends=40&amp;amp;shape=41&amp;amp;spotlight=42&amp;amp;row%20guides=43&amp;amp;flow%20guides=44&amp;amp;lens%20size=45&amp;amp;magnification=46&amp;amp;interior%20spotlight=47&amp;amp;axes=48&amp;amp;viz=49&amp;amp;sensitivity=50&amp;amp;min%20size=51&amp;amp;min%20light=52&amp;amp;max%20light=53&amp;amp;elem%20scale=54&amp;amp;zero%20hue=55&amp;amp;hue%20gap=56&amp;amp;hue%20spread=57&amp;amp;diag=58&amp;amp;cam=59&amp;amp;x=60&amp;amp;y=61&amp;amp;z=62&amp;amp;cam.target=63&amp;amp;compress=64&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/la2still.jpg&quot; alt=&quot;As in the single matmul examples, the floating arrows point towards the result matrix, blue vane coming from the left argument and red vane from right argument&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next we’ll visualize &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A @ B @ C&lt;/code&gt; with the width of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; &lt;em&gt;narrower&lt;/em&gt; than that of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt;, giving it a bottleneck or “autoencoder” shape (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20C&amp;amp;1=A%20%40%20B%20%40%20C&amp;amp;2=none&amp;amp;12=closed&amp;amp;64=true&amp;amp;3.1=A%20%40%20B&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.12=open&amp;amp;3.2=none&amp;amp;13.1=A&amp;amp;13.4=false&amp;amp;13.5=64&amp;amp;13.6=96&amp;amp;13.7=expr&amp;amp;13.8=&amp;amp;13.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.12=open&amp;amp;14.1=B&amp;amp;14.4=false&amp;amp;14.5=96&amp;amp;14.6=32&amp;amp;14.7=row%20major&amp;amp;14.8=&amp;amp;14.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;14.9=-1&amp;amp;14.10=1&amp;amp;14.11=0&amp;amp;14.12=open&amp;amp;15.16=inherit&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.23=1&amp;amp;24.1=C&amp;amp;24.4=false&amp;amp;24.5=32&amp;amp;24.6=96&amp;amp;24.7=col%20major&amp;amp;24.8=&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.12=open&amp;amp;25.26=none&amp;amp;25.27=12&amp;amp;25.28=false&amp;amp;25.16=none&amp;amp;25.29=0&amp;amp;25.12=closed&amp;amp;30.31=1&amp;amp;30.32=1&amp;amp;30.23=1&amp;amp;33.34=blocks&amp;amp;33.35=5&amp;amp;33.36=0&amp;amp;33.37=1&amp;amp;33.38=0&amp;amp;33.18=negative&amp;amp;33.19=left&amp;amp;33.20=top&amp;amp;33.21=front&amp;amp;33.12=closed&amp;amp;39.40=6&amp;amp;39.41=true&amp;amp;39.42=2&amp;amp;39.43=1&amp;amp;39.44=0.5&amp;amp;39.45=0.5&amp;amp;39.46=10&amp;amp;39.47=false&amp;amp;39.48=false&amp;amp;39.12=open&amp;amp;49.50=semilocal&amp;amp;49.51=0.2&amp;amp;49.52=0.4&amp;amp;49.53=0.6&amp;amp;49.54=1.25&amp;amp;49.55=0.77&amp;amp;49.56=0.74&amp;amp;49.57=0.04&amp;amp;49.12=open&amp;amp;58.8=&amp;amp;59.60=-125.71162036288077&amp;amp;59.61=101.84279252909485&amp;amp;59.62=122.50425255743914&amp;amp;63.60=-14.817097084822203&amp;amp;63.61=-9.723209466639396&amp;amp;63.62=-5.4699873376955646&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;left.left=13&amp;amp;left.right=14&amp;amp;left.anim=15&amp;amp;alg=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.block=22&amp;amp;k%20blocks=23&amp;amp;right=24&amp;amp;anim=25&amp;amp;fuse=26&amp;amp;speed=27&amp;amp;hide%20inputs=28&amp;amp;spin=29&amp;amp;block=30&amp;amp;i%20blocks=31&amp;amp;j%20blocks=32&amp;amp;layout=33&amp;amp;scheme=34&amp;amp;gap=35&amp;amp;scatter=36&amp;amp;molecule=37&amp;amp;blast=38&amp;amp;deco=39&amp;amp;legends=40&amp;amp;shape=41&amp;amp;spotlight=42&amp;amp;row%20guides=43&amp;amp;flow%20guides=44&amp;amp;lens%20size=45&amp;amp;magnification=46&amp;amp;interior%20spotlight=47&amp;amp;axes=48&amp;amp;viz=49&amp;amp;sensitivity=50&amp;amp;min%20size=51&amp;amp;min%20light=52&amp;amp;max%20light=53&amp;amp;elem%20scale=54&amp;amp;zero%20hue=55&amp;amp;hue%20gap=56&amp;amp;hue%20spread=57&amp;amp;diag=58&amp;amp;cam=59&amp;amp;x=60&amp;amp;y=61&amp;amp;z=62&amp;amp;cam.target=63&amp;amp;compress=64&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/lacontract.jpg&quot; alt=&quot;visualize A @ B @ C with the width of B narrower than that of A or C&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This pattern of alternating convex and concave blocks extends to chains of arbitrary length: for example this multilayer bottleneck (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;1=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;2=none&amp;amp;23=closed&amp;amp;63=true&amp;amp;3.2=none&amp;amp;4.5=inherit&amp;amp;6.7=1&amp;amp;8.9=positive&amp;amp;8.10=left&amp;amp;8.11=bottom&amp;amp;8.12=back&amp;amp;13.0=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;13.1=A%20%40%20B%20%40%20C&amp;amp;13.2=none&amp;amp;14.1=A%20%40%20B&amp;amp;14.15=true&amp;amp;14.16=32&amp;amp;14.17=32&amp;amp;14.18=row%20major&amp;amp;14.19=&amp;amp;14.20=-1&amp;amp;14.21=1&amp;amp;14.22=0&amp;amp;14.23=open&amp;amp;14.2=none&amp;amp;24.1=A&amp;amp;24.15=false&amp;amp;24.16=64&amp;amp;24.17=96&amp;amp;24.18=expr&amp;amp;24.19=&amp;amp;24.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.20=-1&amp;amp;24.21=1&amp;amp;24.22=0&amp;amp;24.23=open&amp;amp;25.1=B&amp;amp;25.15=false&amp;amp;25.16=96&amp;amp;25.17=64&amp;amp;25.18=row%20major&amp;amp;25.19=&amp;amp;25.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;25.20=-1&amp;amp;25.21=1&amp;amp;25.22=0&amp;amp;25.23=open&amp;amp;26.5=inherit&amp;amp;27.9=positive&amp;amp;27.10=left&amp;amp;27.11=bottom&amp;amp;27.12=back&amp;amp;28.7=1&amp;amp;29.1=C&amp;amp;29.15=false&amp;amp;29.16=64&amp;amp;29.17=32&amp;amp;29.18=col%20major&amp;amp;29.19=&amp;amp;29.20=-1&amp;amp;29.21=1&amp;amp;29.22=0&amp;amp;29.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;29.23=open&amp;amp;30.31=none&amp;amp;30.32=12&amp;amp;30.33=false&amp;amp;30.5=none&amp;amp;30.34=0&amp;amp;30.23=closed&amp;amp;35.36=1&amp;amp;35.7=1&amp;amp;37.9=negative&amp;amp;37.10=left&amp;amp;37.11=top&amp;amp;37.12=front&amp;amp;38.39=6&amp;amp;38.40=true&amp;amp;38.41=2&amp;amp;38.42=1&amp;amp;38.43=0.5&amp;amp;38.44=0.5&amp;amp;38.45=10&amp;amp;38.46=false&amp;amp;38.47=false&amp;amp;38.23=open&amp;amp;48.49=semilocal&amp;amp;48.50=0.2&amp;amp;48.51=0.4&amp;amp;48.52=0.6&amp;amp;48.53=1.25&amp;amp;48.54=0.77&amp;amp;48.55=0.74&amp;amp;48.56=0.04&amp;amp;48.23=open&amp;amp;57.19=&amp;amp;58.59=-125.71162036288077&amp;amp;58.60=101.84279252909485&amp;amp;58.61=122.50425255743914&amp;amp;62.59=-14.817097084822203&amp;amp;62.60=-9.723209466639396&amp;amp;62.61=-5.4699873376955646&amp;amp;13.23=open&amp;amp;13.63=true&amp;amp;13.15=true&amp;amp;64.1=D&amp;amp;64.15=false&amp;amp;64.16=32&amp;amp;64.17=64&amp;amp;64.18=col%20major&amp;amp;64.19=&amp;amp;64.20=-1&amp;amp;64.21=1&amp;amp;64.22=0&amp;amp;64.0=&amp;amp;64.23=open&amp;amp;3.1=A%20%40%20B%20%40%20C%20%40%20D&amp;amp;3.15=true&amp;amp;65.1=E&amp;amp;65.15=false&amp;amp;65.16=64&amp;amp;65.17=96&amp;amp;65.18=col%20major&amp;amp;65.19=&amp;amp;65.20=-1&amp;amp;65.21=1&amp;amp;65.22=0&amp;amp;65.0=&amp;amp;66.31=none&amp;amp;66.32=12&amp;amp;66.33=false&amp;amp;66.5=none&amp;amp;66.34=0&amp;amp;66.23=closed&amp;amp;67.36=1&amp;amp;67.68=1&amp;amp;67.7=1&amp;amp;69.70=blocks&amp;amp;69.71=5&amp;amp;69.72=0&amp;amp;69.73=1&amp;amp;69.74=0&amp;amp;69.9=negative&amp;amp;69.10=left&amp;amp;69.11=top&amp;amp;69.12=front&amp;amp;69.23=closed&amp;amp;75.39=5.28&amp;amp;75.40=true&amp;amp;75.41=2&amp;amp;75.42=1&amp;amp;75.43=0.5&amp;amp;75.44=0.5&amp;amp;75.45=10&amp;amp;75.46=false&amp;amp;75.47=false&amp;amp;75.23=open&amp;amp;76.49=semilocal&amp;amp;76.50=0.2&amp;amp;76.51=0.4&amp;amp;76.52=0.6&amp;amp;76.53=1.25&amp;amp;76.54=0.77&amp;amp;76.55=0.74&amp;amp;76.56=0.04&amp;amp;76.23=open&amp;amp;77.19=&amp;amp;78.59=-163.23429720087873&amp;amp;78.60=132.20892347209139&amp;amp;78.61=159.04014894666057&amp;amp;79.59=-14.817097084822203&amp;amp;79.60=-9.723209466639396&amp;amp;79.61=-5.4699873376955646&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;left.anim=4&amp;amp;alg=5&amp;amp;left.block=6&amp;amp;k%20blocks=7&amp;amp;left.layout=8&amp;amp;polarity=9&amp;amp;left%20placement=10&amp;amp;right%20placement=11&amp;amp;result%20placement=12&amp;amp;left.left=13&amp;amp;left.left.left=14&amp;amp;matmul=15&amp;amp;h=16&amp;amp;w=17&amp;amp;init=18&amp;amp;url=19&amp;amp;min=20&amp;amp;max=21&amp;amp;dropout=22&amp;amp;folder=23&amp;amp;left.left.left.left=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.left.block=28&amp;amp;left.left.right=29&amp;amp;left.left.anim=30&amp;amp;fuse=31&amp;amp;speed=32&amp;amp;hide%20inputs=33&amp;amp;spin=34&amp;amp;left.left.block=35&amp;amp;i%20blocks=36&amp;amp;left.left.layout=37&amp;amp;left.left.deco=38&amp;amp;legends=39&amp;amp;shape=40&amp;amp;spotlight=41&amp;amp;row%20guides=42&amp;amp;flow%20guides=43&amp;amp;lens%20size=44&amp;amp;magnification=45&amp;amp;interior%20spotlight=46&amp;amp;axes=47&amp;amp;left.left.viz=48&amp;amp;sensitivity=49&amp;amp;min%20size=50&amp;amp;min%20light=51&amp;amp;max%20light=52&amp;amp;elem%20scale=53&amp;amp;zero%20hue=54&amp;amp;hue%20gap=55&amp;amp;hue%20spread=56&amp;amp;left.left.diag=57&amp;amp;left.left.cam=58&amp;amp;x=59&amp;amp;y=60&amp;amp;z=61&amp;amp;left.left.cam.target=62&amp;amp;compress=63&amp;amp;left.right=64&amp;amp;right=65&amp;amp;anim=66&amp;amp;block=67&amp;amp;j%20blocks=68&amp;amp;layout=69&amp;amp;scheme=70&amp;amp;gap=71&amp;amp;scatter=72&amp;amp;molecule=73&amp;amp;blast=74&amp;amp;deco=75&amp;amp;viz=76&amp;amp;diag=77&amp;amp;cam=78&amp;amp;cam.target=79&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/nlayerbottleneck.jpg&quot; alt=&quot;pattern of alternating convex and concave blocks extends to chains of arbitrary length&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3b-right-associative-expressions&quot;&gt;3b Right associative expressions&lt;/h3&gt;

&lt;p&gt;Next we’ll visualize a right-associative expression &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A @ (B @ C)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In the same way left-associative expressions extend horizontally - sprouting from the left argument of the root expression, so to speak - right-associative chains extend vertically, sprouting from the root’s right argument.&lt;/p&gt;

&lt;p&gt;One sometimes sees an MLP formulated right-associatively, i.e. with columnar input on the right and weight layers running right to left. Using the matrices from the 2-layer FFN example pictured above - suitably transposed - here’s what that looks like, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; now playing the role of the input, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; the first layer and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; the second layer (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20(B%20%40%20C)&amp;amp;1=A%20%40%20(B%20%40%20C)&amp;amp;2=none&amp;amp;12=closed&amp;amp;64=true&amp;amp;3.1=A&amp;amp;3.4=false&amp;amp;3.5=32&amp;amp;3.6=96&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;3.12=open&amp;amp;13.1=B%20%40%20C&amp;amp;13.4=true&amp;amp;13.5=32&amp;amp;13.6=32&amp;amp;13.7=col%20major&amp;amp;13.8=&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.2=none&amp;amp;14.15=inherit&amp;amp;16.17=1&amp;amp;18.19=positive&amp;amp;18.20=right&amp;amp;18.21=top&amp;amp;18.22=back&amp;amp;23.1=B&amp;amp;23.4=false&amp;amp;23.5=96&amp;amp;23.6=32&amp;amp;23.7=col%20major&amp;amp;23.8=&amp;amp;23.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.12=open&amp;amp;24.1=C&amp;amp;24.4=false&amp;amp;24.5=32&amp;amp;24.6=64&amp;amp;24.7=expr&amp;amp;24.8=&amp;amp;24.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.12=open&amp;amp;13.12=open&amp;amp;25.26=none&amp;amp;25.27=12&amp;amp;25.28=false&amp;amp;25.15=none&amp;amp;25.29=0&amp;amp;25.12=closed&amp;amp;30.31=1&amp;amp;30.32=1&amp;amp;30.17=1&amp;amp;33.34=blocks&amp;amp;33.35=5&amp;amp;33.36=0&amp;amp;33.37=1&amp;amp;33.38=0&amp;amp;33.19=negative&amp;amp;33.20=left&amp;amp;33.21=top&amp;amp;33.22=front&amp;amp;33.12=closed&amp;amp;39.40=6&amp;amp;39.41=true&amp;amp;39.42=2&amp;amp;39.43=1&amp;amp;39.44=0.5&amp;amp;39.45=0.5&amp;amp;39.46=10&amp;amp;39.47=false&amp;amp;39.48=false&amp;amp;39.12=closed&amp;amp;49.50=semilocal&amp;amp;49.51=0.2&amp;amp;49.52=0.4&amp;amp;49.53=0.6&amp;amp;49.54=1.25&amp;amp;49.55=0.77&amp;amp;49.56=0.74&amp;amp;49.57=0.04&amp;amp;49.12=closed&amp;amp;58.8=&amp;amp;58.12=open&amp;amp;59.60=-105.78213185291946&amp;amp;59.61=96.67420268229331&amp;amp;59.62=113.6419504179439&amp;amp;63.60=-4.617417891034972&amp;amp;63.61=-3.695553245058398&amp;amp;63.62=-1.8863985145585351&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;right=13&amp;amp;right.anim=14&amp;amp;alg=15&amp;amp;right.block=16&amp;amp;k%20blocks=17&amp;amp;right.layout=18&amp;amp;polarity=19&amp;amp;left%20placement=20&amp;amp;right%20placement=21&amp;amp;result%20placement=22&amp;amp;right.left=23&amp;amp;right.right=24&amp;amp;anim=25&amp;amp;fuse=26&amp;amp;speed=27&amp;amp;hide%20inputs=28&amp;amp;spin=29&amp;amp;block=30&amp;amp;i%20blocks=31&amp;amp;j%20blocks=32&amp;amp;layout=33&amp;amp;scheme=34&amp;amp;gap=35&amp;amp;scatter=36&amp;amp;molecule=37&amp;amp;blast=38&amp;amp;deco=39&amp;amp;legends=40&amp;amp;shape=41&amp;amp;spotlight=42&amp;amp;row%20guides=43&amp;amp;flow%20guides=44&amp;amp;lens%20size=45&amp;amp;magnification=46&amp;amp;interior%20spotlight=47&amp;amp;axes=48&amp;amp;viz=49&amp;amp;sensitivity=50&amp;amp;min%20size=51&amp;amp;min%20light=52&amp;amp;max%20light=53&amp;amp;elem%20scale=54&amp;amp;zero%20hue=55&amp;amp;hue%20gap=56&amp;amp;hue%20spread=57&amp;amp;diag=58&amp;amp;cam=59&amp;amp;x=60&amp;amp;y=61&amp;amp;z=62&amp;amp;cam.target=63&amp;amp;compress=64&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/raffn.jpg&quot; alt=&quot;an MLP formulated right-associatively&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aside: in addition to the color of the arrow vanes (blue for left, red for right), a second visual cue for distinguishing left and right arguments is their &lt;em&gt;orientation&lt;/em&gt;: the rows of the left argument are coplanar with those of the result - they stack along the same axis (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt;). Both cues tell us for example that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is the left argument to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B @ C)&lt;/code&gt; above.&lt;/p&gt;

&lt;h3 id=&quot;3c-binary-expressions&quot;&gt;3c Binary expressions&lt;/h3&gt;

&lt;p&gt;For a visualization tool to be useful beyond simple didactic examples, visualizations need to remain legible as expressions get more complicated. A key structural component in real-world use cases is binary expressions - matmuls with subexpressions on both the left and right.&lt;/p&gt;

&lt;p&gt;Here we’ll visualize the simplest such expression shape, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(A @ B) @ (C @ D)&lt;/code&gt; (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20(C%20%40%20D)&amp;amp;1=A%20%40%20B%20%40%20(C%20%40%20D)&amp;amp;2=none&amp;amp;12=closed&amp;amp;69=true&amp;amp;3.1=A%20%40%20B&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.12=open&amp;amp;3.2=none&amp;amp;13.1=A&amp;amp;13.4=false&amp;amp;13.5=64&amp;amp;13.6=64&amp;amp;13.7=expr&amp;amp;13.8=&amp;amp;13.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.12=closed&amp;amp;14.1=B&amp;amp;14.4=false&amp;amp;14.5=64&amp;amp;14.6=64&amp;amp;14.7=row%20major&amp;amp;14.8=&amp;amp;14.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;14.9=-1&amp;amp;14.10=1&amp;amp;14.11=0&amp;amp;14.12=closed&amp;amp;15.16=inherit&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.23=1&amp;amp;24.1=C%20%40%20D&amp;amp;24.4=true&amp;amp;24.5=32&amp;amp;24.6=32&amp;amp;24.7=col%20major&amp;amp;24.8=&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.2=none&amp;amp;25.16=inherit&amp;amp;26.23=1&amp;amp;27.18=positive&amp;amp;27.19=right&amp;amp;27.20=top&amp;amp;27.21=back&amp;amp;28.1=C&amp;amp;28.4=false&amp;amp;28.5=64&amp;amp;28.6=64&amp;amp;28.7=col%20major&amp;amp;28.8=&amp;amp;28.9=-1&amp;amp;28.10=1&amp;amp;28.11=0&amp;amp;28.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;28.12=open&amp;amp;29.1=D&amp;amp;29.4=false&amp;amp;29.5=64&amp;amp;29.6=64&amp;amp;29.7=expr&amp;amp;29.8=&amp;amp;29.9=-1&amp;amp;29.10=1&amp;amp;29.11=0&amp;amp;29.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;29.12=open&amp;amp;30.31=none&amp;amp;30.32=12&amp;amp;30.33=false&amp;amp;30.16=none&amp;amp;30.34=0&amp;amp;30.12=closed&amp;amp;35.36=1&amp;amp;35.37=1&amp;amp;35.23=1&amp;amp;38.39=blocks&amp;amp;38.40=5&amp;amp;38.41=0&amp;amp;38.42=1&amp;amp;38.43=0&amp;amp;38.18=negative&amp;amp;38.19=left&amp;amp;38.20=top&amp;amp;38.21=front&amp;amp;38.12=closed&amp;amp;44.45=6&amp;amp;44.46=true&amp;amp;44.47=2&amp;amp;44.48=1&amp;amp;44.49=0.5&amp;amp;44.50=0.5&amp;amp;44.51=10&amp;amp;44.52=false&amp;amp;44.53=false&amp;amp;44.12=closed&amp;amp;54.55=semilocal&amp;amp;54.56=0.4&amp;amp;54.57=0.4&amp;amp;54.58=0.6&amp;amp;54.59=1.5&amp;amp;54.60=0.77&amp;amp;54.61=0.74&amp;amp;54.62=0.04&amp;amp;54.12=open&amp;amp;63.8=&amp;amp;64.65=-149.45958189074523&amp;amp;64.66=140.76437147298853&amp;amp;64.67=162.13832534246401&amp;amp;68.65=-4.044017278625395&amp;amp;68.66=-2.123834827920271&amp;amp;68.67=-2.551083969824457&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;left.left=13&amp;amp;left.right=14&amp;amp;left.anim=15&amp;amp;alg=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.block=22&amp;amp;k%20blocks=23&amp;amp;right=24&amp;amp;right.anim=25&amp;amp;right.block=26&amp;amp;right.layout=27&amp;amp;right.left=28&amp;amp;right.right=29&amp;amp;anim=30&amp;amp;fuse=31&amp;amp;speed=32&amp;amp;hide%20inputs=33&amp;amp;spin=34&amp;amp;block=35&amp;amp;i%20blocks=36&amp;amp;j%20blocks=37&amp;amp;layout=38&amp;amp;scheme=39&amp;amp;gap=40&amp;amp;scatter=41&amp;amp;molecule=42&amp;amp;blast=43&amp;amp;deco=44&amp;amp;legends=45&amp;amp;shape=46&amp;amp;spotlight=47&amp;amp;row%20guides=48&amp;amp;flow%20guides=49&amp;amp;lens%20size=50&amp;amp;magnification=51&amp;amp;interior%20spotlight=52&amp;amp;axes=53&amp;amp;viz=54&amp;amp;sensitivity=55&amp;amp;min%20size=56&amp;amp;min%20light=57&amp;amp;max%20light=58&amp;amp;elem%20scale=59&amp;amp;zero%20hue=60&amp;amp;hue%20gap=61&amp;amp;hue%20spread=62&amp;amp;diag=63&amp;amp;cam=64&amp;amp;x=65&amp;amp;y=66&amp;amp;z=67&amp;amp;cam.target=68&amp;amp;compress=69&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/binary4.jpg&quot; alt=&quot;binary expressions - matmuls with subexpressions on both the left and right&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3d-quick-aside-partitioning-and-parallelism&quot;&gt;3d Quick aside: partitioning and parallelism&lt;/h3&gt;

&lt;p&gt;A full presentation of this topic is out of scope for this note, though we’ll see it in action later in the context of attention heads. But as a warmup, two quick examples should give a sense of how this style of visualization makes reasoning about parallelizing compound expressions very intuitive, via the simple geometry of partitioning.&lt;/p&gt;

&lt;p&gt;In the first example we’ll apply the canonical “data parallel” partitioning to the left-associative multilayer bottleneck example above. We partition along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt;, segmenting the initial left argument (“batch”) and all intermediate results (“activations”), but none of the subsequent arguments (“weights”) - the geometry making it obvious which participants in the expression are segmented and which remain whole (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;1=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;2=none&amp;amp;23=closed&amp;amp;63=true&amp;amp;3.1=A%20%40%20B%20%40%20C%20%40%20D&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.0=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;21.1=A%20%40%20B%20%40%20C&amp;amp;21.2=none&amp;amp;22.1=A%20%40%20B&amp;amp;22.4=true&amp;amp;22.5=32&amp;amp;22.6=32&amp;amp;22.7=row%20major&amp;amp;22.8=&amp;amp;22.9=-1&amp;amp;22.10=1&amp;amp;22.11=0&amp;amp;22.23=open&amp;amp;22.2=none&amp;amp;24.1=A&amp;amp;24.4=false&amp;amp;24.5=64&amp;amp;24.6=96&amp;amp;24.7=expr&amp;amp;24.8=&amp;amp;24.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.23=open&amp;amp;25.1=B&amp;amp;25.4=false&amp;amp;25.5=96&amp;amp;25.6=64&amp;amp;25.7=row%20major&amp;amp;25.8=&amp;amp;25.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.23=open&amp;amp;26.13=inherit&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.15=1&amp;amp;29.1=C&amp;amp;29.4=false&amp;amp;29.5=64&amp;amp;29.6=32&amp;amp;29.7=col%20major&amp;amp;29.8=&amp;amp;29.9=-1&amp;amp;29.10=1&amp;amp;29.11=0&amp;amp;29.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;29.23=open&amp;amp;30.31=none&amp;amp;30.32=12&amp;amp;30.33=false&amp;amp;30.13=none&amp;amp;30.34=0&amp;amp;30.23=closed&amp;amp;35.36=1&amp;amp;35.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;38.39=6&amp;amp;38.40=true&amp;amp;38.41=2&amp;amp;38.42=1&amp;amp;38.43=0.5&amp;amp;38.44=0.5&amp;amp;38.45=10&amp;amp;38.46=false&amp;amp;38.47=false&amp;amp;38.23=open&amp;amp;48.49=semilocal&amp;amp;48.50=0.2&amp;amp;48.51=0.4&amp;amp;48.52=0.6&amp;amp;48.53=1.25&amp;amp;48.54=0.77&amp;amp;48.55=0.74&amp;amp;48.56=0.04&amp;amp;48.23=open&amp;amp;57.8=&amp;amp;58.59=-125.71162036288077&amp;amp;58.60=101.84279252909485&amp;amp;58.61=122.50425255743914&amp;amp;62.59=-14.817097084822203&amp;amp;62.60=-9.723209466639396&amp;amp;62.61=-5.4699873376955646&amp;amp;21.23=open&amp;amp;21.63=true&amp;amp;21.4=true&amp;amp;64.1=D&amp;amp;64.4=false&amp;amp;64.5=32&amp;amp;64.6=64&amp;amp;64.7=col%20major&amp;amp;64.8=&amp;amp;64.9=-1&amp;amp;64.10=1&amp;amp;64.11=0&amp;amp;64.0=&amp;amp;64.23=open&amp;amp;65.1=E&amp;amp;65.4=false&amp;amp;65.5=64&amp;amp;65.6=96&amp;amp;65.7=col%20major&amp;amp;65.8=&amp;amp;65.9=-1&amp;amp;65.10=1&amp;amp;65.11=0&amp;amp;65.0=&amp;amp;66.31=none&amp;amp;66.32=12&amp;amp;66.33=false&amp;amp;66.13=none&amp;amp;66.34=0&amp;amp;66.23=closed&amp;amp;67.36=8&amp;amp;67.68=1&amp;amp;67.15=1&amp;amp;67.23=open&amp;amp;69.70=blocks&amp;amp;69.71=5&amp;amp;69.72=0&amp;amp;69.73=1&amp;amp;69.74=0&amp;amp;69.17=negative&amp;amp;69.18=left&amp;amp;69.19=top&amp;amp;69.20=front&amp;amp;69.23=closed&amp;amp;75.39=5.28&amp;amp;75.40=true&amp;amp;75.41=2&amp;amp;75.42=1&amp;amp;75.43=0.5&amp;amp;75.44=0.5&amp;amp;75.45=10&amp;amp;75.46=false&amp;amp;75.47=false&amp;amp;75.23=closed&amp;amp;76.49=semilocal&amp;amp;76.50=0.3&amp;amp;76.51=0.4&amp;amp;76.52=0.6&amp;amp;76.53=1.5&amp;amp;76.54=0.77&amp;amp;76.55=0.74&amp;amp;76.56=0.04&amp;amp;76.23=closed&amp;amp;77.8=&amp;amp;78.59=-174.76129648411032&amp;amp;78.60=141.54502619212317&amp;amp;78.61=170.2709730709386&amp;amp;79.59=-14.817097084822203&amp;amp;79.60=-9.723209466639396&amp;amp;79.61=-5.4699873376955646&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;folder=23&amp;amp;left.left.left.left=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.left.block=28&amp;amp;left.left.right=29&amp;amp;left.left.anim=30&amp;amp;fuse=31&amp;amp;speed=32&amp;amp;hide%20inputs=33&amp;amp;spin=34&amp;amp;left.left.block=35&amp;amp;i%20blocks=36&amp;amp;left.left.layout=37&amp;amp;left.left.deco=38&amp;amp;legends=39&amp;amp;shape=40&amp;amp;spotlight=41&amp;amp;row%20guides=42&amp;amp;flow%20guides=43&amp;amp;lens%20size=44&amp;amp;magnification=45&amp;amp;interior%20spotlight=46&amp;amp;axes=47&amp;amp;left.left.viz=48&amp;amp;sensitivity=49&amp;amp;min%20size=50&amp;amp;min%20light=51&amp;amp;max%20light=52&amp;amp;elem%20scale=53&amp;amp;zero%20hue=54&amp;amp;hue%20gap=55&amp;amp;hue%20spread=56&amp;amp;left.left.diag=57&amp;amp;left.left.cam=58&amp;amp;x=59&amp;amp;y=60&amp;amp;z=61&amp;amp;left.left.cam.target=62&amp;amp;compress=63&amp;amp;left.right=64&amp;amp;right=65&amp;amp;anim=66&amp;amp;block=67&amp;amp;j%20blocks=68&amp;amp;layout=69&amp;amp;scheme=70&amp;amp;gap=71&amp;amp;scatter=72&amp;amp;molecule=73&amp;amp;blast=74&amp;amp;deco=75&amp;amp;viz=76&amp;amp;diag=77&amp;amp;cam=78&amp;amp;cam.target=79&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/bottleneck_part.jpg&quot; alt=&quot;the canonical &amp;quot;data parallel&amp;quot; partitioning to the left-associative multilayer bottleneck example&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second example would (for me, anyway) be much harder to build intuition about without clear geometry to support it: it shows how a binary expression can be parallelized by partitioning the left subexpression along its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; axis, the right subexpression along its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; axis, and the parent expression along its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; axis (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20(C%20%40%20D)&amp;amp;1=A%20%40%20B%20%40%20(C%20%40%20D)&amp;amp;2=none&amp;amp;12=closed&amp;amp;69=true&amp;amp;3.1=A%20%40%20B&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.12=open&amp;amp;3.2=none&amp;amp;13.1=A&amp;amp;13.4=false&amp;amp;13.5=64&amp;amp;13.6=64&amp;amp;13.7=expr&amp;amp;13.8=&amp;amp;13.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.12=closed&amp;amp;14.1=B&amp;amp;14.4=false&amp;amp;14.5=64&amp;amp;14.6=64&amp;amp;14.7=row%20major&amp;amp;14.8=&amp;amp;14.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;14.9=-1&amp;amp;14.10=1&amp;amp;14.11=0&amp;amp;14.12=closed&amp;amp;15.16=inherit&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.23=1&amp;amp;24.1=C%20%40%20D&amp;amp;24.4=true&amp;amp;24.5=32&amp;amp;24.6=32&amp;amp;24.7=col%20major&amp;amp;24.8=&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.2=none&amp;amp;25.16=inherit&amp;amp;26.23=1&amp;amp;27.18=positive&amp;amp;27.19=right&amp;amp;27.20=top&amp;amp;27.21=back&amp;amp;28.1=C&amp;amp;28.4=false&amp;amp;28.5=64&amp;amp;28.6=64&amp;amp;28.7=col%20major&amp;amp;28.8=&amp;amp;28.9=-1&amp;amp;28.10=1&amp;amp;28.11=0&amp;amp;28.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;28.12=open&amp;amp;29.1=D&amp;amp;29.4=false&amp;amp;29.5=64&amp;amp;29.6=64&amp;amp;29.7=expr&amp;amp;29.8=&amp;amp;29.9=-1&amp;amp;29.10=1&amp;amp;29.11=0&amp;amp;29.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;29.12=open&amp;amp;30.31=none&amp;amp;30.32=12&amp;amp;30.33=false&amp;amp;30.16=none&amp;amp;30.34=0&amp;amp;30.12=closed&amp;amp;35.36=1&amp;amp;35.37=1&amp;amp;35.23=8&amp;amp;35.12=open&amp;amp;38.39=blocks&amp;amp;38.40=5&amp;amp;38.41=0&amp;amp;38.42=1&amp;amp;38.43=0&amp;amp;38.18=negative&amp;amp;38.19=left&amp;amp;38.20=top&amp;amp;38.21=front&amp;amp;38.12=closed&amp;amp;44.45=6&amp;amp;44.46=true&amp;amp;44.47=2&amp;amp;44.48=1&amp;amp;44.49=0.5&amp;amp;44.50=0.5&amp;amp;44.51=10&amp;amp;44.52=false&amp;amp;44.53=false&amp;amp;44.12=closed&amp;amp;54.55=semilocal&amp;amp;54.56=0.4&amp;amp;54.57=0.4&amp;amp;54.58=0.6&amp;amp;54.59=1.5&amp;amp;54.60=0.77&amp;amp;54.61=0.74&amp;amp;54.62=0.04&amp;amp;54.12=open&amp;amp;63.8=&amp;amp;64.65=-163.0431410622342&amp;amp;64.66=153.55767080483412&amp;amp;64.67=176.87418575632128&amp;amp;68.65=-4.044017278625395&amp;amp;68.66=-2.123834827920271&amp;amp;68.67=-2.551083969824457&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;left.left=13&amp;amp;left.right=14&amp;amp;left.anim=15&amp;amp;alg=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.block=22&amp;amp;k%20blocks=23&amp;amp;right=24&amp;amp;right.anim=25&amp;amp;right.block=26&amp;amp;right.layout=27&amp;amp;right.left=28&amp;amp;right.right=29&amp;amp;anim=30&amp;amp;fuse=31&amp;amp;speed=32&amp;amp;hide%20inputs=33&amp;amp;spin=34&amp;amp;block=35&amp;amp;i%20blocks=36&amp;amp;j%20blocks=37&amp;amp;layout=38&amp;amp;scheme=39&amp;amp;gap=40&amp;amp;scatter=41&amp;amp;molecule=42&amp;amp;blast=43&amp;amp;deco=44&amp;amp;legends=45&amp;amp;shape=46&amp;amp;spotlight=47&amp;amp;row%20guides=48&amp;amp;flow%20guides=49&amp;amp;lens%20size=50&amp;amp;magnification=51&amp;amp;interior%20spotlight=52&amp;amp;axes=53&amp;amp;viz=54&amp;amp;sensitivity=55&amp;amp;min%20size=56&amp;amp;min%20light=57&amp;amp;max%20light=58&amp;amp;elem%20scale=59&amp;amp;zero%20hue=60&amp;amp;hue%20gap=61&amp;amp;hue%20spread=62&amp;amp;diag=63&amp;amp;cam=64&amp;amp;x=65&amp;amp;y=66&amp;amp;z=67&amp;amp;cam.target=68&amp;amp;compress=69&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/binary_part.jpg&quot; alt=&quot;a binary expression can be parallelized by partitioning the left subexpression along its j axis, the right subexpression along its i axis, and the parent expression along its k axis&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-inside-an-attention-head&quot;&gt;4 Inside an Attention Head&lt;/h2&gt;

&lt;p&gt;Let’s look at a GPT2 attention head - specifically layer 5, head 4 of the “gpt2” (small) configuration (layers=12, heads=12, embed=768) from &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;NanoGPT&lt;/a&gt;, using OpenAI weights via HuggingFace. Input activations are taken from a forward pass on an OpenWebText training sample of 256 tokens.&lt;/p&gt;

&lt;p&gt;There’s nothing particularly unusual about this particular head; I chose it mainly because it computes a fairly common attention pattern and lives in the middle of the model, where activations have become structured and show some interesting texture. (Aside: in a subsequent note I’ll present an attention head explorer that lets you visualize all layers and heads of this model, along with some travel notes.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=10&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;53.49=open&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0.655&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=closed&amp;amp;78.8=&amp;amp;78.49=closed&amp;amp;79.80=-1149.3128801149742&amp;amp;79.81=1143.004532598807&amp;amp;79.82=1754.3660479535383&amp;amp;83.80=-6.708919569777563&amp;amp;83.81=75.05036284609801&amp;amp;83.82=-216.66743330111652&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;Open in mm&lt;/a&gt; (may take a few seconds to fetch model weights)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/mha1.jpg&quot; alt=&quot;There's nothing particularly unusual about this particular head&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4a-structure&quot;&gt;4a Structure&lt;/h3&gt;

&lt;p&gt;The entire attention head is visualized as a single compound expression, starting with input and ending with projected output. (Note: to keep things self-contained we do per-head output projection as described in &lt;a href=&quot;https://arxiv.org/pdf/1909.08053.pdf&quot;&gt;Megatron-LM&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The computation contains six matmuls:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Q = input @ wQ        // 1
K_t = wK_t @ input_t  // 2
V = input @ wV        // 3
attn = sdpa(Q @ K_t)  // 4
head_out = attn @ V   // 5
out = head_out @ wO   // 6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A thumbnail description of what we’re looking at:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the blades of the windmill are matmuls 1, 2, 3 and 6: the former group are the in-projections from input to Q, K and V; the latter is the out-projection from attn @ V back to the embedding dimension.&lt;/li&gt;
  &lt;li&gt;at the hub is the double matmul that first calculates attention scores (convex cube in back), then uses them to produce output tokens from the values vector (concave cube in front). Causality means that the attention scores form a lower triangle.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But I’d encourage &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=closed&amp;amp;69.70=local&amp;amp;69.71=0&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=-1212.5184472916683&amp;amp;79.81=1205.8631771144878&amp;amp;79.82=1850.8460431010271&amp;amp;83.80=-6.708919569777563&amp;amp;83.81=75.05036284609801&amp;amp;83.82=-216.66743330111652&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;exploring this example in the tool itself&lt;/a&gt;, rather than relying on the screenshot or the video below to convey just how much signal can be absorbed from it - both about its structure and the actual values flowing through the computation.&lt;/p&gt;

&lt;h3 id=&quot;4b-computation-and-values&quot;&gt;4b Computation and Values&lt;/h3&gt;

&lt;p&gt;Here’s an animation of the attention head computation. Specifically, we’re watching&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sdpa(input @ wQ @ K_t) @ V @ wO
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(i.e., matmuls 1, 4 , 5 and 6 above, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K_t&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; precomputed) being computed as a fused chain of vector-matrix products: each item in the sequence goes all the way from input through attention to output in one step. More on this animation choice in the later section on parallelization, but first let’s look at what the values being computed tell us.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_5_4&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_5&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_5&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_5_4&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_5_4&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=vmprod&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-382.8684269325278&amp;amp;79.81=293.7591554956184&amp;amp;79.82=395.95878922315694&amp;amp;83.80=-14.023727291338966&amp;amp;83.81=-38.22974037070054&amp;amp;83.82=-84.10726407282482&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;Open in mm&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/gpt2_big2b.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;There’s a lot of interesting stuff going on here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Before we even get to the attention calculation, it’s quite striking how low-rank &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K_t&lt;/code&gt; are. &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;24=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;23.24=closed&amp;amp;25.1=wQ_5_4&amp;amp;25.4=false&amp;amp;25.5=768&amp;amp;25.6=64&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;25.0=&amp;amp;26.13=none&amp;amp;26.24=open&amp;amp;27.15=1&amp;amp;28.17=positive&amp;amp;28.18=left&amp;amp;28.19=bottom&amp;amp;28.20=back&amp;amp;22.24=open&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.17=positive&amp;amp;32.18=right&amp;amp;32.19=top&amp;amp;32.20=back&amp;amp;33.1=wK_t_5_4&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t_0_5&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=vmprod&amp;amp;36.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;21.24=closed&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input_0_5&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV_5_4&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.17=negative&amp;amp;43.18=right&amp;amp;43.19=top&amp;amp;43.20=back&amp;amp;3.24=closed&amp;amp;44.1=wO_5_4&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=4&amp;amp;45.48=false&amp;amp;45.13=vmprod&amp;amp;45.49=0&amp;amp;45.24=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.24=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-0.30816774330149777&amp;amp;79.81=333.6054152134701&amp;amp;79.82=155.72856559616935&amp;amp;83.80=-0.11764216999897817&amp;amp;83.81=-38.43510027180947&amp;amp;83.82=-78.52287109278605&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;folder=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;Zooming in on the Q @ K_t vector-matrix product animation&lt;/a&gt;, the situation is even more vivid: a significant number of channels (embedding positions) in &lt;em&gt;both&lt;/em&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; look more or less constant across the sequence, implying that the useful attention signal is potentially driven by a only smallish subset of the embedding. Understanding and exploiting this phenomenon is one of the threads we’re pulling on as part of the SysML ATOM transformer efficiency project.&lt;/li&gt;
  &lt;li&gt;Perhaps most familiar is the strong-but-not-perfect diagonal that emerges in the attention matrix. This is a common pattern, showing up in many of the attention heads of this model (and those of many transformers). It produces &lt;em&gt;localized&lt;/em&gt; attention: the value tokens in the small neighborhood immediately preceding an output token’s position largely determine that output token’s content pattern.&lt;/li&gt;
  &lt;li&gt;However, the size of this neighborhood and the influence of individual tokens within it vary nontrivially - this can be seen both in the off-diagonal frost in the attention grid, and in the &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;26=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;25.26=open&amp;amp;27.15=1&amp;amp;28.17=positive&amp;amp;28.18=left&amp;amp;28.19=bottom&amp;amp;28.20=back&amp;amp;22.26=closed&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.17=positive&amp;amp;32.18=right&amp;amp;32.19=top&amp;amp;32.20=back&amp;amp;33.1=wK_t_5_4&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t_0_5&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=none&amp;amp;35.26=open&amp;amp;36.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;21.26=open&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input_0_5&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV_5_4&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.17=negative&amp;amp;43.18=right&amp;amp;43.19=top&amp;amp;43.20=back&amp;amp;3.26=open&amp;amp;44.1=wO_5_4&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=vmprod&amp;amp;45.49=0&amp;amp;45.26=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.26=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-12.838747258760423&amp;amp;79.81=224.62765397316576&amp;amp;79.82=274.71626756027933&amp;amp;83.80=-13.049253781233714&amp;amp;83.81=-55.16215322834755&amp;amp;83.82=-70.26525235295296&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;folder=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;fluctuating patterns of the attn[i] @ V vector-matrix product plane&lt;/a&gt; as it descends the attention matrix on its way through the sequence.&lt;/li&gt;
  &lt;li&gt;But note that the local neighborhood isn’t the only thing that’s attracting attention: the leftmost column of the attention grid, corresponding to the first token of the sequence, is entirely filled with nonzero (but fluctuating) values, meaning every output token will be influenced to some degree by the first value token.&lt;/li&gt;
  &lt;li&gt;Moreover there’s an &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_5_4&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_5&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_5&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_5_4&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_5_4&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-328.8286059935543&amp;amp;79.81=-64.64788859858083&amp;amp;79.82=156.66189435044396&amp;amp;83.80=-6.5479856531724625&amp;amp;83.81=-27.630477427688977&amp;amp;83.82=-64.70186279804427&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;inexact but discernible oscillation in attention score dominance&lt;/a&gt; between the current token neighborhood and the initial token. The period of the oscillation varies, but broadly speaking starts short and then lengthens as one travels down the sequence (evocatively correlated with the quantity of candidate attention tokens for each row, given causality).&lt;/li&gt;
  &lt;li&gt;To get a feel for how (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V)&lt;/code&gt; is formed, it’s important not to focus on attention in isolation - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; is an equal player. Each output item is a weighted average of the entire &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; vector: at the limit when attention is a perfect diagonal, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; is simply an exact copy of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;. Here we see &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out+%3D+%28attn+%3D+%28Q+%3D+input_0_5+%40+wQ_5_4%29+%40+%28K_t+%3D+wK_t_5_4+%40+input_t_0_5%29%29+%40+%28V+%3D+input_0_5+%40+wV_5_4%29+%40+wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;51=closed&amp;amp;84=true&amp;amp;3.1=attn+%40+V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row+major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;14.16=1&amp;amp;14.17=1&amp;amp;18.19=positive&amp;amp;18.20=left&amp;amp;18.21=bottom&amp;amp;18.22=back&amp;amp;23.1=attn&amp;amp;23.4=true&amp;amp;23.2=softmax%28tril%28x%2Fsqrt%28k%29%29%29&amp;amp;24.1=Q&amp;amp;24.4=true&amp;amp;24.2=none&amp;amp;25.1=input_0_5&amp;amp;25.4=false&amp;amp;25.5=256&amp;amp;25.6=768&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;25.0=&amp;amp;26.1=wQ_5_4&amp;amp;26.4=false&amp;amp;26.5=768&amp;amp;26.6=64&amp;amp;26.7=url&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;26.0=&amp;amp;27.13=vmprod&amp;amp;28.15=1&amp;amp;28.16=1&amp;amp;28.17=1&amp;amp;29.19=positive&amp;amp;29.20=left&amp;amp;29.21=bottom&amp;amp;29.22=back&amp;amp;30.2=none&amp;amp;31.13=none&amp;amp;32.15=1&amp;amp;32.16=1&amp;amp;32.17=1&amp;amp;33.19=positive&amp;amp;33.20=right&amp;amp;33.21=top&amp;amp;33.22=back&amp;amp;34.1=wK_t_5_4&amp;amp;34.4=false&amp;amp;34.5=64&amp;amp;34.6=768&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;35.1=input_t_0_5&amp;amp;35.4=false&amp;amp;35.5=768&amp;amp;35.6=256&amp;amp;35.7=url&amp;amp;35.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;35.9=-1&amp;amp;35.10=1&amp;amp;35.11=0&amp;amp;35.0=&amp;amp;30.1=K_t&amp;amp;30.4=true&amp;amp;36.13=vmprod&amp;amp;37.15=1&amp;amp;37.16=1&amp;amp;37.17=1&amp;amp;38.19=negative&amp;amp;38.20=left&amp;amp;38.21=top&amp;amp;38.22=front&amp;amp;39.1=V&amp;amp;39.4=true&amp;amp;39.2=none&amp;amp;40.1=input_0_5&amp;amp;40.4=false&amp;amp;40.5=256&amp;amp;40.6=768&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;40.0=&amp;amp;41.1=wV_5_4&amp;amp;41.4=false&amp;amp;41.5=768&amp;amp;41.6=64&amp;amp;41.7=url&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;41.0=&amp;amp;42.13=none&amp;amp;43.15=1&amp;amp;43.16=1&amp;amp;43.17=1&amp;amp;44.19=negative&amp;amp;44.20=right&amp;amp;44.21=top&amp;amp;44.22=back&amp;amp;45.1=wO_5_4&amp;amp;45.4=false&amp;amp;45.5=64&amp;amp;45.6=768&amp;amp;45.7=url&amp;amp;45.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;45.9=-1&amp;amp;45.10=1&amp;amp;45.11=0&amp;amp;45.0=&amp;amp;46.47=sync&amp;amp;46.48=16&amp;amp;46.49=false&amp;amp;46.13=none&amp;amp;46.50=0&amp;amp;46.51=open&amp;amp;52.16=1&amp;amp;52.15=1&amp;amp;52.17=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.19=negative&amp;amp;53.20=left&amp;amp;53.21=top&amp;amp;53.22=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=20&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.51=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-164.2339403949366&amp;amp;79.81=18.940074323234473&amp;amp;79.82=173.55325640245638&amp;amp;83.80=117.76774477612946&amp;amp;83.81=2.623526996843087&amp;amp;83.82=53.25986191913323&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k+blocks=15&amp;amp;i+blocks=16&amp;amp;j+blocks=17&amp;amp;left.layout=18&amp;amp;polarity=19&amp;amp;left+placement=20&amp;amp;right+placement=21&amp;amp;result+placement=22&amp;amp;left.left=23&amp;amp;left.left.left=24&amp;amp;left.left.left.left=25&amp;amp;left.left.left.right=26&amp;amp;left.left.left.anim=27&amp;amp;left.left.left.block=28&amp;amp;left.left.left.layout=29&amp;amp;left.left.right=30&amp;amp;left.left.right.anim=31&amp;amp;left.left.right.block=32&amp;amp;left.left.right.layout=33&amp;amp;left.left.right.left=34&amp;amp;left.left.right.right=35&amp;amp;left.left.anim=36&amp;amp;left.left.block=37&amp;amp;left.left.layout=38&amp;amp;left.right=39&amp;amp;left.right.left=40&amp;amp;left.right.right=41&amp;amp;left.right.anim=42&amp;amp;left.right.block=43&amp;amp;left.right.layout=44&amp;amp;right=45&amp;amp;anim=46&amp;amp;fuse=47&amp;amp;speed=48&amp;amp;hide+inputs=49&amp;amp;spin=50&amp;amp;folder=51&amp;amp;block=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row+guides=63&amp;amp;flow+guides=64&amp;amp;lens+size=65&amp;amp;magnification=66&amp;amp;interior+spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min+size=71&amp;amp;min+light=72&amp;amp;max+light=73&amp;amp;elem+scale=74&amp;amp;zero+hue=75&amp;amp;hue+gap=76&amp;amp;hue+spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;something more textured&lt;/a&gt;: visible banding where particular tokens have scored high over a contiguous subsequence of attention rows, superimposed on a matrix visibly similar to to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; but with some vertical smearing due to the fat diagonal. (Aside: per the &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;mm reference guide&lt;/a&gt;, long-clicking or control-clicking will reveal the actual numeric values of visualized elements.)&lt;/li&gt;
  &lt;li&gt;Bear in mind that since we’re in a middle layer (5), the input to this attention head is an intermediate representation, not the original tokenized text. So the &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out+%3D+%28attn+%3D+%28Q+%3D+input+%40+wQ%29+%40+%28K_t+%3D+wK_t+%40+input_t%29%29+%40+%28V+%3D+input+%40+wV%29+%40+wO&amp;amp;1=out&amp;amp;2=none&amp;amp;26=closed&amp;amp;84=true&amp;amp;3.1=attn+%40+V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row+major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;14.16=1&amp;amp;14.17=1&amp;amp;18.19=positive&amp;amp;18.20=left&amp;amp;18.21=bottom&amp;amp;18.22=back&amp;amp;23.1=attn&amp;amp;23.4=true&amp;amp;23.2=softmax%28tril%28x%2Fsqrt%28k%29%29%29&amp;amp;24.1=Q&amp;amp;24.4=true&amp;amp;24.2=none&amp;amp;25.1=input&amp;amp;25.4=false&amp;amp;25.5=256&amp;amp;25.6=768&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;25.0=&amp;amp;25.26=open&amp;amp;27.1=wQ&amp;amp;27.4=false&amp;amp;27.5=768&amp;amp;27.6=64&amp;amp;27.7=url&amp;amp;27.9=-1&amp;amp;27.10=1&amp;amp;27.11=0&amp;amp;27.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;27.0=&amp;amp;28.13=vmprod&amp;amp;29.15=1&amp;amp;29.16=1&amp;amp;29.17=1&amp;amp;30.19=positive&amp;amp;30.20=left&amp;amp;30.21=bottom&amp;amp;30.22=back&amp;amp;24.26=open&amp;amp;31.2=none&amp;amp;32.13=none&amp;amp;33.15=1&amp;amp;33.16=1&amp;amp;33.17=1&amp;amp;34.19=positive&amp;amp;34.20=right&amp;amp;34.21=top&amp;amp;34.22=back&amp;amp;35.1=wK_t&amp;amp;35.4=false&amp;amp;35.5=64&amp;amp;35.6=768&amp;amp;35.7=url&amp;amp;35.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;35.9=-1&amp;amp;35.10=1&amp;amp;35.11=0&amp;amp;35.0=&amp;amp;36.1=input_t&amp;amp;36.4=false&amp;amp;36.5=768&amp;amp;36.6=256&amp;amp;36.7=url&amp;amp;36.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;36.9=-1&amp;amp;36.10=1&amp;amp;36.11=0&amp;amp;36.0=&amp;amp;31.1=K_t&amp;amp;31.4=true&amp;amp;37.13=vmprod&amp;amp;38.15=1&amp;amp;38.16=1&amp;amp;38.17=1&amp;amp;39.19=negative&amp;amp;39.20=left&amp;amp;39.21=top&amp;amp;39.22=front&amp;amp;23.26=open&amp;amp;40.1=V&amp;amp;40.4=true&amp;amp;40.2=none&amp;amp;41.1=input&amp;amp;41.4=false&amp;amp;41.5=256&amp;amp;41.6=768&amp;amp;41.7=url&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;41.0=&amp;amp;42.1=wV&amp;amp;42.4=false&amp;amp;42.5=768&amp;amp;42.6=64&amp;amp;42.7=url&amp;amp;42.9=-1&amp;amp;42.10=1&amp;amp;42.11=0&amp;amp;42.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;42.0=&amp;amp;43.13=none&amp;amp;44.15=1&amp;amp;44.16=1&amp;amp;44.17=1&amp;amp;45.19=negative&amp;amp;45.20=right&amp;amp;45.21=top&amp;amp;45.22=back&amp;amp;3.26=open&amp;amp;46.1=wO&amp;amp;46.4=false&amp;amp;46.5=64&amp;amp;46.6=768&amp;amp;46.7=url&amp;amp;46.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;46.9=-1&amp;amp;46.10=1&amp;amp;46.11=0&amp;amp;46.0=&amp;amp;47.48=sync&amp;amp;47.49=16&amp;amp;47.50=false&amp;amp;47.13=none&amp;amp;47.51=0&amp;amp;47.26=open&amp;amp;52.16=1&amp;amp;52.15=1&amp;amp;52.17=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.19=negative&amp;amp;53.20=left&amp;amp;53.21=top&amp;amp;53.22=front&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=5&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.25&amp;amp;59.66=4.632&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.26=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.4&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.26=open&amp;amp;78.8=&amp;amp;78.26=open&amp;amp;79.80=-1126.8641673236093&amp;amp;79.81=-4.707283693510895&amp;amp;79.82=168.0669807860928&amp;amp;83.80=-692.9006907132649&amp;amp;83.81=4.068470706235418&amp;amp;83.82=-171.27561837707958&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k+blocks=15&amp;amp;i+blocks=16&amp;amp;j+blocks=17&amp;amp;left.layout=18&amp;amp;polarity=19&amp;amp;left+placement=20&amp;amp;right+placement=21&amp;amp;result+placement=22&amp;amp;left.left=23&amp;amp;left.left.left=24&amp;amp;left.left.left.left=25&amp;amp;folder=26&amp;amp;left.left.left.right=27&amp;amp;left.left.left.anim=28&amp;amp;left.left.left.block=29&amp;amp;left.left.left.layout=30&amp;amp;left.left.right=31&amp;amp;left.left.right.anim=32&amp;amp;left.left.right.block=33&amp;amp;left.left.right.layout=34&amp;amp;left.left.right.left=35&amp;amp;left.left.right.right=36&amp;amp;left.left.anim=37&amp;amp;left.left.block=38&amp;amp;left.left.layout=39&amp;amp;left.right=40&amp;amp;left.right.left=41&amp;amp;left.right.right=42&amp;amp;left.right.anim=43&amp;amp;left.right.block=44&amp;amp;left.right.layout=45&amp;amp;right=46&amp;amp;anim=47&amp;amp;fuse=48&amp;amp;speed=49&amp;amp;hide+inputs=50&amp;amp;spin=51&amp;amp;block=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row+guides=63&amp;amp;flow+guides=64&amp;amp;lens+size=65&amp;amp;magnification=66&amp;amp;interior+spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min+size=71&amp;amp;min+light=72&amp;amp;max+light=73&amp;amp;elem+scale=74&amp;amp;zero+hue=75&amp;amp;hue+gap=76&amp;amp;hue+spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;patterns seen in the input&lt;/a&gt; are themselves thought-provoking - in particular, the strong vertical threads are particular embedding positions whose values are uniformly high magnitude across long stretches of the sequence - sometimes almost the entire thing.&lt;/li&gt;
  &lt;li&gt;Interestingly, though, the &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO&amp;amp;1=out&amp;amp;2=none&amp;amp;24=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;23.24=open&amp;amp;25.1=wQ&amp;amp;25.4=false&amp;amp;25.5=768&amp;amp;25.6=64&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;25.0=&amp;amp;26.13=vmprod&amp;amp;27.15=1&amp;amp;28.17=positive&amp;amp;28.18=left&amp;amp;28.19=bottom&amp;amp;28.20=back&amp;amp;22.24=open&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.17=positive&amp;amp;32.18=right&amp;amp;32.19=top&amp;amp;32.20=back&amp;amp;33.1=wK_t&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=vmprod&amp;amp;36.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;21.24=open&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.17=negative&amp;amp;43.18=right&amp;amp;43.19=top&amp;amp;43.20=back&amp;amp;3.24=open&amp;amp;44.1=wO&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=none&amp;amp;45.49=0&amp;amp;45.24=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=5&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0&amp;amp;59.66=4.632&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.24=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.4&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.24=open&amp;amp;78.8=&amp;amp;78.24=open&amp;amp;79.80=-905.2149526505231&amp;amp;79.81=126.10717525695773&amp;amp;79.82=-90.1644865901155&amp;amp;83.80=-739.2766627330938&amp;amp;83.81=125.47333863007341&amp;amp;83.82=-229.39828071999955&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;folder=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;first vector in the input sequence is distinctive&lt;/a&gt;, not only breaking the pattern of these high-magnitude columns but carrying atypical values at almost every position (aside: not visualized here, but this pattern is repeated over multiple sample inputs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: apropos of the last two bullet points, it’s worth reiterating that we’re visualizing computation over a &lt;em&gt;single sample input&lt;/em&gt;. In practice I’ve found that each head has a characteristic pattern it will express consistently (though not identically) over a decent collection of samples (and the upcoming attention head browser will provide a collection of samples to play with), but when looking at any visualization that includes activations, it’s important to bear in mind that a full distribution of inputs may influence the ideas and intuitions it provokes it in subtle ways.&lt;/p&gt;

&lt;p&gt;Finally, one more pitch to &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_5_4&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_5&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_5&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_5_4&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_5_4&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=vmprod&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-382.8684269325278&amp;amp;79.81=293.7591554956184&amp;amp;79.82=395.95878922315694&amp;amp;83.80=-14.023727291338966&amp;amp;83.81=-38.22974037070054&amp;amp;83.82=-84.10726407282482&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;explore the animation directly&lt;/a&gt;!&lt;/p&gt;

&lt;h3 id=&quot;4c-heads-are-different-in-interesting-ways&quot;&gt;4c Heads are different in interesting ways&lt;/h3&gt;

&lt;p&gt;Before we move on, here’s one more demonstration of the usefulness of simply poking around a model to see how it works in detail.&lt;/p&gt;

&lt;p&gt;This is another attention head from GPT2. It behaves quite differently from layer 5, head 4 above - as one might expect, given that it’s in a very different part of the model. This head is in the very first layer: layer 0, head 2 (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=-217.09372134188362&amp;amp;79.81=412.82010718887307&amp;amp;79.82=523.3596617096426&amp;amp;83.80=127.59196458710655&amp;amp;83.81=35.32022663933653&amp;amp;83.82=87.43354119148215&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;, may take a few seconds to load model weights):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/gpt2_0_2c.jpg&quot; alt=&quot;This is another attention head from GPT2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Things to note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This head spreads attention very evenly. This has the effect of delivering a relatively &lt;em&gt;unweighted&lt;/em&gt; average of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; (or rather, the appropriate causal prefix of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;) to each row in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt;, as can be seen in &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=vmprod&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=closed&amp;amp;69.70=local&amp;amp;69.71=0.4&amp;amp;69.72=0.4&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=closed&amp;amp;79.80=11.34872888812131&amp;amp;79.81=324.07536950158396&amp;amp;79.82=239.8893041928473&amp;amp;83.80=11.804686909150822&amp;amp;83.81=25.33948904441141&amp;amp;83.82=46.896270190786204&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;this animation&lt;/a&gt;: as we move down the attention score triangle, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn[i] @ V&lt;/code&gt; vector-matrix product is small fluctuations away from being simply a downscaled, progressively revealed copy of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; has &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=-152.24249732960024&amp;amp;79.81=115.78244265148294&amp;amp;79.82=89.29496035154&amp;amp;83.80=20.231661185991296&amp;amp;83.81=61.75722293832386&amp;amp;83.82=52.45120329048098&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;striking vertical uniformity&lt;/a&gt; - in large columnar regions of the embedding, the same value patterns persist over &lt;em&gt;the entire sequence&lt;/em&gt;. One can think of these as properties shared by every token.&lt;/li&gt;
  &lt;li&gt;Aside: on the one hand one might expect &lt;em&gt;some&lt;/em&gt; uniformity in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; given the effect of very evenly spread attention. But each row has been constructed from only a causal subsequence of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; rather than the whole thing - why is that not causing more variation, like a progressive morphing as one moves down the sequence? &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=8.296178745251016&amp;amp;79.81=-533.8678069620822&amp;amp;79.82=35.64126972299759&amp;amp;83.80=8.29674894856322&amp;amp;83.81=36.25749961529174&amp;amp;83.82=35.64126624185369&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;By visual inspection V isn’t uniform along its length&lt;/a&gt;, so the answer must lie in some more subtle property of its distribution of values.&lt;/li&gt;
  &lt;li&gt;Finally, this head’s output is &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=41.37507219272118&amp;amp;79.81=4.367136718959145&amp;amp;79.82=430.5595129727994&amp;amp;83.80=607.5332301692057&amp;amp;83.81=-2.548000389888877&amp;amp;83.82=-122.74351758382484&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;even more vertically uniform after out-projection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;the strong impression being that the bulk of the information being delivered by this attention head consists of properties which are shared by every token in the sequence. The composition of its &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=136.19712021298585&amp;amp;79.81=351.47497630691043&amp;amp;79.82=254.9066405965837&amp;amp;83.80=554.2569175811409&amp;amp;83.81=-39.63963114876448&amp;amp;83.82=-109.72659308933949&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;output projection weights&lt;/a&gt; reinforces this intuition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overall, it’s hard to resist the idea that the extremely regular, highly structured information this attention head produces might be obtained by computational means that are a bit… less lavish. Of course this isn’t an unexplored area, but the specificity and richness of signal of the visualized computation has been useful in generating new ideas, and reasoning about existing ones.&lt;/p&gt;

&lt;h3 id=&quot;4d-revisiting-the-pitch-invariants-for-free&quot;&gt;4d Revisiting the pitch: invariants for free&lt;/h3&gt;

&lt;p&gt;Stepping back, it’s worth reiterating that the reason we can visualize nontrivially compound operations like attention heads and have them remain intuitive is that important algebraic properties - like how argument shapes are constrained, or which parallelization axes intersect which operations - &lt;em&gt;don’t require additional thinking&lt;/em&gt;: they arise directly from the geometry of the visualized object, rather than being additional rules to keep in mind.&lt;/p&gt;

&lt;p&gt;For example, in these attention head visualizations it’s immediately obvious that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; are the same length, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; are the same length, and the lengths of these pairs are independent of each other&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; are the same width, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; are the same width, and the widths of these pairs are independent of each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These properties are true by construction, as a simple consequence of which parts of the compound structure the constituents inhabit and how they are oriented.&lt;/p&gt;

&lt;p&gt;This “properties for free” benefit can be especially useful when exploring variations on a canonical structure - an obvious example being the one-row-high attention matrix in autoregressive token-at-a-time decoding (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;24=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(x%2Fsqrt(k))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=1&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;23.24=open&amp;amp;25.1=wQ_5_4&amp;amp;25.4=false&amp;amp;25.5=768&amp;amp;25.6=64&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;25.0=&amp;amp;26.13=vmprod&amp;amp;27.15=1&amp;amp;28.17=positive&amp;amp;28.18=left&amp;amp;28.19=bottom&amp;amp;28.20=back&amp;amp;22.24=open&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.17=positive&amp;amp;32.18=right&amp;amp;32.19=top&amp;amp;32.20=back&amp;amp;33.1=wK_t_5_4&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t_0_5&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=vmprod&amp;amp;36.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;21.24=open&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input_0_5&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV_5_4&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.17=negative&amp;amp;43.18=right&amp;amp;43.19=top&amp;amp;43.20=back&amp;amp;3.24=closed&amp;amp;44.1=wO_5_4&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=none&amp;amp;45.49=0&amp;amp;45.24=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.24=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-289.3020871171715&amp;amp;79.81=176.55051931108687&amp;amp;79.82=202.12550566094345&amp;amp;83.80=6.09420901744693&amp;amp;83.81=-60.94451681776672&amp;amp;83.82=-55.94371166611936&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;folder=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/gpt2_decode2.jpg&quot; alt=&quot;the one-row-high attention matrix in autoregressive token-at-a-time decoding&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-parallelizing-attention&quot;&gt;5 Parallelizing attention&lt;/h2&gt;

&lt;p&gt;In the animation of head 5, layer 4 above, we visualize 4 of the 6 matmuls in the attention head&lt;/p&gt;

&lt;p&gt;as a fused chain of vector-matrix products, confirming the geometric intuition that the entire left-associative chain from input to output is &lt;em&gt;laminar&lt;/em&gt; along the shared &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; axis, and can be parallelized.&lt;/p&gt;

&lt;h3 id=&quot;5a-example-partitioning-along-i&quot;&gt;5a Example: partitioning along &lt;code&gt;i&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;To parallelize the computation in practice, we would partition the input into blocks along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; axis. We can visualize this partition in the tool, by specifying that a given axis be partitioned into a particular number of blocks - in these examples we’ll use 8, but there’s nothing special about that number.&lt;/p&gt;

&lt;p&gt;Among other things, this visualization makes clear that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wQ&lt;/code&gt; (for in-projection), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K_t&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; (for attention) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wO&lt;/code&gt; (for out-projection) are needed in their entirety by each parallel computation, since they’re adjacent to the partitioned matrices along those matrices’ unpartitioned dimensions (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_5_4&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_5&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_5&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_5_4&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_5_4&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=closed&amp;amp;50.51=8&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;50.49=open&amp;amp;53.54=blocks&amp;amp;53.55=10&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;53.49=closed&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.507&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.3&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=closed&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=-452.09425433307837&amp;amp;79.81=-10.01467989007457&amp;amp;79.82=392.9851223674549&amp;amp;83.80=-27.91725760321879&amp;amp;83.81=-18.858991089590095&amp;amp;83.82=-140.6826497984033&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/gpt2_parti.jpg&quot; alt=&quot;wQ (for in-projection), K_t and V (for attention) and wO (for out-projection) are needed in their entirety by each parallel computation&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5b-example-double-partitioning&quot;&gt;5b Example: double partitioning&lt;/h3&gt;

&lt;p&gt;As an example of partitioning along &lt;em&gt;multiple&lt;/em&gt; axes, we can visualize some recent work which innovates in this space (&lt;a href=&quot;https://arxiv.org/pdf/2305.19370.pdf&quot;&gt;Block Parallel Transformer&lt;/a&gt;, building on work done in e.g. &lt;a href=&quot;https://arxiv.org/pdf/2205.14135.pdf&quot;&gt;Flash Attention&lt;/a&gt; and its antecedents).&lt;/p&gt;

&lt;p&gt;First, BPT partitions along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; as described above - and actually extends this horizontal partitioning of the sequence into chunks all the way through the second (FFN) half of the attention layer as well. (We’ll visualize this in a later section.)&lt;/p&gt;

&lt;p&gt;To fully attack the context length problem, a second partitioning is then added to MHA - that of the attention calculation itself (i.e., a partition along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; axis of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q @ K_t&lt;/code&gt;). The two partitions together divide attention into a grid of blocks (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;16=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=8&amp;amp;14.16=open&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.1=attn&amp;amp;22.4=true&amp;amp;22.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;23.1=Q&amp;amp;23.4=true&amp;amp;23.2=none&amp;amp;24.1=input_0_5&amp;amp;24.4=false&amp;amp;24.5=256&amp;amp;24.6=768&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;24.0=&amp;amp;25.1=wQ_5_4&amp;amp;25.4=false&amp;amp;25.5=768&amp;amp;25.6=64&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;25.0=&amp;amp;26.13=vmprod&amp;amp;27.15=1&amp;amp;28.18=positive&amp;amp;28.19=left&amp;amp;28.20=bottom&amp;amp;28.21=back&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.18=positive&amp;amp;32.19=right&amp;amp;32.20=top&amp;amp;32.21=back&amp;amp;33.1=wK_t_5_4&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t_0_5&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=vmprod&amp;amp;36.15=1&amp;amp;37.18=negative&amp;amp;37.19=left&amp;amp;37.20=top&amp;amp;37.21=front&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input_0_5&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV_5_4&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.18=negative&amp;amp;43.19=right&amp;amp;43.20=top&amp;amp;43.21=back&amp;amp;3.16=open&amp;amp;44.1=wO_5_4&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=none&amp;amp;45.49=0&amp;amp;45.16=closed&amp;amp;50.51=8&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;50.16=closed&amp;amp;53.54=blocks&amp;amp;53.55=10&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.18=negative&amp;amp;53.19=left&amp;amp;53.20=top&amp;amp;53.21=front&amp;amp;53.16=closed&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.507&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.16=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.3&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.16=closed&amp;amp;78.8=&amp;amp;78.16=open&amp;amp;79.80=-459.733038437248&amp;amp;79.81=-10.183892342609507&amp;amp;79.82=399.6251724834292&amp;amp;83.80=-27.91725760321879&amp;amp;83.81=-18.858991089590095&amp;amp;83.82=-140.6826497984033&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;folder=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.left=22&amp;amp;left.left.left=23&amp;amp;left.left.left.left=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/gpt2_ik.jpg&quot; alt=&quot;The two partitions together divide attention into a grid of blocks&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This visualization makes clear&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the effectiveness of this double partitioning as an attack on the context length problem, since we’ve now visibly partitioned every occurrence of sequence length in the attention calculation&lt;/li&gt;
  &lt;li&gt;the “reach” of this second partitioning: it’s clear from the geometry that the in-projection computations of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; can be partitioned along with the core double matmul&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note one subtlety: the visual implication here is that we can also parallelize the subsequent matmul &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; and sum the partial results &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#parallelized-reductions&quot;&gt;split-k style&lt;/a&gt;, thus parallelizing the entire double matmul. But the row-wise softmax in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sdpa()&lt;/code&gt; adds the requirement that each row have all its segments normalized before the corresponding row of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; can be computed, adding an extra row-wise step between the attention calculation and the final matmul.&lt;/p&gt;

&lt;h2 id=&quot;6-sizes-in-an-attention-layer&quot;&gt;6 Sizes in an Attention Layer&lt;/h2&gt;

&lt;p&gt;The first (MHA) half of an attention layer is famously computationally demanding because of its quadratic complexity, but the second (FFN) half is demanding in its own right due to the width of its hidden dimension, typically 4 times that of the model’s embedding dimension. Visualizing the biomass of a full attention layer can be useful in building intuition about how the two halves of the layer compare to each other.&lt;/p&gt;

&lt;h3 id=&quot;6a-visualizing-the-full-layer&quot;&gt;6a Visualizing the full layer&lt;/h3&gt;

&lt;p&gt;Below is a full attention layer with the first half (MHA) in the background and the second (FFN) in the foreground. As usual, arrows point in the direction of computation.&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This visualization doesn’t depict individual attention heads, but instead shows the unsliced Q/K/V weights and projections surrounding a central double matmul. Of course this isn’t a faithful visualization of the full MHA operation - but the goal here is to give a clearer sense of the relative matrix &lt;em&gt;sizes&lt;/em&gt; in the two halves of the layer, rather than the relative amounts of computation each half performs. (Also, randomized values are used rather than real weights.)&lt;/li&gt;
  &lt;li&gt;The dimensions used here are downsized to keep the browser (relatively) happy, but the proportions are preserved (from &lt;a href=&quot;https://github.com/karpathy/nanoGPT/blob/master/model.py#L217&quot;&gt;NanoGPT’s small config&lt;/a&gt;): model embedding dimension = 192 (from 768), FFN embedding dimension = 768 (from 3072), sequence length = 256 (from 1024), although sequence length is not fundamental to the model. (Visually, changes in sequence length would appear as changes in the width of the input blades, and consequently in the size of the attention hub and the height of the downstream vertical planes.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out%20%3D%20(attn_out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO)%20%40%20FFN_1%20%40%20FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;94=true&amp;amp;3.1=attn_out%20%40%20FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=open&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.2=layernorm&amp;amp;23.13=inherit&amp;amp;24.15=1&amp;amp;24.16=open&amp;amp;25.18=negative&amp;amp;25.19=left&amp;amp;25.20=top&amp;amp;25.21=front&amp;amp;26.1=attn%20%40%20V&amp;amp;26.4=true&amp;amp;26.5=32&amp;amp;26.6=32&amp;amp;26.7=row%20major&amp;amp;26.8=&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.2=none&amp;amp;27.13=vmprod&amp;amp;28.15=1&amp;amp;28.16=open&amp;amp;29.18=positive&amp;amp;29.19=left&amp;amp;29.20=bottom&amp;amp;29.21=back&amp;amp;30.1=attn&amp;amp;30.4=true&amp;amp;30.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;31.1=Q&amp;amp;31.4=true&amp;amp;31.2=none&amp;amp;32.1=input&amp;amp;32.4=false&amp;amp;32.5=256&amp;amp;32.6=192&amp;amp;32.7=gaussian&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;32.0=&amp;amp;32.16=open&amp;amp;33.1=wQ&amp;amp;33.4=false&amp;amp;33.5=192&amp;amp;33.6=192&amp;amp;33.7=gaussian&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;33.0=&amp;amp;33.16=open&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.18=positive&amp;amp;36.19=left&amp;amp;36.20=bottom&amp;amp;36.21=back&amp;amp;31.16=closed&amp;amp;37.2=none&amp;amp;38.13=none&amp;amp;39.15=1&amp;amp;40.18=positive&amp;amp;40.19=right&amp;amp;40.20=top&amp;amp;40.21=back&amp;amp;41.1=wK_t&amp;amp;41.4=false&amp;amp;41.5=192&amp;amp;41.6=192&amp;amp;41.7=gaussian&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;41.0=&amp;amp;41.16=open&amp;amp;42.1=input_t&amp;amp;42.4=false&amp;amp;42.5=192&amp;amp;42.6=256&amp;amp;42.7=gaussian&amp;amp;42.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;42.9=-1&amp;amp;42.10=1&amp;amp;42.11=0&amp;amp;42.0=&amp;amp;42.16=open&amp;amp;37.1=K_t&amp;amp;37.4=true&amp;amp;37.16=closed&amp;amp;43.13=vmprod&amp;amp;44.15=1&amp;amp;44.16=open&amp;amp;45.18=negative&amp;amp;45.19=left&amp;amp;45.20=top&amp;amp;45.21=front&amp;amp;30.16=open&amp;amp;46.1=V&amp;amp;46.4=true&amp;amp;46.2=none&amp;amp;47.1=input&amp;amp;47.4=false&amp;amp;47.5=256&amp;amp;47.6=192&amp;amp;47.7=gaussian&amp;amp;47.9=-1&amp;amp;47.10=1&amp;amp;47.11=0&amp;amp;47.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;47.0=&amp;amp;47.16=open&amp;amp;48.1=wV&amp;amp;48.4=false&amp;amp;48.5=192&amp;amp;48.6=192&amp;amp;48.7=gaussian&amp;amp;48.9=-1&amp;amp;48.10=1&amp;amp;48.11=0&amp;amp;48.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;48.0=&amp;amp;48.16=open&amp;amp;49.13=none&amp;amp;50.15=1&amp;amp;51.18=negative&amp;amp;51.19=right&amp;amp;51.20=top&amp;amp;51.21=back&amp;amp;46.16=open&amp;amp;26.16=open&amp;amp;52.1=wO&amp;amp;52.4=false&amp;amp;52.5=192&amp;amp;52.6=192&amp;amp;52.7=gaussian&amp;amp;52.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;52.9=-1&amp;amp;52.10=0.996&amp;amp;52.11=0&amp;amp;52.0=&amp;amp;52.16=closed&amp;amp;22.1=attn_out&amp;amp;22.4=true&amp;amp;22.16=open&amp;amp;53.1=FFN_1&amp;amp;53.4=false&amp;amp;53.5=192&amp;amp;53.6=768&amp;amp;53.7=gaussian&amp;amp;53.8=&amp;amp;53.9=-1&amp;amp;53.10=1&amp;amp;53.11=0&amp;amp;53.0=&amp;amp;53.16=closed&amp;amp;3.16=open&amp;amp;54.1=FFN_2&amp;amp;54.4=false&amp;amp;54.5=768&amp;amp;54.6=192&amp;amp;54.7=gaussian&amp;amp;54.8=&amp;amp;54.9=-1&amp;amp;54.10=1&amp;amp;54.11=0&amp;amp;54.0=&amp;amp;54.16=closed&amp;amp;55.56=sync&amp;amp;55.57=16&amp;amp;55.58=false&amp;amp;55.13=none&amp;amp;55.59=0&amp;amp;55.16=closed&amp;amp;60.61=1&amp;amp;60.62=1&amp;amp;60.15=1&amp;amp;60.16=closed&amp;amp;63.64=blocks&amp;amp;63.65=8&amp;amp;63.66=0&amp;amp;63.67=1&amp;amp;63.68=0&amp;amp;63.18=negative&amp;amp;63.19=left&amp;amp;63.20=top&amp;amp;63.21=front&amp;amp;63.16=closed&amp;amp;69.70=10&amp;amp;69.71=true&amp;amp;69.72=4&amp;amp;69.73=0.507&amp;amp;69.74=0.524&amp;amp;69.75=0.5&amp;amp;69.76=12&amp;amp;69.77=false&amp;amp;69.78=false&amp;amp;69.16=closed&amp;amp;79.80=local&amp;amp;79.81=0.1&amp;amp;79.82=0.2&amp;amp;79.83=0.9&amp;amp;79.84=2&amp;amp;79.85=0.75&amp;amp;79.86=0.75&amp;amp;79.87=0.03&amp;amp;79.16=closed&amp;amp;88.8=&amp;amp;88.16=open&amp;amp;89.90=-738.1526976199144&amp;amp;89.91=919.9001193338946&amp;amp;89.92=957.7418906526483&amp;amp;93.90=0&amp;amp;93.91=0&amp;amp;93.92=0&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;folder=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.left=22&amp;amp;left.left.anim=23&amp;amp;left.left.block=24&amp;amp;left.left.layout=25&amp;amp;left.left.left=26&amp;amp;left.left.left.anim=27&amp;amp;left.left.left.block=28&amp;amp;left.left.left.layout=29&amp;amp;left.left.left.left=30&amp;amp;left.left.left.left.left=31&amp;amp;left.left.left.left.left.left=32&amp;amp;left.left.left.left.left.right=33&amp;amp;left.left.left.left.left.anim=34&amp;amp;left.left.left.left.left.block=35&amp;amp;left.left.left.left.left.layout=36&amp;amp;left.left.left.left.right=37&amp;amp;left.left.left.left.right.anim=38&amp;amp;left.left.left.left.right.block=39&amp;amp;left.left.left.left.right.layout=40&amp;amp;left.left.left.left.right.left=41&amp;amp;left.left.left.left.right.right=42&amp;amp;left.left.left.left.anim=43&amp;amp;left.left.left.left.block=44&amp;amp;left.left.left.left.layout=45&amp;amp;left.left.left.right=46&amp;amp;left.left.left.right.left=47&amp;amp;left.left.left.right.right=48&amp;amp;left.left.left.right.anim=49&amp;amp;left.left.left.right.block=50&amp;amp;left.left.left.right.layout=51&amp;amp;left.left.right=52&amp;amp;left.right=53&amp;amp;right=54&amp;amp;anim=55&amp;amp;fuse=56&amp;amp;speed=57&amp;amp;hide%20inputs=58&amp;amp;spin=59&amp;amp;block=60&amp;amp;i%20blocks=61&amp;amp;j%20blocks=62&amp;amp;layout=63&amp;amp;scheme=64&amp;amp;gap=65&amp;amp;scatter=66&amp;amp;molecule=67&amp;amp;blast=68&amp;amp;deco=69&amp;amp;legends=70&amp;amp;shape=71&amp;amp;spotlight=72&amp;amp;row%20guides=73&amp;amp;flow%20guides=74&amp;amp;lens%20size=75&amp;amp;magnification=76&amp;amp;interior%20spotlight=77&amp;amp;axes=78&amp;amp;viz=79&amp;amp;sensitivity=80&amp;amp;min%20size=81&amp;amp;min%20light=82&amp;amp;max%20light=83&amp;amp;elem%20scale=84&amp;amp;zero%20hue=85&amp;amp;hue%20gap=86&amp;amp;hue%20spread=87&amp;amp;diag=88&amp;amp;cam=89&amp;amp;x=90&amp;amp;y=91&amp;amp;z=92&amp;amp;cam.target=93&amp;amp;compress=94&quot;&gt;Open in mm&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/attnlayer2.jpg&quot; alt=&quot;a full attention layer with the first half (MHA) in the background and the second (FFN) in the foreground&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6b-visualizing-the-bpt-partitioned-layer&quot;&gt;6b Visualizing the BPT partitioned layer&lt;/h3&gt;

&lt;p&gt;Revisiting &lt;a href=&quot;https://arxiv.org/pdf/2305.19370.pdf&quot;&gt;Blockwise Parallel Transformer&lt;/a&gt; briefly, here we visualize BPT’s parallelization scheme in the context of an entire attention layer (with individual heads elided per above). In particular, note how the partitioning along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; (of sequence blocks) extends through both MHA and FFN halves (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out%20%3D%20(attn_out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO)%20%40%20FFN_1%20%40%20FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;94=true&amp;amp;3.1=attn_out%20%40%20FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=closed&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.2=layernorm&amp;amp;23.13=inherit&amp;amp;24.15=1&amp;amp;24.16=closed&amp;amp;25.18=negative&amp;amp;25.19=left&amp;amp;25.20=top&amp;amp;25.21=front&amp;amp;26.1=attn%20%40%20V&amp;amp;26.4=true&amp;amp;26.5=32&amp;amp;26.6=32&amp;amp;26.7=row%20major&amp;amp;26.8=&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.2=none&amp;amp;27.13=vmprod&amp;amp;28.15=8&amp;amp;28.16=open&amp;amp;29.18=positive&amp;amp;29.19=left&amp;amp;29.20=bottom&amp;amp;29.21=back&amp;amp;30.1=attn&amp;amp;30.4=true&amp;amp;30.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;31.1=Q&amp;amp;31.4=true&amp;amp;31.2=none&amp;amp;32.1=input&amp;amp;32.4=false&amp;amp;32.5=256&amp;amp;32.6=192&amp;amp;32.7=gaussian&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;32.0=&amp;amp;32.16=open&amp;amp;33.1=wQ&amp;amp;33.4=false&amp;amp;33.5=192&amp;amp;33.6=192&amp;amp;33.7=gaussian&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;33.0=&amp;amp;33.16=open&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.18=positive&amp;amp;36.19=left&amp;amp;36.20=bottom&amp;amp;36.21=back&amp;amp;31.16=closed&amp;amp;37.2=none&amp;amp;38.13=none&amp;amp;39.15=1&amp;amp;40.18=positive&amp;amp;40.19=right&amp;amp;40.20=top&amp;amp;40.21=back&amp;amp;41.1=wK_t&amp;amp;41.4=false&amp;amp;41.5=192&amp;amp;41.6=192&amp;amp;41.7=gaussian&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;41.0=&amp;amp;41.16=open&amp;amp;42.1=input_t&amp;amp;42.4=false&amp;amp;42.5=192&amp;amp;42.6=256&amp;amp;42.7=gaussian&amp;amp;42.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;42.9=-1&amp;amp;42.10=1&amp;amp;42.11=0&amp;amp;42.0=&amp;amp;42.16=open&amp;amp;37.1=K_t&amp;amp;37.4=true&amp;amp;37.16=closed&amp;amp;43.13=vmprod&amp;amp;44.15=1&amp;amp;44.16=open&amp;amp;45.18=negative&amp;amp;45.19=left&amp;amp;45.20=top&amp;amp;45.21=front&amp;amp;30.16=closed&amp;amp;46.1=V&amp;amp;46.4=true&amp;amp;46.2=none&amp;amp;47.1=input&amp;amp;47.4=false&amp;amp;47.5=256&amp;amp;47.6=192&amp;amp;47.7=gaussian&amp;amp;47.9=-1&amp;amp;47.10=1&amp;amp;47.11=0&amp;amp;47.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;47.0=&amp;amp;47.16=open&amp;amp;48.1=wV&amp;amp;48.4=false&amp;amp;48.5=192&amp;amp;48.6=192&amp;amp;48.7=gaussian&amp;amp;48.9=-1&amp;amp;48.10=1&amp;amp;48.11=0&amp;amp;48.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;48.0=&amp;amp;48.16=open&amp;amp;49.13=none&amp;amp;50.15=1&amp;amp;51.18=negative&amp;amp;51.19=right&amp;amp;51.20=top&amp;amp;51.21=back&amp;amp;46.16=closed&amp;amp;26.16=open&amp;amp;52.1=wO&amp;amp;52.4=false&amp;amp;52.5=192&amp;amp;52.6=192&amp;amp;52.7=gaussian&amp;amp;52.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;52.9=-1&amp;amp;52.10=0.996&amp;amp;52.11=0&amp;amp;52.0=&amp;amp;52.16=closed&amp;amp;22.1=attn_out&amp;amp;22.4=true&amp;amp;22.16=open&amp;amp;53.1=FFN_1&amp;amp;53.4=false&amp;amp;53.5=192&amp;amp;53.6=768&amp;amp;53.7=gaussian&amp;amp;53.8=&amp;amp;53.9=-1&amp;amp;53.10=1&amp;amp;53.11=0&amp;amp;53.0=&amp;amp;53.16=closed&amp;amp;3.16=open&amp;amp;54.1=FFN_2&amp;amp;54.4=false&amp;amp;54.5=768&amp;amp;54.6=192&amp;amp;54.7=gaussian&amp;amp;54.8=&amp;amp;54.9=-1&amp;amp;54.10=1&amp;amp;54.11=0&amp;amp;54.0=&amp;amp;54.16=closed&amp;amp;55.56=sync&amp;amp;55.57=16&amp;amp;55.58=false&amp;amp;55.13=none&amp;amp;55.59=0&amp;amp;55.16=closed&amp;amp;60.61=8&amp;amp;60.62=1&amp;amp;60.15=1&amp;amp;60.16=open&amp;amp;63.64=blocks&amp;amp;63.65=8&amp;amp;63.66=0&amp;amp;63.67=1&amp;amp;63.68=0&amp;amp;63.18=negative&amp;amp;63.19=left&amp;amp;63.20=top&amp;amp;63.21=front&amp;amp;63.16=closed&amp;amp;69.70=10&amp;amp;69.71=true&amp;amp;69.72=4&amp;amp;69.73=0.507&amp;amp;69.74=0.524&amp;amp;69.75=0.5&amp;amp;69.76=12&amp;amp;69.77=false&amp;amp;69.78=false&amp;amp;69.16=closed&amp;amp;79.80=local&amp;amp;79.81=0.1&amp;amp;79.82=0.2&amp;amp;79.83=0.9&amp;amp;79.84=2&amp;amp;79.85=0.75&amp;amp;79.86=0.75&amp;amp;79.87=0.03&amp;amp;79.16=closed&amp;amp;88.8=&amp;amp;88.16=open&amp;amp;89.90=-766.4372214429399&amp;amp;89.91=955.1488380935747&amp;amp;89.92=994.4406298292719&amp;amp;93.90=0&amp;amp;93.91=0&amp;amp;93.92=0&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;folder=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.left=22&amp;amp;left.left.anim=23&amp;amp;left.left.block=24&amp;amp;left.left.layout=25&amp;amp;left.left.left=26&amp;amp;left.left.left.anim=27&amp;amp;left.left.left.block=28&amp;amp;left.left.left.layout=29&amp;amp;left.left.left.left=30&amp;amp;left.left.left.left.left=31&amp;amp;left.left.left.left.left.left=32&amp;amp;left.left.left.left.left.right=33&amp;amp;left.left.left.left.left.anim=34&amp;amp;left.left.left.left.left.block=35&amp;amp;left.left.left.left.left.layout=36&amp;amp;left.left.left.left.right=37&amp;amp;left.left.left.left.right.anim=38&amp;amp;left.left.left.left.right.block=39&amp;amp;left.left.left.left.right.layout=40&amp;amp;left.left.left.left.right.left=41&amp;amp;left.left.left.left.right.right=42&amp;amp;left.left.left.left.anim=43&amp;amp;left.left.left.left.block=44&amp;amp;left.left.left.left.layout=45&amp;amp;left.left.left.right=46&amp;amp;left.left.left.right.left=47&amp;amp;left.left.left.right.right=48&amp;amp;left.left.left.right.anim=49&amp;amp;left.left.left.right.block=50&amp;amp;left.left.left.right.layout=51&amp;amp;left.left.right=52&amp;amp;left.right=53&amp;amp;right=54&amp;amp;anim=55&amp;amp;fuse=56&amp;amp;speed=57&amp;amp;hide%20inputs=58&amp;amp;spin=59&amp;amp;block=60&amp;amp;i%20blocks=61&amp;amp;j%20blocks=62&amp;amp;layout=63&amp;amp;scheme=64&amp;amp;gap=65&amp;amp;scatter=66&amp;amp;molecule=67&amp;amp;blast=68&amp;amp;deco=69&amp;amp;legends=70&amp;amp;shape=71&amp;amp;spotlight=72&amp;amp;row%20guides=73&amp;amp;flow%20guides=74&amp;amp;lens%20size=75&amp;amp;magnification=76&amp;amp;interior%20spotlight=77&amp;amp;axes=78&amp;amp;viz=79&amp;amp;sensitivity=80&amp;amp;min%20size=81&amp;amp;min%20light=82&amp;amp;max%20light=83&amp;amp;elem%20scale=84&amp;amp;zero%20hue=85&amp;amp;hue%20gap=86&amp;amp;hue%20spread=87&amp;amp;diag=88&amp;amp;cam=89&amp;amp;x=90&amp;amp;y=91&amp;amp;z=92&amp;amp;cam.target=93&amp;amp;compress=94&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/bptlayer.jpg&quot; alt=&quot;visualize BPT's parallelization scheme in the context of an entire attention layer&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6c-partitioning-the-ffn&quot;&gt;6c Partitioning the FFN&lt;/h3&gt;

&lt;p&gt;The visualization suggests an additional partitioning, orthogonal to the ones described above - in the FFN half of the attention layer, splitting the double matmul &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(attn_out @ FFN_1) @ FFN_2&lt;/code&gt;, first along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn_out @ FFN_1&lt;/code&gt;, then along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; in the subsequent matmul with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FFN_2&lt;/code&gt;. This partition slices both layers of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FFN&lt;/code&gt; weights, reducing the capacity requirements of each participant in the computation at the cost of a final summation of the partial results.&lt;/p&gt;

&lt;p&gt;Here’s what this partition looks like applied to an otherwise unpartitioned attention layer (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out+%3D+%28attn_out+%3D+%28attn+%3D+%28Q+%3D+input+%40+wQ%29+%40+%28K_t+%3D+wK_t+%40+input_t%29%29+%40+%28V+%3D+input+%40+wV%29+%40+wO%29+%40+FFN_1+%40+FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;94=true&amp;amp;3.1=attn_out+%40+FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row+major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=open&amp;amp;14.17=1&amp;amp;14.18=8&amp;amp;19.20=positive&amp;amp;19.21=left&amp;amp;19.22=bottom&amp;amp;19.23=back&amp;amp;24.2=layernorm&amp;amp;25.13=inherit&amp;amp;26.15=1&amp;amp;26.16=closed&amp;amp;26.17=1&amp;amp;26.18=1&amp;amp;27.20=negative&amp;amp;27.21=left&amp;amp;27.22=top&amp;amp;27.23=front&amp;amp;28.1=attn+%40+V&amp;amp;28.4=true&amp;amp;28.5=32&amp;amp;28.6=32&amp;amp;28.7=row+major&amp;amp;28.8=&amp;amp;28.9=-1&amp;amp;28.10=1&amp;amp;28.11=0&amp;amp;28.2=none&amp;amp;29.13=vmprod&amp;amp;30.15=1&amp;amp;30.16=open&amp;amp;30.17=1&amp;amp;30.18=1&amp;amp;31.20=positive&amp;amp;31.21=left&amp;amp;31.22=bottom&amp;amp;31.23=back&amp;amp;32.1=attn&amp;amp;32.4=true&amp;amp;32.2=softmax%28tril%28x%2Fsqrt%28k%29%29%29&amp;amp;33.1=Q&amp;amp;33.4=true&amp;amp;33.2=none&amp;amp;34.1=input&amp;amp;34.4=false&amp;amp;34.5=256&amp;amp;34.6=192&amp;amp;34.7=gaussian&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;34.0=&amp;amp;34.16=open&amp;amp;35.1=wQ&amp;amp;35.4=false&amp;amp;35.5=192&amp;amp;35.6=192&amp;amp;35.7=gaussian&amp;amp;35.9=-1&amp;amp;35.10=1&amp;amp;35.11=0&amp;amp;35.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;35.0=&amp;amp;35.16=open&amp;amp;36.13=vmprod&amp;amp;37.15=1&amp;amp;37.17=1&amp;amp;37.18=1&amp;amp;38.20=positive&amp;amp;38.21=left&amp;amp;38.22=bottom&amp;amp;38.23=back&amp;amp;33.16=closed&amp;amp;39.2=none&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;41.17=1&amp;amp;41.18=1&amp;amp;42.20=positive&amp;amp;42.21=right&amp;amp;42.22=top&amp;amp;42.23=back&amp;amp;43.1=wK_t&amp;amp;43.4=false&amp;amp;43.5=192&amp;amp;43.6=192&amp;amp;43.7=gaussian&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;43.16=open&amp;amp;44.1=input_t&amp;amp;44.4=false&amp;amp;44.5=192&amp;amp;44.6=256&amp;amp;44.7=gaussian&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;44.16=open&amp;amp;39.1=K_t&amp;amp;39.4=true&amp;amp;39.16=closed&amp;amp;45.13=vmprod&amp;amp;46.15=1&amp;amp;46.16=open&amp;amp;46.17=1&amp;amp;46.18=1&amp;amp;47.20=negative&amp;amp;47.21=left&amp;amp;47.22=top&amp;amp;47.23=front&amp;amp;32.16=closed&amp;amp;48.1=V&amp;amp;48.4=true&amp;amp;48.2=none&amp;amp;49.1=input&amp;amp;49.4=false&amp;amp;49.5=256&amp;amp;49.6=192&amp;amp;49.7=gaussian&amp;amp;49.9=-1&amp;amp;49.10=1&amp;amp;49.11=0&amp;amp;49.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;49.0=&amp;amp;49.16=open&amp;amp;50.1=wV&amp;amp;50.4=false&amp;amp;50.5=192&amp;amp;50.6=192&amp;amp;50.7=gaussian&amp;amp;50.9=-1&amp;amp;50.10=1&amp;amp;50.11=0&amp;amp;50.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;50.0=&amp;amp;50.16=open&amp;amp;51.13=none&amp;amp;52.15=1&amp;amp;52.17=1&amp;amp;52.18=1&amp;amp;53.20=negative&amp;amp;53.21=right&amp;amp;53.22=top&amp;amp;53.23=back&amp;amp;48.16=closed&amp;amp;28.16=open&amp;amp;54.1=wO&amp;amp;54.4=false&amp;amp;54.5=192&amp;amp;54.6=192&amp;amp;54.7=gaussian&amp;amp;54.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;54.9=-1&amp;amp;54.10=0.996&amp;amp;54.11=0&amp;amp;54.0=&amp;amp;54.16=closed&amp;amp;24.1=attn_out&amp;amp;24.4=true&amp;amp;24.16=closed&amp;amp;55.1=FFN_1&amp;amp;55.4=false&amp;amp;55.5=192&amp;amp;55.6=768&amp;amp;55.7=gaussian&amp;amp;55.8=&amp;amp;55.9=-1&amp;amp;55.10=1&amp;amp;55.11=0&amp;amp;55.0=&amp;amp;55.16=closed&amp;amp;3.16=open&amp;amp;56.1=FFN_2&amp;amp;56.4=false&amp;amp;56.5=768&amp;amp;56.6=192&amp;amp;56.7=gaussian&amp;amp;56.8=&amp;amp;56.9=-1&amp;amp;56.10=1&amp;amp;56.11=0&amp;amp;56.0=&amp;amp;56.16=closed&amp;amp;57.58=sync&amp;amp;57.59=16&amp;amp;57.60=false&amp;amp;57.13=none&amp;amp;57.61=0&amp;amp;57.16=closed&amp;amp;62.17=1&amp;amp;62.15=8&amp;amp;62.18=1&amp;amp;62.16=closed&amp;amp;63.64=blocks&amp;amp;63.65=8&amp;amp;63.66=0&amp;amp;63.67=1&amp;amp;63.68=0&amp;amp;63.20=negative&amp;amp;63.21=left&amp;amp;63.22=top&amp;amp;63.23=front&amp;amp;63.16=closed&amp;amp;69.70=10&amp;amp;69.71=true&amp;amp;69.72=4&amp;amp;69.73=0.507&amp;amp;69.74=0.524&amp;amp;69.75=0.5&amp;amp;69.76=12&amp;amp;69.77=false&amp;amp;69.78=false&amp;amp;69.16=closed&amp;amp;79.80=local&amp;amp;79.81=0.1&amp;amp;79.82=0.2&amp;amp;79.83=0.9&amp;amp;79.84=2&amp;amp;79.85=0.75&amp;amp;79.86=0.75&amp;amp;79.87=0.03&amp;amp;79.16=closed&amp;amp;88.8=&amp;amp;88.16=open&amp;amp;89.90=-725.0392607527422&amp;amp;89.91=909.2543497392985&amp;amp;89.92=1420.9035091451585&amp;amp;93.90=84.4062135237143&amp;amp;93.91=2.295441349889614&amp;amp;93.92=60.16668289640925&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k+blocks=15&amp;amp;folder=16&amp;amp;i+blocks=17&amp;amp;j+blocks=18&amp;amp;left.layout=19&amp;amp;polarity=20&amp;amp;left+placement=21&amp;amp;right+placement=22&amp;amp;result+placement=23&amp;amp;left.left=24&amp;amp;left.left.anim=25&amp;amp;left.left.block=26&amp;amp;left.left.layout=27&amp;amp;left.left.left=28&amp;amp;left.left.left.anim=29&amp;amp;left.left.left.block=30&amp;amp;left.left.left.layout=31&amp;amp;left.left.left.left=32&amp;amp;left.left.left.left.left=33&amp;amp;left.left.left.left.left.left=34&amp;amp;left.left.left.left.left.right=35&amp;amp;left.left.left.left.left.anim=36&amp;amp;left.left.left.left.left.block=37&amp;amp;left.left.left.left.left.layout=38&amp;amp;left.left.left.left.right=39&amp;amp;left.left.left.left.right.anim=40&amp;amp;left.left.left.left.right.block=41&amp;amp;left.left.left.left.right.layout=42&amp;amp;left.left.left.left.right.left=43&amp;amp;left.left.left.left.right.right=44&amp;amp;left.left.left.left.anim=45&amp;amp;left.left.left.left.block=46&amp;amp;left.left.left.left.layout=47&amp;amp;left.left.left.right=48&amp;amp;left.left.left.right.left=49&amp;amp;left.left.left.right.right=50&amp;amp;left.left.left.right.anim=51&amp;amp;left.left.left.right.block=52&amp;amp;left.left.left.right.layout=53&amp;amp;left.left.right=54&amp;amp;left.right=55&amp;amp;right=56&amp;amp;anim=57&amp;amp;fuse=58&amp;amp;speed=59&amp;amp;hide+inputs=60&amp;amp;spin=61&amp;amp;block=62&amp;amp;layout=63&amp;amp;scheme=64&amp;amp;gap=65&amp;amp;scatter=66&amp;amp;molecule=67&amp;amp;blast=68&amp;amp;deco=69&amp;amp;legends=70&amp;amp;shape=71&amp;amp;spotlight=72&amp;amp;row+guides=73&amp;amp;flow+guides=74&amp;amp;lens+size=75&amp;amp;magnification=76&amp;amp;interior+spotlight=77&amp;amp;axes=78&amp;amp;viz=79&amp;amp;sensitivity=80&amp;amp;min+size=81&amp;amp;min+light=82&amp;amp;max+light=83&amp;amp;elem+scale=84&amp;amp;zero+hue=85&amp;amp;hue+gap=86&amp;amp;hue+spread=87&amp;amp;diag=88&amp;amp;cam=89&amp;amp;x=90&amp;amp;y=91&amp;amp;z=92&amp;amp;cam.target=93&amp;amp;compress=94&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/attnlayer_ffnsplitk.jpg&quot; alt=&quot;what this partition looks like applied to an otherwise unpartitioned attention layer&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here it is applied to a layer partitioned a la BPT (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out+%3D+%28attn_out+%3D+%28attn+%3D+%28Q+%3D+input+%40+wQ%29+%40+%28K_t+%3D+wK_t+%40+input_t%29%29+%40+%28V+%3D+input+%40+wV%29+%40+wO%29+%40+FFN_1+%40+FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;94=true&amp;amp;3.1=attn_out+%40+FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row+major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=open&amp;amp;14.17=8&amp;amp;14.18=8&amp;amp;19.20=positive&amp;amp;19.21=left&amp;amp;19.22=bottom&amp;amp;19.23=back&amp;amp;24.2=layernorm&amp;amp;25.13=inherit&amp;amp;26.15=1&amp;amp;26.16=closed&amp;amp;26.17=8&amp;amp;26.18=1&amp;amp;27.20=negative&amp;amp;27.21=left&amp;amp;27.22=top&amp;amp;27.23=front&amp;amp;28.1=attn+%40+V&amp;amp;28.4=true&amp;amp;28.5=32&amp;amp;28.6=32&amp;amp;28.7=row+major&amp;amp;28.8=&amp;amp;28.9=-1&amp;amp;28.10=1&amp;amp;28.11=0&amp;amp;28.2=none&amp;amp;29.13=vmprod&amp;amp;30.15=8&amp;amp;30.16=open&amp;amp;30.17=8&amp;amp;30.18=1&amp;amp;31.20=positive&amp;amp;31.21=left&amp;amp;31.22=bottom&amp;amp;31.23=back&amp;amp;32.1=attn&amp;amp;32.4=true&amp;amp;32.2=softmax%28tril%28x%2Fsqrt%28k%29%29%29&amp;amp;33.1=Q&amp;amp;33.4=true&amp;amp;33.2=none&amp;amp;34.1=input&amp;amp;34.4=false&amp;amp;34.5=256&amp;amp;34.6=192&amp;amp;34.7=gaussian&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;34.0=&amp;amp;34.16=open&amp;amp;35.1=wQ&amp;amp;35.4=false&amp;amp;35.5=192&amp;amp;35.6=192&amp;amp;35.7=gaussian&amp;amp;35.9=-1&amp;amp;35.10=1&amp;amp;35.11=0&amp;amp;35.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;35.0=&amp;amp;35.16=open&amp;amp;36.13=vmprod&amp;amp;37.15=1&amp;amp;37.17=8&amp;amp;37.18=1&amp;amp;38.20=positive&amp;amp;38.21=left&amp;amp;38.22=bottom&amp;amp;38.23=back&amp;amp;33.16=closed&amp;amp;39.2=none&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;41.17=1&amp;amp;41.18=1&amp;amp;42.20=positive&amp;amp;42.21=right&amp;amp;42.22=top&amp;amp;42.23=back&amp;amp;43.1=wK_t&amp;amp;43.4=false&amp;amp;43.5=192&amp;amp;43.6=192&amp;amp;43.7=gaussian&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;43.16=open&amp;amp;44.1=input_t&amp;amp;44.4=false&amp;amp;44.5=192&amp;amp;44.6=256&amp;amp;44.7=gaussian&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;44.16=open&amp;amp;39.1=K_t&amp;amp;39.4=true&amp;amp;39.16=closed&amp;amp;45.13=vmprod&amp;amp;46.15=1&amp;amp;46.16=open&amp;amp;46.17=8&amp;amp;46.18=1&amp;amp;47.20=negative&amp;amp;47.21=left&amp;amp;47.22=top&amp;amp;47.23=front&amp;amp;32.16=closed&amp;amp;48.1=V&amp;amp;48.4=true&amp;amp;48.2=none&amp;amp;49.1=input&amp;amp;49.4=false&amp;amp;49.5=256&amp;amp;49.6=192&amp;amp;49.7=gaussian&amp;amp;49.9=-1&amp;amp;49.10=1&amp;amp;49.11=0&amp;amp;49.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;49.0=&amp;amp;49.16=open&amp;amp;50.1=wV&amp;amp;50.4=false&amp;amp;50.5=192&amp;amp;50.6=192&amp;amp;50.7=gaussian&amp;amp;50.9=-1&amp;amp;50.10=1&amp;amp;50.11=0&amp;amp;50.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;50.0=&amp;amp;50.16=open&amp;amp;51.13=none&amp;amp;52.15=1&amp;amp;52.17=1&amp;amp;52.18=1&amp;amp;53.20=negative&amp;amp;53.21=right&amp;amp;53.22=top&amp;amp;53.23=back&amp;amp;48.16=closed&amp;amp;28.16=open&amp;amp;54.1=wO&amp;amp;54.4=false&amp;amp;54.5=192&amp;amp;54.6=192&amp;amp;54.7=gaussian&amp;amp;54.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;54.9=-1&amp;amp;54.10=0.996&amp;amp;54.11=0&amp;amp;54.0=&amp;amp;54.16=closed&amp;amp;24.1=attn_out&amp;amp;24.4=true&amp;amp;24.16=closed&amp;amp;55.1=FFN_1&amp;amp;55.4=false&amp;amp;55.5=192&amp;amp;55.6=768&amp;amp;55.7=gaussian&amp;amp;55.8=&amp;amp;55.9=-1&amp;amp;55.10=1&amp;amp;55.11=0&amp;amp;55.0=&amp;amp;55.16=closed&amp;amp;3.16=open&amp;amp;56.1=FFN_2&amp;amp;56.4=false&amp;amp;56.5=768&amp;amp;56.6=192&amp;amp;56.7=gaussian&amp;amp;56.8=&amp;amp;56.9=-1&amp;amp;56.10=1&amp;amp;56.11=0&amp;amp;56.0=&amp;amp;56.16=closed&amp;amp;57.58=sync&amp;amp;57.59=16&amp;amp;57.60=false&amp;amp;57.13=none&amp;amp;57.61=0&amp;amp;57.16=closed&amp;amp;62.17=8&amp;amp;62.15=8&amp;amp;62.18=1&amp;amp;62.16=open&amp;amp;63.64=blocks&amp;amp;63.65=8&amp;amp;63.66=0&amp;amp;63.67=1&amp;amp;63.68=0&amp;amp;63.20=negative&amp;amp;63.21=left&amp;amp;63.22=top&amp;amp;63.23=front&amp;amp;63.16=closed&amp;amp;69.70=10&amp;amp;69.71=true&amp;amp;69.72=4&amp;amp;69.73=0.507&amp;amp;69.74=0.524&amp;amp;69.75=0.5&amp;amp;69.76=12&amp;amp;69.77=false&amp;amp;69.78=false&amp;amp;69.16=closed&amp;amp;79.80=local&amp;amp;79.81=0.1&amp;amp;79.82=0.2&amp;amp;79.83=0.9&amp;amp;79.84=2&amp;amp;79.85=0.75&amp;amp;79.86=0.75&amp;amp;79.87=0.03&amp;amp;79.16=closed&amp;amp;88.8=&amp;amp;88.16=open&amp;amp;89.90=-908.5990219796431&amp;amp;89.91=1012.984380609292&amp;amp;89.92=1378.7815259698948&amp;amp;93.90=57.978218494193676&amp;amp;93.91=-30.847130586978256&amp;amp;93.92=41.66771129059017&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k+blocks=15&amp;amp;folder=16&amp;amp;i+blocks=17&amp;amp;j+blocks=18&amp;amp;left.layout=19&amp;amp;polarity=20&amp;amp;left+placement=21&amp;amp;right+placement=22&amp;amp;result+placement=23&amp;amp;left.left=24&amp;amp;left.left.anim=25&amp;amp;left.left.block=26&amp;amp;left.left.layout=27&amp;amp;left.left.left=28&amp;amp;left.left.left.anim=29&amp;amp;left.left.left.block=30&amp;amp;left.left.left.layout=31&amp;amp;left.left.left.left=32&amp;amp;left.left.left.left.left=33&amp;amp;left.left.left.left.left.left=34&amp;amp;left.left.left.left.left.right=35&amp;amp;left.left.left.left.left.anim=36&amp;amp;left.left.left.left.left.block=37&amp;amp;left.left.left.left.left.layout=38&amp;amp;left.left.left.left.right=39&amp;amp;left.left.left.left.right.anim=40&amp;amp;left.left.left.left.right.block=41&amp;amp;left.left.left.left.right.layout=42&amp;amp;left.left.left.left.right.left=43&amp;amp;left.left.left.left.right.right=44&amp;amp;left.left.left.left.anim=45&amp;amp;left.left.left.left.block=46&amp;amp;left.left.left.left.layout=47&amp;amp;left.left.left.right=48&amp;amp;left.left.left.right.left=49&amp;amp;left.left.left.right.right=50&amp;amp;left.left.left.right.anim=51&amp;amp;left.left.left.right.block=52&amp;amp;left.left.left.right.layout=53&amp;amp;left.left.right=54&amp;amp;left.right=55&amp;amp;right=56&amp;amp;anim=57&amp;amp;fuse=58&amp;amp;speed=59&amp;amp;hide+inputs=60&amp;amp;spin=61&amp;amp;block=62&amp;amp;layout=63&amp;amp;scheme=64&amp;amp;gap=65&amp;amp;scatter=66&amp;amp;molecule=67&amp;amp;blast=68&amp;amp;deco=69&amp;amp;legends=70&amp;amp;shape=71&amp;amp;spotlight=72&amp;amp;row+guides=73&amp;amp;flow+guides=74&amp;amp;lens+size=75&amp;amp;magnification=76&amp;amp;interior+spotlight=77&amp;amp;axes=78&amp;amp;viz=79&amp;amp;sensitivity=80&amp;amp;min+size=81&amp;amp;min+light=82&amp;amp;max+light=83&amp;amp;elem+scale=84&amp;amp;zero+hue=85&amp;amp;hue+gap=86&amp;amp;hue+spread=87&amp;amp;diag=88&amp;amp;cam=89&amp;amp;x=90&amp;amp;y=91&amp;amp;z=92&amp;amp;cam.target=93&amp;amp;compress=94&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/bptlayer_ffnsplitk.jpg&quot; alt=&quot;applied to a layer partitioned a la BPT&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6d-visualizing-token-at-a-time-decoding&quot;&gt;6d Visualizing token-at-a-time decoding&lt;/h3&gt;

&lt;p&gt;During autoregressive token-at-a-time decoding, the query vector consists of a single token. It’s instructive to have a mental picture of what an attention layer looks like in that situation - a single embedding row working its way through an enormous tiled plane of weights.&lt;/p&gt;

&lt;p&gt;Aside from the emphasizing the sheer immensity of weights compared to activations, this view is also evocative of the notion that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K_t&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; function like dynamically generated layers in a 6-layer MLP, although the mux/demux computations of MHA itself (papered over here, per above) make the correspondence inexact (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out%20%3D%20(attn_out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20K_t)%20%40%20V%20%40%20wO)%20%40%20FFN_1%20%40%20FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;84=true&amp;amp;3.1=attn_out%20%40%20FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=open&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.2=layernorm&amp;amp;23.13=inherit&amp;amp;24.15=1&amp;amp;24.16=open&amp;amp;25.18=negative&amp;amp;25.19=left&amp;amp;25.20=top&amp;amp;25.21=front&amp;amp;26.1=attn%20%40%20V&amp;amp;26.4=true&amp;amp;26.5=32&amp;amp;26.6=32&amp;amp;26.7=row%20major&amp;amp;26.8=&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.2=none&amp;amp;27.13=vmprod&amp;amp;28.15=1&amp;amp;28.16=open&amp;amp;29.18=positive&amp;amp;29.19=left&amp;amp;29.20=bottom&amp;amp;29.21=back&amp;amp;30.1=attn&amp;amp;30.4=true&amp;amp;30.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;31.1=Q&amp;amp;31.4=true&amp;amp;31.2=none&amp;amp;32.1=input&amp;amp;32.4=false&amp;amp;32.5=1&amp;amp;32.6=192&amp;amp;32.7=gaussian&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;32.0=&amp;amp;32.16=open&amp;amp;33.1=wQ&amp;amp;33.4=false&amp;amp;33.5=192&amp;amp;33.6=192&amp;amp;33.7=gaussian&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;33.0=&amp;amp;33.16=open&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.18=positive&amp;amp;36.19=left&amp;amp;36.20=bottom&amp;amp;36.21=back&amp;amp;31.16=open&amp;amp;37.1=K_t&amp;amp;37.4=false&amp;amp;37.5=192&amp;amp;37.6=256&amp;amp;37.7=gaussian&amp;amp;37.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;37.9=-1&amp;amp;37.10=1&amp;amp;37.11=0&amp;amp;37.0=&amp;amp;37.16=open&amp;amp;38.13=vmprod&amp;amp;39.15=1&amp;amp;39.16=open&amp;amp;40.18=negative&amp;amp;40.19=left&amp;amp;40.20=top&amp;amp;40.21=front&amp;amp;30.16=open&amp;amp;41.1=V&amp;amp;41.4=false&amp;amp;41.16=open&amp;amp;41.5=256&amp;amp;41.6=192&amp;amp;41.7=gaussian&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;41.0=&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;26.16=open&amp;amp;42.1=wO&amp;amp;42.4=false&amp;amp;42.5=192&amp;amp;42.6=192&amp;amp;42.7=gaussian&amp;amp;42.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;42.9=-1&amp;amp;42.10=0.996&amp;amp;42.11=0&amp;amp;42.0=&amp;amp;42.16=closed&amp;amp;22.1=attn_out&amp;amp;22.4=true&amp;amp;22.16=open&amp;amp;43.1=FFN_1&amp;amp;43.4=false&amp;amp;43.5=192&amp;amp;43.6=768&amp;amp;43.7=gaussian&amp;amp;43.8=&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;43.16=closed&amp;amp;3.16=open&amp;amp;44.1=FFN_2&amp;amp;44.4=false&amp;amp;44.5=768&amp;amp;44.6=192&amp;amp;44.7=gaussian&amp;amp;44.8=&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;44.16=closed&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=none&amp;amp;45.49=0&amp;amp;45.16=closed&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;50.16=closed&amp;amp;53.54=blocks&amp;amp;53.55=8&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.18=negative&amp;amp;53.19=left&amp;amp;53.20=top&amp;amp;53.21=front&amp;amp;53.16=closed&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.507&amp;amp;59.64=0.524&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.16=closed&amp;amp;69.70=local&amp;amp;69.71=0.1&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.16=closed&amp;amp;78.8=&amp;amp;78.16=open&amp;amp;79.80=-621.3352100945762&amp;amp;79.81=774.3199147754915&amp;amp;79.82=806.172978523008&amp;amp;83.80=0&amp;amp;83.81=0&amp;amp;83.82=0&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;folder=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.left=22&amp;amp;left.left.anim=23&amp;amp;left.left.block=24&amp;amp;left.left.layout=25&amp;amp;left.left.left=26&amp;amp;left.left.left.anim=27&amp;amp;left.left.left.block=28&amp;amp;left.left.left.layout=29&amp;amp;left.left.left.left=30&amp;amp;left.left.left.left.left=31&amp;amp;left.left.left.left.left.left=32&amp;amp;left.left.left.left.left.right=33&amp;amp;left.left.left.left.left.anim=34&amp;amp;left.left.left.left.left.block=35&amp;amp;left.left.left.left.left.layout=36&amp;amp;left.left.left.left.right=37&amp;amp;left.left.left.left.anim=38&amp;amp;left.left.left.left.block=39&amp;amp;left.left.left.left.layout=40&amp;amp;left.left.left.right=41&amp;amp;left.left.right=42&amp;amp;left.right=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/decoding.jpg&quot; alt=&quot;the mux/demux computations of MHA itself&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;7-lora&quot;&gt;7 LoRA&lt;/h2&gt;

&lt;p&gt;The recent LoRA paper (&lt;a href=&quot;https://arxiv.org/pdf/2106.09685.pdf&quot;&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/a&gt;) describes an efficient finetuning technique based on the idea that weight deltas introduced during finetuning are low-rank. Per the paper, this “allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers’ change during adaptation […], while keeping the pre-trained weights frozen.”&lt;/p&gt;

&lt;h3 id=&quot;7a-the-basic-idea&quot;&gt;7a The basic idea&lt;/h3&gt;

&lt;p&gt;In a nutshell, the key move is to train the &lt;em&gt;factors&lt;/em&gt; of a weight matrix rather than the matrix itself: replace an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I x J&lt;/code&gt; weights tensor with a matmul of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I x K&lt;/code&gt; tensor and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K x J&lt;/code&gt; tensor, holding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; to some small number.&lt;/p&gt;

&lt;p&gt;If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; is small enough the size win can be huge, but the tradeoff is that lowering it lowers the rank of what the product can express. As a quick illustration of both the size savings and the structuring effect on the result, here’s a matmul of random &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;128 x 4&lt;/code&gt; left and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4 x 128&lt;/code&gt; right arguments - a.k.a. a rank-4 factorization of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;128 x 128&lt;/code&gt; matrix. Notice the vertical and horizontal patterning in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L @ R&lt;/code&gt; (&lt;a href=&quot;https://bhosmer.github.io/mm/?0=L%20%40%20R&amp;amp;1=L%20%40%20R&amp;amp;2=none&amp;amp;12=closed&amp;amp;59=true&amp;amp;3.1=L&amp;amp;3.4=false&amp;amp;3.5=128&amp;amp;3.6=4&amp;amp;3.7=gaussian&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.0=&amp;amp;3.12=open&amp;amp;13.1=R&amp;amp;13.4=false&amp;amp;13.5=4&amp;amp;13.6=128&amp;amp;13.7=gaussian&amp;amp;13.8=&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.0=&amp;amp;14.15=none&amp;amp;14.16=1&amp;amp;14.17=false&amp;amp;14.18=none&amp;amp;14.19=0&amp;amp;14.20=1&amp;amp;14.21=1&amp;amp;14.22=1&amp;amp;23.20=1&amp;amp;23.22=1&amp;amp;23.21=1&amp;amp;24.25=blocks&amp;amp;24.26=11.214&amp;amp;24.27=0&amp;amp;24.28=1&amp;amp;24.29=0&amp;amp;24.30=negative&amp;amp;24.31=left&amp;amp;24.32=top&amp;amp;24.33=front&amp;amp;24.12=open&amp;amp;34.35=10&amp;amp;34.36=true&amp;amp;34.37=2&amp;amp;34.38=1&amp;amp;34.39=0&amp;amp;34.40=0.5&amp;amp;34.41=10&amp;amp;34.42=false&amp;amp;34.43=false&amp;amp;34.12=open&amp;amp;44.45=local&amp;amp;44.46=0.3&amp;amp;44.47=0.5&amp;amp;44.48=0.7&amp;amp;44.49=1.25&amp;amp;44.50=0.77&amp;amp;44.51=0.74&amp;amp;44.52=0.04&amp;amp;44.12=open&amp;amp;53.8=&amp;amp;54.55=-147.50937470998977&amp;amp;54.56=141.1312665550063&amp;amp;54.57=104.24779022699425&amp;amp;58.55=-7.97607936427614&amp;amp;58.56=3.391570600844088&amp;amp;58.57=-11.685048965074932&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;right=13&amp;amp;anim=14&amp;amp;fuse=15&amp;amp;speed=16&amp;amp;hide%20inputs=17&amp;amp;alg=18&amp;amp;spin=19&amp;amp;i%20blocks=20&amp;amp;k%20blocks=21&amp;amp;j%20blocks=22&amp;amp;block=23&amp;amp;layout=24&amp;amp;scheme=25&amp;amp;gap=26&amp;amp;scatter=27&amp;amp;molecule=28&amp;amp;blast=29&amp;amp;polarity=30&amp;amp;left%20placement=31&amp;amp;right%20placement=32&amp;amp;result%20placement=33&amp;amp;deco=34&amp;amp;legends=35&amp;amp;shape=36&amp;amp;spotlight=37&amp;amp;row%20guides=38&amp;amp;flow%20guides=39&amp;amp;lens%20size=40&amp;amp;magnification=41&amp;amp;interior%20spotlight=42&amp;amp;axes=43&amp;amp;viz=44&amp;amp;sensitivity=45&amp;amp;min%20size=46&amp;amp;min%20light=47&amp;amp;max%20light=48&amp;amp;elem%20scale=49&amp;amp;zero%20hue=50&amp;amp;hue%20gap=51&amp;amp;hue%20spread=52&amp;amp;diag=53&amp;amp;cam=54&amp;amp;x=55&amp;amp;y=56&amp;amp;z=57&amp;amp;cam.target=58&amp;amp;compress=59&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/lora_single.jpg&quot; alt=&quot;a matmul of random 128 x 4 left and 4 x 128 right arguments&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;7b-applying-lora-to-an-attention-head&quot;&gt;7b Applying LoRA to an attention head&lt;/h3&gt;

&lt;p&gt;The way LoRA applies this factoring move to the fine tuning process is to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;create a low-rank factorization for each weight tensor to be fine-tuned and train the factors, keeping the original weights frozen&lt;/li&gt;
  &lt;li&gt;after fine tuning, multiply each pair of low-rank factors to get a matrix in the shape of the original weights tensor, and add it to the original pretrained weights tensor&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following visualization shows an attention head with the weight tensors &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wQ&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wK_t&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wV&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wO&lt;/code&gt; replaced by low rank factorizations &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wQ_A @ wQ_B&lt;/code&gt;, etc. Visually, the factor matrices show up as low fences along the edges of the windmill blades (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20(wQ%20%3D%20wQ_A%20%40%20wQ_B))%20%40%20(K_t%20%3D%20(wK_t%20%3D%20wK_t_A%20%40%20wK_t_B)%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20(wV%20%3D%20wV_A%20%40%20wV_B))%20%40%20(wO%20%3D%20wO_A%20%40%20wO_B)&amp;amp;1=out&amp;amp;2=none&amp;amp;15=closed&amp;amp;104=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=rows&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=inherit&amp;amp;12.14=1&amp;amp;12.15=open&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input&amp;amp;23.4=false&amp;amp;23.5=64&amp;amp;23.6=96&amp;amp;23.7=gaussian&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;23.15=closed&amp;amp;24.1=wQ&amp;amp;24.4=true&amp;amp;24.2=none&amp;amp;25.1=wQ_A&amp;amp;25.4=false&amp;amp;25.5=96&amp;amp;25.6=8&amp;amp;25.7=gaussian&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq0_768_64.csv&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.0=&amp;amp;25.15=closed&amp;amp;26.1=wQ_B&amp;amp;26.4=false&amp;amp;26.5=8&amp;amp;26.6=32&amp;amp;26.7=gaussian&amp;amp;26.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq0_768_64.csv&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.0=&amp;amp;26.15=open&amp;amp;27.13=inherit&amp;amp;27.14=1&amp;amp;27.15=closed&amp;amp;28.17=negative&amp;amp;28.18=right&amp;amp;28.19=top&amp;amp;28.20=back&amp;amp;29.30=1&amp;amp;24.15=closed&amp;amp;31.13=inherit&amp;amp;31.14=1&amp;amp;31.15=closed&amp;amp;32.17=positive&amp;amp;32.18=left&amp;amp;32.19=bottom&amp;amp;32.20=back&amp;amp;33.30=1&amp;amp;22.15=closed&amp;amp;34.2=none&amp;amp;35.13=inherit&amp;amp;35.14=1&amp;amp;35.15=closed&amp;amp;36.17=positive&amp;amp;36.18=right&amp;amp;36.19=top&amp;amp;36.20=back&amp;amp;37.1=wK_t&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=wK_t_A&amp;amp;38.4=false&amp;amp;38.5=32&amp;amp;38.6=8&amp;amp;38.7=gaussian&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t0_64_768.csv&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.0=&amp;amp;38.15=open&amp;amp;39.1=wK_t_B&amp;amp;39.4=false&amp;amp;39.5=8&amp;amp;39.6=96&amp;amp;39.7=gaussian&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t0_64_768.csv&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.0=&amp;amp;39.15=closed&amp;amp;40.13=inherit&amp;amp;40.14=1&amp;amp;40.15=open&amp;amp;41.17=negative&amp;amp;41.18=left&amp;amp;41.19=bottom&amp;amp;41.20=back&amp;amp;42.30=1&amp;amp;37.15=closed&amp;amp;43.1=input_t&amp;amp;43.4=false&amp;amp;43.5=96&amp;amp;43.6=64&amp;amp;43.7=gaussian&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;43.15=closed&amp;amp;34.1=K_t&amp;amp;34.4=true&amp;amp;44.30=1&amp;amp;34.15=closed&amp;amp;45.13=inherit&amp;amp;45.14=1&amp;amp;45.15=closed&amp;amp;46.17=negative&amp;amp;46.18=left&amp;amp;46.19=top&amp;amp;46.20=front&amp;amp;47.30=1&amp;amp;21.15=closed&amp;amp;48.1=V&amp;amp;48.4=true&amp;amp;48.2=none&amp;amp;49.1=input&amp;amp;49.4=false&amp;amp;49.5=64&amp;amp;49.6=96&amp;amp;49.7=gaussian&amp;amp;49.9=-1&amp;amp;49.10=1&amp;amp;49.11=0&amp;amp;49.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;49.0=&amp;amp;49.15=closed&amp;amp;50.1=wV&amp;amp;50.4=true&amp;amp;50.2=none&amp;amp;51.1=wV_A&amp;amp;51.4=false&amp;amp;51.5=96&amp;amp;51.6=8&amp;amp;51.7=gaussian&amp;amp;51.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv0_768_64.csv&amp;amp;51.9=-1&amp;amp;51.10=1&amp;amp;51.11=0&amp;amp;51.0=&amp;amp;51.15=open&amp;amp;52.1=wV_B&amp;amp;52.4=false&amp;amp;52.5=8&amp;amp;52.6=32&amp;amp;52.7=gaussian&amp;amp;52.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv0_768_64.csv&amp;amp;52.9=-1&amp;amp;52.10=1&amp;amp;52.11=0&amp;amp;52.0=&amp;amp;52.15=open&amp;amp;53.13=inherit&amp;amp;53.14=1&amp;amp;53.15=open&amp;amp;54.17=positive&amp;amp;54.18=left&amp;amp;54.19=bottom&amp;amp;54.20=back&amp;amp;55.30=1&amp;amp;50.15=closed&amp;amp;56.13=inherit&amp;amp;56.14=1&amp;amp;56.15=closed&amp;amp;57.17=negative&amp;amp;57.18=right&amp;amp;57.19=top&amp;amp;57.20=back&amp;amp;58.30=1&amp;amp;48.15=closed&amp;amp;3.15=closed&amp;amp;59.30=1&amp;amp;60.1=wO&amp;amp;60.4=true&amp;amp;60.5=32&amp;amp;60.6=32&amp;amp;60.7=cols&amp;amp;60.8=&amp;amp;60.9=-1&amp;amp;60.10=1&amp;amp;60.11=0&amp;amp;60.2=none&amp;amp;61.1=wO_A&amp;amp;61.4=false&amp;amp;61.5=32&amp;amp;61.6=8&amp;amp;61.7=gaussian&amp;amp;61.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo0_64_768.csv&amp;amp;61.9=-1&amp;amp;61.10=1&amp;amp;61.11=0&amp;amp;61.0=&amp;amp;61.15=open&amp;amp;62.1=wO_B&amp;amp;62.4=false&amp;amp;62.5=8&amp;amp;62.6=96&amp;amp;62.7=gaussian&amp;amp;62.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo0_64_768.csv&amp;amp;62.9=-1&amp;amp;62.10=1&amp;amp;62.11=0&amp;amp;62.0=&amp;amp;62.15=closed&amp;amp;63.13=inherit&amp;amp;63.14=1&amp;amp;63.15=closed&amp;amp;64.17=positive&amp;amp;64.18=right&amp;amp;64.19=top&amp;amp;64.20=back&amp;amp;60.15=closed&amp;amp;65.30=1&amp;amp;66.67=none&amp;amp;66.68=100&amp;amp;66.69=false&amp;amp;66.13=none&amp;amp;66.70=-3&amp;amp;66.71=1&amp;amp;66.30=1&amp;amp;66.14=1&amp;amp;66.15=open&amp;amp;72.71=1&amp;amp;72.14=1&amp;amp;72.30=1&amp;amp;73.74=blocks&amp;amp;73.75=2&amp;amp;73.76=8&amp;amp;73.77=2&amp;amp;73.78=0&amp;amp;73.17=negative&amp;amp;73.18=left&amp;amp;73.19=top&amp;amp;73.20=front&amp;amp;73.15=closed&amp;amp;79.80=6.5&amp;amp;79.81=false&amp;amp;79.82=1&amp;amp;79.83=1&amp;amp;79.84=0.757&amp;amp;79.85=0.5&amp;amp;79.86=10&amp;amp;79.87=false&amp;amp;79.88=false&amp;amp;79.15=open&amp;amp;89.90=local&amp;amp;89.91=0.2&amp;amp;89.92=0.3&amp;amp;89.93=1&amp;amp;89.94=1.25&amp;amp;89.95=0.75&amp;amp;89.96=0.75&amp;amp;89.97=0.03&amp;amp;89.15=closed&amp;amp;98.8=&amp;amp;98.15=closed&amp;amp;99.100=-172.19348886030096&amp;amp;99.101=179.87607098671913&amp;amp;99.102=291.20723943546824&amp;amp;103.100=-6.083689158200286&amp;amp;103.101=-2.2203698054118064&amp;amp;103.102=-20.406063431589725&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;j%20blocks=14&amp;amp;folder=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.right.left=25&amp;amp;left.left.left.right.right=26&amp;amp;left.left.left.right.anim=27&amp;amp;left.left.left.right.layout=28&amp;amp;left.left.left.right.block=29&amp;amp;k%20blocks=30&amp;amp;left.left.left.anim=31&amp;amp;left.left.left.layout=32&amp;amp;left.left.left.block=33&amp;amp;left.left.right=34&amp;amp;left.left.right.anim=35&amp;amp;left.left.right.layout=36&amp;amp;left.left.right.left=37&amp;amp;left.left.right.left.left=38&amp;amp;left.left.right.left.right=39&amp;amp;left.left.right.left.anim=40&amp;amp;left.left.right.left.layout=41&amp;amp;left.left.right.left.block=42&amp;amp;left.left.right.right=43&amp;amp;left.left.right.block=44&amp;amp;left.left.anim=45&amp;amp;left.left.layout=46&amp;amp;left.left.block=47&amp;amp;left.right=48&amp;amp;left.right.left=49&amp;amp;left.right.right=50&amp;amp;left.right.right.left=51&amp;amp;left.right.right.right=52&amp;amp;left.right.right.anim=53&amp;amp;left.right.right.layout=54&amp;amp;left.right.right.block=55&amp;amp;left.right.anim=56&amp;amp;left.right.layout=57&amp;amp;left.right.block=58&amp;amp;left.block=59&amp;amp;right=60&amp;amp;right.left=61&amp;amp;right.right=62&amp;amp;right.anim=63&amp;amp;right.layout=64&amp;amp;right.block=65&amp;amp;anim=66&amp;amp;fuse=67&amp;amp;speed=68&amp;amp;hide%20inputs=69&amp;amp;spin=70&amp;amp;i%20blocks=71&amp;amp;block=72&amp;amp;layout=73&amp;amp;scheme=74&amp;amp;gap=75&amp;amp;scatter=76&amp;amp;molecule=77&amp;amp;blast=78&amp;amp;deco=79&amp;amp;legends=80&amp;amp;shape=81&amp;amp;spotlight=82&amp;amp;row%20guides=83&amp;amp;flow%20guides=84&amp;amp;lens%20size=85&amp;amp;magnification=86&amp;amp;interior%20spotlight=87&amp;amp;axes=88&amp;amp;viz=89&amp;amp;sensitivity=90&amp;amp;min%20size=91&amp;amp;min%20light=92&amp;amp;max%20light=93&amp;amp;elem%20scale=94&amp;amp;zero%20hue=95&amp;amp;hue%20gap=96&amp;amp;hue%20spread=97&amp;amp;diag=98&amp;amp;cam=99&amp;amp;x=100&amp;amp;y=101&amp;amp;z=102&amp;amp;cam.target=103&amp;amp;compress=104&quot;&gt;open in mm&lt;/a&gt; - spacebar stops the spin):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/lora_spin4b.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h2 id=&quot;8-wrapup&quot;&gt;8 Wrapup&lt;/h2&gt;

&lt;h3 id=&quot;8a-call-for-feedback&quot;&gt;8a Call for feedback&lt;/h3&gt;

&lt;p&gt;I’ve found this way of visualizing matmul expressions extremely helpful for building intuition and reasoning about not just matrix multiplication itself, but also many aspects of ML models and their computation, from efficiency to interpretability.&lt;/p&gt;

&lt;p&gt;if you try it out and have suggestions or comments, I definitely want to hear, either in the comments here or &lt;a href=&quot;https://github.com/bhosmer/mm&quot;&gt;in the repo&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;8b-next-steps&quot;&gt;8b Next steps&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;There’s a &lt;a href=&quot;https://bhosmer.github.io/mm/examples/attngpt2/index.html&quot;&gt;GPT2 attention head explorer&lt;/a&gt; built on top of the tool which I’m currently using to inventory and classify the attention head traits found in that model. (This was the tool I used to find and explore the attention heads in this note.) Once complete I plan to post a note with the inventory.&lt;/li&gt;
  &lt;li&gt;As mentioned up top, embedding these visualizations in Python notebooks is &lt;a href=&quot;https://colab.research.google.com/drive/1wZIoU20eRWKtRNCW7e5Iugm3MhfaE1f7?usp=sharing&quot;&gt;dead simple&lt;/a&gt;. But session URLs can get… unwieldy, so it will be useful to have Python-side utilities for constructing them from configuration objects, similar to the simple JavaScript helpers used in the &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;reference guide&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;If you’ve got a use case you think might benefit from visualizations like this but it’s not obvious how to use the tool to do it, get in touch! I’m not necessarily looking to expand its core visualization capabilities that much further (right tool for the job, etc.), but e.g. the API for driving it programmatically is pretty basic, there’s plenty that can be done there.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Use 3D to visualize matrix multiplication expressions, attention heads with real weights, and more.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated CPU Inference with PyTorch Inductor using torch.compile</title>
      <link href="https://pytorch.org/blog/accelerated-cpu-inference/" rel="alternate" type="text/html" title="Accelerated CPU Inference with PyTorch Inductor using torch.compile" />
      <published>2023-09-13T00:00:00-07:00</published>
      <updated>2023-09-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-cpu-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-cpu-inference/">&lt;h2 id=&quot;story-at-a-glance&quot;&gt;Story at a Glance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Although the PyTorch* Inductor C++/OpenMP* backend has enabled users to take advantage of modern CPU architectures and parallel processing, it has lacked optimizations, resulting in the backend performing worse than eager mode in terms of end-to-end performance.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Intel optimized the Inductor backend using a hybrid strategy that classified operations into two categories: Conv/GEMM and non-Conv/GEMM element-wise and reduction ops.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;For popular deep learning models, this hybrid strategy demonstrates promising performance improvements compared to eager mode and improves the C++/OpenMP backend’s efficiency and reliability for PyTorch models.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;inductor-backend-challenges&quot;&gt;Inductor Backend Challenges&lt;/h2&gt;

&lt;p&gt;The PyTorch Inductor C++/OpenMP backend enables users to take advantage of modern CPU architectures and parallel processing to accelerate computations.&lt;/p&gt;

&lt;p&gt;However, during the early stages of its development, the backend lacked some optimizations, which prevented it from fully utilizing the CPU computation capabilities. As a result, for most models the C++/OpenMP backend performed worse than eager mode in terms of end-to-end performance, with 45% of TorchBench, 100% of Hugging Face, and 75% of TIMM models performing worse than eager mode.&lt;/p&gt;

&lt;p&gt;In this post, we highlight Intel’s optimizations to the Inductor CPU backend, including the technologies and results.&lt;/p&gt;

&lt;p&gt;We optimized the backend by using a hybrid strategy that classified operations into two categories: Conv/GEMM and non-Conv/GEMM element-wise and reduction ops. Post-op fusion and weight prepacking using the oneDNN performance library were utilized to optimize the former, while explicit vectorization in C++ codegen was used to optimize the latter.&lt;/p&gt;

&lt;p&gt;This hybrid strategy demonstrated promising performance improvements compared to eager mode, particularly on popular deep learning models such as Inductor Hugging Face, Inductor TorchBench and Inductor TIMM. Overall, Intel’s optimizations improve the C++/OpenMP backend’s efficiency and reliability for PyTorch models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f1-pytorch-inference-speedup-ratio-trend-multi.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 1. Performance Speedup Ratio Trend&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Performance Speedup Ratio Trend&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;performance-status-of-intel-hybrid-optimizations&quot;&gt;Performance Status of Intel Hybrid Optimizations&lt;/h3&gt;

&lt;p&gt;Compared to eager mode with the hybrid optimizations, the C++/OpenMP backend shows promising performance improvements. We measured the performance of the three Inductor benchmark suites—TorchBench, Hugging Face, and TIMM—and the results are as follows. (&lt;em&gt;Note: we publish our performance data twice per week on &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/93531&quot;&gt;GitHub&lt;/a&gt;.&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Overall, these optimizations help to ensure that the C++/OpenMP backend provides efficient and reliable support for PyTorch models.&lt;/p&gt;

&lt;h3 id=&quot;passrate&quot;&gt;Passrate&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor | 93%, 56/60 | 96%, 44/46  | 100%, 61/61 |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;geometric-mean-speedup-single-socket-multi-threads&quot;&gt;Geometric mean speedup (Single-Socket Multi-threads)&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor |   1.39x    |    1.20x    |    1.73x    |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;individual-model-performance&quot;&gt;Individual Model Performance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f2-torchbench-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 2. TorchBench FP32 Performance (Single-Socket Multi-threads)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: TorchBench FP32 Performance (Single-Socket Multi-threads)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f3-huggingface-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 3. Hugging Face FP32 Performance (Single-Socket Multi-thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Hugging Face FP32 Performance (Single-Socket Multi-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f4-timm-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 4. TIMM FP32 Performance (Single-Socket Multi-threads)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: TIMM FP32 Performance (Single-Socket Multi-threads)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;geometric-mean-speedup-single-core-single-thread&quot;&gt;Geometric mean speedup (Single-core Single-thread)&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor |    1.29x   |    1.15x    |    1.37x    |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f5-torchbench-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 5. TorchBench FP32 Performance (Single-Socket Single-thread)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: TorchBench FP32 Performance (Single-Socket Single-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f6-huggingface-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 6. Hugging Face FP32 Performance (Single-Socket Single Thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: Hugging Face FP32 Performance (Single-Socket Single Thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f7-timm-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 7. TIMM FP32 Performance (Single-Socket Single-thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;: TIMM FP32 Performance (Single-Socket Single-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;technical-deep-dive&quot;&gt;Technical Deep Dive&lt;/h2&gt;

&lt;p&gt;Now, let’s take a closer look at the two primary optimizations used in the Inductor C++/OpenMP backend:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;weight prepacking and post-operation fusion via oneDNN library&lt;/li&gt;
  &lt;li&gt;explicit vectorization in Inductor C++ codegen&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;weight-prepackaging--post-op-fusion-via-onednn&quot;&gt;Weight Prepackaging &amp;amp; Post-op Fusion via oneDNN&lt;/h3&gt;

&lt;p&gt;Shorthand for Intel® oneAPI Deep Neural Network Library, oneDNN library provides a range of post-op fusions (i.e., fuse convolution and matmal with its consecutive operation) that can benefit popular models. The &lt;a href=&quot;https://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; has implemented most of these fusions and has achieved significant performance improvements. As a result, we have upstreamed all of these fusions that have been applied in Intel’s PyTorch extension to Inductor, enabling a wider range of models to benefit from these optimizations. We have defined these fusions as operators under the mkldnn namespace. This allows the Python module to invoke these mkldnn operations directly.&lt;/p&gt;

&lt;p&gt;Currently, the defined fused operations are as follows. You can find these defined fused operations at &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/aten/src/ATen/native/mkldnn/RegisterMkldnnOpContextClass.cpp#L35-#L48&quot;&gt;RegisterMkldnnOpContextClass.cpp&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_linear_pointwise&lt;/code&gt;: Fuses Linear and its post-unary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_linear_pointwise.binary&lt;/code&gt;: Fuses Linear and its post-binary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_convolution_pointwise&lt;/code&gt;: Fuses Convolution and its post-unary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_convolution_pointwise.binary&lt;/code&gt;: Fuses Convolution and its post-binary element-wise operations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The detailed fusion patterns are defined in the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/mkldnn.py#L774-#L818&quot;&gt;mkldnn.py&lt;/a&gt; file: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;convolution/linear + sigmoid/hardsigmoid/tanh/hardtanh/hardswish/leaky_relu/gelu/relu/relu6/siluconvolution/linear + add/add_/iadd/sub/sub_&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;On the Inductor side, we apply these fusions on the FX graph that has been lowered. We have defined &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/mkldnn.py#L491&quot;&gt;mkldnn_fuse_fx&lt;/a&gt; as the entry point to apply all the fusions. The code snippet for this is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def mkldnn_fuse_fx(gm: torch.fx.GraphModule, example_inputs):
    ...
    gm = fuse_unary(gm)
    gm = fuse_binary(gm)
    ...
    if config.cpp.weight_prepack:
        gm = pack_module(gm)
    return gm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkldnn_fuse_fx&lt;/code&gt; function, we apply fusion on the FX graph that hasn’t been lowered yet. To fuse convolution/linear and its consecutive elementwise operations, we invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fuse_unary&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fuse_binary&lt;/code&gt; as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   gm = fuse_unary(gm)
   gm = fuse_binary(gm)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to the post-op fusion, we apply weight prepacking to improve the Conv/GEMM performance further:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   gm = pack_module(gm)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Weight prepacking involves rearranging the weight tensor in a blocked layout, which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;can improve vectorization and cache reuse compared to plain formats like NCHW or NHWC and;&lt;/li&gt;
  &lt;li&gt;can help avoid weight reordering at runtime, which can reduce overhead and improve performance and;&lt;/li&gt;
  &lt;li&gt;increases memory usage as the tradeoff.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For these reasons, we provide &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config.cpp.weight_prepack&lt;/code&gt; flag in Inductor to provide users with more control over this optimization, allowing them to enable it based on their specific needs.&lt;/p&gt;

&lt;h3 id=&quot;explicit-vectorization-in-inductor-c-codegen&quot;&gt;Explicit Vectorization in Inductor C++ Codegen&lt;/h3&gt;

&lt;p&gt;Vectorization is a key optimization technique that can significantly improve the performance of numerical computations. By utilizing SIMD (Single Instruction, Multiple Data) instructions, vectorization enables multiple computations to be performed simultaneously on a single processor core, which can lead to significant performance improvements.&lt;/p&gt;

&lt;p&gt;In the Inductor C++/OpenMP backend, we use &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L372&quot;&gt;Intel® AVX2&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L359&quot;&gt;Intel® AVX-512&lt;/a&gt; ISA (Instruction Set Architecture) options for vectorization by leveraging the aten vectorization library to facilitate the implementation. Aten vectorization supports multiple platforms, including x86 and Arm, as well as multiple data types. It can be extended to support other ISAs easily by adding more &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L275&quot;&gt;VecISA&lt;/a&gt; sub-classes. This allows Inductor to easily support other platforms and data types in the future.&lt;/p&gt;

&lt;p&gt;Due to differences in platforms, the C++/OpenMP backend of Inductor starts by detecting the CPU features to determine the vectorization bit width at the beginning of code generation. By default, if the machine supports both AVX-512 and AVX2, the backend will choose 512-bit vectorization.&lt;/p&gt;

&lt;p&gt;If the hardware supports vectorization, the C++/OpenMP backend first detects if the loop body can be vectorized or not. There are primarily three scenarios that we are not able to generate kernel with vectorization:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Loop body lacks vector intrinsics support, e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rand&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;atomic_add&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Loop body lacks efficient vector intrinsics support, e.g., non-contiguous &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load/store&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Data types with vectorization not yet supported but work in progress, e.g., integer, double, half, and bfloat16.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To address this issue, the C++/OpenMP backend uses &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1396&quot;&gt;CppVecKernelChecker&lt;/a&gt; to detect whether all operations in a particular loop body can be vectorized or not. In general, we classified the operations into two categories by identifying if they depend on the context.&lt;/p&gt;

&lt;p&gt;For most elementwise operations such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relu&lt;/code&gt;, vectorization is straightforward, and their execution does not depend on context.&lt;/p&gt;

&lt;p&gt;However, for certain other operations, their semantics are more complex and their execution depends on context through static analysis.&lt;/p&gt;

&lt;p&gt;For example, let’s consider the where operation that takes in mask, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true_value&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false_value&lt;/code&gt; while the mask value is loaded from a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8&lt;/code&gt; tensor. The fx graph could be as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;graph():
    %ops : [#users=9] = placeholder[target=ops]
    %get_index : [#users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
    %load : [#users=1] = call_method[target=load](args = (%ops, arg1_1, %get_index), kwargs = {})
    %to_dtype : [#users=1] = call_method[target=to_dtype](args = (%ops, %load, torch.bool), kwargs = {})
    ...
    %where : [#users=1] = call_method[target=where](args = (%ops, %to_dtype, %to_dtype_2, %to_dtype_3), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Regarding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8&lt;/code&gt;, it is a general data type and could be used for computation but is not limited to being used as Boolean for mask. Hence, we need to analyze its context statically. In particular, the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1396&quot;&gt;CppVecKernelChecker&lt;/a&gt; will check whether a uint8 tensor is only used by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_dtype&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_dtype&lt;/code&gt; is only used by where. If yes, it could be vectorized. Otherwise, it will fall back to the scalar version. The generated code could be as follows:&lt;/p&gt;

&lt;p&gt;Scalar Version&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto tmp0 = in_ptr0[i1 + (17*i0)];
auto tmp3 = in_ptr1[i1 + (17*i0)];
auto tmp1 = static_cast&amp;lt;bool&amp;gt;(tmp0);
auto tmp2 = static_cast&amp;lt;float&amp;gt;(-33.0);
auto tmp4 = tmp1 ? tmp2 : tmp3;
tmp5 = std::max(tmp5, tmp4);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Vectorization Version&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float g_tmp_buffer_in_ptr0[16] = {0};
// Convert the flag to float for vectorization. 
flag_to_float(in_ptr0 + (16*i1) + (17*i0), g_tmp_buffer_in_ptr0, 16);
auto tmp0 = at::vec::Vectorized&amp;lt;float&amp;gt;::loadu(g_tmp_buffer_in_ptr0);
auto tmp3 = at::vec::Vectorized&amp;lt;float&amp;gt;::loadu(in_ptr1 + (16*i1) + (17*i0));
auto tmp1 = (tmp0);
auto tmp2 = at::vec::Vectorized&amp;lt;float&amp;gt;(static_cast&amp;lt;float&amp;gt;(-33.0));
auto tmp4 = decltype(tmp2)::blendv(tmp3, tmp2, tmp1);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to context analysis, the C++/OpenMP backend also incorporates several other vectorization-related optimizations. These include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tiled kernel implementation for supporting transpose load - &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1211&quot;&gt;cpp.py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Data type demotion based on value range - &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1647-#L1672&quot;&gt;cpp.py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Replacement of &lt;a href=&quot;http://github.com/shibatch/sleef/tree/e0a003ee838b75d11763aa9c3ef17bf71a725bff&quot;&gt;sleef&lt;/a&gt; implementation with oneDNN/oneMKL implementation for optimizing aten vectorization - &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/94577&quot;&gt;#94577&lt;/a&gt;, &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/92289&quot;&gt;#92289&lt;/a&gt;, &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/91613&quot;&gt;#91613&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, we examined vectorization optimization in Inductor C++ backend for FP32 training and inference of 150 benchmark models with 90% of inference kernels and 71% of training kernels being vectorized.&lt;/p&gt;

&lt;p&gt;In terms of inference, a total of 28,185 CPP kernels were generated, with 25,579 (90%) of them being vectorized, while the remaining 10% were scalar. As for training, 103,084 kernels were generated, with 73,909 (71%) being vectorized and 29% not vectorized.&lt;/p&gt;

&lt;p&gt;The results indicate that &lt;strong&gt;the vectorization of inference kernels is quite impressive&lt;/strong&gt; (there is still some work to be done in training kernels since we just started to work on the training). The remaining non-vectorized kernels are analyzed in different categories, highlighting the next steps to improve vectorization coverage: index-related operations, int64 support, vertical reduction, vectorization with fallback, and more.&lt;/p&gt;

&lt;p&gt;In addition, we also optimized the C++/OpenMP backend with other optimizations like buffer-reuse and CppWrapper.&lt;/p&gt;

&lt;h4 id=&quot;future-work&quot;&gt;Future Work&lt;/h4&gt;

&lt;p&gt;The next step, we will continue optimizing the C++/OpenMP backend and extend it to support more data types as the next step. This includes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Improve vectorization coverage&lt;/li&gt;
  &lt;li&gt;Support and optimize low precision kernel including BF16, FP16, Quantization&lt;/li&gt;
  &lt;li&gt;Training optimization&lt;/li&gt;
  &lt;li&gt;Loop tiling&lt;/li&gt;
  &lt;li&gt;Autotune&lt;/li&gt;
  &lt;li&gt;Further fusion optimization of Conv/GEMM kernels.&lt;/li&gt;
  &lt;li&gt;Explore alternative codegen paths: clang/llvm/triton&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Inductor C++/OpenMP backend is a flexible and efficient backend for the CPU. This blog describes the optimizations used in the C++/OpenMP backend of Inductor for inference and training of three benchmark suites – TorchBench, Hugging&lt;/p&gt;

&lt;p&gt;Face and TIMM. The primary optimizations include weight prepacking and post-operation fusion via the oneDNN library, as well as explicit vectorization in Inductor C++ codegen using AVX2 and AVX-512 instructions.&lt;/p&gt;

&lt;p&gt;The results show that 90% of inference kernels and 71% of training kernels are vectorized, indicating impressive vectorization for inference and room for improvement in training. In addition, we also applied other optimizations like buffer-reuse and CppWrapper. And we will continuously focus on the future work mentioned above to further improve the performance.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;The results presented in this blog post are the culmination of a collaborative effort between the Intel PyTorch team and Meta. We would like to express our sincere gratitude to &lt;a href=&quot;http://dev-discuss.pytorch.org/u/jansel&quot;&gt;@jansel&lt;/a&gt;, &lt;a href=&quot;http://dev-discuss.pytorch.org/u/desertfire&quot;&gt;@desertfire&lt;/a&gt;, and &lt;a href=&quot;http://dev-discuss.pytorch.org/u/chillee&quot;&gt;@Chillee&lt;/a&gt; for their invaluable contributions and unwavering support throughout the development process. Their expertise and dedication have been instrumental in achieving the optimizations and performance improvements discussed here.&lt;/p&gt;

&lt;h3 id=&quot;configuration-details&quot;&gt;Configuration Details&lt;/h3&gt;

&lt;h4 id=&quot;hardware-details&quot;&gt;Hardware Details&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Item &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Value &lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Manufacturer 
   &lt;/td&gt;
   &lt;td&gt;
Amazon EC2 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Product Name 
   &lt;/td&gt;
   &lt;td&gt;
c6i.16xlarge 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
CPU Model 
   &lt;/td&gt;
   &lt;td&gt;
Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Installed Memory 
   &lt;/td&gt;
   &lt;td&gt;
128GB (1x128GB DDR4 3200 MT/s [Unknown]) 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
OS 
   &lt;/td&gt;
   &lt;td&gt;
Ubuntu 22.04.2 LTS 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Kernel 
   &lt;/td&gt;
   &lt;td&gt;
5.19.0-1022-aws 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Microcode 
   &lt;/td&gt;
   &lt;td&gt;
0xd000389 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
GCC 
   &lt;/td&gt;
   &lt;td&gt;
gcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
GLIBC 
   &lt;/td&gt;
   &lt;td&gt;
ldd (Ubuntu GLIBC 2.35-0ubuntu3.1) 2.35 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Binutils 
   &lt;/td&gt;
   &lt;td&gt;
GNU ld (GNU Binutils for Ubuntu) 2.38 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Python 
   &lt;/td&gt;
   &lt;td&gt;
Python 3.10.6 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
OpenSSL 
   &lt;/td&gt;
   &lt;td&gt;
OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022) 
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;software-details&quot;&gt;Software Details&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;SW&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Nightly commit&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Main commit&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Pytorch
   &lt;/td&gt;
   &lt;td&gt;
a977a12
   &lt;/td&gt;
   &lt;td&gt;
0b1b063
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Torchbench
   &lt;/td&gt;
   &lt;td&gt;
/
   &lt;/td&gt;
   &lt;td&gt;
a0848e19
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchaudio
   &lt;/td&gt;
   &lt;td&gt;
0a652f5
   &lt;/td&gt;
   &lt;td&gt;
d5b2996
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchtext
   &lt;/td&gt;
   &lt;td&gt;
c4ad5dd
   &lt;/td&gt;
   &lt;td&gt;
79100a6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchvision
   &lt;/td&gt;
   &lt;td&gt;
f2009ab
   &lt;/td&gt;
   &lt;td&gt;
b78d98b
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchdata
   &lt;/td&gt;
   &lt;td&gt;
5cb3e6d
   &lt;/td&gt;
   &lt;td&gt;
f2bfd3d
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
dynamo_benchmarks
   &lt;/td&gt;
   &lt;td&gt;
fea73cb
   &lt;/td&gt;
   &lt;td&gt;
/
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;configuration&quot;&gt;Configuration&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Intel OpenMP&lt;/li&gt;
  &lt;li&gt;Jemalloc - oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:-1,muzzy_decay_ms:-1&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-Socket Multi-threads:&lt;/strong&gt; #of Instances: 1; Cores/Instance: 32&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-Core Single-thread:&lt;/strong&gt; #of Instances: 1; Cores/Instance: 1&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Story at a Glance</summary>
      

      
      
    </entry>
  
</feed>


