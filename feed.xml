<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-11-06T17:01:43-08:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs</title>
      <link href="https://pytorch.org/blog/high-performance-llama-2/" rel="alternate" type="text/html" title="High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs" />
      <published>2023-11-06T00:00:00-08:00</published>
      <updated>2023-11-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/high-performance-llama-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/high-performance-llama-2/">&lt;p&gt;In a landscape where AI innovation is accelerating at an unprecedented pace, Meta’s &lt;a href=&quot;https://ai.meta.com/llama/&quot;&gt;Llama&lt;/a&gt; family of open sourced large language models (LLMs) stands out as a notable breakthrough. &lt;a href=&quot;https://ai.meta.com/blog/large-language-model-llama-meta-ai/&quot;&gt;Llama&lt;/a&gt; marked a significant step forward for LLMs, demonstrating the power of pre-trained architectures for a wide range of applications. &lt;a href=&quot;https://about.fb.com/news/2023/07/llama-2/&quot;&gt;Llama 2&lt;/a&gt; further pushed the boundaries of scale and capabilities, inspiring advancements in language understanding, generation, and beyond.&lt;/p&gt;

&lt;p&gt;Shortly after the announcement of Llama, we published a &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;blog post&lt;/a&gt; showcasing ultra-low inference latency for Llama using PyTorch/XLA on Cloud TPU v4. Building on these results, today, we are proud to share Llama 2 training and inference performance using &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;PyTorch/XLA&lt;/a&gt; on Cloud TPU v4 and our newest AI supercomputer, &lt;a href=&quot;https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga&quot;&gt;Cloud TPU v5e&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post, we use Llama 2 as an example model to demonstrate the power of PyTorch/XLA on Cloud TPUs for LLM training and inference. We discuss the computation techniques and optimizations used to improve inference throughput and training model FLOPs utilization (MFU). &lt;strong&gt;For Llama 2 70B parameters, we deliver 53% training MFU, 17 ms/token inference latency, 42 tokens/s/chip throughput powered by PyTorch/XLA on Google Cloud TPU.&lt;/strong&gt; We offer a &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/SPMD_USER_GUIDE.md&quot;&gt;training user guide&lt;/a&gt; and an &lt;a href=&quot;https://github.com/pytorch-tpu/llama/blob/llama2-google-next-inference/TORCH_XLA_USER_GUIDE.md&quot;&gt;inference user guide&lt;/a&gt; for reproducing the results in this article. Additionally, you may find our &lt;a href=&quot;https://www.youtube.com/watch?v=PSpmRtWuMs8&quot;&gt;Google Next 2023 presentation here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;model-overview&quot;&gt;Model Overview&lt;/h2&gt;

&lt;p&gt;Llama 2 comes in various sizes, ranging from 7B to 70B parameters, catering to different needs, computational resources, and training / inference budgets. Whether it’s small-scale projects or large-scale deployments, Llama models offer versatility and scalability to accommodate a wide range of applications.&lt;/p&gt;

&lt;p&gt;Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The largest, 70B model, uses grouped-query attention, which speeds up inference without sacrificing quality. &lt;a href=&quot;https://arxiv.org/pdf/2307.09288.pdf&quot;&gt;Llama 2 is trained on 2 trillion tokens&lt;/a&gt; (40% more data than Llama) and has the context length of 4,096 tokens for inference (double the context length of Llama), which enables more accuracy, fluency, and creativity for the model.&lt;/p&gt;

&lt;p&gt;Llama 2 is a state-of-the-art LLM that outperforms many other open source language models on many benchmarks, including reasoning, coding, proficiency, and knowledge tests. The model’s scale and complexity place many demands on AI accelerators, making it an ideal benchmark for LLM training and inference performance of PyTorch/XLA on Cloud TPUs.&lt;/p&gt;

&lt;h2 id=&quot;performance-challenge-of-llms&quot;&gt;Performance Challenge of LLMs&lt;/h2&gt;

&lt;p&gt;Large-scale distributed training for LLMs such as Llama 2 introduces technical challenges that require practical solutions to make the most efficient use of TPUs. Llama’s size can strain both memory and processing resources of TPUs. To address this, we use model sharding, which involves breaking down the model into smaller segments, each fitting within the capacity of a single TPU core. This enables parallelism across multiple TPUs, improving training speed while reducing communication overhead.&lt;/p&gt;

&lt;p&gt;Another challenge is managing the large datasets required for training Llama 2 efficiently, which requires effective data distribution and synchronization methods. Additionally, optimizing factors like learning rate schedules, gradient aggregation, and weight synchronization across distributed TPUs is crucial for achieving convergence.&lt;/p&gt;

&lt;p&gt;After pretraining or fine-tuning Llama 2, running inference on the model checkpoint creates additional technical challenges. All of the challenges discussed in our &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;previous blog post&lt;/a&gt;, such as autoregressive decoding, variable input prompt lengths, and the need for model sharding and quantization still apply for Llama 2. In addition, Llama 2 introduced two new capabilities: grouped-query attention and early stopping. We discuss how PyTorch/XLA handles these challenges to enable high-performance, cost-efficient training and inference of Llama 2 on Cloud TPU v4 and v5e.&lt;/p&gt;

&lt;h2 id=&quot;large-scale-distributed-training&quot;&gt;Large-Scale Distributed Training&lt;/h2&gt;

&lt;p&gt;PyTorch/XLA offers two major ways of doing large-scale distributed training: &lt;a href=&quot;https://pytorch.org/blog/pytorch-xla-spmd/&quot;&gt;SPMD&lt;/a&gt;, which utilizes the XLA compiler to transform and partition a single-device program into a multi-device distributed program; and &lt;a href=&quot;https://pytorch.org/blog/large-scale-training-hugging-face/&quot;&gt;FSDP&lt;/a&gt;, which implements the widely-adopted &lt;a href=&quot;https://engineering.fb.com/2021/07/15/open-source/fsdp/&quot;&gt;Fully Sharded Data Parallel&lt;/a&gt; algorithm.&lt;/p&gt;

&lt;p&gt;In this blog post, we show how to use the SPMD API to annotate the &lt;a href=&quot;https://huggingface.co/blog/llama2&quot;&gt;HuggingFace (HF) Llama 2&lt;/a&gt; implementation to maximize performance. For comparison, we also show our FSDP results with the same configurations; read about &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/fsdp.md&quot;&gt;PyTorch/XLA FSDP API here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;spmd-overview&quot;&gt;SPMD Overview&lt;/h3&gt;

&lt;p&gt;Let’s briefly review the fundamentals of SPMD. For details, please refer to our &lt;a href=&quot;https://pytorch.org/blog/pytorch-xla-spmd/&quot;&gt;blog post&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/spmd.md&quot;&gt;user guide&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;mesh&quot;&gt;Mesh&lt;/h4&gt;

&lt;p&gt;A multidimensional array that describes the logical topology of the TPU devices:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Assuming you are running on a TPU host that has 8 devices attached
num_devices = xr.global_runtime_device_count()
# mesh shape will be (4,2) in this example
mesh_shape = (num_devices // 2, 2)
device_ids = np.array(range(num_devices))
# axis_names 'x' and 'y' are optional
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;partition-spec&quot;&gt;Partition Spec&lt;/h4&gt;

&lt;p&gt;A tuple that describes how the corresponding tensor’s dimensions are sharded across the mesh:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;partition_spec = ('x', 'y')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;mark-sharding&quot;&gt;Mark Sharding&lt;/h4&gt;

&lt;p&gt;An API that takes a mesh and a partition_spec, and then generates a sharding annotation for the XLA compiler.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor = torch.randn(4, 4).to('xla')
# Let's resue the above mesh and partition_spec.
# It means the tensor's 0th dim is sharded 4 way and 1th dim is sharded 2 way.
xs.mark_sharding(tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2d-sharding-with-spmd&quot;&gt;2D Sharding with SPMD&lt;/h3&gt;

&lt;p&gt;In our &lt;a href=&quot;https://pytorch.org/blog/pytorch-xla-spmd/&quot;&gt;SPMD blog post&lt;/a&gt;, we demonstrated using 1D FSDP style sharding. Here, we introduce a more powerful sharding strategy, called &lt;a href=&quot;https://arxiv.org/pdf/2105.04663.pdf&quot;&gt;2D sharding&lt;/a&gt;, where both the parameters and activations are sharded. This new sharding strategy not only allows fitting a larger model but also boosts the MFU to up to &lt;strong&gt;54.3%&lt;/strong&gt;. For more details, read the Benchmarks section.&lt;/p&gt;

&lt;p&gt;This section introduces a set of general rules that applies to most LLMs, and for convenience we directly reference the variable names and configuration names from &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/modeling_llama.py&quot;&gt;HF Llama&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, let’s create a 2D Mesh with corresponding axis names: data and model. The data axis is usually where we distribute the input data, and the model axis is where we further distribute the model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mesh = Mesh(device_ids, mesh_shape, ('data', 'model'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mesh_shape&lt;/code&gt; can be a hyper-parameter that is tuned for different model sizes and hardware configurations. The same mesh will be reused in all following sharding annotations. In the next few sections, we will cover how to use the mesh to shard parameters, activations and input data.&lt;/p&gt;

&lt;h4 id=&quot;parameter-sharding&quot;&gt;Parameter Sharding&lt;/h4&gt;

&lt;p&gt;Below is a table that summarizes all parameters of HF Llama 2 and corresponding partition specifications. Example HF code can be found &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/examples/pytorch/language-modeling/run_clm.py#L572&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Parameter Name&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Explanation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Parameter Shape&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Partition Spec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;embed_tokens&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;embedding layer
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;vocab_size&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(model, data)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;q_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;num_heads&lt;/code&gt; &lt;code&gt;x&lt;/code&gt; &lt;code&gt;head_dim&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;k_proj / v_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;num_key_value_heads&lt;/code&gt; &lt;code&gt;x&lt;/code&gt; &lt;code&gt;head_dim&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;o_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;hidden_size&lt;/code&gt;, &lt;code&gt;num_heads x head_dim&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(model, data)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;gate_proj / up_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;MLP weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;intermediate_size&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(model, data)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;down_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;MLP weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;hidden_size&lt;/code&gt;, &lt;code&gt;intermediate_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;lm_head&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;HF output embedding 
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;vocab_size&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(model, data)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1: SPMD 2D Sharding Parameter Partition Spec&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The rule is to shard the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidden_size&lt;/code&gt; dim of any weights except QKVO projections according to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; axis of the mesh, then shard the other dim with the remaining &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; axis. For QKVO, do the opposite. This model-data axis rotation methodology is similar to that of &lt;a href=&quot;https://arxiv.org/pdf/1909.08053.pdf&quot;&gt;Megatron-LM&lt;/a&gt; to reduce communication overhead. For &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;layernorm&lt;/code&gt; weights, we implicitly mark them as replicated across different devices given they are 1D tensors.&lt;/p&gt;

&lt;h4 id=&quot;activation-sharding&quot;&gt;Activation Sharding&lt;/h4&gt;

&lt;p&gt;In order to better utilize the device memory, very often we need to annotate the output of some memory bound ops. That way the compiler is forced to only keep partial output on devices instead of the full output. In Llama 2, we explicitly annotate all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.matmul&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Linear&lt;/code&gt; outputs. Table 2 summarizes the corresponding annotations; the example HF code can be found &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/modeling_llama.py#L235&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Output Name&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Explanation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Output Shape&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Partition Spec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;inputs_embeds&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;embedding layer output
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;query_states&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention nn.Linear output
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;num_heads x head_dim&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;key_states / value_states&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention nn.Linear output
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;num_key_value_heads x head_dim&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;attn_weights&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention weights
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;num_attention_heads&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, model, None, None)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;attn_output&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;attention layer output
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;up_proj / gate_proj / down_proj&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;MLP &lt;code&gt;nn.Linear&lt;/code&gt; outputs
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;intermediate_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;code&gt;logits&lt;/code&gt;
   &lt;/td&gt;
   &lt;td&gt;HF output embedding output 
   &lt;/td&gt;
   &lt;td&gt;(&lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;hidden_size&lt;/code&gt;)
   &lt;/td&gt;
   &lt;td&gt;(data, None, model)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 2: SPMD 2D Sharding Activation Partition Spec&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The rule is to shard the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size&lt;/code&gt; dim of any outputs according to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; axis of the mesh, then replicate the length dims of any outputs, and finally shard the last dim along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; axis.&lt;/p&gt;

&lt;h4 id=&quot;input-sharding&quot;&gt;Input Sharding&lt;/h4&gt;

&lt;p&gt;For input sharding, the rule is to shard the batch dim along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; axis of the mesh, and replicate the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence_length&lt;/code&gt; dim. Below is the example code, and the corresponding HF change may be found &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/trainer.py#L1456&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;partition_spec = ('data', None)
sharding_spec = xs.ShardingSpec(mesh, partition_spec)
# MpDeviceLoader will shard the input data before sending to the device.
pl.MpDeviceLoader(dataloader, self.args.device, input_sharding=sharding_spec, ...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, all the data and model tensors that require sharding are covered!&lt;/p&gt;

&lt;h4 id=&quot;optimizer-states--gradients&quot;&gt;Optimizer States &amp;amp; Gradients&lt;/h4&gt;

&lt;p&gt;You may be wondering whether it is necessary to shard the optimizer states and gradients as well. Great news: the sharding propagation feature of the XLA compiler automates the sharding annotation in these two scenarios, without needing more hints to improve performance.&lt;/p&gt;

&lt;p&gt;It is important to note that optimizer states are typically initialized within the first iteration of the training loop. From the standpoint of the XLA compiler, the optimizer states are the outputs of the first graph, and therefore have the sharding annotation propagated. For subsequent iterations, the optimizer states become inputs to the second graph, with the sharding annotation propagated from the first one. This is also why PyTorch/XLA typically produces two graphs for the training loops. If the optimizer states are somehow initialized before the first iteration, users will have to manually annotate them, just like the model weights.&lt;/p&gt;

&lt;p&gt;Again, all concrete examples of the above sharding annotation can be found in our fork of HF Transformers &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/tree/llama2-google-next-training&quot;&gt;here&lt;/a&gt;. The repo also contains code for our experimental feature &lt;a href=&quot;https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads&quot;&gt;MultiSlice&lt;/a&gt;, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HybridMesh&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dcn&lt;/code&gt; axis, which follows the same principles mentioned above.&lt;/p&gt;

&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;

&lt;p&gt;While using SPMD for training, there are a few important things to pay attention to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.einsum&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.matmul&lt;/code&gt;; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.matmul&lt;/code&gt; usually flattens tensors and does a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.mm&lt;/code&gt; at the end, and that’s bad for SPMD when the combined axes are sharded. The XLA compiler will have a hard time determining how to propagate the sharding.&lt;/li&gt;
  &lt;li&gt;PyTorch/XLA provides patched &lt;code&gt;[nn.Linear](https://github.com/pytorch/xla/blob/master/torch_xla/experimental/xla_sharding.py#L570)&lt;/code&gt; to overcome the above constraint:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch_xla.experimental.xla_sharding as xs
from torch_xla.distributed.fsdp.utils import apply_xla_patch_to_nn_linear

 model = apply_xla_patch_to_nn_linear(model, xs.xla_patched_nn_linear_forward)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Always reuse the same mesh across all shardings&lt;/li&gt;
  &lt;li&gt;Always specify &lt;code&gt;--dataloader_drop_last yes&lt;/code&gt;. The last smaller data is hard to annotate.&lt;/li&gt;
  &lt;li&gt;Large models which are initialized on the host can induce host-side OOM. One way to avoid this issue is to initialize parameters on the &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/examples/pytorch/language-modeling/run_clm.py#L501&quot;&gt;meta device&lt;/a&gt;, then create and shard real tensors layer-by-layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;infrastructure-improvements&quot;&gt;Infrastructure Improvements&lt;/h3&gt;

&lt;p&gt;Besides the above modeling techniques, we have developed additional features and improvements to maximize performance, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We enable asynchronous collective communication. This requires enhancements on the XLA compiler’s latency hiding scheduler to better optimize for the Llama 2 PyTorch code.&lt;/li&gt;
  &lt;li&gt;We now allow sharding annotations in the middle of the IR graph, just like JAX’s &lt;a href=&quot;https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html&quot;&gt;jax.lax.with_sharding_constraint&lt;/a&gt;. Previously, only graph inputs were annotated.&lt;/li&gt;
  &lt;li&gt;We also propagate replicated sharding spec from the compiler to the graph outputs. This allows us to shard the optimizer states automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;inference-optimizations&quot;&gt;Inference Optimizations&lt;/h2&gt;

&lt;p&gt;All the PyTorch/XLA &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;optimizations&lt;/a&gt; implemented for Llama inference are applied to Llama 2 as well. That includes &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/#fairscale-sharding&quot;&gt;Tensor Parallelism + Dynamo (torch.compile) using torch-xla collective ops&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/#autoregressive-decoding-on-pytorchxla&quot;&gt;autoregressive decoding logic improvement to avoid recompilation&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/#input-prompt-optimization&quot;&gt;bucketized prompt length&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/#kv-cache-optimization&quot;&gt;KV-cache with compilation friendly index ops&lt;/a&gt;. Llama 2 introduces two new changes: Grouped Query Attention, and Early Stopping when eos is reached for all prompts. We applied corresponding changes to promote better performance and flexibility with PyTorch/XLA.&lt;/p&gt;

&lt;h3 id=&quot;grouped-query-attention&quot;&gt;Grouped Query Attention&lt;/h3&gt;

&lt;p&gt;Llama 2 enables &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;&gt;Grouped Query Attention&lt;/a&gt; for the 70B models. It allows the number of Key and Value heads to be smaller than the number of Query heads, while still supporting KV-cache sharding up to the number of KV heads. For the 70B models, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_kv_heads&lt;/code&gt; is 8, which limits the tensor parallelism to be less or equal to 8. In order to shard the model checkpoint to run on more devices, the K, V projection weights need to be replicated first, and then split into multiple pieces. For example, to shard the 70B model checkpoint from 8 pieces to 16 pieces, the K, V projection weights are duplicated and split into 2 pieces for each shard. We provide a &lt;a href=&quot;https://github.com/pytorch-tpu/llama/blob/llama2-google-next-inference/reshard_checkpoints.py&quot;&gt;reshard_checkpoints.py&lt;/a&gt; script to handle that, and to make sure the sharded checkpoint performs mathematically identical to the original checkpoint.&lt;/p&gt;

&lt;h3 id=&quot;eos-early-stopping&quot;&gt;EOS Early Stopping&lt;/h3&gt;

&lt;p&gt;The Llama 2 generation code added &lt;a href=&quot;https://github.com/facebookresearch/llama/blob/ea9f33d6d3ea8ed7d560d270986407fd6c2e52b7/llama/generation.py#L159&quot;&gt;the early stopping logic&lt;/a&gt;. A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos_reached&lt;/code&gt; tensor is used to track the completion of all the prompt generations, and if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos&lt;/code&gt; token is reached for all the prompts in the batch, the generation would stop early. The similar change is incorporated in the PyTorch/XLA optimized version as well, with some minor tweaks.&lt;/p&gt;

&lt;p&gt;In PyTorch/XLA, checking the value of a tensor like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos_reached&lt;/code&gt; as part of the control flow condition would invoke a blocking device-to-host transfer. The tensor would be transferred from device memory to CPU memory to evaluate its value, while all other logics are waiting. This introduced a delay on the scale of ms after every new token generation. As a trade-off, we reduce the rate of checking the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos_reached&lt;/code&gt; value to be &lt;a href=&quot;https://github.com/pytorch-tpu/llama/blob/b89dd0f2351c42fef367670d9d2c5b65cd0ae932/llama/generation.py#L268C13-L270C26&quot;&gt;once every 10 new token generations&lt;/a&gt;. With this change, the impact of the blocking device-to-host transfer would be reduced by 10x, while the early stopping would still be effective, and at most 9 unnecessary tokens would be generated after each sequence reaches the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eos&lt;/code&gt; token.&lt;/p&gt;

&lt;h3 id=&quot;model-serving&quot;&gt;Model Serving&lt;/h3&gt;

&lt;p&gt;PyTorch/XLA is working on a serving strategy to enable the PyTorch community to serve their deep learning applications via &lt;a href=&quot;https://pytorch.org/docs/stable/export.html&quot;&gt;Torch.Export&lt;/a&gt;, &lt;a href=&quot;https://github.com/openxla/stablehlo&quot;&gt;StableHLO&lt;/a&gt;, and &lt;a href=&quot;https://www.tensorflow.org/guide/saved_model&quot;&gt;SavedModel&lt;/a&gt;. PyTorch/XLA Serving is an experimental feature in &lt;a href=&quot;https://github.com/pytorch/xla/releases&quot;&gt;PyTorch/XLA 2.1 release&lt;/a&gt;; for details visit our &lt;a href=&quot;https://github.com/pytorch/xla/blob/r2.1/docs/stablehlo.md#convert-saved-stablehlo-for-serving&quot;&gt;serving user guide&lt;/a&gt;. Users can take advantage of TorchServe to run their single-host workloads.&lt;/p&gt;

&lt;h2 id=&quot;benchmarks&quot;&gt;Benchmarks&lt;/h2&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;To measure training performance, we use the industry-standard metric: &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;&gt;Model FLOPS Utilization (MFU)&lt;/a&gt;. Model FLOPS are the floating point operations required to perform a single forward and backward pass. Model FLOPs are hardware and implementation independent and only depend on the underlying model. MFU measures how effectively the model is using the actual hardware during training. Achieving 100% MFU means that the model is using the hardware perfectly.&lt;/p&gt;

&lt;p&gt;To measure inference performance, we use the industry-standard metric of throughput. First, we measure latency per token when the model has been compiled and loaded. Then, we calculate throughput by dividing batch size (BS) over latency per chip. As a result, throughput measures how the model is performing in production environments regardless of how many chips are used.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;h4 id=&quot;training-evaluation&quot;&gt;Training Evaluation&lt;/h4&gt;

&lt;p&gt;Figure 1 shows Llama 2 SPMD 2D sharding training results on a range of Google TPU v4 hardware with &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;PyTorch/XLA FSDP&lt;/a&gt; as the baseline. We increased MFU by &lt;strong&gt;28%&lt;/strong&gt; across all sizes of Llama 2 compared to FSDP running on the same hardware configuration. This performance improvement is largely due to: 1) 2D Sharding has less communication overhead than FSDP, and 2) asynchronous collective communication is enabled in SPMD which allows communication and computation overlapping. Also note that as the model size scales, we maintain the high MFU. Table 3 shows all the hardware configurations plus some hyperparameters used in the training benchmarks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig1.jpg&quot; alt=&quot;Figure 1. Llama 2 Training MFU on TPU v4 Hardware&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: Llama 2 Training MFU on TPU v4 Hardware&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The results in Figure 1 are produced with sequence length 1,024. Figure 2 shows how the performance behaves with larger sequence lengths. It shows our performance also scales linearly with sequence lengths. The MFU is expected to decrease a little as a smaller per device batch size is needed to accommodate the additional memory pressure introduced by the larger sequence length since the sequence length axis is not sharded in 2D sharding. And TPU is very sensitive to batch size. For Llama 2, 70B parameters, the performance decrease is as low as &lt;strong&gt;4%&lt;/strong&gt;. At the time of preparing these results, &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/tokenization_llama.py#L48&quot;&gt;Hugging Face Llama 2 tokenizer&lt;/a&gt; limits the max model input to 2,048, preventing us from evaluating larger sequence lengths.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig2.jpg&quot; alt=&quot;Figure 2. Llama 2 SPMD Training MFU on TPU v4 with Different Sequence Lengths&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: Llama 2 SPMD Training MFU on TPU v4 with Different Sequence Lengths&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model Size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;7B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;13B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;70B&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;TPU NumCores&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;V4-32
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;V4-64
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;V4-256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Mesh Shape&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;(16, 1)
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;(32, 1)
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;(32, 4)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Seq Len&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;1,024
   &lt;/td&gt;
   &lt;td&gt;2,048
   &lt;/td&gt;
   &lt;td&gt;1,024
   &lt;/td&gt;
   &lt;td&gt;2,048
   &lt;/td&gt;
   &lt;td&gt;1,024
   &lt;/td&gt;
   &lt;td&gt;2,048
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Global Batch&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Per Device Batch&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;16
   &lt;/td&gt;
   &lt;td&gt;8
   &lt;/td&gt;
   &lt;td&gt;8
   &lt;/td&gt;
   &lt;td&gt;4
   &lt;/td&gt;
   &lt;td&gt;16
   &lt;/td&gt;
   &lt;td&gt;8
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 3: Llama 2 SPMD Training Benchmark TPU Configurations and Hyperparameters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One last thing to call out is that we use &lt;a href=&quot;https://arxiv.org/abs/1804.04235&quot;&gt;adafactor&lt;/a&gt; as the optimizer for better memory utilization. And once again, here is the &lt;a href=&quot;https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/SPMD_USER_GUIDE.md&quot;&gt;user guide&lt;/a&gt; to reproduce the benchmark results listed above.&lt;/p&gt;

&lt;h4 id=&quot;inference-evaluation&quot;&gt;Inference Evaluation&lt;/h4&gt;

&lt;p&gt;In this section, we extend our &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;previous evaluation of Llama on Cloud v4 TPU&lt;/a&gt;. Here, we demonstrate the performance properties of TPU v5e for inference applications.&lt;/p&gt;

&lt;p&gt;We define inference throughput as the number of tokens produced by a model per second per TPU chip. Figure 3 shows Llama 2 70B throughput on a v5e-16 TPU node. Given Llama is a memory bound application, we see that applying weight-only quantization unblocks extending the model batch size to 32. Higher throughput results would be possible on larger TPU v5e hardware up to the point where the ICI network bandwidth between chips throttle the TPU slice from delivering higher throughput. Exploring the upper bound limits of TPU v5e on Llama 2 was outside of the scope of this work. Notice, to make the Llama 2 70B model run on v5e-16, we replicated the attention heads to have one head per chip as discussed in the Inference section above. As discussed &lt;a href=&quot;https://pytorch.org/blog/path-achieve-low-inference-latency/&quot;&gt;previously&lt;/a&gt;, with increasing model batch size, per-token latency grows proportionally; quantization improves overall latency by reducing memory I/O demand.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig3.jpg&quot; alt=&quot;Figure 3. Llama 2 70B Inference Per-Chip Throughput on TPU v5e vs. Batch Size&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 3&lt;/strong&gt;: Llama 2 70B Inference Per-Chip Throughput on TPU v5e vs. Batch Size&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Figure 4 shows inference throughput results across different model sizes. These results highlight the largest throughput given the hardware configuration when using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bf16&lt;/code&gt; precision. With weight only quantization, this throughput reaches 42 on the 70B model. As mentioned above, increasing hardware resources may lead to performance gains.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig4.jpg&quot; alt=&quot;Figure 4. Llama 2 Inference Per-Chip Throughput on TPU v5e&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 4&lt;/strong&gt;: Llama 2 Inference Per-Chip Throughput on TPU v5e&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Figure 5 shows the cost of serving Llama 2 models (from Figure 4) on Cloud TPU v5e. We report the TPU v5e per-chip cost based on the 3-year commitment (reserved) price in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-west4&lt;/code&gt; region. All model sizes use maximum sequence length of 2,048 and maximum generation length of 1,000 tokens. Note that with quantization, the cost for the 70B model drops to &lt;strong&gt;$0.0036 per 1,000 tokens&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig5.jpg&quot; alt=&quot;Figure 5. Llama 2 Inference Per-Chip Cost on TPU v5e&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 5&lt;/strong&gt;: Llama 2 Inference Per-Chip Cost on TPU v5e&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Figure 6 summarizes our best Llama 2 inference latency results on TPU v5e. Llama 2 7B results are obtained from our non-quantized configuration (BF16 Weight, BF16 Activation) while the 13B and 70B results are from the quantized (INT8 Weight, BF16 Activation) configuration. We attribute this observation to the inherent memory saving vs. compute overhead tradeoff of quantization; as a result, for smaller models, quantization may not lead to lower inference latency.&lt;/p&gt;

&lt;p&gt;Additionally, prompt length has a strong effect on the memory requirements of LLMs. For instance, we observe a latency of 1.2ms / token (i.e. 201 tokens / second / chip) when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_len=256&lt;/code&gt; at batch size of 1 with no quantization on v5e-4 running Llama2 7B.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama-2/fig6.jpg&quot; alt=&quot;Figure 6. Llama 2 Inference Latency on TPU v5e&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 6&lt;/strong&gt;: Llama 2 Inference Latency on TPU v5e&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;The recent wave of AI innovation has been nothing short of transformative, with breakthroughs in LLMs at the forefront. Meta’s Llama and Llama 2 models stand as notable milestones in this wave of progress. PyTorch/XLA uniquely enables high-performance, cost-efficient training and inference for Llama 2 and other LLMs and generative AI models on Cloud TPUs, including the new Cloud TPU v5e. Looking forward, PyTorch/XLA will continue to push the performance limits on Cloud TPUs in both throughput and scalability and at the same time maintain the same PyTorch user experience.&lt;/p&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; so that we can openly collaborate. You can also &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;try out&lt;/a&gt; PyTorch/XLA for yourself on various XLA devices including TPUs and GPUs.&lt;/p&gt;

&lt;p&gt;We would like to extend our special thanks to Marcello Maggioni, Tongfei Guo, Andy Davis, Berkin Ilbeyi for their support and collaboration in this effort.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br /&gt;
The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Jiewen Tan, Jon Bolin, Yeounoh Chung, Liyang Lu, Siyuan Liu, Wonjoo Lee, Manfei Bai, Meghan Cowan, Jack Cao, Milad Mohammadi, Shauheen Zahirazami, Alex Spiridonov</name>
        
        
      </author>

      

      

      
        <summary type="html">In a landscape where AI innovation is accelerating at an unprecedented pace, Meta’s Llama family of open sourced large language models (LLMs) stands out as a notable breakthrough. Llama marked a significant step forward for LLMs, demonstrating the power of pre-trained architectures for a wide range of applications. Llama 2 further pushed the boundaries of scale and capabilities, inspiring advancements in language understanding, generation, and beyond.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Inference on x86-64 Machines with oneDNN Graph</title>
      <link href="https://pytorch.org/blog/accelerating-inference/" rel="alternate" type="text/html" title="Accelerating Inference on x86-64 Machines with oneDNN Graph" />
      <published>2023-11-02T00:00:00-07:00</published>
      <updated>2023-11-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-inference/">&lt;p&gt;&lt;em&gt;Supported in PyTorch 2.0 as a beta feature, oneDNN Graph leverages aggressive fusion patterns to accelerate inference on x86-64 machines, especially Intel® Xeon® Scalable processors.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on AI hardware. It automatically identifies the graph partitions to be accelerated via fusion. The &lt;a href=&quot;http://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul, and their neighbor operations for both inference and training use cases.&lt;/p&gt;

&lt;p&gt;In PyTorch 2.0 and beyond, oneDNN Graph can help accelerate inference on x86-64 CPUs (primarily, Intel Xeon processor-based machines) with Float32 and BFloat16 (with PyTorch’s Automatic Mixed Precision support) datatypes. With BFloat16, speedup is limited to machines that support AVX512_BF16 ISA (Instruction Set Architecture), as well as machines that also support AMX_BF16 ISA.&lt;/p&gt;

&lt;h2 id=&quot;onednn-graph-usage&quot;&gt;oneDNN Graph Usage&lt;/h2&gt;

&lt;p&gt;From a user’s perspective, the usage is quite simple and intuitive, &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;with the only change in code being an API invocation&lt;/a&gt;. To leverage oneDNN Graph with &lt;a href=&quot;http://pytorch.org/docs/stable/generated/torch.jit.trace.html&quot;&gt;JIT-tracing&lt;/a&gt;, a model is profiled with an example input as shown below in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-onednn-graph-api-code-snippet.png&quot; alt=&quot;Figure 1. A code-snippet that demonstrates using oneDNN Graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: A code-snippet that demonstrates using oneDNN Graph&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;oneDNN Graph receives the model’s graph and identifies candidates for operator-fusion with respect to the input shape of the example input. Currently, only static shapes are supported. This means that any other input shape would neither be supported nor receive any performance-benefit.&lt;/p&gt;

&lt;h2 id=&quot;measurements&quot;&gt;Measurements&lt;/h2&gt;

&lt;p&gt;To ensure reproducibility of results, we used a &lt;a href=&quot;http://github.com/sanchitintel/benchmark/tree/onednn-graph-preview2&quot;&gt;fork&lt;/a&gt; of &lt;a href=&quot;http://github.com/pytorch/benchmark&quot;&gt;TorchBench&lt;/a&gt; to measure inference speed-up of some Vision models on an &lt;a href=&quot;http://aws.amazon.com/ec2/instance-types/m7i/&quot;&gt;AWS m7i.16xlarge&lt;/a&gt; instance, which uses 4th Gen Intel® Xeon® Scalable processors.&lt;/p&gt;

&lt;p&gt;The baseline for comparison was &lt;a href=&quot;http://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html&quot;&gt;torch.jit.optimize_for_inference&lt;/a&gt; which only supports Float32 datatype. The batch-size for each model was based on the respective batch size being used for them in TorchBench.&lt;/p&gt;

&lt;p&gt;In Figure 2, we depict the inference speedup of using oneDNN Graph over PyTorch alone. The geomean speedup with oneDNN Graph &lt;strong&gt;for Float32 datatype was 1.24x&lt;/strong&gt;, and the geomean speedup &lt;strong&gt;for BFloat16 datatype was 3.31x&lt;/strong&gt;1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-inference-speedup-with-onednn-graph.png&quot; alt=&quot;Figure 2. Inference speedup with oneDNN Graph over default CPU JIT Fuser (which only uses Float32 datatype)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: Inference speedup with oneDNN Graph over default CPU JIT Fuser (which only uses Float32 datatype)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt;

&lt;p&gt;oneDNN Graph is currently supported in PyTorch through TorchScript, but work is already underway by Intel to integrate it with the Inductor-CPU backend as a prototype feature in a future PyTorch release and Dynamo make supporting dynamic shapes easier with PyTorch, and we would like to introduce Dynamic shape support with Inductor-CPU. We also plan to add int8 quantization support.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;The results presented in this blog are a joint effort between Meta and the Intel PyTorch team. Special thanks to Elias Ellison from Meta who spent precious time thoroughly reviewing the PRs and gave us helpful feedback.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Supported in PyTorch 2.0 as a beta feature, oneDNN Graph leverages aggressive fusion patterns to accelerate inference on x86-64 machines, especially Intel® Xeon® Scalable processors.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AMD Extends Support for PyTorch Machine Learning Development on Select RDNA™ 3 GPUs with ROCm™ 5.7</title>
      <link href="https://pytorch.org/blog/amd-extends-support-for-pt-ml/" rel="alternate" type="text/html" title="AMD Extends Support for PyTorch Machine Learning Development on Select RDNA™ 3 GPUs with ROCm™ 5.7" />
      <published>2023-10-31T00:00:00-07:00</published>
      <updated>2023-10-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/amd-extends-support-for-pt-ml</id>
      <content type="html" xml:base="https://pytorch.org/blog/amd-extends-support-for-pt-ml/">&lt;p&gt;Researchers and developers working with Machine Learning (ML) models and algorithms using PyTorch can now use AMD ROCm 5.7 on Ubuntu® Linux® to tap into the parallel computing power of the Radeon™ RX 7900 XTX and the Radeon™ PRO W7900 graphics cards which are based on the AMD RDNA™ 3 GPU architecture.&lt;/p&gt;

&lt;p&gt;A client solution built on these two high-end GPUs enables a local, private, and cost-effective workflow for ML training and inference for those who previously relied on cloud-based solutions alone.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2281965-ROCm-development-radeon.jpg&quot; alt=&quot;ML Development on Desktop&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;accelerate-machine-learning-with-pytorch-on-your-desktop&quot;&gt;Accelerate Machine Learning With Pytorch On Your Desktop&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A local PC or workstation system running PyTorch with a Radeon 7900 series GPU presents a capable, yet affordable solution to address these growing workflow challenges thanks to large GPU memory sizes of 24GB and even 48GB.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unified-software-stack-for-the-desktop-and-the-datacenter&quot;&gt;Unified Software Stack For The Desktop And The Datacenter&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The latest AMD ROCm 5.7 software stack for GPU programming unlocks the massively parallel compute power of these RDNA™ 3 architecture-based GPUs for use with PyTorch, one of the leading ML frameworks. The same unified software stack also supports the CDNA™ GPU architecture of the AMD Instinct™ MI series accelerators.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;freedom-to-customize&quot;&gt;Freedom To Customize&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The AMD ROCm platform is primarily Open-Source Software (OSS).  It allows developers the freedom to customize and tailor their GPU software for their own needs while collaborating with a community of other developers, and helping each other find solutions in an agile, flexible, and rapid manner. The AMD ROCm platform’s goal is to allow users to maximize their GPU hardware investment. The AMD ROCm platform is designed to help develop, test, and deploy GPU accelerated HPC, AI, scientific computing, CAD, and other applications in a free, open source, integrated and secure software ecosystem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As the industry moves towards an ecosystem that supports a broad set of systems, frameworks and accelerators, AMD is determined to continue to make AI more accessible to PyTorch developers and researchers that benefit from a local client-based setup for ML development using RDNA™ 3 architecture-based desktop GPUs.&lt;/p&gt;

&lt;h2 id=&quot;learn-more&quot;&gt;Learn More&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amd.com/en/developer/resources/ml-radeon.html&quot;&gt;https://www.amd.com/en/developer/resources/ml-radeon.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;download-software&quot;&gt;Download Software&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amd.com/en/support/linux-drivers&quot;&gt;https://www.amd.com/en/support/linux-drivers&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;visit-thedocumentation-portalto-get-started-training-ml-models-on-your-local-desktop&quot;&gt;Visit the Documentation Portal to get started training ML models on your local desktop&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://rocm.docs.amd.com/projects/radeon/en/latest/&quot;&gt;https://rocm.docs.amd.com/projects/radeon/en/latest/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://rocm.docs.amd.com/projects/radeon/en/latest/docs/prerequisites.html&quot;&gt;https://rocm.docs.amd.com/projects/radeon/en/latest/docs/prerequisites.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-guide&quot;&gt;How to Guide&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/howto.html&quot;&gt;https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/howto.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;© 2023 Advanced Micro Devices, Inc. All rights reserved. AMD, the AMD Arrow logo, CDNA, Radeon, ROCm, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Linux® is the registered trademark of Linus Torvalds in the U.S. and other countries. Microsoft and Windows are registered trademarks of Microsoft Corporation in the US and/or other countries. PyTorch, the PyTorch logo and any related marks are trademarks of The Linux Foundation. TensorFlow, the TensorFlow logo and any related marks are trademarks of Google Inc. Ubuntu and the Ubuntu logo are registered trademarks of Canonical Ltd. Other product names used in this publication are for identification purposes only and may be trademarks of their respective owners.&lt;/p&gt;

&lt;p&gt;Radeon™ AI technology is compatible with all AMD Radeon 7000 Series graphics cards and newer. Please check with your system manufacturer for feature availability prior to purchase. GD-232.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Based on AMD internal measurements, November 2022, comparing the Radeon RX 7900 XTX at 2.5GHz boost clock with 96 CUs issuing 2X the Bfloat16 math operations per clocks vs. the RX 6900 XT GPU at 2.25 GHz boost clock and 80 CUs issue 1X the Bfloat16 math operations per clock. RX-821&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>AMD</name>
        
        
      </author>

      

      

      
        <summary type="html">Researchers and developers working with Machine Learning (ML) models and algorithms using PyTorch can now use AMD ROCm 5.7 on Ubuntu® Linux® to tap into the parallel computing power of the Radeon™ RX 7900 XTX and the Radeon™ PRO W7900 graphics cards which are based on the AMD RDNA™ 3 GPU architecture.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Compiling NumPy code into C++ or CUDA via torch.compile</title>
      <link href="https://pytorch.org/blog/compiling-numpy-code/" rel="alternate" type="text/html" title="Compiling NumPy code into C++ or CUDA via torch.compile" />
      <published>2023-10-17T00:00:00-07:00</published>
      <updated>2023-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/compiling-numpy-code</id>
      <content type="html" xml:base="https://pytorch.org/blog/compiling-numpy-code/">&lt;p&gt;Quansight engineers have implemented support for tracing through NumPy code via 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; in PyTorch 2.1. This feature leverages PyTorch’s compiler to 
generate efficient fused vectorized code without having to modify your original 
NumPy code. Even more, it also allows for executing NumPy code on CUDA 
just by running it through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.device(&quot;cuda&quot;)&lt;/code&gt;!&lt;/p&gt;

&lt;p&gt;In this post, we go over how to use this feature and give a few tips and tricks 
to make the most out of it.&lt;/p&gt;

&lt;h2 id=&quot;compiling-numpy-code-into-parallel-c&quot;&gt;Compiling NumPy code into Parallel C++&lt;/h2&gt;

&lt;p&gt;We take as our running example one step in a K-Means algorithm. 
This piece of code is borrowed from this &lt;a href=&quot;https://realpython.com/numpy-array-programming/#clustering-algorithms&quot;&gt;NumPy book&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np

def kmeans(X, means):
    return np.argmin(np.linalg.norm(X - means[:, None], axis=2), axis=0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We create a synthetic dataset with 20M random 2-D points. We can see that, 
given that the means are chosen appropriately, the function returns the correct 
cluster for all of them&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npts = 10_000_000
X = np.repeat([[5, 5], [10, 10]], [npts, npts], axis=0)
X = X + np.random.randn(*X.shape)  # 2 distinct &quot;blobs&quot;
means = np.array([[5, 5], [10, 10]])
np_pred = kmeans(X, means)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Benchmarking this function gives us a baseline of &lt;strong&gt;1.26s&lt;/strong&gt; on an AMD 3970X CPU.&lt;/p&gt;

&lt;p&gt;Compiling this function is now as easy as wrapping it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; and 
executing it with the example inputs&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch

compiled_fn = torch.compile(kmeans)
compiled_pred = compiled_fn(X, means)
assert np.allclose(np_pred, compiled_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The compiled function yields a 9x speed-up when running it on 1 core. Even 
better, as opposed to NumPy, our generated code does take advantage of all the 
cores in a processor. As such, when we run it on 32 cores, we get a &lt;strong&gt;57x 
speed-up&lt;/strong&gt;. Note that PyTorch always uses all the available cores unless 
explicitly restricted, so this is the default behavior you get when using 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We may inspect the generated C++ code by running the script with the 
environment variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_LOGS=output_code&lt;/code&gt;. When doing so, we can see that 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; was able to compile the broadcasting and the two reductions 
into just one for-loop, and parallelize it using OpenMP&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;extern &quot;C&quot; void kernel(const double* in_ptr0, const long* in_ptr1, long* out_ptr0) {
    #pragma omp parallel num_threads(32)
    #pragma omp for
    for(long i0=0L; i0&amp;lt;20000000L; i0+=1L) {
        auto tmp0 = in_ptr0[2L*i0];
        auto tmp1 = in_ptr1[0L];
        auto tmp5 = in_ptr0[1L + (2L*i0)];
        auto tmp6 = in_ptr1[1L];
        // Rest of the kernel omitted for brevity
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;compiling-numpy-code-into-cuda&quot;&gt;Compiling NumPy code into CUDA&lt;/h2&gt;

&lt;p&gt;Compiling our code so that it runs on CUDA is as simple as setting the 
default device to be CUDA&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.device(&quot;cuda&quot;):
    cuda_pred = compiled_fn(X, means)
assert np.allclose(np_pred, cuda_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By inspecting the generated code via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_LOGS=output_code&lt;/code&gt;, we see that, 
rather than generating CUDA code directly, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; generates rather 
readable &lt;a href=&quot;https://triton-lang.org/main/index.html&quot;&gt;triton&lt;/a&gt; code&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def triton_(in_ptr0, in_ptr1, out_ptr0, XBLOCK : tl.constexpr):
    xnumel = 20000000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &amp;lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (2*x0), xmask)
    tmp1 = tl.load(in_ptr1 + (0))
    // Rest of the kernel omitted for brevity
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running this small snippet on an RTX 2060 gives an &lt;strong&gt;8x speed-up&lt;/strong&gt; over the 
original NumPy code. This is something, but it is not particularly impressive, 
given the speed-ups we have seen on CPU. Let’s have a look into how to squeeze 
the most out of our GPU via a couple minor changes.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float64&lt;/code&gt; vs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float32&lt;/code&gt;. Many GPUs, in particular consumer-grade ones, are 
rather sluggish when running operations on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float64&lt;/code&gt;. For this reason, changing 
the data generation to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float32&lt;/code&gt;, the original NumPy code just gets a bit 
faster, about a 9%, but our CUDA code gets &lt;strong&gt;40% faster&lt;/strong&gt;, yielding a &lt;strong&gt;11x 
speed-up&lt;/strong&gt; over the plain NumPy code.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, by default, respects the NumPy semantics, and as such, it uses 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.float64&lt;/code&gt; as its default dtype for all its creation ops. As discussed, this 
can hinder performance, so it is possible to change this default by setting&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch._dynamo import config
config.numpy_default_float = &quot;float32&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;CPU &amp;lt;&amp;gt; CUDA copies&lt;/strong&gt;. An 11x speed-up is good, but it is not even close to 
the CPU numbers. This is caused by a small transformation that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile 
&lt;/code&gt;does behind the scenes. The code above takes NumPy arrays and returns NumPy 
arrays. All of these arrays are on CPU, but the computations are performed on 
the GPU. This means that every time the function is called, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; has 
to copy all these arrays from CPU to the GPU, and then copy the result back to 
CPU to preserve the original semantics. There is no native solution to this 
issue in NumPy, as NumPy does not have the notion of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device&lt;/code&gt;. That being 
said, we can work around it by creating a wrapper to this function so that it 
accepts PyTorch tensors and returns PyTorch tensors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@torch.compile
def tensor_fn(X, means):
    X, means = X.numpy(), means.numpy()
    ret = kmeans(X, means)
    return torch.from_numpy(ret)

def cuda_fn(X, means):
    with torch.device(&quot;cuda&quot;):
        return tensor_fn(X, means)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function now takes tensors in CUDA memory and returns tensors in CUDA 
memory, but the function itself is written in NumPy! &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; uses the 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy()&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from_numpy()&lt;/code&gt; calls as hints, and optimizes them away, and 
internally it simply works with PyTorch tensors without moving the memory at 
all. When we keep the tensors in CUDA and perform the computations in 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float32&lt;/code&gt;, we see a &lt;strong&gt;200x speed-up&lt;/strong&gt; over the initial NumPy implementation on 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float32&lt;/code&gt; arrays.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mixing NumPy and PyTorch&lt;/strong&gt;. In this example, we had to write a small adaptor 
to convert tensors to ndarrays and then back to tensors. In programs that mix 
PyTorch and NumPy converting a tensor into an ndarray is often implemented as 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.detach().cpu().numpy()&lt;/code&gt;, or simply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.numpy(force=True)&lt;/code&gt;. Since when running 
under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; we can run NumPy code in CUDA, we can implement this 
conversion pattern as call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.numpy()&lt;/code&gt;, as we did above. Doing so and 
running the resulting code under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device(&quot;cuda&quot;)&lt;/code&gt; will generate efficient CUDA 
code from original NumPy calls without copying the data from CUDA to CPU at 
all. Note that the resulting code does not run without &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. For it 
to run in eager mode one would need to rollback to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.numpy(force=True)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;further-speed-up-tricks&quot;&gt;Further Speed-up tricks&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;General advice&lt;/strong&gt;. The CUDA code we have shown is already quite efficient, but 
it is true that the running example is rather short. When dealing with larger 
programs, we may need to tweak parts of it to make it more efficient. A good 
place to start is the multiple &lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler.html#read-more&quot;&gt;tutorials and FAQs for torch.compile&lt;/a&gt;. 
This showcases a number of ways to inspect the tracing process, and how to 
identify problematic code that may cause slowdowns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advice when compiling NumPy code&lt;/strong&gt;. NumPy, even if rather similar to PyTorch, 
is often used very differently. It is rather common to perform computations in 
NumPy and then do an if/else depending on values within the array, or perform 
operations in-place, perhaps via boolean masks. These constructions, while 
supported by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, hamper its performance. Changes like writing the 
code in a branchless way to avoid graph breaks, or avoiding in-place ops can go 
a long way.&lt;/p&gt;

&lt;p&gt;To write fast NumPy code, it is best to avoid loops, but sometimes they are 
unavoidable. When tracing through a loop, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; will try to fully 
unroll it. This is sometimes desirable, but sometimes it may not even be 
possible, like when we have a dynamic stopping condition, like in a while loop. 
In these cases, it may be best to just compile the body of the loop, perhaps a 
few iterations at a time (loop unrolling).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Debugging NumPy code&lt;/strong&gt;. Debugging is rather tricky when a compiler is 
involved. To figure out whether an error you are hitting is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile 
&lt;/code&gt;error, or an error from the program, you can execute your NumPy program without 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; by replacing the NumPy import by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import torch._numpy as np&lt;/code&gt;. 
This is should just be used for &lt;strong&gt;debugging purposes&lt;/strong&gt; and is in no way a 
replacement for the PyTorch API, as it is &lt;strong&gt;much slower&lt;/strong&gt; and, as a private API, 
&lt;strong&gt;may change without notice&lt;/strong&gt;. See also &lt;a href=&quot;https://pytorch.org/docs/stable/torch.compiler_faq.html#does-numpy-work-with-torch-compile&quot;&gt;this FAQ&lt;/a&gt; for other tricks.&lt;/p&gt;

&lt;h2 id=&quot;differences-between-numpy-and-torchcompile-numpy&quot;&gt;Differences between NumPy and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; NumPy&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;NumPy scalars&lt;/strong&gt;. NumPy returns NumPy scalars in almost any case where PyTorch 
would return a 0-D tensor (e.g. from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.sum&lt;/code&gt;). Under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, NumPy 
scalars are treated as 0-D arrays. This is just fine in most cases. The only 
case when their behavior diverges is when NumPy scalars are implicitly used as 
Python scalars. For example,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.asarray(2) * [1, 2, 3]  # 0-D array is an array-like
array([2, 4, 6])
&amp;gt;&amp;gt;&amp;gt; u = np.int32(2)
&amp;gt;&amp;gt;&amp;gt; u * [1, 2, 3]              # scalar decays into a Python int
[1, 2, 3, 1, 2, 3]
&amp;gt;&amp;gt;&amp;gt; torch.compile(lambda: u * [1, 2, 3])()
array([2, 4, 6])               # acts as a 0-D array, not as a scalar ?!?!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we compile the first two lines, we see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; treats &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u&lt;/code&gt; as a 
0-D array. To recover the eager semantics, we just need to make the casting 
explicit&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; torch.compile(lambda: int(u) * [1, 2, 3])()
[1, 2, 3, 1, 2, 3]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Type promotion and versioning&lt;/strong&gt;. NumPy’s type promotion rules may be, at 
times, a bit surprising&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.zeros(1, dtype=np.int8) + 127
array([127], dtype=int8)
&amp;gt;&amp;gt;&amp;gt; np.zeros(1, dtype=np.int8) + 128
array([128], dtype=int16)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;NumPy 2.0 is changing these rules to follow others that are closer to those 
PyTorch. The relevant technical document is &lt;a href=&quot;https://numpy.org/neps/nep-0050-scalar-promotion.html&quot;&gt;NEP 50&lt;/a&gt;. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; went ahead and implemented NEP 50 rather than the about-to-be-deprecated rules.&lt;/p&gt;

&lt;p&gt;In general, NumPy within torch.compile follows NumPy 2.0 pre-release.&lt;/p&gt;

&lt;h2 id=&quot;beyond-numpy-scipy-and-scikit-learn&quot;&gt;Beyond NumPy: SciPy and scikit-learn&lt;/h2&gt;

&lt;p&gt;In parallel to this effort of making &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; understand NumPy code, 
other Quansight engineers have designed and proposed a way to support PyTorch 
tensors within scikit-learn and SciPy. This was received enthusiastically by 
other maintainers from these libraries, as it was shown that using PyTorch as a 
backend would often yield considerable speed-ups. Both projects have now merged 
initial support for PyTorch tensors across a number of APIs and submodules.&lt;/p&gt;

&lt;p&gt;This sets the stepping stone to move towards a future where PyTorch tensors can 
be used within other libraries in the Python data ecosystem. Even more, this 
will enable running these other libraries on GPUs and even compiling code 
mixing these libraries and PyTorch, similar to what we have been discussed in 
this post.&lt;/p&gt;

&lt;p&gt;If you want to learn more about this effort, how to use it, or how to help 
moving it forward, see &lt;a href=&quot;https://labs.quansight.org/blog/array-api-support-scikit-learn&quot;&gt;this other blogpost&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PyTorch has committed since its inception to be a framework compatible with the 
rest of the Python ecosystem. Enabling compiling NumPy programs, and 
establishing the tools necessary to do the same for other prominent libraries 
are two more steps in this direction. Quansight and Meta continue working hand 
on hand, improving the compatibility between PyTorch and the rest of the 
ecosystem.&lt;/p&gt;

&lt;p&gt;From Quansight, we would like to thank Mengwei, Voz, and Ed for their 
invaluable help in integrating our work with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. We would also 
like to thank Meta for funding this project as well as previous work on 
improving NumPy compatibility within PyTorch, and the project that led to 
supporting PyTorch within scikit-learn and SciPy. These are giant leaps towards 
consolidating PyTorch as the framework of choice within the open source Python 
data ecosystem.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Evgeni Burovski, Ralf Gommers and Mario Lezcano</name>
        
        
      </author>

      

      

      
        <summary type="html">Quansight engineers have implemented support for tracing through NumPy code via torch.compile in PyTorch 2.1. This feature leverages PyTorch’s compiler to generate efficient fused vectorized code without having to modify your original NumPy code. Even more, it also allows for executing NumPy code on CUDA just by running it through torch.compile under torch.device(&quot;cuda&quot;)!</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Huawei Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/huawei-joins-pytorch/" rel="alternate" type="text/html" title="Huawei Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-10-17T00:00:00-07:00</published>
      <updated>2023-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/huawei-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/huawei-joins-pytorch/">&lt;p&gt;Today, the PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, announced that Huawei has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Huawei has been a long-standing supporter and contributor to the PyTorch Ecosystem, and, through the release of progressive diverse computing, provides easier access to the PyTorch ecosystem for more hardware vendors. By joining as a premier member, Huawei will continue to optimize PyTorch to fully unleash Ascend computing capabilities.&lt;/p&gt;

&lt;p&gt;“We are delighted to join the PyTorch Foundation, and hope to further collaborate with other member companies and expand the community to a wider audience,” said by Zhang Dixuan, President of Huawei Ascend Computing Business, “This move benefits both Huawei, PyTorch, and the wider AI ecosystem. It also aligns with our long-held beliefs in openness, innovation, collaboration, and shared success, and we are confident that it will spur new innovations in the global AI community.”&lt;/p&gt;

&lt;p&gt;Huawei unveiled the All Intelligence strategy to accelerate intelligence across all industries. To cater the demand for AI computing needs, Huawei invests in the system-level technologies, and that belief is centered on open hardware and software that enables partners and fosters talent. This strategy aligns with the PyTorch Foundation’s mission to develop AI as part of a sustainable open source ecosystem and produce inclusive technological feats.&lt;/p&gt;

&lt;p&gt;PyTorch Foundation Executive Director Ibrahim Haddad said, “We are delighted to welcome Huawei to the PyTorch Foundation. Huawei is a leading body in researching computer vision, natural language processing, speech recognition, and other emerging areas, and has proven experience in the field of foundation models. We have no doubt that we will benefit from their support and guidance.”&lt;/p&gt;

&lt;p&gt;As a premier member, Huawei is granted one seat to the PyTorch Foundation Governing Board, and will help set policies, bylaws, and mission and vision statements that define the overarching scope of the PyTorch Foundation’s initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;The Board welcomes Huawei representative Fred Li, Head of Computing Open Source Development Team at Huawei. Fred leads an active and creative team in R&amp;amp;D and operations projects under the principle of “upstream first”, which aims to make diverse computing power ubiquitous.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-huawei&quot;&gt;About Huawei&lt;/h2&gt;

&lt;p&gt;Founded in 1987, Huawei is a leading global provider of information and communications technology (ICT) infrastructure and smart devices. We have 207,000 employees and operate in over 170 countries and regions, serving more than three billion people around the world. We are committed to bringing digital to every person, home and organization for a fully connected, intelligent world.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/legal/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;

&lt;hr class=&quot;mt-5 mb-5&quot; /&gt;

&lt;p&gt;华为成为PyTorch基金会Primer会员&lt;/p&gt;

&lt;p&gt;PyTorch 基金会是深度学习社区在开源 PyTorch 框架和生态系统上进行协作的中立家园，今天宣布华为已作为Primer会员加入。&lt;/p&gt;

&lt;p&gt;华为长期以来一直是PyTorch生态系统的支持者和贡献者，通过推进多样性算力支持与改进，帮助更多厂商后端能够更加轻松地接入PyTorch生态，并积极致力于PyTorch优化，从而充分释放昇腾的算力。&lt;/p&gt;

&lt;p&gt;“通过加入PyTorch基金会，我们可以进一步与其他成员公司共同协作，加速PyTorch社区的发展。”华为昇腾计算业务总裁张迪煊表示，“我们相信这对华为和 PyTorch 生态系统是互惠互利的，也符合我们长期以来开放创新，协作共赢的开源理念，为全球人工智能社区带来更多的兴奋和创新。”&lt;/p&gt;

&lt;p&gt;华为发布全面智能化战略，加速千行万业智能化的转型，持续通过系统级持续创新，坚持硬件开放、软件开源、使能伙伴、发展人才，以满足各行各业多样性的AI算力需求。这与 PyTorch 基金会的使命完美契合且相互补充，即通过培育和维持开源生态系统来推动人工智能的发展，并使每个人都能使用这些技术创新。&lt;/p&gt;

&lt;p&gt;“华为在计算机视觉、自然语言处理、语音识别等领域进行了广泛的研究，并且在大模型领域也积累了成熟的研究经验。我们相信 PyTorch 基金会将从他们对我们的成员和生态系统的支持中受益匪浅。”PyTorch 基金会执行董事 Ibrahim Haddad 说道。&lt;/p&gt;

&lt;p&gt;作为 Primer 会员，华为获得了 PyTorch 基金会董事会的一个席位。董事会通过我们的章程、使命和愿景声明制定政策，描述基金会计划、技术愿景和方向的总体范围。&lt;/p&gt;

&lt;p&gt;我们很高兴欢迎华为计算开源业务总经理李永乐加入我们的董事会。李永乐目前负责华为计算产品线开源业务，他领导着一支极具创新又充满活力的技术和运营团队，他们秉持着“Upstream first”的原则，让多样性算力无处不在。&lt;/p&gt;

&lt;p&gt;要了解有关如何成为 PyTorch 基金会一部分的更多信息，请访问我们的&lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;网站&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;关于华为&lt;/p&gt;

&lt;p&gt;华为创立于1987年，是全球领先的ICT（信息与通信）基础设施和智能终端提供商。我们的20.7万员工遍及170多个国家和地区，为全球30多亿人口提供服务。我们致力于把数字世界带入每个人、每个家庭、每个组织，构建万物互联的智能世界。&lt;/p&gt;

&lt;p&gt;关于PyTorch基金会&lt;/p&gt;

&lt;p&gt;PyTorch 基金会是深度学习社区在开源 PyTorch 框架和生态系统上进行协作的中立家园。 PyTorch 基金会得到其成员和 PyTorch 开源项目主要贡献者的支持。基金会利用成员和贡献者提供的资源来促进社区讨论和协作。&lt;/p&gt;

&lt;p&gt;关于Linux基金会&lt;/p&gt;

&lt;p&gt;Linux 基金会是世界领先的开源软件、硬件、标准和数据协作中心。 Linux 基金会项目对世界基础设施至关重要，包括 Linux、Kubernetes、Node.js、ONAP、PyTorch、RISC-V、SPDX、OpenChain 等。 Linux 基金会专注于利用最佳实践并满足贡献者、用户和解决方案提供商的需求，以创建可持续的开放协作模型。欲了解更多信息，请访问我们的 linuxfoundation.org。 Linux 基金会已注册商标并使用商标。有关 Linux 基金会的商标列表，请参阅其商标使用页面：www.linuxfoundation.org/trademark-usage。 Linux 是 Linus Torvalds 的注册商标。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, the PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, announced that Huawei has joined as a premier member.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Lightning AI Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/lightning-ai-joins-pytorch/" rel="alternate" type="text/html" title="Lightning AI Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-10-17T00:00:00-07:00</published>
      <updated>2023-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/lightning-ai-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/lightning-ai-joins-pytorch/">&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Lightning AI has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Lightning AI is the company behind PyTorch Lightning, the platform and open-source framework for companies to build and deploy AI products leveraging the latest generative AI models.&lt;/p&gt;

&lt;p&gt;“This is a very important milestone for Lightning AI and the PyTorch Lightning community,” remarks Luca Antiga, Chief Technology Officer of Lightning AI. “By joining the PyTorch Foundation, we are strengthening our commitment to boost the adoption of PyTorch across industries. We look forward to partnering with the Foundation to push the vision of PyTorch forward.”&lt;/p&gt;

&lt;p&gt;PyTorch Lightning is one of the leading projects in the PyTorch ecosystem, allowing developers to build, train, fine-tune and deploy AI models at scale. PyTorch Lightning is helping drive the rapid adoption of PyTorch by both the research community and the enterprise.&lt;/p&gt;

&lt;p&gt;“Lightning AI has been a great steward of the AI community, and notably a key contributor to PyTorch over the years,” said PyTorch Foundation Executive Director Ibrahim Haddad. “Their goal of making AI research scalable directly aligns with our mission at the foundation.”&lt;/p&gt;

&lt;p&gt;As a premier member, Lightning AI is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Luca Antiga, Chief Technology Officer at Lightning AI, to our board. Luca joined the Lightning AI team in April 2021 when the Tensorwerk team joined Grid AI. Prior to joining Lightning AI, Luca co-founded Orobix, an applied AI company, and Tensorwerk. He was an early core contributor to PyTorch and co-authored Deep Learning with PyTorch (Manning).&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-lightning-ai&quot;&gt;About Lightning AI&lt;/h2&gt;

&lt;p&gt;Lightning AI is the creator of PyTorch Lightning, the deep learning platform and open-source framework of choice for developers and companies seeking to build and deploy AI products.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/legal/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Lightning AI has joined as a premier member.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Edge: Enabling On-Device Inference Across Mobile and Edge Devices with ExecuTorch</title>
      <link href="https://pytorch.org/blog/pytorch-edge/" rel="alternate" type="text/html" title="PyTorch Edge: Enabling On-Device Inference Across Mobile and Edge Devices with ExecuTorch" />
      <published>2023-10-17T00:00:00-07:00</published>
      <updated>2023-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-edge</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-edge/">&lt;p&gt;We are excited to announce ExecuTorch, our all-new solution for enabling on-device inference capabilities across mobile and edge devices with the backing of industry leaders like Arm, Apple, and Qualcomm Innovation Center.&lt;/p&gt;

&lt;p&gt;As part of PyTorch Edge’s vision for the future of the on-device AI stack and ecosystem, ExecuTorch addresses the fragmentation in the on-device AI ecosystem. It offers a design that provides extension points for seamless third-party integration to accelerate ML models on specialized hardware. Our partners have contributed custom delegate implementations to optimize model inference execution on their respective hardware platforms.&lt;/p&gt;

&lt;p&gt;We have created extensive documentation that provides more details about ExecuTorch’s architecture, its high-level components, example ML models running on ExecuTorch, and end-to-end tutorials for exporting and running a model on various hardware devices. We are excited to see all of the innovative use cases of ExecuTorch built by the community.&lt;/p&gt;

&lt;h2 id=&quot;key-components-of-executorch&quot;&gt;Key Components of ExecuTorch&lt;/h2&gt;

&lt;p&gt;ExecuTorch offers a compact runtime with a lightweight operator registry to cover the PyTorch ecosystem of models, and a streamlined path to execute PyTorch programs on edge devices. These devices range from mobile phones to embedded hardware powered by specific delegates built by our partners. In addition, ExecuTorch ships with a Software Developer Kit (SDK) and toolchain that provide an ergonomic UX for ML Developers to go from model authoring to training and device delegation in a single PyTorch workflow. This suite of tools enables ML developers to perform on-device model profiling and better ways of debugging the original PyTorch model.&lt;/p&gt;

&lt;p&gt;ExecuTorch is architected from the ground up in a composable manner to allow ML developers to make decisions on what components to leverage as well as entry points to extend them if needed. This design provides the following benefits to the ML community:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Portability&lt;/strong&gt;: Compatibility with a wide variety of computing platforms, from high-end mobile phones to highly constrained embedded systems and microcontrollers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Productivity&lt;/strong&gt;: Enabling developers to use the same toolchains and SDK from PyTorch model authoring and conversion, to debugging and deployment to a wide variety of platforms, resulting in productivity gains.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Providing end users with a seamless and high-performance experience due to a lightweight runtime as well as its ability to utilize full hardware capabilities, including general purpose CPUs and specialized purpose microprocessors such as NPUs and DSPs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pytorch-edge-from-pytorch-mobile-to-executorch&quot;&gt;PyTorch Edge: from PyTorch Mobile to ExecuTorch&lt;/h2&gt;

&lt;p&gt;Bringing research and production environments closer together is a fundamental goal of PyTorch. ML engineers increasingly use PyTorch to author and deploy machine learning models in highly dynamic and ever-evolving environments, from servers to edge devices such as mobile phones and embedded hardware.&lt;/p&gt;

&lt;p&gt;With the increasing adoption of AI in Augmented Reality (AR), Virtual Reality (VR), Mixed Reality (MR), Mobile, IoT and other domains, there is a growing need for an end-to-end on-device solution that is extensible, modular, and aligned with the PyTorch stack.&lt;/p&gt;

&lt;p&gt;PyTorch Edge builds on the same fundamental principle of improving research to production by enabling the deployment of various ML models (spanning vision, speech, NLP, translation, ranking, integrity and content creation tasks) to edge devices via a low-friction development and deployment process. It provides a framework stack that spans the universe of on-device use-cases that the PyTorch community cares about.&lt;/p&gt;

&lt;p&gt;PyTorch Edge provides portability of core components that is required to reach a wide spectrum of devices which are characterized by differing hardware configurations, performance and efficiency. Such portability is achieved by allowing optimization that are custom developed for the target use-cases, and developer productivity via well defined entry-points, representations, and tools to tie all this together into a thriving ecosystem.&lt;/p&gt;

&lt;p&gt;PyTorch Edge is the future of the on-device AI stack and ecosystem for PyTorch. We are excited to see what the community builds with ExecuTorch’s on-device inference capabilities across mobile and edge devices backed by our industry partner delegates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/executorch/stable/index.html&quot;&gt;Learn more about PyTorch Edge and ExecuTorch&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>the PyTorch Edge Team</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce ExecuTorch, our all-new solution for enabling on-device inference capabilities across mobile and edge devices with the backing of industry leaders like Arm, Apple, and Qualcomm Innovation Center.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Flash-Decoding for long-context inference</title>
      <link href="https://pytorch.org/blog/flash-decoding/" rel="alternate" type="text/html" title="Flash-Decoding for long-context inference" />
      <published>2023-10-13T00:00:00-07:00</published>
      <updated>2023-10-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/flash-decoding</id>
      <content type="html" xml:base="https://pytorch.org/blog/flash-decoding/">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Large language models (LLM) such as ChatGPT or Llama have received unprecedented attention lately. However, they remain massively expensive to run. Even though generating a single response can cost about $0.01 (a few seconds of an 8xA100 instance on AWS), the costs quickly add up when scaling to billions of users, who could have multiple daily interactions with such LLMs. Some use cases are more expensive, like code auto-completion, because it runs whenever a new character is typed. As LLM applications multiply, even small efficiency gains to the generation time can have a massive impact.&lt;/p&gt;

&lt;p&gt;LLM inference (or “decoding”) is an iterative process: tokens are generated one at a time. Generating full sentences of N tokens requires N forward passes through the model. Fortunately, it is possible to cache previously calculated tokens: this means that a single generation step does not depend on the context length, except for a single operation, the attention. This operation does not scale well with context length.&lt;/p&gt;

&lt;p&gt;There are a number of important emerging use cases of LLMs that utilize a long context. With a longer context, LLMs can reason about longer documents, either to summarize or answer questions about them, they can keep track of longer conversations, or even process entire codebases before writing code. As an example, most LLMs had a context length of up to 2k in 2022 (GPT-3), but we now have open-source LLMs scaling up to 32k (&lt;a href=&quot;https://together.ai/blog/llama-2-7b-32k&quot;&gt;Llama-2-32k&lt;/a&gt;), or even 100k more recently (&lt;a href=&quot;https://about.fb.com/news/2023/08/code-llama-ai-for-coding/&quot;&gt;CodeLlama&lt;/a&gt;). In this setting, attention takes a significant fraction of time during inference.&lt;/p&gt;

&lt;p&gt;When scaling on the batch size dimension, the attention can also become a bottleneck even with relatively small contexts. This is because the amount of memory to read scales with the batch dimension, whereas it only depends on the model size for the rest of the model.&lt;/p&gt;

&lt;p&gt;We present a technique, Flash-Decoding, that significantly speeds up attention during inference, bringing up to 8x faster generation for very long sequences. The main idea is to load the keys and values in parallel as fast as possible, then separately rescale and combine the results to maintain the right attention outputs.&lt;/p&gt;

&lt;h2 id=&quot;multi-head-attention-for-decoding&quot;&gt;Multi-head attention for decoding&lt;/h2&gt;

&lt;p&gt;During decoding, every new token that is generated needs to attend to all previous tokens, to compute:&lt;/p&gt;

&lt;p&gt;softmax(queries @ keys.transpose) @ values&lt;/p&gt;

&lt;p&gt;This operation has been optimized with FlashAttention (v1 and v2 recently) in the training case, where the bottleneck is the memory bandwidth to read and write the intermediate results (e.g. Q @ K^T). However, these optimizations don’t apply directly to the inference case, because the bottlenecks are different. For training, FlashAttention parallelizes across the batch size and query length dimensions. During inference, the query length is typically 1: this means that if the batch size is smaller than the number of streaming multiprocessors (SMs) on the GPU (108 for an A100), the operation will only use a small part of the GPU! This is especially the case when using long contexts, because it requires smaller batch sizes to fit in GPU memory. With a batch size of 1, FlashAttention will use less than 1% of the GPU!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Inference_regular_attn.gif&quot; alt=&quot;FlashAttention&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;FlashAttention parallelizes across blocks of queries and batch size only, and does not manage to occupy the entire GPU during decoding&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The attention can also be done using matrix multiplication primitives - without using FlashAttention. In this case, the operation occupies the GPU entirely, but launches many kernels that write and read intermediate results, which is not optimal.&lt;/p&gt;

&lt;h2 id=&quot;a-faster-attention-for-decoding-flash-decoding&quot;&gt;A faster attention for decoding: Flash-Decoding&lt;/h2&gt;

&lt;p&gt;Our new approach Flash-Decoding is based on FlashAttention, and adds a new parallelization dimension: the keys/values sequence length. It combines the benefits of the 2 approaches from above. Like FlashAttention, it stores very little extra data to global memory, however it fully utilizes the GPU even when the batch size is small, as long as the context length is large enough.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inference_splitkv.gif&quot; alt=&quot;Flash-Decoding&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Flash-Decoding also parallelizes across keys and values, at the cost of a small final reduction step&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Flash-Decoding works in 3 steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, we split the keys/values in smaller chunks.&lt;/li&gt;
  &lt;li&gt;We compute the attention of the query with each of these splits in parallel using FlashAttention. We also write 1 extra scalar per row and per split: the log-sum-exp of the attention values.&lt;/li&gt;
  &lt;li&gt;Finally, we compute the actual output by reducing over all the splits, using the log-sum-exp to scale the contribution of each split.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All of this is possible because the attention/softmax can be calculated iteratively. In Flash-Decoding, it is used at 2 levels: within splits (like FlashAttention), and across splits to perform the final reduction.&lt;/p&gt;

&lt;p&gt;In practice, step (1) does not involve any GPU operation, as the key/value chunks are views of the full key/value tensors. We then have 2 separate kernels to perform respectively (2) and (3).&lt;/p&gt;

&lt;h2 id=&quot;benchmarks-on-codellama-34b&quot;&gt;Benchmarks on CodeLlama 34B&lt;/h2&gt;

&lt;p&gt;To validate this approach, we benchmark the decoding throughput of the CodeLLaMa-34b. This model has the same architecture as Llama 2, and more generally results should generalize across many LLMs. We measure the decoding speed in tok/s at various sequence lengths, from 512 to 64k, and compare multiple ways of calculating the attention:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pytorch: Running the attention using pure PyTorch primitives (without using FlashAttention)&lt;/li&gt;
  &lt;li&gt;FlashAttention v2&lt;/li&gt;
  &lt;li&gt;FasterTransformer: Uses the FasterTransformer attention kernel&lt;/li&gt;
  &lt;li&gt;Flash-Decoding&lt;/li&gt;
  &lt;li&gt;And an upper bound calculated as the time it takes to read from memory the entire model along with the KV-cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Flash-Decoding unlocks up to 8x speedups in decoding speed for very large sequences, and scales much better than alternative approaches.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/decoding_codellama34b.png&quot; alt=&quot;CodeLlama&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All approaches perform similarly for small prompts, but scale poorly as the sequence length increases from 512 to 64k, except Flash-Decoding. In this regime (batch size 1) with Flash-Decoding, scaling the sequence length has little impact on generation speed&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;component-level-micro-benchmarks&quot;&gt;Component-level micro-benchmarks&lt;/h2&gt;

&lt;p&gt;We also micro-benchmark the scaled multi-head attention for various sequence lengths and batch sizes on A100 with inputs in f16. We set the batch size to 1, and use 16 query heads of dimension 128, for 2 key/value heads (grouped-query attention), which matches the dimensions used in CodeLLaMa-34b when running on 4 GPUs.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Setting \ Algorithm&lt;/td&gt;
      &lt;td&gt;PyTorch Eager (us)&lt;/td&gt;
      &lt;td&gt;Flash-Attention v2.0.9 (us)&lt;/td&gt;
      &lt;td&gt;Flash-Decoding (us)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=256, seqlen=256&lt;/td&gt;
      &lt;td&gt;3058.6&lt;/td&gt;
      &lt;td&gt;390.5&lt;/td&gt;
      &lt;td&gt;63.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=128, seqlen=512&lt;/td&gt;
      &lt;td&gt;3151.4&lt;/td&gt;
      &lt;td&gt;366.3&lt;/td&gt;
      &lt;td&gt;67.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=64, seqlen=1024&lt;/td&gt;
      &lt;td&gt;3160.4&lt;/td&gt;
      &lt;td&gt;364.8&lt;/td&gt;
      &lt;td&gt;77.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=32, seqlen=2048&lt;/td&gt;
      &lt;td&gt;3158.3&lt;/td&gt;
      &lt;td&gt;352&lt;/td&gt;
      &lt;td&gt;58.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=16, seqlen=4096&lt;/td&gt;
      &lt;td&gt;3157&lt;/td&gt;
      &lt;td&gt;401.7&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=8, seqlen=8192&lt;/td&gt;
      &lt;td&gt;3173.1&lt;/td&gt;
      &lt;td&gt;529.2&lt;/td&gt;
      &lt;td&gt;56.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=4, seqlen=16384&lt;/td&gt;
      &lt;td&gt;3223&lt;/td&gt;
      &lt;td&gt;582.7&lt;/td&gt;
      &lt;td&gt;58.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=2, seqlen=32768&lt;/td&gt;
      &lt;td&gt;3224.1&lt;/td&gt;
      &lt;td&gt;1156.1&lt;/td&gt;
      &lt;td&gt;60.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=1, seqlen=65536&lt;/td&gt;
      &lt;td&gt;1335.6&lt;/td&gt;
      &lt;td&gt;2300.6&lt;/td&gt;
      &lt;td&gt;64.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=1, seqlen=131072&lt;/td&gt;
      &lt;td&gt;2664&lt;/td&gt;
      &lt;td&gt;4592.2&lt;/td&gt;
      &lt;td&gt;106.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Micro-benchmark of the multi-head attention, run-time in us. Flash-Decoding achieves almost constant run-time as the sequence length scales to up to 64k.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The up to 8x speedup end-to-end measured earlier is made possible because the attention itself is up to 50x faster than FlashAttention. Up until sequence length 32k, the attention time is roughly constant, because Flash-Decoding manages to fully utilize the GPU.&lt;/p&gt;

&lt;h2 id=&quot;using-flash-decoding&quot;&gt;Using Flash-Decoding&lt;/h2&gt;

&lt;p&gt;Flash-decoding is available:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention/tree/main&quot;&gt;FlashAttention&lt;/a&gt; package, starting at version 2.2&lt;/li&gt;
  &lt;li&gt;Through &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; starting at version 0.0.22 through `xformers.ops.memory_efficient_attention`. The dispatcher will automatically use either the Flash-Decoding or FlashAttention approaches depending on the problem size. When these approaches are not supported, it can dispatch to an efficient triton kernel that implements the Flash-Decoding algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A full example of decoding with LLaMa v2 / CodeLLaMa is available in the FlashAttention repo &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention/tree/main/examples/inference&quot;&gt;here&lt;/a&gt; and in the xFormers &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;repo&lt;/a&gt; here. We also provide a &lt;a href=&quot;https://github.com/facebookresearch/xformers/tree/main/examples/llama_inference&quot;&gt;minimal example&lt;/a&gt; of an efficient decoding code for LLaMa v1/v2 models, meant to be fast, easy to read, educational and hackable.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Thanks to Erich Elsen, Ashish Vaswani, and Michaël Benesty for suggesting this idea of splitting the KVcache loading. We want to thank Jeremy Reizenstein, Patrick Labatut and Andrew Tulloch for the valuable discussions, and Quentin Carbonneaux for contributing the efficient decoding example to xFormers. We also want to thank Geeta Chauhan and Gregory Chanan for helping with the writing and more broadly contributing to getting this published on the PyTorch blog.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Tri Dao, Daniel Haziza, Francisco Massa, Grigory Sizov</name>
        
        
      </author>

      

      

      
        <summary type="html">Motivation</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ML Model Server Resource Saving - Transition From High-Cost GPUs to Intel CPUs and oneAPI powered Software with performance</title>
      <link href="https://pytorch.org/blog/ml-model-server-resource-saving/" rel="alternate" type="text/html" title="ML Model Server Resource Saving - Transition From High-Cost GPUs to Intel CPUs and oneAPI powered Software with performance" />
      <published>2023-10-11T00:00:00-07:00</published>
      <updated>2023-10-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/ml-model-server-resource-saving</id>
      <content type="html" xml:base="https://pytorch.org/blog/ml-model-server-resource-saving/">&lt;p&gt;Reviewers: &lt;a href=&quot;https://www.linkedin.com/in/yunsang-ju/&quot;&gt;Yunsang Ju&lt;/a&gt;(Naver GplaceAI Leader), Min Jean Cho(Intel), Jing Xu(Intel), Mark Saroufim(Meta)&lt;/p&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;Here, We will be sharing our experience in moving AI workloads from our GPU servers to our Intel CPU servers without any performance or quality degradation, and &lt;strong&gt;saving annual costs of approximately 340 thousand U.S. Dollar&lt;/strong&gt; (refer to the &lt;strong&gt;Conclusion&lt;/strong&gt;) in the process.&lt;/p&gt;

&lt;p&gt;We aim to provide value to our consumers by serving various AI models that enhance the Online to Offline (O2O) experience. With the ongoing growth in the demand for new models and the limited nature of high-cost resource GPUs, we needed to transition relatively lightweight AI models from GPU servers to Intel CPU servers for reducing resource consumption. In the same setting, however, the CPU server had issues where performance of rps, inference time, etc. was reduced by tens of times. We applied various engineering techniques and lightweighted the model to solve this problem, and we were able to successfully transition to the Intel CPU servers with the same performance or better performance as the GPU servers with just a three-fold scale out.&lt;/p&gt;

&lt;p&gt;For a more detailed introduction about our team, please refer to the &lt;a href=&quot;https://medium.com/naver-place-dev/introduction-to-naver-place-ai-development-team-a8b0630e3b23&quot;&gt;Introduction to NAVER Place AI Development Team&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ll mention it again in the middle, but I’ve received a lot of help from &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;Grokking Pytorch Intel CPU Performance From First Principles&lt;/a&gt; written by Intel and PyTorch in the overall work.&lt;/p&gt;

&lt;h2 id=&quot;problem-definition&quot;&gt;Problem Definition&lt;/h2&gt;

&lt;h3 id=&quot;1-service-architecture&quot;&gt;1: Service Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg1.jpg&quot; alt=&quot;Simplified service architecture&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simplified service architecture (Image Source: NAVER GplaceAI)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To facilitate understanding, a brief introduction to our service architecture will be provided. CPU intensive tasks such as preprocessing input to tensor format (then forwarded to the model) and post processing inference results to human readable output (e.g. natural language and image formats) are performed on the App Server(FastAPI) The Model Server(TorchServe) exclusively handles inference operations. For stable operation of the service, the following actions need to be performed with sufficient throughput and low latency.&lt;/p&gt;

&lt;p&gt;The specific processing sequence is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The client submits a request to the app server via the Traefik gateway.&lt;/li&gt;
  &lt;li&gt;The app server pre-processes the input by performing actions such as resizing and transforming, and converting it into a Torch tensor before then requesting the model server.&lt;/li&gt;
  &lt;li&gt;The model server performs inference and returns the feature to the app server&lt;/li&gt;
  &lt;li&gt;The app server converts the feature into a format understandable by humans through post-processing and returns it to the client&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-throughput-and-latency-measurement&quot;&gt;2:  Throughput and Latency Measurement&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg1-1.jpg&quot; alt=&quot;Comparison of Image Scoring Models&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparison of Image Scoring Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With all other conditions remaining the same, deploying on a threefold increase CPU server pod, yet, notably, the RPS (requests per second) and response time deteriorated by more than tenfold. While it was not surprising that CPU inference performance is inferior to GPUs, the challenging situation was evident. Given the goal of maintaining performance within limited resources, achieving an approximate &lt;strong&gt;10 to 20 times performance improvement&lt;/strong&gt; was necessary Barring any additional scaling.&lt;/p&gt;

&lt;h3 id=&quot;3-challenges-from-a-throughput-perspective&quot;&gt;3: Challenges From a Throughput Perspective&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                        37     0(0.00%) |   9031    4043   28985   8200 |    1.00        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                        37     0(0.00%) |   9031    4043   28985   8200 |    1.00        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One of the first steps TorchServer framework users might take in order to improve throughput is to increase the number of workers in TorchServe. This approach is effective on GPU servers Because of parallel workload processing, excluding the linear memory usage increase as workers scale. However, we were experiencing worse performance when increasing the number of workers. Identifying the cause of performance degradation on CPU servers required further investigation.&lt;/p&gt;

&lt;h3 id=&quot;4challenges-from-a-latency-perspective&quot;&gt;4: Challenges From a Latency Perspective&lt;/h3&gt;

&lt;p&gt;Our primary concern was latency. Throughput improvement is normally achievable when a system’s implementation is faithful to scale-out principles, except for perhaps very rare worst-case scenarios. However, in the case of the Image Scoring model example, even performing a single inference took more than 1 second, and as the request volume increased, latency increased to as much as 4 seconds. It was a situation where the timeout criteria to satisfy the client could not be met even with a single inference.&lt;/p&gt;

&lt;h2 id=&quot;proposed-solutions&quot;&gt;Proposed Solutions&lt;/h2&gt;

&lt;p&gt;Improvements were needed from both an ML and an engineering perspective. It was essential to fundamentally reduce the inference time on the CPU and to identify the causes of performance degradation when applying config that generally enhances performance, in order to find the optimal configuration values. To accomplish this, collaboration was established with MLE professionals to concurrently execute tasks encompassing ‘model lightweighting without compromising performance’, and ‘Identify optimal configurations for achieving peak performance’. Using the aforementioned approaches we were able to effectively transition workload handling to our CPU servers.&lt;/p&gt;

&lt;h3 id=&quot;1-resolving-low-rps-from-an-engineering-perspective&quot;&gt;1: Resolving Low RPS from an Engineering Perspective&lt;/h3&gt;

&lt;p&gt;First, the reason for performance degradation even after increasing the worker number was the front-end bound caused by logical threads in GEMM operations. Generally, when increasing the number of workers, the expected improvement effect is the increase in parallelism. Conversely, if performance decreases, one can infer the corresponding trade-off effect.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg2.jpg&quot; alt=&quot;CPU + GPU&quot; style=&quot;width:100%; max-width: 420px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image Source: &lt;a href=&quot;https://blogs.nvidia.com/blog/2018/06/11/what-is-a-virtual-gpu/&quot;&gt;Nvidia&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As many are aware, the reason model inference performance on CPUs is inferior to GPUs lies in the difference in hardware design, particularly in terms of multi-threading capabilities. Diving deeper, model inference is fundamentally a repetition of &lt;strong&gt;GEMM (General Matrix Multiply)&lt;/strong&gt; operations, and these GEMM operations are executed independently in &lt;strong&gt;“fused-multiply-add” (FMA)&lt;/strong&gt; or &lt;strong&gt;“dot-product” (DP)&lt;/strong&gt; execution units. If the GEMM operation becomes a bottleneck on the CPU, increasing parallelism might actually result in decreased performance. While researching the problem we found relevant information within the &lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/advanced/cpu_affinity.html#binding-processes-to-physical-cores&quot;&gt;PyTorch documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;While two logical threads run GEMM at the same time, they will be sharing the same core resources causing front-end bound&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This information highlighted that logical threads could cause a bottleneck in CPU GEMM operations, which helped us intuitively understand why performance decreased when increasing the worker num. This is because the default value of the torch thread corresponds to the physical core value of the CPU.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@test-pod:/# lscpu
  …
Thread(s) per core: 2
Core(s) per socket: 12
  …
root@test-pod:/# python
&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; print(torch.get_num_threads())
24
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the worker_num increases, the total thread count increases by the product of the physical core * worker number. Consequently, logical threads are utilized. In order to improve performance, the total number of threads per worker was adjusted to align with the physical core count. Below, it can be observed that the metric RPS &lt;strong&gt;increased approximately threefold&lt;/strong&gt; to 6.3(from the previous value of 2.1) when the worker_num was increased to 4 and the total thread count was aligned with the number of physical cores.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                       265     0(0.00%) |   3154    1885    4008   3200 |    6.30        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       265     0(0.00%) |   3154    1885    4008   3200 |    6.30        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 1&lt;/strong&gt;: Our team is Using Kubernetes to maintain our deployments. So we are adjusting the which required us to adjust according to the CPU resource limit of the pod, rather than the physical core count of the node that can be checked using the lscpu command. (Setting the torch thread of each worker to 8/4 = 2, or 24/4 = 6 resulted in performance degradation.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 2&lt;/strong&gt;: Since torch thread settings for each worker &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.set_num_threads.html&quot;&gt;can only be configured as integers&lt;/a&gt;, it’s advisable to set the CPU limit divisible by the worker_num in order to adequately utilize CPU usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg3.jpg&quot; alt=&quot;example&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ex) core=8, In the case of worker_num=3: int(8/worker_num) = 2, 2*worker_num/8 = 75%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg4.jpg&quot; alt=&quot;example&quot; style=&quot;width:100%; margin-top: 30px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ex) core=8, In the case of worker_num=4: int(8/worker_num) = 2, 2*worker_num/8 = 100%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We also analyzed the model containers to see why we got a mere threefold improvement in performance despite a four times increase in the number of workers. Various resources were monitored, and among them, the core utilization rate was identified as the underlying cause.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg5.jpg&quot; alt=&quot;threads&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even when the total thread count was adjusted to match the CPU(2nd Generation, Intel(R) Xeon(R) Silver 4214) limit(8 core), there were instances where computations were executed from logical thread to logical core. Due to the presence of 24 physical cores, the cores numbered 25 to 48 are classified as logical cores. The possibility of confining thread execution solely within physical cores seemed to offer the potential for further performance enhancement. The reference to this solution could be found within the source document mentioned in the PyTorch-geometric article that warned about CPU GEMM bottlenecks.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reference Documentation: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;Grokking Pytorch Intel CPU Performance From First Principles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As per the instructions in the document, Intel provides Intel® Extension for PyTorch where we can simply pin cores to specific sockets. The application method is also made very simple, by adding the following settings to the &lt;strong&gt;torchserve config.properties&lt;/strong&gt; file.(used intel_extension_for_pytorch==1.13.0)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ipex_enable=true
CPU_launcher_enable=true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg6.jpg&quot; alt=&quot;two-socket configuration&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image Source: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Beyond the removal of logical threads through socket pinning, there is an additional effect of eliminating UPI cache hit overhead. Since the CPU comprises more than one socket when threads scheduled on socket 1 are rescheduled on socket 2, cache hits occur in cases of accessing the cache of socket 1 via Intel Ultra Path Interconnect (UPI). At this point, UPI access to the local cache becomes more than twice as slow as local cache access, resulting in more bottlenecks. With threads being pinned to socket units by oneAPI powered Intel® Extension for PyTorch, We observed rps handling increase of up to &lt;strong&gt;four times than when the bottleneck existed&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                       131     0(0.00%) |   3456    1412    6813   3100 |    7.90        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       131     0(0.00%) |   3456    1412    6813   3100 |    7.90        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 1&lt;/strong&gt;: Intel® Extension for PyTorch is specialized in neural network (referred to as “nn” hereafter) inference optimization, so the performance improvement from additional techniques outside nn might be minimal. Indeed, in the instance of the image scoring system highlighted as an example, where svr (support vector regression) is applied post-inference, the performance enhancement was confined to a 4-fold increase. However, for a purely nn inference model such as the food recognition model, &lt;strong&gt;a&lt;/strong&gt; &lt;strong&gt;performance boost of 7-fold (2.5rps -&amp;gt; 17.5rps)&lt;/strong&gt; was detected.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/food-classification                                                 446     0(0.00%) |   1113     249    1804   1200 |   17.50        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       446     0(0.00%) |   1113     249    1804   1200 |   17.50        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 2&lt;/strong&gt;: Applying Intel® Extension for PyTorch requires &lt;strong&gt;torchserve version 0.6.1 or higher&lt;/strong&gt;. Since our team was using version 0.6.0, there was an issue where socket pinning was not functioning correctly. Currently, we have made modifications to the guide document, specifying the required version.&lt;/p&gt;

&lt;p&gt;Within &lt;a href=&quot;https://github.com/pytorch/serve/blob/4236a86dc0a018198ecd3fe261e835b416df739e/frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java&quot;&gt;WorkerLifeCycle.java&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;,&lt;/span&gt; multi-worker pinning is not supported in 0.6.0 and below (ninstance is hardcoded to 1)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// 0.6.0 version

public ArrayList&amp;lt;String&amp;gt; launcherArgsToList() {
   ArrayList&amp;lt;String&amp;gt; arrlist = new ArrayList&amp;lt;String&amp;gt;();
   arrlist.add(&quot;-m&quot;);
   arrlist.add(&quot;intel_extension_for_pytorch.cpu.launch&quot;);
   arrlist.add(&quot; — ninstance&quot;);
   arrlist.add(&quot;1&quot;);
   if (launcherArgs != null &amp;amp;&amp;amp; launcherArgs.length() &amp;gt; 1) {
     String[] argarray = launcherArgs.split(&quot; &quot;);
     for (int i = 0; i &amp;lt; argarray.length; i++) {
       arrlist.add(argarray[i]);
     }
   }
   return arrlist;
 }
// master version

if (this.numWorker &amp;gt; 1) {
   argl.add(&quot; — ninstances&quot;);
   argl.add(String.valueOf(this.numWorker));
   argl.add(&quot; — instance_idx&quot;);
   argl.add(String.valueOf(this.currNumRunningWorkers));
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-addressing-slow-latency-through-model-lightweighting&quot;&gt;2: Addressing Slow Latency Through Model Lightweighting&lt;/h3&gt;

&lt;p&gt;We also streamlined our model using &lt;strong&gt;Knowledge Distillation&lt;/strong&gt; (commonly abbreviated as KD) to further reduce latency. As is widely known, kd is a technique where knowledge from a larger network (Teacher network) is conveyed to a smaller, lightweight network (Student network) which is less resource intensive and can be more readily deployed. For more detailed information, please refer to the paper where this concept was initially introduced, titled &lt;span style=&quot;text-decoration:underline;&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg7.jpg&quot; alt=&quot;neural networks&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a variety of KD techniques available and because we were primarily focused on &lt;strong&gt;accuracy loss minimization&lt;/strong&gt;, we adopted the approach from the paper &lt;a href=&quot;https://arxiv.org/pdf/2205.10536.pdf&quot;&gt;Knowledge Distillation from A Stronger Teacher&lt;/a&gt;, which was published in the year 2022. The concept is straightforward. Unlike the conventional method of distillation that utilizes only the model’s prop values, the chosen approach involves having the student network learn the correlations between classes in the teacher network. When put into actual application, We observed effective model weight reduction to observe the effective reduction in the model’s weight while mainting high accuracy. The following are the outcomes of our experimentation with the mentioned knowledge distillation technique on several candidate student models, where selections were made based on the maintained level of accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg8.jpg&quot; alt=&quot;table of services&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the image scoring system, additional measures were taken to reduce the input size. Considering that the prior use of CPU-based ML technique SVR (Support Vector Regression) was used (2-stage: CNN + SVR), even when this was streamlined into a 1-stage model, significant speed advantages were not observed in CPU inference. In order for streamlining to have significance, the input size of the student model during inference needed further reduction. Consequently, experiments were conducted with the size reduced from 384&lt;em&gt;384 to 224&lt;/em&gt;224.&lt;/p&gt;

&lt;p&gt;Further simplifying transformations, the 2-stage (CNN + SVR) approach was unified into a 1-stage model with a larger ConvNext, and then kd was applied using the lightweight EfficientNet to resolve the accuracy trade-off. During the experiments, we encountered a problem where changing Img_resize to 224 led to a performance drop from 0.4007 to 0.4296 in terms of MAE. Due to the reduction in input size, various preprocessing techniques applied to the original training images (such as Affine, RandomRotate90, Blur, OneOf [GridDistortion, OpticalDistortion, ElasticTransform], VerticalFlip) had a counterproductive effect. By adopting these measures, effective training of the student was achieved, and the &lt;strong&gt;MAE value improved by 25% compared to the previous one (.518 to .3876)&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;validation&quot;&gt;Validation&lt;/h2&gt;

&lt;h3 id=&quot;1-final-performance-measurement&quot;&gt;1: Final Performance Measurement&lt;/h3&gt;

&lt;p&gt;The following shows the final performance improvements using CPU servers, on the three models mentioned throughout this article.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Food photo classifier (pod 3): 2.5rps -&amp;gt; 84 rps

 Type Name                                                                           # reqs # fails | Avg Min Max Med | req/s failures/s
 --------|----------------------------------------------------------------------------|------|------------|-------|------|-------|-------|--------|--------- 
POST /predictions/food-classification 2341 0(0.00%) | 208 130 508 200 | 84.50 0.00 
--------|----------------------------------------------------------------------------|--------|-------------|------|-------|--------|------|--------|----------
         Aggregated                                                                      2341     0(0.00%) |    208     130     508    200 |   84.50        0.00

# Image scoring (pod 3): 2.1rps -&amp;gt; 62rps
 Type Name                                                                               #reqs #fails | Avg Min Max Median | req/s failures/s
 --------|---------------------------------------------------------------------------------|--------|-------------|--------|-------|--------|---------|--------|--------- 
  POST /predictions/image-scoring 1298 0 (0.00%) | 323 99 607 370 | 61.90 0.00 
--------|---------------------------------------------------------------------------------|--------|-------------|--------|------|--------|---------|--------|----------
          Aggregated                                                                          1298     0(0.00%)  |     323      99     607     370  |   61.90        0.00

# receipt classifier(pod 3) : 20rps -&amp;gt; 111.8rps
Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/receipt-classification                                             4024     0(0.00%) |    266     133    2211    200 |   111.8        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                      4020     0(0.00%) |    266     133    2211    200 |   111.8        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-traffic-mirroring&quot;&gt;2:  Traffic Mirroring&lt;/h3&gt;

&lt;p&gt;As previously mentioned, our team’s service architecture employs the tool “traefik” as a gateway in front of the app server, as briefly introduced at the beginning of the article. For final validation, the mirroring feature of this traefik gateway was utilized to mirror traffic from production to staging for a month of validation before applying it to production, which is now operational.&lt;/p&gt;

&lt;p&gt;Details regarding mirroring are beyond the scope of this topic and hence omitted. For those interested, kindly refer to the document at &lt;a href=&quot;https://doc.traefik.io/traefik/routing/services/#mirroring-service&quot;&gt;https://doc.traefik.io/traefik/routing/services/#mirroring-service&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;This concludes the discussion about transitioning from a GPU model server to a CPU server while maintaining service quality. Through this effort, our team &lt;strong&gt;was able to save 15 GPUs each in South Korea and Japan&lt;/strong&gt;, resulting in an &lt;strong&gt;annual cost savings of approximately 340 thousand U.S. Dollar&lt;/strong&gt;. Although we directly purchase and use GPUs within NAVER, we calculated a rough cost reduction &lt;a href=&quot;https://aws.amazon.com/ko/ec2/instance-types/g4/&quot;&gt;based on AWS EC2 instances&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt; &lt;/span&gt;that stably support T4 GPUs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg9.jpg&quot; alt=&quot;instance sizes&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Calculation: 1.306 (1-year reserved instance effective hourly cost) * 24 (hours) * 365 (days) * 15 (number of GPUs) * 2 (KR + JP)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These secured GPUs will be harnessed to further advance and enhance our team’s AI services, delivering exceptional service experiences. We sincerely appreciate your encouragement and anticipation.:)&lt;/p&gt;

&lt;h2 id=&quot;explore-more&quot;&gt;Explore More&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/ecosystem/pytorch-foundation.html&quot;&gt;https://www.intel.com/content/www/us/en/developer/ecosystem/pytorch-foundation.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/advanced/cpu_affinity.html#binding-processes-to-physical-cores&quot;&gt;https://pytorch-geometric.readthedocs.io/en/latest/advanced/CPU_affinity.html#binding-processes-to-physical-cores&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.10536.pdf&quot;&gt;https://arxiv.org/pdf/2205.10536.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Sangjune Park(Naver GplaceAI MLOps), Jooyoung Lee(Naver GplaceAI MLE), Junho Min(Naver GplaceAI MLE)</name>
        
        
      </author>

      

      

      
        <summary type="html">Reviewers: Yunsang Ju(Naver GplaceAI Leader), Min Jean Cho(Intel), Jing Xu(Intel), Mark Saroufim(Meta)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Real-time Audio-visual Speech Recognition</title>
      <link href="https://pytorch.org/blog/real-time-speech-rec/" rel="alternate" type="text/html" title="Real-time Audio-visual Speech Recognition" />
      <published>2023-10-10T00:00:00-07:00</published>
      <updated>2023-10-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/real-time-speech-rec</id>
      <content type="html" xml:base="https://pytorch.org/blog/real-time-speech-rec/">&lt;p&gt;Audio-Visual Speech Recognition (AV-ASR, or AVSR) is the task of transcribing text from audio and visual streams, which has recently attracted a lot of research attention due to its robustness to noise. The vast majority of work to date has focused on developing AV-ASR models for non-streaming recognition; studies on streaming AV-ASR are very limited.&lt;/p&gt;

&lt;p&gt;We have developed a compact real-time speech recognition system based on TorchAudio, a library for audio and signal processing with &lt;a href=&quot;http://pytorch.org&quot;&gt;PyTorch&lt;/a&gt;. It can run locally on a laptop with high accuracy without accessing the cloud. Today, we are releasing &lt;a href=&quot;https://github.com/pytorch/audio/tree/main/examples/avsr&quot;&gt;the real-time AV-ASR recipe&lt;/a&gt; under a permissive open license (BSD-2-Clause license), enabling a broad set of applications and fostering further research on audio-visual models for speech recognition.&lt;/p&gt;

&lt;p&gt;This work is part of our approach to &lt;a href=&quot;https://arxiv.org/abs/2303.14307&quot;&gt;AV-ASR research&lt;/a&gt;. A promising aspect of this approach is its ability to automatically annotate large-scale audio-visual datasets, which enables the training of more accurate and robust speech recognition systems. Furthermore, this technology has the potential to run on smart devices since it achieves the latency and memory efficiency that such devices require for inference.&lt;/p&gt;

&lt;p&gt;In the future, speech recognition systems are expected to power applications in numerous domains. One of the primary applications of AV-ASR is to enhance the performance of ASR in noisy environments. Since visual streams are not affected by acoustic noise, integrating them into an audio-visual speech recognition model can compensate for the performance drop of ASR models. Our AV-ASR system has the potential to serve multiple purposes beyond speech recognition, such as text summarization, translation and even text-to-speech conversion. Moreover, the exclusive use of VSR can be useful in certain scenarios, e.g. where speaking is not allowed, in meetings, and where privacy in public conversations is desired.&lt;/p&gt;

&lt;h1 id=&quot;av-asr&quot;&gt;AV-ASR&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/real-time-speech-rec/pipeline.jpg&quot; alt=&quot;Fig. 1 The pipeline for audio-visual speech recognition system&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: The pipeline for audio-visual speech recognition system&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Our real-time AV-ASR system is presented in Fig. 1. It consists of three components, a data collection module, a pre-processing module and an end-to-end model. The data collection module comprises hardware devices, such as a microphone and camera. Its role is to collect information from the real world. Once the information is collected, the pre-processing module location and crop out face. Next, we feed the raw audio stream and the pre-processed video stream into our end-to-end model for inference.&lt;/p&gt;

&lt;h2 id=&quot;data-collection&quot;&gt;Data collection&lt;/h2&gt;

&lt;p&gt;We use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.io.StreamReader&lt;/code&gt; to capture audio/video from streaming device input, e.g. microphone and camera on laptop. Once the raw video and audio streams are collected, the pre-processing module locates and crops faces. It should be noted that data is immediately deleted during the streaming process.&lt;/p&gt;

&lt;h2 id=&quot;pre-processing&quot;&gt;Pre-processing&lt;/h2&gt;

&lt;p&gt;Before feeding the raw stream into our model, each video sequence has to undergo a specific pre-processing procedure. This involves three critical steps. The first step is to perform face detection. Following that, each individual frame is aligned to a referenced frame, commonly known as the mean face, in order to normalize rotation and size differences across frames. The final step in the pre-processing module is to crop the face region from the aligned face image. We would like to clearly note that our model is fed with raw audio waveforms and pixels of the face, without any further preprocessing like face parsing or landmark detection. An example of the pre-processing procedure is illustrated in Table 1.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/original.gif&quot; alt=&quot;Original image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img src=&quot;/assets/images/real-time-speech-rec/detected.gif&quot; alt=&quot;Detected image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/transformed.gif&quot; alt=&quot;Transformed image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/cropped.gif&quot; alt=&quot;Cropped image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
    0. Original
   &lt;/td&gt;
   &lt;td&gt;
1. Detection
   &lt;/td&gt;
   &lt;td&gt;
2. Alignment
   &lt;/td&gt;
   &lt;td&gt;
3. Crop
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: Preprocessing pipeline.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/real-time-speech-rec/model.jpg&quot; alt=&quot;Fig. 2 The architecture for the audio-visual speech recognition system.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: The architecture for the audio-visual speech recognition system&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;We consider two configurations: Small with 12 Emformer blocks and Large with 28, with 34.9M and 383.3M parameters, respectively. Each AV-ASR model composes front-end encoders, a fusion module, an Emformer encoder, and a transducer model. To be specific, we use convolutional frontends to extract features from raw audio waveforms and facial images. The features are concatenated to form 1024-d features, which are then passed through a two-layer multi-layer perceptron and an Emformer transducer model. The entire network is trained using RNN-T loss. The architecture of the proposed AV-ASR model is illustrated in Fig. 2.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Datasets.&lt;/strong&gt; We follow &lt;a href=&quot;https://arxiv.org/abs/2303.14307&quot;&gt;Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels&lt;/a&gt; to use publicly available audio-visual datasets including &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/lip_reading/&quot;&gt;LRS3&lt;/a&gt;, &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html&quot;&gt;VoxCeleb2&lt;/a&gt; and &lt;a href=&quot;https://looking-to-listen.github.io/avspeech/&quot;&gt;AVSpeech&lt;/a&gt; for training. We do not use mouth ROIs or facial landmarks or attributes during both training and testing stages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparisons with the state-of-the-art.&lt;/strong&gt; Non-streaming evaluation results on LRS3 are presented in Table 2. Our audio-visual model with an algorithmic latency of 800 ms (160ms+1280msx0.5) yields a WER of 1.3%, which is on par with those achieved by state-of-the-art offline models such as AV-HuBERT, RAVEn, and Auto-AVSR.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Method&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Total Hours&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;WER (%)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;ViT3D-CM
   &lt;/td&gt;
   &lt;td&gt;90, 000
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AV-HuBERT
   &lt;/td&gt;
   &lt;td&gt;1, 759
   &lt;/td&gt;
   &lt;td&gt;1.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;RAVEn
   &lt;/td&gt;
   &lt;td&gt;1, 759
   &lt;/td&gt;
   &lt;td&gt;1.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AutoAVSR
   &lt;/td&gt;
   &lt;td&gt;3, 448
   &lt;/td&gt;
   &lt;td&gt;0.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Ours
   &lt;/td&gt;
   &lt;td&gt;3, 068
   &lt;/td&gt;
   &lt;td&gt;1.3
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 2&lt;/strong&gt;: Non-streaming evaluation results for audio-visual models on the LRS3 dataset.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Noisy experiments.&lt;/strong&gt; During training, 16 different noise types are randomly injected to audio waveforms, including 13 types from &lt;a href=&quot;https://zenodo.org/record/1227121&quot;&gt;Demand&lt;/a&gt; database, ‘DLIVING’,’DKITCHEN’, ‘OMEETING’, ‘OOFFICE’, ‘PCAFETER’, ‘PRESTO’, ‘PSTATION’, ‘STRAFFIC’,  ‘SPSQUARE’, ‘SCAFE’, ‘TMETRO’, ‘TBUS’ and ‘TCAR’, two more types of noise from &lt;a href=&quot;https://arxiv.org/abs/1804.03209&quot;&gt;speech commands&lt;/a&gt; database, white and pink and one more type of noise from &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/0167639393900953&quot;&gt;NOISEX-92&lt;/a&gt; database, babble noise. SNR levels in the range of [clean, 7.5dB, 2.5dB, -2.5dB, -7.5dB] are selected from with a uniform distribution. Results of ASR and AV-ASR models, when tested with babble noise, are shown in Table 3. With increasing noise level, the performance advantage of our audio-visual model over our audio-only model grows, indicating that incorporating visual data improves noise robustness.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Type&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;∞&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;10dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;5dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;-5dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;-10dB&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;A
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;1.8
   &lt;/td&gt;
   &lt;td&gt;3.2
   &lt;/td&gt;
   &lt;td&gt;10.9
   &lt;/td&gt;
   &lt;td&gt;27.9
   &lt;/td&gt;
   &lt;td&gt;55.5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;A+V
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;1.7
   &lt;/td&gt;
   &lt;td&gt;2.1
   &lt;/td&gt;
   &lt;td&gt;6.2
   &lt;/td&gt;
   &lt;td&gt;11.7
   &lt;/td&gt;
   &lt;td&gt;27.6
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 3&lt;/strong&gt;: Streaming evaluation WER (%) results at various signal-to-noise ratios for our audio-only (A) and audio-visual (A+V) models on the LRS3 dataset under 0.80-second latency constraints.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-time factor&lt;/strong&gt;. The real-time factor (RTF) is an important measure of a system’s ability to process real-time tasks efficiently. An RTF value of less than 1 indicates that the system meets real-time requirements. We measure RTF using a laptop with an Intel® Core™ i7-12700 CPU running at 2.70 GHz and an NVIDIA 3070 GeForce RTX 3070 Ti GPU. To the best of our knowledge, this is the first AV-ASR model that reports RTFs on the LRS3 benchmark. The Small model achieves a WER of 2.6% and an RTF of 0.87 on CPU (Table 4), demonstrating its potential for real-time on-device inference applications.&lt;/p&gt;

&lt;table class=&quot;table table-bordered text-center&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Device&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Streaming WER [%]&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;RTF&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Large
   &lt;/td&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;0.35
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Small
   &lt;/td&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;2.6
   &lt;/td&gt;
   &lt;td&gt;0.33
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPU
   &lt;/td&gt;
   &lt;td&gt;0.87
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 4&lt;/strong&gt;: Impact of AV-ASR model size and device on WER and RTF. Note that the RTF calculation includes the pre-processing step wherein the Ultra-Lightweight Face Detection Slim 320 model is used to generate face bounding boxes.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Learn more about the system from the published works below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shi, Yangyang, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Mike Seltzer. “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition.” In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6783-6787. IEEE, 2021.&lt;/li&gt;
  &lt;li&gt;Ma, Pingchuan, Alexandros Haliassos, Adriana Fernandez-Lopez, Honglie Chen, Stavros Petridis, and Maja Pantic. “Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels.” In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. IEEE, 2023.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">Audio-Visual Speech Recognition (AV-ASR, or AVSR) is the task of transcribing text from audio and visual streams, which has recently attracted a lot of research attention due to its robustness to noise. The vast majority of work to date has focused on developing AV-ASR models for non-streaming recognition; studies on streaming AV-ASR are very limited.</summary>
      

      
      
    </entry>
  
</feed>


