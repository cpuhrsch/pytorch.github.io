<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-07-09T17:58:00-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Optimizing LibTorch-based inference engine memory usage and thread-pooling</title>
      <link href="https://pytorch.org/blog/optimizing-libtorch/" rel="alternate" type="text/html" title="Optimizing LibTorch-based inference engine memory usage and thread-pooling" />
      <published>2023-06-29T00:00:00-07:00</published>
      <updated>2023-06-29T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/optimizing-libtorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/optimizing-libtorch/">&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;In this blog post we show how to optimize LibTorch-based inference engine to maximize throughput by reducing memory usage and optimizing the thread-pooling strategy. We apply these optimizations to Pattern Recognition engines for audio data, for example, music and speech recognition or acoustic fingerprinting. The optimizations discussed in this blog post allow for memory usage reduction by 50% and reduction in end-to-end latency for Inference by 37.5%. These optimizations are applicable to computer vision and natural language processing.&lt;/p&gt;

&lt;h2 id=&quot;audio-recognition-inferencing&quot;&gt;Audio Recognition Inferencing&lt;/h2&gt;

&lt;p&gt;Audio Recognition (AR) engines can be used to recognize and identify sound patterns. As an example, identifying the type and species of a bird from audio recordings, distinguishing music from the singer’s voice, or detecting an abnormal sound indicating a breach in a building. To identify sounds of interest, AR engines process audio through 4 stages:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;File Validation&lt;/strong&gt;: The AR engine validates the input audio file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feature Extraction&lt;/strong&gt;: Features are extracted from each segment within the audio file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: LibTorch performs inference using CPUs or accelerators. In our case Intel processors on an Elastic Cloud Compute (EC2) instance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Post-processing&lt;/strong&gt;: A post-processing model decodes the results and calculates scores that are used to convert inference output into tags or transcripts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Of these 4 steps, inference is the most computationally intensive and can take up to 50% of the pipeline processing time depending on the model complexity. This means that any optimization at this stage has a significant impact on the overall pipeline. &lt;/p&gt;

&lt;h2 id=&quot;optimizing-the-audio-recognition-engine-with-concurrencyis-not-so-simple&quot;&gt;Optimizing the Audio Recognition engine with concurrency…is not so simple&lt;/h2&gt;

&lt;p&gt;Our objective for this processing pipeline is to extract audio segments into tags or transcripts through a processing. The input data is an audio file composed of several short sound segments (S1 to S6 in Figure 1). The output data corresponds to tags or transcripts ordered by timestamps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im1.jpg&quot; alt=&quot;Figure 1: Example audio file with segment boundaries&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Example audio file with segment boundaries&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Each segment can be processed independently and in an out-of-order fashion. This offers the opportunity to process segments concurrently and in parallel to optimize the overall inference throughput as well as maximize the usage of the resources.&lt;/p&gt;

&lt;p&gt;Parallelization on an instance can be achieved through multi-threading (pThreads, std::threads, OpenMP) or multi-processing. The advantage of multi-threading over multi-processing is the ability to use shared memory. It enables developers to minimize data duplication across threads by sharing data across threads; the AR models in our case (&lt;em&gt;Figure 2&lt;/em&gt;). Furthermore, a reduction in memory allows us to run more pipelines in parallel by increasing the number of engine threads in order to utilize all vCPUs on our Amazon EC2 instance (&lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/c5/&quot;&gt;c5.4xlarge&lt;/a&gt; in our case, it offers 16 vCPUs). In theory, we expect to see higher hardware utilization and higher throughput for our AR engine as a result.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im2.jpg&quot; alt=&quot;Figure 2: Multi-threaded AR Engine&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Multi-threaded AR Engine&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;But we found these assumptions to be wrong. Indeed, we found that increasing the number of threads of the application led to an increase of the end-to-end latency for each audio segment and to a decrease of the engine throughput. For example, increasing the concurrency from 1 to 5 threads led to an increase of the latency by 4x which had a proportional effect on decreasing the throughput. In fact, metrics showed that within the pipeline, the latency of the inference stage alone was 3x higher than it’s single thread baseline. &lt;/p&gt;

&lt;p&gt;Using a profiler, we found that the CPU &lt;a href=&quot;https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/reference/cpu-metrics-reference.html#cpu-metrics-reference_SPIN-AND-OVERHEAD-TIME&quot;&gt;Spin Time&lt;/a&gt; increased, potentially due to CPU oversubscription which impacts system and application performance. Given our control over the application’s multi-thread implementation, we chose to dive deeper into the stack and identify potential conflicts with LibTorch’s default settings.&lt;/p&gt;

&lt;h3 id=&quot;diving-deeper-on-libtorchs-multi-threading-and-its-impact-on-concurrency&quot;&gt;Diving deeper on LibTorch’s multi-threading and its impact on concurrency&lt;/h3&gt;

&lt;p&gt;LibTorch’s parallel implementations on CPU for inference are based on  &lt;a href=&quot;https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#cpu-threading-and-torchscript-inference&quot;&gt;global thread pools&lt;/a&gt;. Examples of implementations are Inter-op and intra-op parallelism, which can be chosen depending on the model’s properties. In both cases, it is possible to set &lt;a href=&quot;https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#tuning-the-number-of-threads&quot;&gt;the number of threads&lt;/a&gt; in each thread-poll to optimize the latency and throughput. &lt;/p&gt;

&lt;p&gt;To test if LibTorch’s parallel default implementation settings had a counter effect on our inference latency, we ran an experiment on a 16 vCPus machine with a 35-minute audio file, keeping the LibTorch inter-threads constant at 1 (because our models didn’t utilize the inter-op thread pool). We collected the following data as shown in Figure 3 and 4. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im3.jpg&quot; alt=&quot;Figure 3: CPU Utilization for different number of engine threads&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: CPU Utilization for different number of engine threads&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im4.jpg&quot; alt=&quot;Figure 4: Processing times for different number of engine threads&quot; style=&quot;max-height:800px; width:100%; margin-top: 4rem;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Processing times for different number of engine threads&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Execution time in Figure 4 is the end-to-end processing time for processing all the segments of the given audio file. We have 4 different configurations of LibTorch intra-threads which are 1, 4, 8, 16 and we change the number of engine threads from 1 to 16 for each intra-thread LibTorch configuration. As we see in Figure 3, CPU utilization increases with an increase in the number of engine threads for all LibTorch intra-thread configurations. But as we see in Figure 4, an increase in CPU utilization doesn’t translate into lower execution time. We found out that in all but one case, as the number of engine threads shot up, so did execution time. The one exception was the case where the intra-thread pool size was 1.&lt;/p&gt;

&lt;h3 id=&quot;resolving-the-global-thread-pool-issue&quot;&gt;Resolving the global thread pool issue&lt;/h3&gt;

&lt;p&gt;Using too many threads with a global thread pool led to performance degradation and caused an over-subscription problem. Without disabling&lt;a href=&quot;https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html&quot;&gt; LibTorch global thread pools&lt;/a&gt;, it was difficult to match the performance of the multi-process engine.&lt;/p&gt;

&lt;p&gt;Disabling the LibTorch global thread pool is as simple as setting the intra-op/inter-op parallelism threads to 1, as shown here:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;at::set_num_threads(1)           // Disables the intraop thread pool.
at::set_num_interop_threads(1). // Disables the interop thread pool.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As shown in Figure 4, the lowest processing time was measured when the LibTorch global thread pool was disabled.&lt;/p&gt;

&lt;p&gt;This solution improved AR engine throughput in several cases. However, when evaluating long datasets (audio files longer than 2 hours in load test), we found that the memory footprint of the engine gradually started to increase.&lt;/p&gt;

&lt;h3 id=&quot;optimizing-memory-usage&quot;&gt;Optimizing memory usage&lt;/h3&gt;

&lt;p&gt;We ran a load-test on the system with two hours long audio files and found out that the observed memory increase was the result of memory fragmentation within a multi-threaded LibTorch inference. We resolved this using&lt;a href=&quot;https://github.com/jemalloc/jemalloc&quot;&gt; jemalloc&lt;/a&gt;, which is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#switch-memory-allocator&quot;&gt;Using jemalloc&lt;/a&gt;, our peak memory usage decreased by an average of 34% and average memory usage decreased by 53%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im5.jpg&quot; alt=&quot;Figure 5: Memory usage over time using the same input file with and without jemalloc&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: Memory usage over time using the same input file with and without jemalloc&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;To optimize the performance of multi-threaded LibTorch-based inference engines, we recommend verifying that there is no oversubscription problem in LibTorch. In our case, all threads in the multi-threaded engine were sharing the LibTorch global thread pool, which caused an oversubscription problem. This was remedied by disabling the global thread pool: we disabled the interop and intraop global thread pool by setting threads to 1. To optimize the memory of a multi-threaded engine, we recommend using Jemalloc as a memory allocator tool rather than the default malloc function.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Himalay Mohanlal Joriwal, Pierre-Yves Aquilanti, Vivek Govindan, Hamid Shojanazeri, Ankith Gunapal, Tristan Rice</name>
        
        
      </author>

      

      

      
        <summary type="html">Outline</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA</title>
      <link href="https://pytorch.org/blog/path-achieve-low-inference-latency/" rel="alternate" type="text/html" title="The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA" />
      <published>2023-06-28T00:00:00-07:00</published>
      <updated>2023-06-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/path-achieve-low-inference-latency</id>
      <content type="html" xml:base="https://pytorch.org/blog/path-achieve-low-inference-latency/">&lt;h2 id=&quot;background--state-of-the-art&quot;&gt;Background &amp;amp; State of the Art&lt;/h2&gt;

&lt;p&gt;In the natural language processing (NLP) space, language models are designed to generate a token (e.g. word) using a sequence of past input tokens. Large Language Models (LLMs) are the latest deep learning innovation in this space built to generate text in a human-like fashion. These models generally use &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;transformers&lt;/a&gt; to improve their attention over a large sequence of input tokens.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;LLaMA&lt;/a&gt;, open sourced by &lt;a href=&quot;https://ai.facebook.com/&quot;&gt;Meta AI&lt;/a&gt;, is a powerful foundation LLM trained on over 1T tokens. LLaMA is competitive with many best-in-class models such as &lt;a href=&quot;https://openai.com/blog/gpt-3-apps&quot;&gt;GPT-3&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2203.15556.pdf&quot;&gt;Chinchilla&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM&lt;/a&gt;. &lt;a href=&quot;https://arxiv.org/pdf/2302.13971.pdf&quot;&gt;LLaMA (13B) outperforms GPT-3 (175B)&lt;/a&gt; highlighting its ability to extract more compute from each model parameter.&lt;/p&gt;

&lt;p&gt;In this blog post, we use LLaMA as an example model to demonstrate the capabilities of PyTorch/XLA for LLM inference. We discuss how the computation techniques and optimizations discussed here improve inference latency by 6.4x on 65B parameter LLaMA models powered by Google Cloud TPU v4 (v4-16).&lt;/p&gt;

&lt;h2 id=&quot;model-overview&quot;&gt;Model Overview&lt;/h2&gt;

&lt;p&gt;We demonstrate the performance capabilities of PyTorch/XLA on &lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;LLaMA&lt;/a&gt;, the latest LLM from Meta. We showcase performance optimizations on a series of common LLaMA configurations. Notice the 175B parameter model configuration is absent in the public domain. For the 175B parameter model mentioned below, we apply &lt;a href=&quot;https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/models/opt/modeling_opt.py#L804&quot;&gt;OPT 175B model configuration&lt;/a&gt; to the LLaMA code base. Unless stated otherwise, in all configurations, we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_len=256&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dtype=bfloat16&lt;/code&gt; for weights and activations.&lt;/p&gt;

&lt;h4 id=&quot;table-1-model-configurations-explored-in-this-article&quot;&gt;Table 1: Model Configurations Explored in this article&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;LLaMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;Model Hyper Parameters&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;# Parameters&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Dimensions&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;N Heads&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;N Layers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Max Seq Len&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;7B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;4,096
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;33B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;6,656
   &lt;/td&gt;
   &lt;td&gt;52
   &lt;/td&gt;
   &lt;td&gt;60
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;65B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;8,192
   &lt;/td&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;80
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;175B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;12,288
   &lt;/td&gt;
   &lt;td&gt;96
   &lt;/td&gt;
   &lt;td&gt;96
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;performance-challenges-of-llms&quot;&gt;Performance Challenges of LLMs&lt;/h2&gt;

&lt;p&gt;LLMs have a few properties that make them challenging for compiler optimizations. (a) LLMs use autoregressive decoding to generate the next token baked on the previous ones; this means prompt tensors and coaches have a dynamic shape. (b) LLMs must work with variable input prompt lengths without triggering recompilation due to input tensor shape changes; input tensors must be properly bucketized and padded to avoid recompilation. (c) LLMs often require more memory than a single TPU (or GPU) device can support. A model-sharding scheme is required to fit the model across a distributed compute architecture. For instance, a LLaMA model with 65B parameters can fit on a v4-16 Cloud TPU, which is comparable to 8 A100 GPUs. (d) running LLMs in production can be expensive; one way to improve performance per total cost of ownership (Perf/TCO) is via quantization; quantization can potentially reduce hardware requirements.&lt;/p&gt;

&lt;h2 id=&quot;inference-tech-stack-in-pytorchxla&quot;&gt;Inference Tech Stack in PyTorch/XLA&lt;/h2&gt;

&lt;p&gt;Our goal is to offer the AI community a high performance inference stack. PyTorch/XLA integrates with &lt;a href=&quot;https://pytorch.org/docs/stable/dynamo/index.html&quot;&gt;TorchDynamo&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla/#pjrt-runtime-beta&quot;&gt;PjRt&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla-path-forward/&quot;&gt;OpenXLA&lt;/a&gt;, and various model parallelism schemes. TorchDynamo eliminates tracing overhead at runtime, PjRt enables efficient host-device communication; PyTorch/XLA traceable collectives enable model and data parallelism on LLaMA via &lt;a href=&quot;https://pytorch.org/docs/stable/dynamo/index.html&quot;&gt;TorchDynamo&lt;/a&gt;. To try our results, please use our custom &lt;a href=&quot;https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly+20230422-cp38-cp38-linux_x86_64.whl&quot;&gt;torch&lt;/a&gt;, &lt;a href=&quot;https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly+20230422-cp38-cp38-linux_x86_64.whl&quot;&gt;torch-xla&lt;/a&gt; wheels to reproduce our &lt;a href=&quot;https://github.com/pytorch-tpu/llama/tree/blog&quot;&gt;LLaMA inference solution&lt;/a&gt;. PyTorch/XLA 2.1 will support the features discussed in this post by default.&lt;/p&gt;

&lt;h2 id=&quot;parallel-computing&quot;&gt;Parallel Computing&lt;/h2&gt;

&lt;h3 id=&quot;fairscale-sharding&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/fairscale&quot;&gt;FairScale&lt;/a&gt; Sharding&lt;/h3&gt;

&lt;p&gt;LLaMA uses FairScale model sharding API (&lt;a href=&quot;https://github.com/facebookresearch/llama/blob/main/llama/model.py#L13-L17&quot;&gt;fairscale.nn.model_parallel.layers&lt;/a&gt;). We built an equivalent representation of this API using PyTorch/XLA communication collective (CC) ops such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all-reduce&lt;/code&gt; to communicate program state (e.g. activations) between accelerators. TorchDynamo does not fully support capturing CC ops currently (a.k.a. &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/93173&quot;&gt;traceable collectives&lt;/a&gt;). Without this support, a TorchDynamo FX graph would be cut at every device communication, meaning at every model layer. Graph cuts lead to performance loss as the underlying XLA compiler loses full graph optimization opportunities. To resolve this, we offer PyTorch/XLA traceable collectives by integrating the dispatcher collectives into our existing CC APIs. The difference is we don’t need to insert &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c10d.wait()&lt;/code&gt; ops after collectives, given the lazy execution nature of PyTorch/XLA. With support for traceable collectives, PyTorch/XLA allows singular FX graph generation in TorchDynamo.&lt;/p&gt;

&lt;h2 id=&quot;autoregressive-decoding-on-pytorchxla&quot;&gt;Autoregressive Decoding on PyTorch/XLA&lt;/h2&gt;

&lt;p&gt;LLMs need autoregressive decoding to feed the previous word as a prompt to predict the next token. Autoregressive decoding leads to unbounded dynamic shape problems, which in turn causes recompilation of every prompt. We optimized the LLaMA autoregressive decoder to operate with fixed shapes that in-place updates the KV-cache, output sequences, and attention masks during every token generation. With a combination of padding, masking, and index ops, we avoided excessive graph recompilation, thereby achieving efficient autoregressive decoding.&lt;/p&gt;

&lt;h3 id=&quot;kv-cache-optimization&quot;&gt;KV-Cache Optimization&lt;/h3&gt;

&lt;p&gt;LLaMA implements autoregressive decoding with KV-cache. For every generated token, the KV-cache stores the attention key/value activations of each Transformer layer. Thus, upon decoding a new token, the key/values of prior tokens no longer need recomputation.&lt;/p&gt;

&lt;p&gt;In LLaMA, the KV-cache tensor slices are updated in-place; this leads to recompilation events every time a token is generated. To address this issue, we use index tensors and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.index_copy()&lt;/code&gt; ops to replace the in-place slice updates. Attention masks and output sequences also benefit from the same optimization.&lt;/p&gt;

&lt;h2 id=&quot;input-prompt-optimization&quot;&gt;Input Prompt Optimization&lt;/h2&gt;

&lt;p&gt;Variable length input prompts are common in LLM applications. This property causes input tensor shape dynamism and in turn recompilation events. When processing a prompt to fill the KV-cache, we either (a) process the input prompt token-by-token, or (b) process the whole prompt in one iteration. The pros and cons of each method are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pre-compile 1 graph and process a prompt token-by-token
    &lt;ul&gt;
      &lt;li&gt;Practical: 1 graph is compiled during warm-up&lt;/li&gt;
      &lt;li&gt;Slow: &lt;em&gt;O(L)&lt;/em&gt; to process an input prompt length &lt;em&gt;L&lt;/em&gt; - a disadvantage for long prompts&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pre-compile all graphs with input lengths ranging from 1 to max_seq_len (e.g. 2,048)
    &lt;ul&gt;
      &lt;li&gt;Impractical: pre-compile and cache &lt;em&gt;max_seq_len&lt;/em&gt; graphs during warm-up time&lt;/li&gt;
      &lt;li&gt;Fast: 1 graph execution to process the full prompt&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We introduce prompt length bucketization, an optimization to strike a balance between the two alternatives. We define a set of ascending bucket sizes, &lt;em&gt;(b&lt;sub&gt;0&lt;/sub&gt;,b&lt;sub&gt;1&lt;/sub&gt;,b&lt;sub&gt;2&lt;/sub&gt;,…,b&lt;sub&gt;B-1&lt;/sub&gt;)&lt;/em&gt;, and then pre-compile program graphs with input sizes according to these bucket values, &lt;em&gt;(G&lt;sub&gt;0&lt;/sub&gt;,G&lt;sub&gt;1&lt;/sub&gt;,G&lt;sub&gt;2&lt;/sub&gt;,…,G&lt;sub&gt;B-1&lt;/sub&gt;)&lt;/em&gt;; &lt;em&gt;B&lt;/em&gt; is the number of buckets. For a given input prompt, we round up the prompt length to the closest bucket value &lt;em&gt;b&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt;, pad the sequence, and use &lt;em&gt;G&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt; to process the prompt in one iteration. The computation on the padding tokens is discarded. For prompts larger than the largest bucket size, we process them section-by-section.&lt;/p&gt;

&lt;p&gt;The optimal bucket sizes should be determined by prompt length distribution in a target application. Here, we adopt bucket lengths: 128, 256, 384, 512. Any input prompt with up to 2,047 tokens requires up to 4 graph executions. For example, a 1,500 input prompt with generation length of 256 requires 260 graph executions - 4 to process the input, and 256 to generate the output.&lt;/p&gt;

&lt;h2 id=&quot;quantization&quot;&gt;Quantization&lt;/h2&gt;

&lt;p&gt;Quantization reduces the number of bits necessary to represent a value; it reduces the bandwidth to communicate data across multiple accelerator nodes (via collectives) and lowers the hardware requirements to serve a specific model size.&lt;/p&gt;

&lt;p&gt;Normally, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BF16&lt;/code&gt; weights, a 175B parameter model would consume about 351GB of memory, and therefore require a v4-32 instance to accommodate the model. By quantizing the weights to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;INT8&lt;/code&gt;, we reduced the model size by roughly 50%, allowing it to run on a smaller v4-16 instance. Because LLaMA shards model activations, quantization offers negligible communication gain.&lt;/p&gt;

&lt;p&gt;In our experiments, we quantized the linear layer. Since LLaMA model checkpoints are unavailable publicly, and our goal is to evaluate performance, the quantized model is initialized with random weights.Recent literature such as &lt;a href=&quot;https://arxiv.org/pdf/2306.00978.pdf&quot;&gt;AWQ&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/2305.12356.pdf&quot;&gt;Integer or Floating Point?&lt;/a&gt; offer insights into performance properties of LLaMA under various low-bit quantization schemes.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-batch-size-on-quantization-performance&quot;&gt;Effect of Batch Size on Quantization Performance&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2304.01433.pdf&quot;&gt;TPU v4&lt;/a&gt; is programmed to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul&lt;/code&gt; on the Matrix Multiply Unit (MXU) when the model batch size (BS) &amp;gt; 1. For BS = 1, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul&lt;/code&gt; runs on the Vector Processor Unit (VPU). Since MXU is more efficient than VPU, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;INT8&lt;/code&gt; quantization gains performance at BS&amp;gt;1. See &lt;a href=&quot;#heading=h.4xqv3t16rl42&quot;&gt;Performance Analysis&lt;/a&gt; section for details.&lt;/p&gt;

&lt;h2 id=&quot;op-support&quot;&gt;Op Support&lt;/h2&gt;

&lt;p&gt;Occasionally, new models introduce new mathematical operations that require PyTorch/XLA to extend its supported op set for compilation. For LLaMA, we supported: &lt;a href=&quot;https://github.com/pytorch/xla/issues/4839&quot;&gt;multinomial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;LLaMA works on PyTorch/XLA out of the box on LazyTensorCore. We use this configuration as a baseline for our follow up analysis. All experiments assume 256-long input prompts. In the absence of a publicly available model checkpoint, we used random tensor initialization for this inference stack optimization effort. A model checkpoint is not expected to change latency results discussed here.&lt;/p&gt;

&lt;h3 id=&quot;model-sizing&quot;&gt;Model Sizing&lt;/h3&gt;

&lt;p&gt;Assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; is the number of parameters, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dimensions&lt;/code&gt; is the hidden size, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_layers&lt;/code&gt; is the number of layers, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_heads&lt;/code&gt; is the number of attention heads, the equation below can be used to approximate the model size. See the &lt;a href=&quot;#heading=h.tehlvi942ssk&quot;&gt;Model Overview&lt;/a&gt; section for details.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N = (dimensions)^2 * n_layers * 12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_heads&lt;/code&gt; doesn’t affect &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;, but the following equation holds for the open sourced model configs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dim = 128 * n_heads
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;cache-sizing&quot;&gt;Cache Sizing&lt;/h4&gt;

&lt;p&gt;Both model parameters and the cache layers in the Attention block contribute to memory consumption. Since the default LLaMA model uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BF16&lt;/code&gt; weights, the memory consumption calculation in this section is based on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BF16&lt;/code&gt; weights.&lt;/p&gt;

&lt;p&gt;The size of the cache layer is calculated by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cache_size = max_batch_size * max_seq_len * dimensions&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_batch_size = 1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_len = 256 &lt;/code&gt;are used as an example configuration in the following calculations. There are 2 cache layers in each Attention block. So, the total LLaMA cache size (in Bytes) is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;total_cache_size = n_layers * 2 * cache_size * (2 bytes)&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;tpu-v4-hardware-sizing&quot;&gt;TPU v4 Hardware Sizing&lt;/h4&gt;

&lt;p&gt;Each TPU v4 chip has 32GB of available High-Bandwidth Memory (HBM). Table 2 has the details on memory consumption and the number of required TPU chips to hold a LLaMA model.&lt;/p&gt;

&lt;h4 id=&quot;table-2-llama-tpu-v4-hbm-requirements-ie-tpu-v4-chip-requirements&quot;&gt;Table 2: LLaMA TPU v4 HBM requirements (i.e. TPU v4 chip requirements)&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;# Parameters&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Parameter (MB)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Cache (MB)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Total (GB)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Min # of TPU v4 Chips&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;7B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;14,000
   &lt;/td&gt;
   &lt;td&gt;134
   &lt;/td&gt;
   &lt;td&gt;14.128
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;33B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;66,000
   &lt;/td&gt;
   &lt;td&gt;408
   &lt;/td&gt;
   &lt;td&gt;66.41
   &lt;/td&gt;
   &lt;td&gt;3
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;65B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;130,000
   &lt;/td&gt;
   &lt;td&gt;671
   &lt;/td&gt;
   &lt;td&gt;130.67
   &lt;/td&gt;
   &lt;td&gt;5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;175B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;350,000
   &lt;/td&gt;
   &lt;td&gt;1,208
   &lt;/td&gt;
   &lt;td&gt;351.21
   &lt;/td&gt;
   &lt;td&gt;11
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;Below are useful metrics to measure inference speed. Assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;/code&gt; is the total time, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is the batch size, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; is the decoded sequence length.&lt;/p&gt;

&lt;h4 id=&quot;latency-definition&quot;&gt;Latency Definition&lt;/h4&gt;

&lt;p&gt;Latency is the time it takes to get the decoded result at target length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt;, regardless of the batch size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt;. Latency represents how long the user should wait to get the response from the generation model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Latency = T (s)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;per-token-latency&quot;&gt;Per-token latency&lt;/h4&gt;

&lt;p&gt;One step of autoregressive decoding generates a token for each sample in the batch. Per-token latency is the average time for that one step.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Per-token latency = T / L (s/token)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;throughput&quot;&gt;Throughput&lt;/h4&gt;

&lt;p&gt;Throughput measures how many tokens are generated per unit time. While it’s not a useful metric for evaluating online serving it is useful to measure the speed of batch processing.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Throughput = B * L / T (tokens/s)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To minimize confusion and misinterpretation, it’s better to avoid metrics like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T / (B * L)&lt;/code&gt;, which mixes latency and throughput.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;Figure 1 shows latency / token results for LLaMA 7B to 175B models. In each case, the model is run on a range of TPU v4 configurations. For instance, LLaMA 7B shows 4.7ms/token and 3.8ms/token on v4-8 and v4-16 respectively. For more comparison, visit the HuggingFace &lt;a href=&quot;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&quot;&gt;LLM performance leaderboard&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the absence of the features discussed in this blog post, the LLaMA 65B running on v4-32 delivers 120ms/token instead of 14.5ms/token obtained here, leading to &lt;strong&gt;8.3x&lt;/strong&gt; speedup. As discussed earlier, developers are encouraged to try our custom &lt;a href=&quot;https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly+20230422-cp38-cp38-linux_x86_64.whl&quot;&gt;torch&lt;/a&gt;, &lt;a href=&quot;https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly+20230422-cp38-cp38-linux_x86_64.whl&quot;&gt;torch-xla&lt;/a&gt; wheels that unlock the repro of &lt;a href=&quot;https://github.com/pytorch-tpu/llama/tree/blog&quot;&gt;LLaMA inference&lt;/a&gt; results shared here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im1.svg&quot; alt=&quot;Figure 1: LLaMA Inference Performance on TPU v4 hardware&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: LLaMA Inference Performance on TPU v4 hardware&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;PyTorch/XLA:GPU performance is better than PyTorch:GPU eager and similar to PyTorch Inductor. PyTorch/XLA:TPU performance is superior to PyTorch/XLA:GPU. In the near future, XLA:GPU will deliver optimizations that bring parity with XLA:TPU. The single A100 configuration only fits LLaMA 7B, and the 8-A100 doesn’t fit LLaMA 175B.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im2.svg&quot; alt=&quot;Figure 2: LLaMA Inference Performance on GPU A100 hardware&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: LLaMA Inference Performance on GPU A100 hardware&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;As the batch size increases, we observe a sublinear increase in per-token latency highlighting the tradeoff between hardware utilization and latency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im3.svg&quot; alt=&quot;Figure 3: LLaMA Inference Performance across different batch sizes&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: LLaMA Inference Performance across different batch sizes&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Our studies suggest the impact of maximum sequence input length (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_len&lt;/code&gt;) on inference latency is relatively minimal. We attribute this to the sequential and iterative nature of token generation. The small difference in performance can be due to KV cache access latency changes as the storage size increases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im4.svg&quot; alt=&quot;Figure 4: LLaMA Inference Performance across different prompt lengths&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: LLaMA Inference Performance across different prompt lengths&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;LLMs are often memory bound applications; thus, by quantizing model parameters we enable loading and executing a larger tensor on MXUs per unit time (i.e. HBM ⇒ CMEM and CMEM ⇒ MXU data moevment). Figure 5 shows &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;INT8&lt;/code&gt; weight-only quantization offers 1.6x-1.9x speedup allowing running a larger model on a given hardware.&lt;/p&gt;

&lt;p&gt;When BS=1, INT8 tensors are dispatched to VPU which is smaller than MXU (see the &lt;a href=&quot;https://arxiv.org/pdf/2304.01433.pdf&quot;&gt;TPU v4 paper&lt;/a&gt;); otherwise, MXU is used. As a result, when BS=1, quantization memory bandwidth gains are offset by lack of MXU utilization. When BS&amp;gt;1, however, memory gains deliver superior latency on the quantized model. For example, in the case of 175B parameters LLaMA, v4-16 with quantiztion and v4-32 without quantiztion deliver similar performance. Note we do not provied &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FP8&lt;/code&gt; comparisons because PyTorch is yet to offer this data type.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im5.svg&quot; alt=&quot;Figure 5: LLaMA Inference Performance vs. weight-only quantization. The missing blue bars suggest the model size doesn’t fit in the specified TPU hardware.&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: LLaMA Inference Performance vs. weight-only quantization. The missing blue bars suggest the model size doesn’t fit in the specified TPU hardware.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Figure 6 demonstrates the steady performance advantage of PyTorch/XLA as the input prompt length grows from 10 tokens to 1,500 tokens. This strong scaling capability suggests minimal PyTorch/XLA recompilation events enabling a wide range of real-world applications. In this experiment, the maximum length is 2,048 and maximum generation length is 256.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im6.svg&quot; alt=&quot;Figure 6: LLaMA Inference Performance vs. Input Prompt Length&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: LLaMA Inference Performance vs. Input Prompt Length&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; so that we can openly collaborate. You can also &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;try out&lt;/a&gt; PyTorch/XLA for yourself on various XLA devices including TPUs and GPUs.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br /&gt;
The PyTorch/XLA Team at Google&lt;br /&gt;
#PoweredByPyTorch&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Milad Mohammadi, Jiewen Tan, Liyang Lu, Siyuan Liu, Yeounoh Chung,  Wonjoo Lee, Manfei Bai, Steven Krawczyk, Shauheen Zahirazami, Alex Wertheim, Meghan Cowan, Jack Cao,  Joe Spisak</name>
        
        
      </author>

      

      

      
        <summary type="html">Background &amp;amp; State of the Art</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Optimized PyTorch 2.0 Inference with AWS Graviton processors</title>
      <link href="https://pytorch.org/blog/optimized-pytorch-w-graviton/" rel="alternate" type="text/html" title="Optimized PyTorch 2.0 Inference with AWS Graviton processors" />
      <published>2023-06-22T00:00:00-07:00</published>
      <updated>2023-06-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/optimized-pytorch-w-graviton</id>
      <content type="html" xml:base="https://pytorch.org/blog/optimized-pytorch-w-graviton/">&lt;p&gt;New generations of CPUs offer significant performance improvement in machine learning (ML) inference due to specialized built-in instructions. Combined with their flexibility, high speed of development, and low operating cost, these general-purpose processors offer an alternative ML inference solution to other existing hardware solutions.&lt;/p&gt;

&lt;p&gt;AWS, Arm, Meta, and others helped optimize the performance of PyTorch 2.0 inference for Arm-based processors. As a result, we are delighted to announce that Arm-based AWS Graviton instance inference performance for PyTorch 2.0 is up to 3.5 times the speed for ResNet-50 compared to the previous PyTorch release, and up to 1.4 times the speed for BERT, making Graviton-based instances the fastest compute optimized instances on AWS for these models (see the following graph).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im1.png&quot; alt=&quot;Relative speed improvement achieved by upgrading PyTorch to 2.0&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Relative speed improvement achieved by upgrading from PyTorch version 1.13 to 2.0 (higher is better). The performance is measured on c7g.4xlarge instances.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;As shown in the next graph, we measured up to 50% cost savings for PyTorch inference with Graviton3-based c7g instances across Torch Hub ResNet-50 and multiple Hugging Face models compared to comparable x86-based compute optimized Amazon EC2 instances. For that graph, we first measured the cost per million inference for the five instance types. Then, we normalized the cost per million inference results to a c5.4xlarge instance, which is the baseline measure of “1” on the Y-axis of the chart.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im2.png&quot; alt=&quot;Relative cost of PyTorch inference running on different AWS instances&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Relative cost of PyTorch inference running on different AWS instances (lower is better). &lt;br /&gt;Source: AWS ML Blog on &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/optimized-pytorch-2-0-inference-with-aws-graviton-processors/&quot;&gt;Graviton PyTorch2.0 inference performance&lt;/a&gt;.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Similar to the preceding inference cost comparison graph, the following graph shows the model p90 latency for the same five instance types. We normalized the latency results to the c5.4xlarge instance, which is the baseline measure of “1” on the Y-axis of the chart. The c7g.4xlarge (AWS Graviton3) model inference latency is up to 50% better than the latencies measured on c5.4xlarge, c6i.4xlarge, and c6a.4xlarge. \&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im3.png&quot; alt=&quot;Relative latency (p90) of PyTorch inference running on different AWS instances&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: Relative latency (p90) of PyTorch inference running on different AWS instances (lower is better). &lt;br /&gt;Source: AWS ML Blog on &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/optimized-pytorch-2-0-inference-with-aws-graviton-processors/&quot;&gt;Graviton PyTorch2.0 inference performance&lt;/a&gt;.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimization-details&quot;&gt;Optimization details&lt;/h2&gt;

&lt;p&gt;PyTorch supports Compute Library for the Arm® Architecture (ACL) GEMM kernels via the oneDNN backend (previously called “MKL-DNN”) for AArch64 platforms. The optimizations are primarily for PyTorch ATen CPU BLAS, ACL kernels for fp32 and bfloat16, and oneDNN primitive caching. There are no frontend API changes, so no changes are required at the application level to get these optimizations working on Graviton3-based instances.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-level-optimizations&quot;&gt;PyTorch level optimizations&lt;/h3&gt;

&lt;p&gt;We extended the ATen CPU BLAS interface to accelerate more operators and tensor configurations via oneDNN backend for aarch64 platform. The following diagram highlights (in orange) the optimized components that improved the PyTorch inference performance on aarch64 platform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im4.png&quot; alt=&quot;PyTorch software stack highlighting (in orange) the components optimized for inference performance improvement on AArch64 platform&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 4&lt;/strong&gt;: PyTorch software stack highlighting (in orange) the components optimized for inference performance improvement on AArch64 platform&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;acl-kernels-and-bfloat16-fpmath-mode&quot;&gt;ACL kernels and BFloat16 FPmath mode&lt;/h3&gt;

&lt;p&gt;The ACL library provides Neon and SVE optimized GEMM kernels for both fp32 and bfloat16 formats: These kernels improve the SIMD hardware utilization and reduce the end to end inference latencies. The bfloat16 support in Graviton3 allows efficient deployment of models trained using bfloat16, fp32 and Automatic Mixed Precision (AMP). The standard fp32 models use bfloat16 kernels via oneDNN FPmath mode without model quantization. They provide up to two times faster performance compared to existing fp32 model inference without bfloat16 FPmath support. For more details on ACL GEMM kernel support, refer to &lt;a href=&quot;https://github.com/ARM-software/ComputeLibrary&quot;&gt;Arm Compute Library github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;primitive-caching&quot;&gt;Primitive Caching&lt;/h3&gt;

&lt;p&gt;The following call sequence diagram shows how ACL operators are integrated into oneDNN backend. As shown in the diagram, ACL objects are handled as oneDNN resources instead of the primitive objects. This is because the ACL objects are stateful and mutable. Since the ACL objects are handled as resource objects, they are not cacheable with the default primitive caching feature supported in oneDNN. We implemented primitive caching at ideep operator level for “convolution”, “matmul” and “inner product” operators to avoid redundant GEMM kernel initialization and tensor allocation overhead.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im5.png&quot; alt=&quot;Call sequence diagram showing how the Compute Library for the Arm® Architecture (ACL) GEMM kernels are integrated into oneDNN backend&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 5&lt;/strong&gt;: Call sequence diagram showing how the Compute Library for the Arm® Architecture (ACL) GEMM kernels are integrated into oneDNN backend&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-take-advantage-of-the-optimizations&quot;&gt;How to take advantage of the optimizations&lt;/h2&gt;

&lt;p&gt;Install the PyTorch 2.0 wheel from the official repo and set environment variables to enable the additional optimizations.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Install Python
sudo apt-get update
sudo apt-get install -y python3 python3-pip

# Upgrade pip3 to the latest version
python3 -m pip install --upgrade pip

# Install PyTorch and extensions
python3 -m pip install torch
python3 -m pip install torchvision torchaudio torchtext

# Turn on Graviton3 optimization
export DNNL_DEFAULT_FPMATH_MODE=BF16
export LRU_CACHE_CAPACITY=1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;running-an-inference&quot;&gt;Running an inference&lt;/h2&gt;

&lt;p&gt;You can use PyTorch &lt;a href=&quot;https://github.com/pytorch/benchmark&quot;&gt;torchbench&lt;/a&gt; to measure the CPU inference performance improvements, or to compare different instance types.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Pre-requisite:
# pip install PyTorch2.0 wheels and set the above mentioned environment variables

# Clone PyTorch benchmark repo
git clone https://github.com/pytorch/benchmark.git

# Setup ResNet-50 benchmark
cd benchmark
python3 install.py resnet50

# Install the dependent wheels
python3 -m pip install numba

# Run ResNet-50 inference in jit mode. On successful completion of the inference runs,
# the script prints the inference latency and accuracy results
python3 run.py resnet50 -d cpu -m jit -t eval --use_cosine_similarity
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;performance-analysis&quot;&gt;Performance Analysis&lt;/h2&gt;

&lt;p&gt;Now, we will analyze the inference performance of ResNet-50 on Graviton3-based c7g instance using PyTorch profiler. We run the code below with PyTorch 1.13 and PyTorch 2.0 and run the inference for a few iterations as a warmup before measuring the performance.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Turn on Graviton3 optimization
export DNNL_DEFAULT_FPMATH_MODE=BF16
export LRU_CACHE_CAPACITY=1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torchvision import models
sample_input = [torch.rand(1, 3, 224, 224)]
eager_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
model = torch.jit.script(eager_model, example_inputs=[sample_input, ])

model = model.eval()
model = torch.jit.optimize_for_inference(model)

with torch.no_grad():
    # warmup runs
    for i in range(10):
        model(*sample_input)
    prof = torch.profiler.profile(
      on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'), record_shapes=True, with_stack=True)
    # profile after warmup
    prof.start()
    model(*sample_input)
    prof.stop()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We use tensorboard to view results of the profiler and analyze model performance.&lt;/p&gt;

&lt;p&gt;Install PyTorch Profiler Tensorboard plugin as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install torch_tb_profiler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Launch the tensorboard using&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensorboard --logdir=./logs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Launch the following in the browser to view the profiler output. The profiler supports ‘Overview’, ‘Operator’, ‘Trace’ and ‘Module’ views to get insight into the inference execution.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://localhost:6006/#pytorch_profiler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following diagram is the profiler ‘Trace’ view which shows the call stack along with the execution time of each function. In the profiler, we selected the forward() function to get the overall inference time. As shown in the diagram, the inference time for the ResNet-50 model on Graviton3-based c7g instance is around 3 times faster in PyTorch 2.0 compared to PyTorch 1.13.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im6.png&quot; alt=&quot;Profiler Trace view: Forward pass wall duration on PyTorch 1.13 and PyTorch 2.0&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 6&lt;/strong&gt;: Profiler Trace view: Forward pass wall duration on PyTorch 1.13 and PyTorch 2.0&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The next diagram is the ‘Operator’ view which shows the list of PyTorch operators and their execution time. Similar to the preceding Trace view, the Operator view shows that the operator host duration for the ResNet-50 model on Graviton3-based c7g instance is around 3 times faster in PyTorch 2.0 compared to PyTorch 1.13.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im7.png&quot; alt=&quot;Profiler Operator view: Forward operator Host duration on PyTorch 1.13 and PyTorch 2.0&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 7&lt;/strong&gt;: Profiler Operator view: Forward operator Host duration on PyTorch 1.13 and PyTorch 2.0&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-hugging-face-models&quot;&gt;Benchmarking Hugging Face models&lt;/h2&gt;

&lt;p&gt;You can use the&lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html&quot;&gt; Amazon SageMaker Inference Recommender&lt;/a&gt; utility to automate performance benchmarking across different instances. With Inference Recommender, you can find the real-time inference endpoint that delivers the best performance at the lowest cost for a given ML model. We collected the preceding data using the Inference Recommender notebooks by deploying the models on production endpoints. For more details on Inference Recommender, refer to the&lt;a href=&quot;https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-inference-recommender/huggingface-inference-recommender/huggingface-inference-recommender.ipynb&quot;&gt; amazon-sagemaker-examples&lt;/a&gt; GitHub repo. We benchmarked the following models for this post:&lt;a href=&quot;https://pytorch.org/hub/pytorch_vision_resnet/&quot;&gt; ResNet50 image classification&lt;/a&gt;,&lt;a href=&quot;https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english&quot;&gt; DistilBERT sentiment analysis&lt;/a&gt;,&lt;a href=&quot;https://huggingface.co/roberta-base&quot;&gt; RoBERTa fill mask&lt;/a&gt;, and&lt;a href=&quot;https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment&quot;&gt; RoBERTa sentiment analysis&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;For PyTorch 2.0, the Graviton3-based C7g instance is the most cost-effective compute optimized Amazon EC2 instance for inference. These instances are available on&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-adds-new-graviton-based-instances-model-deployment/&quot;&gt; SageMaker&lt;/a&gt; and&lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/c7g/&quot;&gt; Amazon EC2&lt;/a&gt;. The&lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt; AWS Graviton Technical Guide&lt;/a&gt; provides the list of optimized libraries and best practices that will help you achieve cost benefit with Graviton instances across different workloads.&lt;/p&gt;

&lt;p&gt;If you find use cases where similar performance gains are not observed on Graviton, please open an issue on the &lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt;aws-graviton-getting-started&lt;/a&gt; github to let us know about it. We will continue to add more performance improvements to make AWS Graviton-based instances the most cost-effective and efficient general purpose processor for inference using PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;We would like to thank Ali Saidi (Sr. Principal Engineer) and Csaba Csoma (Sr. Manager, Software Development) from AWS, Ashok Bhat (Sr. Product Manager), Nathan Sircombe (Sr. Engineering Manager) and Milos Puzovic (Principal Software Engineer) from Arm for their support during the Graviton PyTorch inference optimization work. We would also like to thank Geeta Chauhan (Engineering Leader, Applied AI) from Meta for her guidance on this blog.&lt;/p&gt;

&lt;h2 id=&quot;about-the-authors&quot;&gt;About the authors&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Sunita Nadampalli&lt;/strong&gt; is a ML Engineer and Software Development Manager at AWS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ankith Gunapal&lt;/strong&gt; is an AI Partner Engineer at Meta(PyTorch).&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sunita Nadampalli from AWS &amp; Ankith Gunapal from Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">New generations of CPUs offer significant performance improvement in machine learning (ML) inference due to specialized built-in instructions. Combined with their flexibility, high speed of development, and low operating cost, these general-purpose processors offer an alternative ML inference solution to other existing hardware solutions.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">🎉 PyTorch Docathon H1 2023 Wrap-up 🎉</title>
      <link href="https://pytorch.org/blog/docathon-h1-2023-wrap-up/" rel="alternate" type="text/html" title="🎉 PyTorch Docathon H1 2023 Wrap-up 🎉" />
      <published>2023-06-16T00:00:00-07:00</published>
      <updated>2023-06-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/docathon-h1-2023-wrap-up</id>
      <content type="html" xml:base="https://pytorch.org/blog/docathon-h1-2023-wrap-up/">&lt;p&gt;Thank you to all who participated in our first ever PyTorch Docathon, the results have been nothing short of amazing! We want to extend our sincerest gratitude to all the participants who made this event a resounding success. Your passion, talent, and hard work have left an indelible mark on the PyTorch documentation.&lt;/p&gt;

&lt;p&gt;The virtual Docathon ran from May 31 through June 15 with more than 230 registrants and more than 110 participants joining the Docathon Slack channel, the energy and enthusiasm were palpable. Entrants were judged on the difficulty of submissions that resulted in over 40 merged pull requests and the publication of four new tutorials and addition of one new example.&lt;/p&gt;

&lt;p&gt;We want to give a special shout-out to our top contributors, who went above and beyond during this event. Your dedication and expertise have been invaluable in enhancing the PyTorch documentation and empowering developers worldwide. See the full list of contributors &lt;a href=&quot;https://github.com/pytorch/tutorials/blob/main/docathon-leaderboard.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Meet the top contributors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First place: &lt;a href=&quot;https://github.com/JoseLuisC99&quot;&gt;JoseLuisC99&lt;/a&gt;, &lt;a href=&quot;https://github.com/QasimKhan5x&quot;&gt;QasimKhan5x&lt;/a&gt;, &lt;a href=&quot;https://github.com/bjhargrave&quot;&gt;bjhargrave&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Second place: &lt;a href=&quot;https://github.com/Aidyn-A&quot;&gt;Aidyn-A&lt;/a&gt;, &lt;a href=&quot;https://github.com/CaoE&quot;&gt;CaoE&lt;/a&gt;, &lt;a href=&quot;https://github.com/HemanthSai7&quot;&gt;HemanthSai7&lt;/a&gt;, &lt;a href=&quot;https://github.com/leslie-fang-intel&quot;&gt;leslie-fang-intel&lt;/a&gt;, 	&lt;a href=&quot;https://github.com/Valentine233&quot;&gt;Valentine233&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Third place: &lt;a href=&quot;https://github.com/TheMemoryDealer&quot;&gt;TheMemoryDealer&lt;/a&gt;, &lt;a href=&quot;https://github.com/arunppsg&quot;&gt;arunppsg&lt;/a&gt;, &lt;a href=&quot;https://github.com/noqqaqq&quot;&gt;noqqaqq&lt;/a&gt;, &lt;a href=&quot;https://github.com/zabboud&quot;&gt;zabboud&lt;/a&gt;, &lt;a href=&quot;https://github.com/kiersten-stokes&quot;&gt;kiersten-stokes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Honorable mentions: &lt;a href=&quot;https://github.com/frasertajima&quot;&gt;frasertajima&lt;/a&gt;, &lt;a href=&quot;https://github.com/nairbv&quot;&gt;nairbv&lt;/a&gt;, &lt;a href=&quot;https://github.com/mikebrow&quot;&gt;mikebrow&lt;/a&gt;, &lt;a href=&quot;https://github.com/NeoKish&quot;&gt;NeoKish&lt;/a&gt;, &lt;a href=&quot;https://github.com/fabiogomez11c&quot;&gt;fabiogomez11c&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we bring this Docathon to a close, we encourage each and every one of you to stay inspired and keep  contributing to PyTorch &lt;a href=&quot;https://github.com/pytorch/tutorials#contributing&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md&quot;&gt;code&lt;/a&gt;, and pushing the boundaries of what’s possible with PyTorch. Your collective efforts are shaping the landscape of deep learning and fostering innovation in the AI community.&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Thank you to all who participated in our first ever PyTorch Docathon, the results have been nothing short of amazing! We want to extend our sincerest gratitude to all the participants who made this event a resounding success. Your passion, talent, and hard work have left an indelible mark on the PyTorch documentation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Join the PyTorch Foundation: Membership Now Open</title>
      <link href="https://pytorch.org/blog/join-pytorch/" rel="alternate" type="text/html" title="Join the PyTorch Foundation: Membership Now Open" />
      <published>2023-06-07T00:00:00-07:00</published>
      <updated>2023-06-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/join-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/join-pytorch/">&lt;p&gt;In September 2022, we welcomed PyTorch to the Linux Foundation from Meta, which formed the PyTorch Foundation with founding members AMD, Amazon Web Services (AWS), Google, Meta, Microsoft, and NVIDIA.&lt;/p&gt;

&lt;p&gt;Since then, &lt;a href=&quot;https://www.linuxfoundation.org/blog/pytorch-foundation-the-first-six-months&quot;&gt;we’ve seen significant growth&lt;/a&gt;, including a &lt;strong&gt;39% increase in commits&lt;/strong&gt; across all repositories, &lt;strong&gt;27% increase of unique contributors&lt;/strong&gt;, and a &lt;strong&gt;12% increase community contributions&lt;/strong&gt; – all in the last 90 days! We’re grateful to our founding members for their support to move the foundation forward.&lt;/p&gt;

&lt;p&gt;Today, we’re announcing that &lt;strong&gt;membership is now open to join the PyTorch Foundation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As a member of the PyTorch Foundation, you’ll have access to resources that allow you to be stewards of stable, secure, and long-lasting codebases. You can collaborate on training and certification programs, local and regional events, open source developer tooling, academic research, and guides to help new users and contributors have a productive experience.&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation’s goal is to help end users navigate the PyTorch ecosystem, recruit talent, and adopt PyTorch and support open source AI technologies successfully.&lt;/p&gt;

&lt;h2 id=&quot;why-join-as-a-member&quot;&gt;Why join as a member&lt;/h2&gt;

&lt;p&gt;Being a part of the PyTorch Foundation grants opportunities to help build the future of end-to-end machine learning frameworks alongside your industry peers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Membership benefits include:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gain technical traction and insight for your organization’s products by immersing your teams with other industry leaders.&lt;/li&gt;
  &lt;li&gt;Influence technical priorities, approaches, and code.&lt;/li&gt;
  &lt;li&gt;Support the PyTorch project community by helping fund programs and services that the project and its community rely on.&lt;/li&gt;
  &lt;li&gt;Engage with the PyTorch project ecosystem, network with fellow members, and contribute to building and maintaining an engaging and strong PyTorch ecosystem.&lt;/li&gt;
  &lt;li&gt;Provide thought leadership and participate in unique, wide-reaching networking and marketing programs expanding industry awareness as PyTorch amplifies member progress.&lt;/li&gt;
  &lt;li&gt;Retain, attract, and increase engineering skills and employees and build your innovation partner network, supply chain, and customer pipeline.&lt;/li&gt;
  &lt;li&gt;As an active member of the PyTorch community, you can deepen your engagement and leadership in local and industry developer networks and conferences.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-join&quot;&gt;How to join&lt;/h2&gt;

&lt;p&gt;Premier members &lt;a href=&quot;https://docs.google.com/forms/d/1JVzFIaFu-El5Ug0IlzpHKwPbZLe9MvaAUXl0FZgnNQw/edit&quot;&gt;must submit an application &lt;/a&gt;to be considered for board level membership. General and associate members are welcome to &lt;a href=&quot;https://enrollment.lfx.linuxfoundation.org/?project=pytorch&quot;&gt;join automatically&lt;/a&gt;. See below for specific tiering and details on each type of membership.&lt;/p&gt;

&lt;h3 id=&quot;premier-members&quot;&gt;Premier Members&lt;/h3&gt;

&lt;p&gt;Premier members are the highest tier. They will appoint one voting representative in any subcommittees or activities of the PTF Governing Board, and receive prominent placement in displays of membership including website, landscape and marketing materials, exclusive live webinars with PyTorch online programs and everything included within a “general” membership. The annual fee is $150,000 + an LF Silver Membership.&lt;/p&gt;

&lt;h3 id=&quot;general-members&quot;&gt;General Members&lt;/h3&gt;

&lt;p&gt;General members will participate in all marketing, community and thought leadership opportunities, as well as discounts on event sponsorships and training courses. General members also have the opportunity to be considered for a PTF board position. The annual fee is dependent on the size of your organization. More details can be found &lt;a href=&quot;http://pytorch.org/join&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;associate-members&quot;&gt;Associate Members&lt;/h3&gt;

&lt;p&gt;Associate members are free to join and will receive support and participation opportunities with the PyTorch Foundation team. More information can be found &lt;a href=&quot;http://pytorch.org/join&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;hear-from-our-founding-members&quot;&gt;Hear from our founding members&lt;/h2&gt;

&lt;h3 id=&quot;amd&quot;&gt;AMD&lt;/h3&gt;

&lt;p&gt;“AMD strongly believes in and supports an open software ecosystem. We are very proud to be a founding member of the PyTorch Foundation, helping to develop an open and collaborative community for AI and ML. AI and ML have the opportunity to impact everything we do, and the work done through the PyTorch Foundation is critical in developing an open framework that is vendor neutral and helps democratize AI for all.”&lt;/p&gt;

&lt;h3 id=&quot;aws&quot;&gt;AWS&lt;/h3&gt;

&lt;p&gt;“AWS is a firm believer in the PyTorch Foundation mission to develop AI and deep learning tools through open collaboration. Our customers use PyTorch every day to build, train, and deploy machine learning models on AWS. Through our involvement, AWS is supporting innovation and helping to make open source tooling more accessible to our customers and the broader community.”&lt;/p&gt;

&lt;h3 id=&quot;google&quot;&gt;Google&lt;/h3&gt;

&lt;p&gt;“The AI revolution is upon us and it’s being built on PyTorch. With new applications like ChatGPT and Stable Diffusion built on PyTorch, the wave of generative AI continues to be felt across every facet of society. We at Google are excited to be a founding member of the PyTorch Foundation and we’re excited for the opportunity to work closely with other leaders in AI to help grow this amazing and innovative community.”&lt;/p&gt;

&lt;h3 id=&quot;meta&quot;&gt;Meta&lt;/h3&gt;

&lt;p&gt;“Meta has a long history of putting open science at the core of our work in AI and PyTorch is no exception. PyTorch was built from the ground up with an open source, community-first philosophy. We transitioned PyTorch to the PyTorch Foundation because we believe this approach enables the fastest progress in building and deploying new systems that will address real-world needs and answer fundamental questions about the nature of intelligence. With the PyTorch Foundation, the entire AI community is positioned to push the field forward in countless exciting new ways.”&lt;/p&gt;

&lt;h3 id=&quot;microsoft&quot;&gt;Microsoft&lt;/h3&gt;

&lt;p&gt;“Microsoft believes strongly in PyTorch and it’s been an honor to be a founding member of the PyTorch Foundation. Internally, we use PyTorch extensively, and an outgrowth of that is the Azure Container for PyTorch, which provides deep optimization for PyTorch development, including ONNX Runtime, DeepSpeed, and Nebula to greatly reduce training cost and accelerate training times on Azure Machine Learning. As part of our ongoing commitment to open source machine learning platforms, we look forward to partnering with industry leaders to continue contributing to the advancement of PyTorch.”&lt;/p&gt;

&lt;h3 id=&quot;nvidia&quot;&gt;NVIDIA&lt;/h3&gt;

&lt;p&gt;“As a leading Python-based AI framework, &lt;a href=&quot;https://www.nvidia.com/en-us/glossary/data-science/pytorch/&quot;&gt;PyTorch&lt;/a&gt; has been fundamental to the development of LLMs and GenAI. NVIDIA’s goal is to deepen our collaboration with the open-source AI community as part of the PyTorch Foundation, and help build the next wave of advanced, energy efficient, and cost-effective applications with &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/solutions/accelerated-computing/&quot;&gt;accelerated computing&lt;/a&gt;.”&lt;/p&gt;

&lt;h2 id=&quot;join-today&quot;&gt;Join today&lt;/h2&gt;

&lt;p&gt;We are excited to see the PyTorch Foundation continue to grow alongside the community through neutral governance and support. We hope you’ll &lt;a href=&quot;/join&quot;&gt;join us as a member&lt;/a&gt;!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">In September 2022, we welcomed PyTorch to the Linux Foundation from Meta, which formed the PyTorch Foundation with founding members AMD, Amazon Web Services (AWS), Google, Meta, Microsoft, and NVIDIA.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Out of the box acceleration and memory savings of 🤗 decoder models with PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/out-of-the-box-acceleration/" rel="alternate" type="text/html" title="Out of the box acceleration and memory savings of 🤗 decoder models with PyTorch 2.0" />
      <published>2023-05-22T00:00:00-07:00</published>
      <updated>2023-05-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/out-of-the-box-acceleration</id>
      <content type="html" xml:base="https://pytorch.org/blog/out-of-the-box-acceleration/">&lt;p&gt;As part of PyTorch 2.0 release, an accelerated implementation of the attention mechanism as part of the “Better Transformer” project (and known in PyTorch as Accelerated Transformers) has been added natively into PyTorch as &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product_attention#torch.nn.functional.scaled_dot_product_attention&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;&lt;/a&gt;. This implementation leverages fused kernels from &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2112.05682&quot;&gt;Memory-efficient attention&lt;/a&gt;, and supports both training and inference.&lt;/p&gt;

&lt;p&gt;We also release a notebook showcasing an example of this integration &lt;a href=&quot;https://colab.research.google.com/drive/1_zuAiiBFoFWpexxeWsTS694tCSlMYydo?usp=sharing&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After seeing &lt;a href=&quot;https://pytorch.org/blog/accelerated-diffusers-pt-20/&quot;&gt;20-30% speedups at inference for diffusion models&lt;/a&gt;, we went ahead and implemented an integration with 🤗 Transformers models through the &lt;a href=&quot;https://huggingface.co/docs/optimum/main/en/bettertransformer/overview&quot;&gt;🤗 Optimum library&lt;/a&gt;. Similar to &lt;a href=&quot;https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2&quot;&gt;the previous integration for encoder models&lt;/a&gt;, the integration replaces modules from Transformers with efficient implementations that use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;. The usage is as follow:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from optimum.bettertransformer import BetterTransformer
from transformers import AutoModelForCausalLM

with torch.device(“cuda”):
model = AutoModelForCausalLM.from_pretrained(“gpt2-large”, torch_dtype=torch.float16)

model = BetterTransformer.transform(model)

# do your inference or training here

# if training and want to save the model
model = BetterTransformer.reverse(model)
model.save_pretrained(“fine_tuned_model”)
model.push_to_hub(“fine_tuned_model”) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Summarizing our findings below about &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is most useful to fit larger models, sequence length, or batch size to train on a given hardware.&lt;/li&gt;
  &lt;li&gt;Memory footprint savings on GPU during training range from 20% to 110%+.&lt;/li&gt;
  &lt;li&gt;Speedups during training range from 10% to 70%.&lt;/li&gt;
  &lt;li&gt;Speedups during inference range from 5% to 20%.&lt;/li&gt;
  &lt;li&gt;Standalone, for small head dimensions, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; speedups go up to 3x, memory savings go as high as 40x (depending on the sequence length).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may be surprised by the wide range of memory savings and speedups. In this blog post, we discuss our benchmarks, where this feature shines and upcoming improvements in future PyTorch releases.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the next release of transformers you will just need to install the proper version of optimum and run:&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = model.to_bettertransformer()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;To convert your model using the BetterTransformer API. You can already try this feature out by installing transformers from source.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmark-and-usage-with--transformers&quot;&gt;Benchmark and usage with 🤗 Transformers&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt; is usable with any architecture that uses standard attention, and namely replaces the boiler-plate code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# native scaled_dot_product_attention is equivalent to the following:
def eager_sdpa(query, key, value, attn_mask, dropout_p, is_causal, scale):
	scale_factor = 1 / math.sqrt(Q.size(-1)) if scale is None else scale
	attn_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0) if is_causal else attn_mask
	attn_mask = attn_mask.masked_fill(not attn_mask, -float('inf')) if attn_mask.dtype==torch.bool else attn_mask
	attn_weight = torch.softmax((Q @ K.transpose(-2, -1) * scale_factor) + attn_mask, dim=-1)
	attn_weight = torch.dropout(attn_weight, dropout_p)
	return attn_weight @ V
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the 🤗 Optimum integration with Transformers models, the following architectures are supported for now: gpt2, gpt-neo, gpt-neox, gptj, t5, bart, codegen, pegasus, opt, LLaMA, blenderbot, m2m100. You can expect this list to be extended in the near future!&lt;/p&gt;

&lt;p&gt;To validate the benefits from the native scaled dot-product attention, we ran inference and training benchmarks, whose results are presented below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig1.jpg&quot; alt=&quot;Inference benchmark on a single A10G GPU, AWS g5.4xlarge instance&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Inference benchmark on a single A10G GPU, AWS g5.4xlarge instance&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig2.jpg&quot; alt=&quot;Training benchmark on a single A10G GPU, AWS g5.4xlarge instance&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Training benchmark on a single A10G GPU, AWS g5.4xlarge instance&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig3.jpg&quot; alt=&quot;Training benchmark on a single A100-SXM4-80GB, Nvidia DGX&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Training benchmark on a single A100-SXM4-80GB, Nvidia DGX&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;Out of this benchmark, the most interesting finding is that native SDPA allows for the usage of longer sequence lengths and batch sizes without running into out of memory issues. Moreover, up to 20% speedups can be seen during inference, and even larger during training.&lt;/p&gt;

&lt;p&gt;As seen on the training benchmarks, it appears that smaller head dimension brings higher speedups and memory savings, which we will discuss in the following section.&lt;/p&gt;

&lt;p&gt;The implementation supports multi-GPU settings as well, thanks to 🤗 Accelerate library by passing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device_map=”auto”&lt;/code&gt; to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from_pretrained&lt;/code&gt; method. Here are some results for training on two A100-SXM4-80GB.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig4.jpg&quot; alt=&quot;Training benchmark on two A100-SXM4-80GB, Nvidia DGX, using 🤗 Accelerate library for distributed training&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Training benchmark on two A100-SXM4-80GB, Nvidia DGX, using 🤗 Accelerate library for distributed training&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;Note that some kernels support only the sm_80 compute capability (which is the one from A100 GPUs), which limits usability on a wide range of hardware, notably if the head dimension is not a power of two. For example, as of PyTorch 2.0.0 during training, opt-2.7b (headim=80) and gpt-neox-20b (headdim=96) can not dispatch to a kernel using flash attention, unless run on an A100 GPU. Better kernels may be developed in the future: https://github.com/pytorch/pytorch/issues/98140#issuecomment-1518101895&lt;/p&gt;

&lt;h2 id=&quot;flash-attention-memory-efficient-attention--math-differences&quot;&gt;Flash Attention, Memory-efficient attention &amp;amp; math differences&lt;/h2&gt;

&lt;p&gt;The native &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; relies on three possible backend implementations: flash attention, memory-efficient attention, and the so-called math implementation which provides a hardware-neutral fallback for all PyTorch platforms.&lt;/p&gt;

&lt;p&gt;When fused kernels are available for a given problem size, flash-attention or memory-efficient attention will be used, effectively allowing for a lower memory footprint, as in the memory-efficient attention case O(N) memory allocations are done on the GPU global memory instead of the classic O(N^2) for the traditional eager attention implementation. With flash attention, a reduced number of memory accesses (read and writes) is expected, hence both giving speedups and memory savings.&lt;/p&gt;

&lt;p&gt;The “math” implementation is simply an &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/aten/src/ATen/native/transformers/attention.cpp#L812-L868&quot;&gt;implementation using the PyTorch’s C++ API&lt;/a&gt;. Interesting to note in this implementation is that the query and key tensors are scaled individually for numerical stability, thus launching two aten::div operations instead of possibly only one in an eager implementation that does not contain this optimization for numerical stability.&lt;/p&gt;

&lt;h3 id=&quot;head-dimension-influence-on-speedups-memory-savings&quot;&gt;Head dimension influence on speedups, memory savings&lt;/h3&gt;

&lt;p&gt;Benchmarking &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;, we notice a decrease in the speedup / memory gains as the head dimension increases. This is an issue for some architectures like EleutherAI/gpt-neo-2.7B, that has a relatively large head dimension of 128, or EleutherAI/gpt-j-6B (and derived models as PygmalionAI/pygmalion-6b) that has a head dimension of 256 (that actually currently do not dispatch on fused kernels as the head dimension is too large).&lt;/p&gt;

&lt;p&gt;This trend can be seen in the figures below, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.scaled_dot_production&lt;/code&gt; is benchmarked standalone versus the above eager implementation. Moreover, we use the &lt;a href=&quot;https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.backends.cuda.sdp_kernel&lt;/code&gt;&lt;/a&gt; context manager to force the usage of respectively math, flash attention, and memory-efficient attention implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig5.jpg&quot; alt=&quot;Using memory-efficient attention SDP kernel (forward-only), A100&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Using memory-efficient attention SDP kernel (forward-only), A100&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig6.jpg&quot; alt=&quot;Using math (without dropout), A100&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Using math (without dropout), A100&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig7.jpg&quot; alt=&quot;Using flash attention SDP kernel (without dropout), A100&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Using flash attention SDP kernel (without dropout), A100&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig8.jpg&quot; alt=&quot;Using memory-efficient attention SDP kernel (without dropout), A100&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Using memory-efficient attention SDP kernel (without dropout), A100&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;We see that for the same problem size, be it for inference-only or training, the speedup decreases with higher head dimension, e.g. from 3.4x for headdim=8 to 1.01x for headdim=128 using flash attention kernel.&lt;/p&gt;

&lt;p&gt;The reduced memory saving is expected with larger head dimensions. Recall the standard attention computation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig9.jpg&quot; alt=&quot;Math equation&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Due to the intermediate computations, the global memory footprint is 2 * N * N + N * d in this standard step by step computation. Memory-efficient attention proposes to iteratively update the softmax renormalization constant and moving its computation at the very end, allowing for only a constant output memory allocation N * d.&lt;/p&gt;

&lt;p&gt;Thus, the memory saving ratio is 2 * N / d + 1, which decreases with larger head dimension.&lt;/p&gt;

&lt;p&gt;In flash attention, the tradeoff is between the head dimension d and the shared memory size M of a GPU streaming multiprocessor, with a total number of memory accesses of O(N² * d²/M). Thus, the memory accesses scale quadratically in the head dimension, contrary to the standard attention that scales linearly. The reason is that in flash attention, for larger head dimension d, the key and value K, V need to be split into more blocks to fit into shared memory, and in turn each block needs to load the full query Q and output O.&lt;/p&gt;

&lt;p&gt;Thus, the highest speedups for flash attention are in a regime where the ratio d² / M is small enough.&lt;/p&gt;

&lt;h2 id=&quot;current-limitations-as-of-pytorch-200&quot;&gt;Current limitations as of PyTorch 2.0.0&lt;/h2&gt;

&lt;h3 id=&quot;absence-of-a-scale-argument&quot;&gt;Absence of a scale argument&lt;/h3&gt;

&lt;p&gt;As of PyTorch 2.0.0, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt; has no scale argument and uses the default square root of the hidden size sqrt(d_k).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig10.jpg&quot; alt=&quot;Math equation&quot; style=&quot;max-height:800px; width:100%; max-width: 400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, some architectures as OPT or T5 do not use a scaling in the attention, which as of Pytorch 2.0.0 forces it to artificially rescale before the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; call. This introduces an unnecessary overhead, as an additional multiplication is necessary, on top of unneeded divisions in the attention.&lt;/p&gt;

&lt;p&gt;A fix for this issue has been merged &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/95259&quot;&gt;in PyTorch repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;support-of-flash-attention--memory-efficient-attention-with-custom-mask&quot;&gt;Support of flash attention / memory-efficient attention with custom mask&lt;/h3&gt;

&lt;p&gt;As of PyTorch 2.0.0, when passing a custom attention mask, flash attention and memory-efficient attention can not be used. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; automatically dispatches to the C++ implementation.&lt;/p&gt;

&lt;p&gt;However, as we have seen, some architectures require a custom attention mask, as T5 that uses positional bias. Moreover, in the case of a batch size larger than one where some inputs may be padded, a custom attention mask also needs to be passed. For this latter case, an alternative would be to use &lt;a href=&quot;https://pytorch.org/docs/stable/nested.html&quot;&gt;NestedTensor&lt;/a&gt;, which SDPA supports.&lt;/p&gt;

&lt;p&gt;This limited support for custom masks thus limits the benefits from SDPA in these specific cases, although we can hope for an extended support &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/96099#issuecomment-1458609375&quot;&gt;in the future&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that xformers, from which PyTorch’s SDPA partially takes inspiration, currently supports arbitrary attention masks: https://github.com/facebookresearch/xformers/blob/658ebab39545f180a6075385b3897921623d6c3b/xformers/ops/fmha/cutlass.py#L147-L156 . HazyResearch implementation of flash attention also supports an equivalent implementation of padding, as a cumulative sequence length array is used along with packed query/key/values - similar in essence to NestedTensor.&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In conclusion&lt;/h2&gt;

&lt;p&gt;Using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt; is a free-lunch optimization, both making your code more readable, uses less memory, and is in most common cases faster.&lt;/p&gt;

&lt;p&gt;Although the implementation in PyTorch 2.0.0 has still minor limitations, inference and training already massively benefit from SDPA in most cases. We encourage you to use this native implementation be it to train or deploy your PyTorch models, and for 🤗 Transformers models as a one-line transformation!&lt;/p&gt;

&lt;p&gt;In the future, we would like to adapt the API to enable users to use SDPA in encoder-based models as well.&lt;/p&gt;

&lt;p&gt;We thank Benjamin Lefaudeux, Daniel Haziza and Francisco Massa for their advice on the head dimension influence, as well as Michael Gschwind, Christian Puhrsch and Driss Guessous for their feedback on the blog post!&lt;/p&gt;

&lt;h2 id=&quot;benchmark-reproduction&quot;&gt;Benchmark reproduction&lt;/h2&gt;

&lt;p&gt;The benchmark presented in this post was done using torch==2.0.0, transformers==4.27.4, accelerate==0.18.0 and optimum==1.8.0.&lt;/p&gt;

&lt;p&gt;The benchmarks can be easily reproduced using the scripts for &lt;a href=&quot;https://github.com/huggingface/optimum/blob/main/tests/benchmark/benchmark_bettertransformer.py&quot;&gt;inference&lt;/a&gt;, &lt;a href=&quot;https://github.com/huggingface/optimum/blob/main/tests/benchmark/benchmark_bettertransformer_training_minimal.py&quot;&gt;training&lt;/a&gt; for 🤗 Transformers models, and &lt;a href=&quot;https://github.com/fxmarty/efficient-attention-benchmark&quot;&gt;standalone SDPA&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Felix Marty, Younes Belkada, Hamid Shojanazeri, Driss Guessous</name>
        
        
      </author>

      

      

      
        <summary type="html">As part of PyTorch 2.0 release, an accelerated implementation of the attention mechanism as part of the “Better Transformer” project (and known in PyTorch as Accelerated Transformers) has been added natively into PyTorch as torch.nn.functional.scaled_dot_product_attention. This implementation leverages fused kernels from FlashAttention and Memory-efficient attention, and supports both training and inference.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Conference 2023: Join us in San Francisco October 16-17</title>
      <link href="https://pytorch.org/blog/pytorch-conference-2023/" rel="alternate" type="text/html" title="PyTorch Conference 2023: Join us in San Francisco October 16-17" />
      <published>2023-05-16T00:00:00-07:00</published>
      <updated>2023-05-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-conference-2023</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-conference-2023/">&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conf-2023.png&quot; alt=&quot;PyTorch Conference 2023&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re thrilled to announce the upcoming &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference 2023&lt;/a&gt;! On October 16-17, the conference will showcase PyTorch 2.0, the next-generation release of the popular machine learning framework. As part of the Linux Foundation, the PyTorch Foundation Conference continues the tradition of bringing together leading researchers, developers, and academic communities to advance the education and development of end-to-end machine learning.&lt;/p&gt;

&lt;p&gt;The conference agenda features an engaging lineup of events, including an opening reception, engaging community and partner discussions, informative panels, poster sessions, enlightening use cases and community stories, as well as discussions on the latest trends in machine learning and deep learning development and deployment.&lt;/p&gt;

&lt;h2 id=&quot;call-for-proposals&quot;&gt;Call for Proposals&lt;/h2&gt;

&lt;p&gt;We are now accepting speaker proposals for the conference until &lt;strong&gt;July 21&lt;/strong&gt;. The program committee will carefully review all submissions, and selected speakers will be notified by &lt;strong&gt;August 8&lt;/strong&gt;. We strongly encourage both experienced and first-time speakers to submit their proposals. This conference provides an excellent opportunity to connect with the PyTorch community, share your ideas, and showcase your work.&lt;/p&gt;

&lt;p&gt;When preparing your proposal, please consider the following guidelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are you hoping to get from your presentation?&lt;/li&gt;
  &lt;li&gt;What do you expect the audience to gain from your presentation?&lt;/li&gt;
  &lt;li&gt;How will your presentation help better the open source ecosystem?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To help you shape your proposal, here are some suggested topics for the conference:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deployments on AWS, Azure&lt;/li&gt;
  &lt;li&gt;Use cases and real-world applications&lt;/li&gt;
  &lt;li&gt;Foundational models&lt;/li&gt;
  &lt;li&gt;AI practices&lt;/li&gt;
  &lt;li&gt;Production considerations&lt;/li&gt;
  &lt;li&gt;PyTorch 2.X features and updates&lt;/li&gt;
  &lt;li&gt;Training techniques and best practices&lt;/li&gt;
  &lt;li&gt;Inference methodologies&lt;/li&gt;
  &lt;li&gt;Hardware advancements and optimizations&lt;/li&gt;
  &lt;li&gt;Edge computing applications&lt;/li&gt;
  &lt;li&gt;Scalability solutions&lt;/li&gt;
  &lt;li&gt;Latest research breakthroughs&lt;/li&gt;
  &lt;li&gt;Optimization strategies&lt;/li&gt;
  &lt;li&gt;Extending PyTorch through customizations and plugins&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We kindly request that you refrain from submitting sales or marketing pitches and avoid discussing unlicensed or closed-source technologies. Such talks tend to detract from the integrity of our events and are not well-received by conference attendees.&lt;/p&gt;

&lt;h2 id=&quot;register-today&quot;&gt;Register Today&lt;/h2&gt;

&lt;p&gt;Registration is now open! Get your ticket today and secure your spot: &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/register/&quot;&gt;https://events.linuxfoundation.org/pytorch-conference/register/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you for your interest, and we look forward to a successful PyTorch Conference 2023!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Language Identification: Building an End-to-End AI Solution using PyTorch</title>
      <link href="https://pytorch.org/blog/language-identification/" rel="alternate" type="text/html" title="Language Identification: Building an End-to-End AI Solution using PyTorch" />
      <published>2023-05-12T00:00:00-07:00</published>
      <updated>2023-05-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/language-identification</id>
      <content type="html" xml:base="https://pytorch.org/blog/language-identification/">&lt;p&gt;Language Identification is the process of identifying the primary language from multiple audio input samples. In natural language processing (NLP), language identification is an important problem and a challenging issue. There are many language-related tasks such as entering text on your phone, finding news articles you enjoy, or discovering answers to questions that you may have. All these tasks are powered by NLP models. To decide which model to invoke at a particular point in time, we must perform language identification.&lt;/p&gt;

&lt;p&gt;This article presents an in-depth solution and code sample for language identification using &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt;, which is a version of the popular PyTorch AI framework optimized for use on Intel® processors, and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&quot;&gt;Intel® Neural Compressor&lt;/a&gt;, which is a tool to accelerate AI inference without sacrificing accuracy.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification&quot;&gt;code sample&lt;/a&gt; demonstrates how to train a model to perform language identification using the Hugging Face SpeechBrain* toolkit and optimize it using the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit-download.html&quot;&gt;Intel® AI Analytics Toolkit (AI Kit)&lt;/a&gt;. The user can modify the code sample and identify up to 93 languages using the Common Voice dataset.&lt;/p&gt;

&lt;h2 id=&quot;proposed-methodology-for-language-identification&quot;&gt;Proposed Methodology for Language Identification&lt;/h2&gt;

&lt;p&gt;In the proposed solution, the user will use an Intel AI Analytics Toolkit container environment to train a model and perform inference leveraging Intel-optimized libraries for PyTorch. There is also an option to quantize the trained model with Intel Neural Compressor to speed up inference.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;http://commonvoice.mozilla.org/en/datasets&quot;&gt;Common Voice&lt;/a&gt; dataset is used and for this code sample, specifically, Common Voice Corpus 11.0 for Japanese and Swedish. This dataset is used to train an &lt;a href=&quot;http://arxiv.org/abs/2005.07143&quot;&gt;Emphasized Channel Attention, Propagation and Aggregation Time Delay Neural Network (ECAPA-TDNN)&lt;/a&gt;, which is implemented using the &lt;a href=&quot;http://huggingface.co/SpeechBrain&quot;&gt;Hugging Face SpeechBrain&lt;/a&gt; library. Time Delay Neural Networks (TDNNs), aka one-dimensional Convolutional Neural Networks (1D CNNs), are multilayer artificial neural network architectures to classify patterns with shift-invariance and model context at each layer of the network. ECAPA-TDNN is a new TDNN-based speaker-embedding extractor for speaker verification; it is built upon the original x-vector architecture and puts more emphasis on channel attention, propagation, and aggregation.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;After downloading the Common Voice dataset, the data is preprocessed by converting the MP3 files into WAV format to avoid information loss and separated into training, validation, and testing sets.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;http://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa&quot;&gt;pretrained VoxLingua107 &lt;/a&gt;model is retrained with the Common Voice dataset using the Hugging Face SpeechBrain library to focus on the languages of interest. &lt;a href=&quot;http://bark.phon.ioc.ee/voxlingua107/&quot;&gt;VoxLingua107&lt;/a&gt; is a speech dataset used for training spoken language recognition models that work well with real-world and varying speech data. This dataset contains data for 107 languages. By default, Japanese and Swedish are used, and more languages can be included. This model is then used for inference on the testing dataset or a user-specified dataset. Also, there is an option to utilize SpeechBrain’s Voice Activity Detection (VAD) where only the speech segments from the audio files are extracted and combined before samples are randomly selected as input into the model. This &lt;a href=&quot;http://huggingface.co/speechbrain/vad-crdnn-libriparty&quot;&gt;link&lt;/a&gt; provides all the necessary tools to perform VAD. To improve performance, the user may quantize the trained model to integer-8 (INT8) using Intel Neural Compressor to decrease latency.&lt;/p&gt;

&lt;h4 id=&quot;training&quot;&gt;Training&lt;/h4&gt;

&lt;p&gt;The copies of training scripts are added to the current working directory, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_wds_shards.py&lt;/code&gt; - for creating the &lt;a href=&quot;http://github.com/webdataset/webdataset&quot;&gt;WebDataset&lt;/a&gt; shards, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt; - to perform the actual training procedure, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_ecapa.yaml&lt;/code&gt; - to configure the training options. The script to create WebDataset shards and YAML file are patched to work with the two languages chosen for this code sample.&lt;/p&gt;

&lt;p&gt;In the data preprocessing phase, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepareAllCommonVoice.py&lt;/code&gt; script is executed to randomly select a specified number of samples to convert the input from MP3 to WAV format. Here, 80% of these samples will be used for training, 10% for validation, and 10% for testing. At least 2000 samples are recommended as the number of input samples and is the default value.&lt;/p&gt;

&lt;p&gt;In the next step, WebDataset shards are created from the training and validation datasets. This stores the audio files as tar files which allows writing purely sequential I/O pipelines for large-scale deep learning in order to achieve high I/O rates from local storage—about 3x-10x faster compared to random access.&lt;/p&gt;

&lt;p&gt;The YAML file will be modified by the user. This includes setting the value for the largest number for the WebDataset shards, output neurons to the number of languages of interest, number of epochs to train over the entire dataset, and the batch size. The batch size should be decreased if the CPU or GPU runs out of memory while running the training script.&lt;/p&gt;

&lt;p&gt;In this code sample, the training script will be executed with CPU. While running the script, “cpu” will be passed as an input parameter. The configurations defined in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_ecapa.yaml&lt;/code&gt; are also passed as parameters.&lt;/p&gt;

&lt;p&gt;The command to run the script to train the model is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python train.py train_ecapa.yaml --device &quot;cpu&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the future, the training script train.py will be designed to work for Intel® GPUs such as the Intel® Data Center GPU Flex Series, Intel® Data Center GPU Max Series, and Intel® Arc™ A-Series with updates from Intel Extension for PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification#train-the-model-with-languages&quot;&gt;Run the training script&lt;/a&gt; to learn how to train the models and execute the training script. The 4th Generation Intel® Xeon® Scalable Processor is recommended for this &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/transfer-learning.html&quot;&gt;transfer learning&lt;/a&gt; application because of its performance improvements through its Intel® Advanced Matrix Extensions (Intel® AMX) instruction set.&lt;/p&gt;

&lt;p&gt;After training, checkpoint files are available. These files are used to load the model for inference.&lt;/p&gt;

&lt;h4 id=&quot;inference&quot;&gt;Inference&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-inference-pipeline-language-identification.png&quot; alt=&quot;Inference Pipeline&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The crucial step before running inference is to patch the SpeechBrain library’s pretrained &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interfaces.py&lt;/code&gt; file so that PyTorch TorchScript* can be run to improve the runtime. TorchScript requires the output of the model to be only tensors.&lt;/p&gt;

&lt;p&gt;Users can choose to run inference using the testing set from Common Voice or their own custom data in WAV format. The following are the options the inference scripts (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inference_custom.py and inference_commonVoice.py&lt;/code&gt;) can be run with:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Input Option&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Description&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;-p
   &lt;/td&gt;
   &lt;td&gt;Specify the data path.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;-d
   &lt;/td&gt;
   &lt;td&gt;Specify the duration of wave sample. The default value is &lt;strong&gt;3&lt;/strong&gt;.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;-s
   &lt;/td&gt;
   &lt;td&gt;Specify size of sample waves, default is &lt;strong&gt;100&lt;/strong&gt;.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--vad
   &lt;/td&gt;
   &lt;td&gt;(`inference_custom.py` only) Enable VAD model to detect active speech. The VAD option will identify speech segments in the audio file and construct a new &lt;strong&gt;.wav&lt;/strong&gt; file containing only the speech segments. This improves the quality of speech data used as input into the language identification model.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--ipex
   &lt;/td&gt;
   &lt;td&gt;Run inference with optimizations from Intel Extension for PyTorch. This option will apply optimizations to the pretrained model. Using this option should result in performance improvements related to latency.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--ground_truth_compare
   &lt;/td&gt;
   &lt;td&gt;(`inference_custom.py` only) Enable comparison of prediction labels to ground truth values.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--verbose
   &lt;/td&gt;
   &lt;td&gt;Print additional debug information, like latency.
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The path to the data must be specified. By default, 100 audio samples of 3-seconds will be randomly selected from the original audio file and used as input to the language identification model.&lt;/p&gt;

&lt;p&gt;A small Convolutional Recurrent Deep Neural Network (CRDNN) pretrained on the &lt;a href=&quot;http://drive.google.com/file/d/1--cAS5ePojMwNY5fewioXAv9YlYAWzIJ/view&quot;&gt;LibriParty&lt;/a&gt; dataset is used to process audio samples and output the segments where speech activity is detected. This can be used in inference with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--vad&lt;/code&gt; option.&lt;/p&gt;

&lt;p&gt;From the figure below, the timestamps where speech will be detected is delivered from the CRDNN model, and these are used to construct a new, shorter audio file with only speech. Sampling from this new audio file will give a better prediction of the primary language spoken.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-timestamps-delivered-from-crdnn-model.png&quot; alt=&quot;Audio wave file visualization&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification#run-inference&quot;&gt;Run the inference script&lt;/a&gt; yourself. An example command of running inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python inference_custom.py -p data_custom -d 3 -s 50 --vad
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will run inference on data you provide located inside the &lt;em&gt;data_custom&lt;/em&gt; folder. This command performs inference on 50 randomly selected 3-second audio samples with voice activity detection.&lt;/p&gt;

&lt;p&gt;If you want to run the code sample for other languages, download Common Voice Corpus 11.0 datasets for other languages.&lt;/p&gt;

&lt;h2 id=&quot;optimizations-with-intel-extension-for-pytorch-and-intel-neural-compressor&quot;&gt;Optimizations with Intel Extension for PyTorch and Intel Neural Compressor&lt;/h2&gt;

&lt;h3 id=&quot;pytorch&quot;&gt;PyTorch&lt;/h3&gt;

&lt;p&gt;The Intel extension expands PyTorch with up-to-date features and optimizations for an extra performance boost on Intel hardware. Check out &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch#installation&quot;&gt;how to install Intel Extension for PyTorch&lt;/a&gt;. The extension can be loaded as a Python module or linked as a C++ library. Python users can enable it dynamically by importing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel_extension_for_pytorch&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/cpu/latest/&quot;&gt;CPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel CPUs. Source code is available at the &lt;a href=&quot;https://github.com/intel/intel-extension-for-pytorch/tree/master&quot;&gt;master branch&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/xpu/latest/&quot;&gt;GPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel GPUs. Source code is available at the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/xpu-master&quot;&gt;xpu-master branch&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To optimize the model for inference using Intel Extension for PyTorch, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--ipex&lt;/code&gt;option can be passed in. The model is optimized using the plug-in. TorchScript speeds up inference because PyTorch is run in graph mode. The command to run with this optimization is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python inference_custom.py -p data_custom -d 3 -s 50 --vad --ipex --verbose
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--verbose&lt;/code&gt; option is required to view the latency measurements.&lt;/p&gt;

&lt;p&gt;Auto-mixed precision such as bfloat16 (BF16) support will be added in a future release of the code sample.&lt;/p&gt;

&lt;h3 id=&quot;intel-neural-compressor&quot;&gt;Intel Neural Compressor&lt;/h3&gt;

&lt;p&gt;This is an open-source Python library that runs on CPUs or GPUs, which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Performs model quantization to reduce the model size and increase the speed of deep learning inference for deployment.&lt;/li&gt;
  &lt;li&gt;Automates popular methods such as quantization, compression, pruning, and knowledge distillation across multiple deep-learning frameworks.&lt;/li&gt;
  &lt;li&gt;Is part of the AI Kit&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model can be quantized from float32 (FP32) precision to integer-8 (INT8) by running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_model.py&lt;/code&gt; script while passing in the path to the model and a validation dataset. The following code can be used to load this INT8 model for inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from neural_compressor.utils.pytorch import load
model_int8 = load(&quot;./lang_id_commonvoice_model_INT8&quot;, self.language_id)
signal = self.language_id.load_audio(data_path)
prediction = self.model_int8(signal)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the original model is required when loading the quantized model. The command to quantize the trained model from FP32 to INT8 by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_model.py&lt;/code&gt; is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python quantize_model.py -p ./lang_id_commonvoice_model -datapath $COMMON_VOICE_PATH/commonVoiceData/commonVoice/dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Try out the above code sample by upgrading the hardware to a 4th Generation Intel Xeon Scalable Processor with Intel AMX and identify up to 93 different languages from Common Voice datasets.&lt;/p&gt;

&lt;p&gt;We encourage you to learn more about and incorporate Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;AI/ML Framework optimizations&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;end-to-end portfolio of tools&lt;/a&gt; into your AI workflow. Also, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI &amp;amp; ML page&lt;/a&gt; covering Intel’s AI software development resources for preparing, building, deploying, and scaling your AI solutions.&lt;/p&gt;

&lt;p&gt;For more details about the new 4th Gen Intel Xeon Scalable processors, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;Intel’s AI Solution Platform portal&lt;/a&gt; where you can learn how Intel is empowering developers to run end-to-end AI pipelines on these powerful CPUs.&lt;/p&gt;

&lt;h3 id=&quot;useful-resources&quot;&gt;Useful resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;Intel AI Developer Tools and resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI unified programming model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-tensorflow.html&quot;&gt;Official documentation - Intel® Optimization for TensorFlow*&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&quot;&gt;Official documentation - Intel® Neural Compressor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/ai-solution-brief.html&quot;&gt;Accelerate AI Workloads with Intel® AMX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;explore-more-ai-code-samples&quot;&gt;Explore more AI code samples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;Optimize PyTorch Models using Intel® Extension for PyTorch (IPEX) Quantization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPyTorch_TrainingOptimizations_AMX_BF16&quot;&gt;PyTorch Training Optimizations with Advanced Matrix Extensions Bfloat16&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Getting-Started-Samples/INC-Sample-for-Tensorflow&quot;&gt;Intel® Neural Compressor TensorFlow* Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/code-samples.html&quot; class=&quot;btn btn-lg with-right-arrow&quot; data-cta=&quot;get-started&quot;&gt;See all code samples&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Language Identification is the process of identifying the primary language from multiple audio input samples. In natural language processing (NLP), language identification is an important problem and a challenging issue. There are many language-related tasks such as entering text on your phone, finding news articles you enjoy, or discovering answers to questions that you may have. All these tasks are powered by NLP models. To decide which model to invoke at a particular point in time, we must perform language identification.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing PyTorch Docathon 2023</title>
      <link href="https://pytorch.org/blog/announcing-docathon/" rel="alternate" type="text/html" title="Announcing PyTorch Docathon 2023" />
      <published>2023-05-03T00:00:00-07:00</published>
      <updated>2023-05-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/announcing-docathon</id>
      <content type="html" xml:base="https://pytorch.org/blog/announcing-docathon/">&lt;p&gt;&lt;img src=&quot;/assets/images/docathon-cover.jpg&quot; alt=&quot;PyTorch Docathon&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are excited to announce the first ever PyTorch Docathon! The Docathon is a hackathon-style event focused on improving the documentation by enlisting the help of the community. Documentation is a crucial aspect of any technology and by improving the documentation, we can make it easier for users to get started with PyTorch, help them understand how to use its features effectively, and ultimately accelerate research to production in the field of machine learning.&lt;/p&gt;

&lt;h2 id=&quot;why-participate&quot;&gt;WHY PARTICIPATE&lt;/h2&gt;

&lt;h3 id=&quot;low-barrier-to-entry&quot;&gt;Low Barrier to Entry&lt;/h3&gt;

&lt;p&gt;Many open-source projects require extensive knowledge of the codebase and prior contributions to the project to participate in any sort of hackathon events. The Docathon, on the other hand, is designed for newcomers. We do expect familiarity with Python, basic knowledge of PyTorch, and ML. But don’t fret, there are some tasks that are related to website issues that won’t require even that.&lt;/p&gt;

&lt;h3 id=&quot;tangible-results&quot;&gt;Tangible Results&lt;/h3&gt;

&lt;p&gt;One of the best things about the Docathon is that you can see the results of your efforts in real time. Improving documentation can have a huge impact on a project’s usability and accessibility and you’ll be able to see those improvements firsthand. Plus having tangible results can be a great motivator to keep contributing.&lt;/p&gt;

&lt;h3 id=&quot;collaborative-environment&quot;&gt;Collaborative Environment&lt;/h3&gt;

&lt;p&gt;The Docathon is a collaborative event which means you’ll have the opportunity to work with other contributors and PyTorch maintainers on improving the documentation. This can be a great way to learn from others, share ideas, and build connections.&lt;/p&gt;

&lt;h3 id=&quot;learning-opportunities&quot;&gt;Learning Opportunities&lt;/h3&gt;

&lt;p&gt;Finally, even if you are not an expert in PyTorch, the Docathon can be a great learning experience. You’ll have the opportunity to explore the PyTorch modules and test some of the tutorials on your machine as well as in the CI.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;EVENT DETAILS&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;May 31&lt;/strong&gt;: Kick-off&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;May 31 - June 11&lt;/strong&gt;:  Submissions and Feedback&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 12 - June 13&lt;/strong&gt;: Final Reviews&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 15&lt;/strong&gt;: Winner Announcements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Details for the Docathon to be announced at the kick-off stream on May 31.&lt;/p&gt;

&lt;p&gt;Please register to join this year’s event: &lt;a href=&quot;https://community.linuxfoundation.org/e/mmbqqb/&quot;&gt;&lt;strong&gt;RSVP&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Image Segmentation using PyTorch</title>
      <link href="https://pytorch.org/blog/accelerated-image-seg/" rel="alternate" type="text/html" title="Accelerated Image Segmentation using PyTorch" />
      <published>2023-05-02T00:00:00-07:00</published>
      <updated>2023-05-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-image-seg</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-image-seg/">&lt;p&gt;&lt;em&gt;Using Intel® Extension for PyTorch to Boost Image Processing Performance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;PyTorch delivers great CPU performance, and it can be further accelerated with Intel® Extension for PyTorch. I trained an AI image segmentation model using PyTorch 1.13.1 (with ResNet34 + UNet architecture) to identify roads and speed limits from satellite images, all on the 4th Gen Intel® Xeon® Scalable processor.&lt;/p&gt;

&lt;p&gt;I will walk you through the steps to work with a satellite image dataset called SpaceNet5 and how I optimized the code to make deep learning workloads feasible on CPUs just by flipping a few key switches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Before we get started, some housekeeping…&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The code accompanying this article is available in the examples folder in the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/master/examples/cpu/usecase_spacenet5&quot;&gt;Intel Extension for PyTorch repository&lt;/a&gt;. I borrowed heavily from the &lt;a href=&quot;http://github.com/avanetten/cresi/&quot;&gt;City-Scale Road Extraction from Satellite Imagery (CRESI) repository&lt;/a&gt;. I adapted it for the 4th Gen Intel Xeon processors with PyTorch optimizations and &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel Extension for PyTorch&lt;/a&gt; optimizations. In particular, I was able to piece together a workflow using the &lt;a href=&quot;http://github.com/avanetten/cresi/tree/main/notebooks&quot;&gt;notebooks here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find the accompanying talk I gave &lt;a href=&quot;http://www.youtube.com/watch?v=LVZWm5GFvAw&quot;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also highly recommend these articles for a detailed explanation of how to get started with the SpaceNet5 data:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-1-imagery-and-label-preparation-598af46d485e&quot;&gt;The SpaceNet 5 Baseline — Part 1: Imagery and Label Preparation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-2-training-a-road-speed-segmentation-model-2bc93de564d7&quot;&gt;The SpaceNet 5 Baseline — Part 2: Training a Road Speed Segmentation Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/the-downlinq/the-spacenet-5-baseline-part-3-extracting-road-speed-vectors-from-satellite-imagery-5d07cd5e1d21&quot;&gt;The SpaceNet 5 Baseline — Part 3: Extracting Road Speed Vectors from Satellite Imagery&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;SpaceNet 5 Winning Model Release: End of the Road&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I referenced two Hugging Face blogs by Julien Simon; he ran his tests on the AWS instance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r7iz.metal-16xl&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://huggingface.co/blog/intel-sapphire-rapids&quot;&gt;Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://huggingface.co/blog/intel-sapphire-rapids-inference&quot;&gt;Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The potential cost savings from using a CPU instance instead of a GPU instance on the major cloud service providers (CSP) can be significant. The latest processors are still being rolled out to the CSPs, so I’m using a 4th Gen Intel Xeon processor that is hosted on the Intel® Developer Cloud (you can sign up for the Beta here: &lt;a href=&quot;http://cloud.intel.com/&quot;&gt;cloud.intel.com&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;On AWS, you can select from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r7iz.*&lt;/code&gt; EC2 instances after you &lt;a href=&quot;http://pages.awscloud.com/R7iz-Preview.html&quot;&gt;sign up for the preview here&lt;/a&gt; (Figure 1). At the time of writing, the new AI-acceleration engine, Intel® Advanced Matrix Extensions (Intel® AMX), is only available on bare metal but it should soon be enabled on the virtual machines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-4th-gen-xeon-aws-instances.png&quot; alt=&quot;List of 4th Gen Xeon  instances on AWS EC2&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;. List of 4th Gen Xeon  instances on AWS EC2 (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;On Google Cloud* Platform, you can select from the 4th Gen Xeon Scalable processors C3 VMs (Figure 2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-4th-gen-xeon-googlecloud-instances.png&quot; alt=&quot;List of 4th Gen Intel Xeon Scalable processor instances on Google Cloud Platform&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. List of 4th Gen Intel Xeon Scalable processor instances on Google Cloud Platform (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;hardware-introduction-and-optimizations&quot;&gt;Hardware Introduction and Optimizations&lt;/h2&gt;

&lt;p&gt;The 4th Gen Intel Xeon processors were released January 2023, and the bare-metal instance I am using has two sockets (each with 56 physical cores), 504 GB of memory, and Intel AMX acceleration. I installed a few key libraries in the backend to take control and monitor the sockets, memory, and cores that I am using on the CPU:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get install numactl&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libjemalloc-dev&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get install libjemalloc&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel-openmp&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda install intel-openmp&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gperftools&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda install gperftools -c conda-forge&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Both PyTorch and Intel Extension for PyTorch have helper scripts so that one does not need to explicitly use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel-openmp&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt;, but they do need to be installed in the backend. In case you want to set them up for other work, here is what I used for OpenMP* …&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export OMP_NUM_THREADS=36
export KMP_AFFINITY=granularity=fine,compact,1,0
export KMP_BLOCKTIME=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;… where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OMP_NUM_THREADS&lt;/code&gt; is the number of threads allocated to the job, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KMP_AFFINITY&lt;/code&gt; affects thread affinity settings (including packing threads close to each other, the state of pinning threads), and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KMP_BLOCKTIME&lt;/code&gt; sets the time in milliseconds that an idle thread should wait before going to sleep.&lt;/p&gt;

&lt;p&gt;Here’s what I used for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt; …&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;numactl -C 0-35 --membind=0 train.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;…where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-C&lt;/code&gt; specifies which cores to use and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--membind&lt;/code&gt; instructs the program to only use one socket (socket 0 in this case).&lt;/p&gt;

&lt;h2 id=&quot;spacenet-data&quot;&gt;SpaceNet Data&lt;/h2&gt;

&lt;p&gt;I am using a satellite image dataset from the &lt;a href=&quot;http://spacenet.ai/sn5-challenge/&quot;&gt;SpaceNet 5 Challenge&lt;/a&gt;. Different cities can be downloaded for free from an AWS S3 bucket:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws s3 ls s3://spacenet-dataset/spacenet/SN5_roads/tarballs/ --human-readable
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2019-09-03 20:59:32    5.8 GiB SN5_roads_test_public_AOI_7_Moscow.tar.gz
2019-09-24 08:43:02    3.2 GiB SN5_roads_test_public_AOI_8_Mumbai.tar.gz
2019-09-24 08:43:47    4.9 GiB SN5_roads_test_public_AOI_9_San_Juan.tar.gz
2019-09-14 13:13:26   35.0 GiB SN5_roads_train_AOI_7_Moscow.tar.gz
2019-09-14 13:13:34   18.5 GiB SN5_roads_train_AOI_8_Mumbai.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can use the following commands to download and unpack a file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws s3 cp s3://spacenet-dataset/spacenet/SN5_roads/tarballs/SN5_roads_train_AOI_7_Moscow.tar.gz .
tar -xvzf ~/spacenet5data/moscow/SN5_roads_train_AOI_7_Moscow.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dataset-preparation&quot;&gt;Dataset Preparation&lt;/h3&gt;

&lt;p&gt;I used the Moscow satellite image dataset, which consists of 1,352 images of 1,300 by 1,300 pixels with corresponding street labels in separate text files. The dataset contains both 8-band multispectral images and 3-band RGB images. Figure 3 shows four sample RGB satellite images and their corresponding generated masks. I used the &lt;a href=&quot;http://github.com/avanetten/cresi/blob/main/cresi/data_prep/speed_masks.py&quot;&gt;speed_masks.py&lt;/a&gt; script from the CRESI repository to generate the segmentation masks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f3-moscow-satellite-image-dataset.png&quot; alt=&quot;Satellite image 3-channel RGB chips from Moscow (top row) and corresponding pixel segmentation masks with varying speed limits&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;. Satellite image 3-channel RGB chips from Moscow (top row) and corresponding pixel segmentation masks with varying speed limits (bottom row) (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;There is a JSON configuration file that must be updated for all remaining components: training and validation split, training, and inference. &lt;a href=&quot;http://github.com/avanetten/cresi/blob/main/cresi/configs/sn5_baseline_aws.json.&quot;&gt;An example configuration can be found here&lt;/a&gt;. I perform an 80:20 training/validation split, making sure to point to the correct folder of satellite images and corresponding masks for training. The configuration parameters are explained in more in the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/master/examples/cpu/usecase_spacenet5&quot;&gt;notebook under examples in GitHub for Intel Extension for PyTorch here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;training-a-resnet34--unet-model&quot;&gt;Training a ResNet34 + UNet Model&lt;/h3&gt;

&lt;p&gt;I made some changes to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cresi&lt;/code&gt; code described below in order to run on a CPU and optimize the training. To run natively on a CPU, replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.model = nn.DataParallel(model).cuda()&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.model = nn.DataParallel(model)&lt;/code&gt; in the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/net/pytorch_utils/train.py&quot;&gt;train.py&lt;/a&gt; script. In the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/01_train.py&quot;&gt;01_train.py&lt;/a&gt; script, remove &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.randn(10).cuda()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To optimize training, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import intel_extension_for_pytorch as ipex&lt;/code&gt; to the import statements in the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/net/pytorch_utils/train.py&quot;&gt;train.py&lt;/a&gt; script. Just after defining the model and optimizer as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model = nn.DataParallel(model)
self.optimizer = optimizer(self.model.parameters(), lr=config.lr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipex.optimize&lt;/code&gt; line to use BF16 precision, instead of FP32: \&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model, self.optimizer = ipex.optimize(self.model, 
    optimizer=self.optimizer,dtype=torch.bfloat16)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add a line to do mixed-precision training just before running a forward pass and calculating the loss function:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.cpu.amp.autocast():
    if verbose:
        print(&quot;input.shape, target.shape:&quot;, input.shape, target.shape)
    output = self.model(input)
    meter = self.calculate_loss_single_channel(output, target, meter, training, iter_size)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that we have optimized our training code, we can move onto training our model.&lt;/p&gt;

&lt;p&gt;Like the &lt;a href=&quot;https://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;winner of the SpaceNet 5 competition&lt;/a&gt;, I trained a ResNet34 encoder + UNet decoder model. It is pretrained from ImageNet weights, and the backbone is left completely unfrozen during training. The training can be run with the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/01_train.py&quot;&gt;01_train.py&lt;/a&gt; script, but in order to control the use of hardware I used a helper script. There are actually two helper scripts: one that comes with stock PyTorch and one that comes with Intel Extension for PyTorch. They both accomplish the same thing, but the first one from stock is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.backends.xeon.run_cpu&lt;/code&gt;, and the second one from Intel Extension for PyTorch is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipexrun&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here is what I ran in the command-line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -m torch.backends.xeon.run_cpu --ninstances 1 \
  --ncores_per_instance 32 \
  --log_path /home/devcloud/spacenet5data/moscow/v10_xeon4_devcloud22.04/logs/run_cpu_logs \
  /home/devcloud/cresi/cresi/01_train.py \
  /home/devcloud/cresi/cresi/configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ipexrun --ninstances 1 \
--ncore_per_instance 32 \
/home/devcloud/cresi/cresi/01_train.py \
/home/devcloud/cresi/cresi/configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In both cases, I am asking PyTorch to run training on one socket with 32 cores. Upon running, I get a printout of what environment variables get set in the backend to understand how PyTorch is using the hardware:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INFO - Use TCMalloc memory allocator
INFO - OMP_NUM_THREADS=32
INFO - Using Intel OpenMP
INFO - KMP_AFFINITY=granularity=fine,compact,1,0
INFO - KMP_BLOCKTIME=1
INFO - LD_PRELOAD=/home/devcloud/.conda/envs/py39/lib/libiomp5.so:/home/devcloud/.conda/envs/py39/lib/libtcmalloc.so
INFO - numactl -C 0-31 -m 0 /home/devcloud/.conda/envs/py39/bin/python -u 01_train.py configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;During training, I make sure that my total loss function is decreasing (i.e., the model is converging on a solution).&lt;/p&gt;

&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;

&lt;p&gt;After training a model, we can start to make predictions from satellite images alone. In the eval.py inference script, add import intel_extension_for_pytorch as ipex to the import statements. After loading the PyTorch model, use Intel Extension for PyTorch to optimize the model for BF16 inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.load(os.path.join(path_model_weights, 
    'fold{}_best.pth'.format(fold)), 
    map_location = lambda storage, 
    loc: storage)
model.eval()
model = ipex.optimize(model, dtype = torch.bfloat16)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just prior to running prediction, add two lines for mixed precision:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.no_grad():
    with torch.cpu.amp.autocast():
        for data in pbar:
            samples = torch.autograd.Variable(data['image'], volatile=True)
            predicted = predict(model, samples, flips=self.flips)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To run inference, we can use the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/02_eval.py&quot;&gt;02_eval.py&lt;/a&gt; script. Now that we have a trained model, we can make predictions on satellite images (Figure 4). We can see that it does seem to map the roads closely to the image!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f4-moscow-satellite-image-complete.png&quot; alt=&quot;Moscow satellite image and accompanying prediction of roads&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;. Moscow satellite image and accompanying prediction of roads (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;I realize that the model I’ve trained is overfit to the Moscow image data and probably won’t generalize well to other cities. However, the &lt;a href=&quot;http://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;winning solution to this challenge&lt;/a&gt; used data from six cities (Las Vegas, Paris, Shanghai, Khartoum, Moscow, Mumbai) and performs well on new cities. In the future, one thing that would be worth testing is training on all six cities and running inference on another city to reproduce their results.&lt;/p&gt;

&lt;h2 id=&quot;note-on-post-processing&quot;&gt;Note on Post-Processing&lt;/h2&gt;

&lt;p&gt;There are further post-processing steps that can be performed to add the mask as graph features to maps. You can read more about the post-processing steps here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-3-extracting-road-speed-vectors-from-satellite-imagery-5d07cd5e1d21&quot;&gt;The SpaceNet 5 Baseline — Part 3: Extracting Road Speed Vectors from Satellite Imagery&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/avanetten/cresi/tree/main/cresi&quot;&gt;Post-processing scripts&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In summary, we:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Created 1,352 image training masks (with speed limits) to correspond to our training satellite image data (from .geojson text file labels)&lt;/li&gt;
  &lt;li&gt;Defined our configuration file for training and inference&lt;/li&gt;
  &lt;li&gt;Split up our data into training and validation sets&lt;/li&gt;
  &lt;li&gt;Optimized our code for CPU training, including using Intel Extension for PyTorch and BF16&lt;/li&gt;
  &lt;li&gt;Trained a performant ResNet34 + UNet model on a 4th Gen Intel Xeon CPU&lt;/li&gt;
  &lt;li&gt;Ran initial inference to see the prediction of a speed limit mask&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can find &lt;a href=&quot;http://edc.intel.com/content/www/us/en/products/performance/benchmarks/4th-generation-intel-xeon-scalable-processors/&quot;&gt;detailed benchmarks here for the 4th Gen Intel Xeon CPU here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Extend the optimizations on an Intel CPU by using the Intel Extension for PyTorch:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install intel-extension-for-pytorch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git clone https://github.com/intel/intel-extension-for-pytorch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://linkedin.com/in/bconsolvo&quot;&gt;Get in touch with me on LinkedIn&lt;/a&gt; if you have any more questions!&lt;/p&gt;

&lt;p&gt;More information about the Intel Extension for PyTorch &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;can be found here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;get-the-software&quot;&gt;Get the Software&lt;/h3&gt;

&lt;p&gt;I encourage you to check out Intel’s other &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Framework&lt;/a&gt;&lt;/strong&gt; optimizations and learn about the open, standards-based &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt;&lt;/strong&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel’s AI software portfolio.&lt;/p&gt;

&lt;p&gt;For more details about 4th Gen Intel Xeon Scalable processor, visit &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI Platform&lt;/a&gt;&lt;/strong&gt; where you can learn about how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-resources&quot;&gt;PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Using Intel® Extension for PyTorch to Boost Image Processing Performance</summary>
      

      
      
    </entry>
  
</feed>


