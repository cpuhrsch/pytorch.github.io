<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-10-19T19:25:08-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Compiling NumPy code into C++ or CUDA via torch.compile</title>
      <link href="https://pytorch.org/blog/compiling-numpy-code/" rel="alternate" type="text/html" title="Compiling NumPy code into C++ or CUDA via torch.compile" />
      <published>2023-10-17T00:00:00-07:00</published>
      <updated>2023-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/compiling-numpy-code</id>
      <content type="html" xml:base="https://pytorch.org/blog/compiling-numpy-code/">&lt;p&gt;Quansight engineers have implemented support for tracing through NumPy code via 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; in PyTorch 2.1. This feature leverages PyTorch’s compiler to 
generate efficient fused vectorized code without having to modify your original 
NumPy code. Even more, it also allows for executing NumPy code on CUDA 
just by running it through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.device(&quot;cuda&quot;)&lt;/code&gt;!&lt;/p&gt;

&lt;p&gt;In this post, we go over how to use this feature and give a few tips and tricks 
to make the most out of it.&lt;/p&gt;

&lt;h2 id=&quot;compiling-numpy-code-into-parallel-c&quot;&gt;Compiling NumPy code into Parallel C++&lt;/h2&gt;

&lt;p&gt;We take as our running example one step in a K-Means algorithm. 
This piece of code is borrowed from this &lt;a href=&quot;https://realpython.com/numpy-array-programming/#clustering-algorithms&quot;&gt;NumPy book&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np

def kmeans(X, means):
    return np.argmin(np.linalg.norm(X - means[:, None], axis=2), axis=0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We create a synthetic dataset with 20M random 2-D points. We can see that, 
given that the means are chosen appropriately, the function returns the correct 
cluster for all of them&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npts = 10_000_000
X = np.repeat([[5, 5], [10, 10]], [npts, npts], axis=0)
X = X + np.random.randn(*X.shape)  # 2 distinct &quot;blobs&quot;
means = np.array([[5, 5], [10, 10]])
np_pred = kmeans(X, means)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Benchmarking this function gives us a baseline of &lt;strong&gt;1.26s&lt;/strong&gt; on an AMD 3970X CPU.&lt;/p&gt;

&lt;p&gt;Compiling this function is now as easy as wrapping it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; and 
executing it with the example inputs&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch

compiled_fn = torch.compile(kmeans)
compiled_pred = compiled_fn(X, means)
assert np.allclose(np_pred, compiled_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The compiled function yields a 9x speed-up when running it on 1 core. Even 
better, as opposed to NumPy, our generated code does take advantage of all the 
cores in a processor. As such, when we run it on 32 cores, we get a &lt;strong&gt;57x 
speed-up&lt;/strong&gt;. Note that PyTorch always uses all the available cores unless 
explicitly restricted, so this is the default behavior you get when using 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We may inspect the generated C++ code by running the script with the 
environment variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_LOGS=output_code&lt;/code&gt;. When doing so, we can see that 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; was able to compile the broadcasting and the two reductions 
into just one for-loop, and parallelize it using OpenMP&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;extern &quot;C&quot; void kernel(const double* in_ptr0, const long* in_ptr1, long* out_ptr0) {
    #pragma omp parallel num_threads(32)
    #pragma omp for
    for(long i0=0L; i0&amp;lt;20000000L; i0+=1L) {
        auto tmp0 = in_ptr0[2L*i0];
        auto tmp1 = in_ptr1[0L];
        auto tmp5 = in_ptr0[1L + (2L*i0)];
        auto tmp6 = in_ptr1[1L];
        // Rest of the kernel omitted for brevity
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;compiling-numpy-code-into-cuda&quot;&gt;Compiling NumPy code into CUDA&lt;/h2&gt;

&lt;p&gt;Compiling our code so that it runs on CUDA is as simple as setting the 
default device to be CUDA&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.device(&quot;cuda&quot;):
    cuda_pred = compiled_fn(X, means)
assert np.allclose(np_pred, cuda_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By inspecting the generated code via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_LOGS=output_code&lt;/code&gt;, we see that, 
rather than generating CUDA code directly, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; generates rather 
readable &lt;a href=&quot;https://triton-lang.org/main/index.html&quot;&gt;triton&lt;/a&gt; code&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def triton_(in_ptr0, in_ptr1, out_ptr0, XBLOCK : tl.constexpr):
    xnumel = 20000000
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex &amp;lt; xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (2*x0), xmask)
    tmp1 = tl.load(in_ptr1 + (0))
    // Rest of the kernel omitted for brevity
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running this small snippet on an RTX 2060 gives an &lt;strong&gt;8x speed-up&lt;/strong&gt; over the 
original NumPy code. This is something, but it is not particularly impressive, 
given the speed-ups we have seen on CPU. Let’s have a look into how to squeeze 
the most out of our GPU via a couple minor changes.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float64&lt;/code&gt; vs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float32&lt;/code&gt;. Many GPUs, in particular consumer-grade ones, are 
rather sluggish when running operations on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float64&lt;/code&gt;. For this reason, changing 
the data generation to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float32&lt;/code&gt;, the original NumPy code just gets a bit 
faster, about a 9%, but our CUDA code gets &lt;strong&gt;40% faster&lt;/strong&gt;, yielding a &lt;strong&gt;11x 
speed-up&lt;/strong&gt; over the plain NumPy code.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, by default, respects the NumPy semantics, and as such, it uses 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.float64&lt;/code&gt; as its default dtype for all its creation ops. As discussed, this 
can hinder performance, so it is possible to change this default by setting&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch._dynamo import config
config.numpy_default_float = &quot;float32&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;CPU &amp;lt;&amp;gt; CUDA copies&lt;/strong&gt;. An 11x speed-up is good, but it is not even close to 
the CPU numbers. This is caused by a small transformation that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile 
&lt;/code&gt;does behind the scenes. The code above takes NumPy arrays and returns NumPy 
arrays. All of these arrays are on CPU, but the computations are performed on 
the GPU. This means that every time the function is called, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; has 
to copy all these arrays from CPU to the GPU, and then copy the result back to 
CPU to preserve the original semantics. There is no native solution to this 
issue in NumPy, as NumPy does not have the notion of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device&lt;/code&gt;. That being 
said, we can work around it by creating a wrapper to this function so that it 
accepts PyTorch tensors and returns PyTorch tensors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@torch.compile
def tensor_fn(X, means):
    X, means = X.numpy(), means.numpy()
    ret = kmeans(X, means)
    return torch.from_numpy(ret)

def cuda_fn(X, means):
    with torch.device(&quot;cuda&quot;):
        return tensor_fn(X, means)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function now takes tensors in CUDA memory and returns tensors in CUDA 
memory, but the function itself is written in NumPy! &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; uses the 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy()&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from_numpy()&lt;/code&gt; calls as hints, and optimizes them away, and 
internally it simply works with PyTorch tensors without moving the memory at 
all. When we keep the tensors in CUDA and perform the computations in 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float32&lt;/code&gt;, we see a &lt;strong&gt;200x speed-up&lt;/strong&gt; over the initial NumPy implementation on 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float32&lt;/code&gt; arrays.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mixing NumPy and PyTorch&lt;/strong&gt;. In this example, we had to write a small adaptor 
to convert tensors to ndarrays and then back to tensors. In programs that mix 
PyTorch and NumPy converting a tensor into an ndarray is often implemented as 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.detach().cpu().numpy()&lt;/code&gt;, or simply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.numpy(force=True)&lt;/code&gt;. Since when running 
under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; we can run NumPy code in CUDA, we can implement this 
conversion pattern as call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.numpy()&lt;/code&gt;, as we did above. Doing so and 
running the resulting code under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device(&quot;cuda&quot;)&lt;/code&gt; will generate efficient CUDA 
code from original NumPy calls without copying the data from CUDA to CPU at 
all. Note that the resulting code does not run without &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. For it 
to run in eager mode one would need to rollback to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.numpy(force=True)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;further-speed-up-tricks&quot;&gt;Further Speed-up tricks&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;General advice&lt;/strong&gt;. The CUDA code we have shown is already quite efficient, but 
it is true that the running example is rather short. When dealing with larger 
programs, we may need to tweak parts of it to make it more efficient. A good 
place to start is the multiple &lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler.html#read-more&quot;&gt;tutorials and FAQs for torch.compile&lt;/a&gt;. 
This showcases a number of ways to inspect the tracing process, and how to 
identify problematic code that may cause slowdowns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advice when compiling NumPy code&lt;/strong&gt;. NumPy, even if rather similar to PyTorch, 
is often used very differently. It is rather common to perform computations in 
NumPy and then do an if/else depending on values within the array, or perform 
operations in-place, perhaps via boolean masks. These constructions, while 
supported by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, hamper its performance. Changes like writing the 
code in a branchless way to avoid graph breaks, or avoiding in-place ops can go 
a long way.&lt;/p&gt;

&lt;p&gt;To write fast NumPy code, it is best to avoid loops, but sometimes they are 
unavoidable. When tracing through a loop, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; will try to fully 
unroll it. This is sometimes desirable, but sometimes it may not even be 
possible, like when we have a dynamic stopping condition, like in a while loop. 
In these cases, it may be best to just compile the body of the loop, perhaps a 
few iterations at a time (loop unrolling).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Debugging NumPy code&lt;/strong&gt;. Debugging is rather tricky when a compiler is 
involved. To figure out whether an error you are hitting is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile 
&lt;/code&gt;error, or an error from the program, you can execute your NumPy program without 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; by replacing the NumPy import by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import torch._numpy as np&lt;/code&gt;. 
This is should just be used for &lt;strong&gt;debugging purposes&lt;/strong&gt; and is in no way a 
replacement for the PyTorch API, as it is &lt;strong&gt;much slower&lt;/strong&gt; and, as a private API, 
&lt;strong&gt;may change without notice&lt;/strong&gt;. See also &lt;a href=&quot;https://hackmd.io/m3qRedShR3OZkLcaOOdISg&quot;&gt;this FAQ&lt;/a&gt; for other tricks.&lt;/p&gt;

&lt;h2 id=&quot;differences-between-numpy-and-torchcompile-numpy&quot;&gt;Differences between NumPy and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; NumPy&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;NumPy scalars&lt;/strong&gt;. NumPy returns NumPy scalars in almost any case where PyTorch 
would return a 0-D tensor (e.g. from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.sum&lt;/code&gt;). Under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, NumPy 
scalars are treated as 0-D arrays. This is just fine in most cases. The only 
case when their behavior diverges is when NumPy scalars are implicitly used as 
Python scalars. For example,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.asarray(2) * [1, 2, 3]  # 0-D array is an array-like
array([2, 4, 6])
&amp;gt;&amp;gt;&amp;gt; u = np.int32(2)
&amp;gt;&amp;gt;&amp;gt; u * [1, 2, 3]              # scalar decays into a Python int
[1, 2, 3, 1, 2, 3]
&amp;gt;&amp;gt;&amp;gt; torch.compile(lambda: u * [1, 2, 3])()
array([2, 4, 6])               # acts as a 0-D array, not as a scalar ?!?!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we compile the first two lines, we see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; treats &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u&lt;/code&gt; as a 
0-D array. To recover the eager semantics, we just need to make the casting 
explicit&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; torch.compile(lambda: int(u) * [1, 2, 3])()
[1, 2, 3, 1, 2, 3]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Type promotion and versioning&lt;/strong&gt;. NumPy’s type promotion rules may be, at 
times, a bit surprising&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; np.zeros(1, dtype=np.int8) + 127
array([127], dtype=int8)
&amp;gt;&amp;gt;&amp;gt; np.zeros(1, dtype=np.int8) + 128
array([128], dtype=int16)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;NumPy 2.0 is changing these rules to follow others that are closer to those 
PyTorch. The relevant technical document is &lt;a href=&quot;https://numpy.org/neps/nep-0050-scalar-promotion.html&quot;&gt;NEP 50&lt;/a&gt;. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; went ahead and implemented NEP 50 rather than the about-to-be-deprecated rules.&lt;/p&gt;

&lt;p&gt;In general, NumPy within torch.compile follows NumPy 2.0 pre-release.&lt;/p&gt;

&lt;h2 id=&quot;beyond-numpy-scipy-and-scikit-learn&quot;&gt;Beyond NumPy: SciPy and scikit-learn&lt;/h2&gt;

&lt;p&gt;In parallel to this effort of making &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; understand NumPy code, 
other Quansight engineers have designed and proposed a way to support PyTorch 
tensors within scikit-learn and SciPy. This was received enthusiastically by 
other maintainers from these libraries, as it was shown that using PyTorch as a 
backend would often yield considerable speed-ups. Both projects have now merged 
initial support for PyTorch tensors across a number of APIs and submodules.&lt;/p&gt;

&lt;p&gt;This sets the stepping stone to move towards a future where PyTorch tensors can 
be used within other libraries in the Python data ecosystem. Even more, this 
will enable running these other libraries on GPUs and even compiling code 
mixing these libraries and PyTorch, similar to what we have been discussed in 
this post.&lt;/p&gt;

&lt;p&gt;If you want to learn more about this effort, how to use it, or how to help 
moving it forward, see &lt;a href=&quot;https://labs.quansight.org/blog/array-api-support-scikit-learn&quot;&gt;this other blogpost&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PyTorch has committed since its inception to be a framework compatible with the 
rest of the Python ecosystem. Enabling compiling NumPy programs, and 
establishing the tools necessary to do the same for other prominent libraries 
are two more steps in this direction. Quansight and Meta continue working hand 
on hand, improving the compatibility between PyTorch and the rest of the 
ecosystem.&lt;/p&gt;

&lt;p&gt;From Quansight, we would like to thank Mengwei, Voz, and Ed for their 
invaluable help in integrating our work with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. We would also 
like to thank Meta for funding this project as well as previous work on 
improving NumPy compatibility within PyTorch, and the project that led to 
supporting PyTorch within scikit-learn and SciPy. These are giant leaps towards 
consolidating PyTorch as the framework of choice within the open source Python 
data ecosystem.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Evgeni Burovski, Ralf Gommers and Mario Lezcano</name>
        
        
      </author>

      

      

      
        <summary type="html">Quansight engineers have implemented support for tracing through NumPy code via torch.compile in PyTorch 2.1. This feature leverages PyTorch’s compiler to generate efficient fused vectorized code without having to modify your original NumPy code. Even more, it also allows for executing NumPy code on CUDA just by running it through torch.compile under torch.device(&quot;cuda&quot;)!</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Huawei Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/huawei-joins-pytorch/" rel="alternate" type="text/html" title="Huawei Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-10-17T00:00:00-07:00</published>
      <updated>2023-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/huawei-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/huawei-joins-pytorch/">&lt;p&gt;Today, the PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, announced that Huawei has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Huawei has been a long-standing supporter and contributor to the PyTorch Ecosystem, and, through the release of progressive diverse computing, provides easier access to the PyTorch ecosystem for more hardware vendors. By joining as a premier member, Huawei will continue to optimize PyTorch to fully unleash Ascend computing capabilities.&lt;/p&gt;

&lt;p&gt;“We are delighted to join the PyTorch Foundation, and hope to further collaborate with other member companies and expand the community to a wider audience,” said by Zhang Dixuan, President of Huawei Ascend Computing Business, “This move benefits both Huawei, PyTorch, and the wider AI ecosystem. It also aligns with our long-held beliefs in openness, innovation, collaboration, and shared success, and we are confident that it will spur new innovations in the global AI community.”&lt;/p&gt;

&lt;p&gt;Huawei unveiled the All Intelligence strategy to accelerate intelligence across all industries. To cater the demand for AI computing needs, Huawei invests in the system-level technologies, and that belief is centered on open hardware and software that enables partners and fosters talent. This strategy aligns with the PyTorch Foundation’s mission to develop AI as part of a sustainable open source ecosystem and produce inclusive technological feats.&lt;/p&gt;

&lt;p&gt;PyTorch Foundation Executive Director Ibrahim Haddad said, “We are delighted to welcome Huawei to the PyTorch Foundation. Huawei is a leading body in researching computer vision, natural language processing, speech recognition, and other emerging areas, and has proven experience in the field of foundation models. We have no doubt that we will benefit from their support and guidance.”&lt;/p&gt;

&lt;p&gt;As a premier member, Huawei is granted one seat to the PyTorch Foundation Governing Board, and will help set policies, bylaws, and mission and vision statements that define the overarching scope of the PyTorch Foundation’s initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;The Board welcomes Huawei representative Fred Li, Head of Computing Open Source Development Team at Huawei. Fred leads an active and creative team in R&amp;amp;D and operations projects under the principle of “upstream first”, which aims to make diverse computing power ubiquitous.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-huawei&quot;&gt;About Huawei&lt;/h2&gt;

&lt;p&gt;Founded in 1987, Huawei is a leading global provider of information and communications technology (ICT) infrastructure and smart devices. We have 207,000 employees and operate in over 170 countries and regions, serving more than three billion people around the world. We are committed to bringing digital to every person, home and organization for a fully connected, intelligent world.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/legal/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;

&lt;hr class=&quot;mt-5 mb-5&quot; /&gt;

&lt;p&gt;华为成为PyTorch基金会Primer会员&lt;/p&gt;

&lt;p&gt;PyTorch 基金会是深度学习社区在开源 PyTorch 框架和生态系统上进行协作的中立家园，今天宣布华为已作为Primer会员加入。&lt;/p&gt;

&lt;p&gt;华为长期以来一直是PyTorch生态系统的支持者和贡献者，通过推进多样性算力支持与改进，帮助更多厂商后端能够更加轻松地接入PyTorch生态，并积极致力于PyTorch优化，从而充分释放昇腾的算力。&lt;/p&gt;

&lt;p&gt;“通过加入PyTorch基金会，我们可以进一步与其他成员公司共同协作，加速PyTorch社区的发展。”华为昇腾计算业务总裁张迪煊表示，“我们相信这对华为和 PyTorch 生态系统是互惠互利的，也符合我们长期以来开放创新，协作共赢的开源理念，为全球人工智能社区带来更多的兴奋和创新。”&lt;/p&gt;

&lt;p&gt;华为发布全面智能化战略，加速千行万业智能化的转型，持续通过系统级持续创新，坚持硬件开放、软件开源、使能伙伴、发展人才，以满足各行各业多样性的AI算力需求。这与 PyTorch 基金会的使命完美契合且相互补充，即通过培育和维持开源生态系统来推动人工智能的发展，并使每个人都能使用这些技术创新。&lt;/p&gt;

&lt;p&gt;“华为在计算机视觉、自然语言处理、语音识别等领域进行了广泛的研究，并且在大模型领域也积累了成熟的研究经验。我们相信 PyTorch 基金会将从他们对我们的成员和生态系统的支持中受益匪浅。”PyTorch 基金会执行董事 Ibrahim Haddad 说道。&lt;/p&gt;

&lt;p&gt;作为 Primer 会员，华为获得了 PyTorch 基金会董事会的一个席位。董事会通过我们的章程、使命和愿景声明制定政策，描述基金会计划、技术愿景和方向的总体范围。&lt;/p&gt;

&lt;p&gt;我们很高兴欢迎华为计算开源业务总经理李永乐加入我们的董事会。李永乐目前负责华为计算产品线开源业务，他领导着一支极具创新又充满活力的技术和运营团队，他们秉持着“Upstream first”的原则，让多样性算力无处不在。&lt;/p&gt;

&lt;p&gt;要了解有关如何成为 PyTorch 基金会一部分的更多信息，请访问我们的&lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;网站&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;关于华为&lt;/p&gt;

&lt;p&gt;华为创立于1987年，是全球领先的ICT（信息与通信）基础设施和智能终端提供商。我们的20.7万员工遍及170多个国家和地区，为全球30多亿人口提供服务。我们致力于把数字世界带入每个人、每个家庭、每个组织，构建万物互联的智能世界。&lt;/p&gt;

&lt;p&gt;关于PyTorch基金会&lt;/p&gt;

&lt;p&gt;PyTorch 基金会是深度学习社区在开源 PyTorch 框架和生态系统上进行协作的中立家园。 PyTorch 基金会得到其成员和 PyTorch 开源项目主要贡献者的支持。基金会利用成员和贡献者提供的资源来促进社区讨论和协作。&lt;/p&gt;

&lt;p&gt;关于Linux基金会&lt;/p&gt;

&lt;p&gt;Linux 基金会是世界领先的开源软件、硬件、标准和数据协作中心。 Linux 基金会项目对世界基础设施至关重要，包括 Linux、Kubernetes、Node.js、ONAP、PyTorch、RISC-V、SPDX、OpenChain 等。 Linux 基金会专注于利用最佳实践并满足贡献者、用户和解决方案提供商的需求，以创建可持续的开放协作模型。欲了解更多信息，请访问我们的 linuxfoundation.org。 Linux 基金会已注册商标并使用商标。有关 Linux 基金会的商标列表，请参阅其商标使用页面：www.linuxfoundation.org/trademark-usage。 Linux 是 Linus Torvalds 的注册商标。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, the PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, announced that Huawei has joined as a premier member.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Lightning AI Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/lightning-ai-joins-pytorch/" rel="alternate" type="text/html" title="Lightning AI Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-10-17T00:00:00-07:00</published>
      <updated>2023-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/lightning-ai-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/lightning-ai-joins-pytorch/">&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Lightning AI has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Lightning AI is the company behind PyTorch Lightning, the platform and open-source framework for companies to build and deploy AI products leveraging the latest generative AI models.&lt;/p&gt;

&lt;p&gt;“This is a very important milestone for Lightning AI and the PyTorch Lightning community,” remarks Luca Antiga, Chief Technology Officer of Lightning AI. “By joining the PyTorch Foundation, we are strengthening our commitment to boost the adoption of PyTorch across industries. We look forward to partnering with the Foundation to push the vision of PyTorch forward.”&lt;/p&gt;

&lt;p&gt;PyTorch Lightning is one of the leading projects in the PyTorch ecosystem, allowing developers to build, train, fine-tune and deploy AI models at scale. PyTorch Lightning is helping drive the rapid adoption of PyTorch by both the research community and the enterprise.&lt;/p&gt;

&lt;p&gt;“Lightning AI has been a great steward of the AI community, and notably a key contributor to PyTorch over the years,” said PyTorch Foundation Executive Director Ibrahim Haddad. “Their goal of making AI research scalable directly aligns with our mission at the foundation.”&lt;/p&gt;

&lt;p&gt;As a premier member, Lightning AI is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Luca Antiga, Chief Technology Officer at Lightning AI, to our board. Luca joined the Lightning AI team in April 2021 when the Tensorwerk team joined Grid AI. Prior to joining Lightning AI, Luca co-founded Orobix, an applied AI company, and Tensorwerk. He was an early core contributor to PyTorch and co-authored Deep Learning with PyTorch (Manning).&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-lightning-ai&quot;&gt;About Lightning AI&lt;/h2&gt;

&lt;p&gt;Lightning AI is the creator of PyTorch Lightning, the deep learning platform and open-source framework of choice for developers and companies seeking to build and deploy AI products.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/legal/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Lightning AI has joined as a premier member.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Edge: Enabling On-Device Inference Across Mobile and Edge Devices with ExecuTorch</title>
      <link href="https://pytorch.org/blog/pytorch-edge/" rel="alternate" type="text/html" title="PyTorch Edge: Enabling On-Device Inference Across Mobile and Edge Devices with ExecuTorch" />
      <published>2023-10-17T00:00:00-07:00</published>
      <updated>2023-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-edge</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-edge/">&lt;p&gt;We are excited to announce ExecuTorch, our all-new solution for enabling on-device inference capabilities across mobile and edge devices with the backing of industry leaders like Arm, Apple, and Qualcomm Innovation Center.&lt;/p&gt;

&lt;p&gt;As part of PyTorch Edge’s vision for the future of the on-device AI stack and ecosystem, ExecuTorch addresses the fragmentation in the on-device AI ecosystem. It offers a design that provides extension points for seamless third-party integration to accelerate ML models on specialized hardware. Our partners have contributed custom delegate implementations to optimize model inference execution on their respective hardware platforms.&lt;/p&gt;

&lt;p&gt;We have created extensive documentation that provides more details about ExecuTorch’s architecture, its high-level components, example ML models running on ExecuTorch, and end-to-end tutorials for exporting and running a model on various hardware devices. We are excited to see all of the innovative use cases of ExecuTorch built by the community.&lt;/p&gt;

&lt;h2 id=&quot;key-components-of-executorch&quot;&gt;Key Components of ExecuTorch&lt;/h2&gt;

&lt;p&gt;ExecuTorch offers a compact runtime with a lightweight operator registry to cover the PyTorch ecosystem of models, and a streamlined path to execute PyTorch programs on edge devices. These devices range from mobile phones to embedded hardware powered by specific delegates built by our partners. In addition, ExecuTorch ships with a Software Developer Kit (SDK) and toolchain that provide an ergonomic UX for ML Developers to go from model authoring to training and device delegation in a single PyTorch workflow. This suite of tools enables ML developers to perform on-device model profiling and better ways of debugging the original PyTorch model.&lt;/p&gt;

&lt;p&gt;ExecuTorch is architected from the ground up in a composable manner to allow ML developers to make decisions on what components to leverage as well as entry points to extend them if needed. This design provides the following benefits to the ML community:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Portability&lt;/strong&gt;: Compatibility with a wide variety of computing platforms, from high-end mobile phones to highly constrained embedded systems and microcontrollers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Productivity&lt;/strong&gt;: Enabling developers to use the same toolchains and SDK from PyTorch model authoring and conversion, to debugging and deployment to a wide variety of platforms, resulting in productivity gains.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Providing end users with a seamless and high-performance experience due to a lightweight runtime as well as its ability to utilize full hardware capabilities, including general purpose CPUs and specialized purpose microprocessors such as NPUs and DSPs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pytorch-edge-from-pytorch-mobile-to-executorch&quot;&gt;PyTorch Edge: from PyTorch Mobile to ExecuTorch&lt;/h2&gt;

&lt;p&gt;Bringing research and production environments closer together is a fundamental goal of PyTorch. ML engineers increasingly use PyTorch to author and deploy machine learning models in highly dynamic and ever-evolving environments, from servers to edge devices such as mobile phones and embedded hardware.&lt;/p&gt;

&lt;p&gt;With the increasing adoption of AI in Augmented Reality (AR), Virtual Reality (VR), Mixed Reality (MR), Mobile, IoT and other domains, there is a growing need for an end-to-end on-device solution that is extensible, modular, and aligned with the PyTorch stack.&lt;/p&gt;

&lt;p&gt;PyTorch Edge builds on the same fundamental principle of improving research to production by enabling the deployment of various ML models (spanning vision, speech, NLP, translation, ranking, integrity and content creation tasks) to edge devices via a low-friction development and deployment process. It provides a framework stack that spans the universe of on-device use-cases that the PyTorch community cares about.&lt;/p&gt;

&lt;p&gt;PyTorch Edge provides portability of core components that is required to reach a wide spectrum of devices which are characterized by differing hardware configurations, performance and efficiency. Such portability is achieved by allowing optimization that are custom developed for the target use-cases, and developer productivity via well defined entry-points, representations, and tools to tie all this together into a thriving ecosystem.&lt;/p&gt;

&lt;p&gt;PyTorch Edge is the future of the on-device AI stack and ecosystem for PyTorch. We are excited to see what the community builds with ExecuTorch’s on-device inference capabilities across mobile and edge devices backed by our industry partner delegates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/executorch/stable/index.html&quot;&gt;Learn more about PyTorch Edge and ExecuTorch&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>the PyTorch Edge Team</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce ExecuTorch, our all-new solution for enabling on-device inference capabilities across mobile and edge devices with the backing of industry leaders like Arm, Apple, and Qualcomm Innovation Center.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Flash-Decoding for long-context inference</title>
      <link href="https://pytorch.org/blog/flash-decoding/" rel="alternate" type="text/html" title="Flash-Decoding for long-context inference" />
      <published>2023-10-13T00:00:00-07:00</published>
      <updated>2023-10-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/flash-decoding</id>
      <content type="html" xml:base="https://pytorch.org/blog/flash-decoding/">&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Large language models (LLM) such as ChatGPT or Llama have received unprecedented attention lately. However, they remain massively expensive to run. Even though generating a single response can cost about $0.01 (a few seconds of an 8xA100 instance on AWS), the costs quickly add up when scaling to billions of users, who could have multiple daily interactions with such LLMs. Some use cases are more expensive, like code auto-completion, because it runs whenever a new character is typed. As LLM applications multiply, even small efficiency gains to the generation time can have a massive impact.&lt;/p&gt;

&lt;p&gt;LLM inference (or “decoding”) is an iterative process: tokens are generated one at a time. Generating full sentences of N tokens requires N forward passes through the model. Fortunately, it is possible to cache previously calculated tokens: this means that a single generation step does not depend on the context length, except for a single operation, the attention. This operation does not scale well with context length.&lt;/p&gt;

&lt;p&gt;There are a number of important emerging use cases of LLMs that utilize a long context. With a longer context, LLMs can reason about longer documents, either to summarize or answer questions about them, they can keep track of longer conversations, or even process entire codebases before writing code. As an example, most LLMs had a context length of up to 2k in 2022 (GPT-3), but we now have open-source LLMs scaling up to 32k (&lt;a href=&quot;https://together.ai/blog/llama-2-7b-32k&quot;&gt;Llama-2-32k&lt;/a&gt;), or even 100k more recently (&lt;a href=&quot;https://about.fb.com/news/2023/08/code-llama-ai-for-coding/&quot;&gt;CodeLlama&lt;/a&gt;). In this setting, attention takes a significant fraction of time during inference.&lt;/p&gt;

&lt;p&gt;When scaling on the batch size dimension, the attention can also become a bottleneck even with relatively small contexts. This is because the amount of memory to read scales with the batch dimension, whereas it only depends on the model size for the rest of the model.&lt;/p&gt;

&lt;p&gt;We present a technique, Flash-Decoding, that significantly speeds up attention during inference, bringing up to 8x faster generation for very long sequences. The main idea is to load the keys and values in parallel as fast as possible, then separately rescale and combine the results to maintain the right attention outputs.&lt;/p&gt;

&lt;h2 id=&quot;multi-head-attention-for-decoding&quot;&gt;Multi-head attention for decoding&lt;/h2&gt;

&lt;p&gt;During decoding, every new token that is generated needs to attend to all previous tokens, to compute:&lt;/p&gt;

&lt;p&gt;softmax(queries @ keys.transpose) @ values&lt;/p&gt;

&lt;p&gt;This operation has been optimized with FlashAttention (v1 and v2 recently) in the training case, where the bottleneck is the memory bandwidth to read and write the intermediate results (e.g. Q @ K^T). However, these optimizations don’t apply directly to the inference case, because the bottlenecks are different. For training, FlashAttention parallelizes across the batch size and query length dimensions. During inference, the query length is typically 1: this means that if the batch size is smaller than the number of streaming multiprocessors (SMs) on the GPU (108 for an A100), the operation will only use a small part of the GPU! This is especially the case when using long contexts, because it requires smaller batch sizes to fit in GPU memory. With a batch size of 1, FlashAttention will use less than 1% of the GPU!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Inference_regular_attn.gif&quot; alt=&quot;FlashAttention&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;FlashAttention parallelizes across blocks of queries and batch size only, and does not manage to occupy the entire GPU during decoding&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The attention can also be done using matrix multiplication primitives - without using FlashAttention. In this case, the operation occupies the GPU entirely, but launches many kernels that write and read intermediate results, which is not optimal.&lt;/p&gt;

&lt;h2 id=&quot;a-faster-attention-for-decoding-flash-decoding&quot;&gt;A faster attention for decoding: Flash-Decoding&lt;/h2&gt;

&lt;p&gt;Our new approach Flash-Decoding is based on FlashAttention, and adds a new parallelization dimension: the keys/values sequence length. It combines the benefits of the 2 approaches from above. Like FlashAttention, it stores very little extra data to global memory, however it fully utilizes the GPU even when the batch size is small, as long as the context length is large enough.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inference_splitkv.gif&quot; alt=&quot;Flash-Decoding&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Flash-Decoding also parallelizes across keys and values, at the cost of a small final reduction step&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Flash-Decoding works in 3 steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, we split the keys/values in smaller chunks.&lt;/li&gt;
  &lt;li&gt;We compute the attention of the query with each of these splits in parallel using FlashAttention. We also write 1 extra scalar per row and per split: the log-sum-exp of the attention values.&lt;/li&gt;
  &lt;li&gt;Finally, we compute the actual output by reducing over all the splits, using the log-sum-exp to scale the contribution of each split.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All of this is possible because the attention/softmax can be calculated iteratively. In Flash-Decoding, it is used at 2 levels: within splits (like FlashAttention), and across splits to perform the final reduction.&lt;/p&gt;

&lt;p&gt;In practice, step (1) does not involve any GPU operation, as the key/value chunks are views of the full key/value tensors. We then have 2 separate kernels to perform respectively (2) and (3).&lt;/p&gt;

&lt;h2 id=&quot;benchmarks-on-codellama-34b&quot;&gt;Benchmarks on CodeLlama 34B&lt;/h2&gt;

&lt;p&gt;To validate this approach, we benchmark the decoding throughput of the CodeLLaMa-34b. This model has the same architecture as Llama 2, and more generally results should generalize across many LLMs. We measure the decoding speed in tok/s at various sequence lengths, from 512 to 64k, and compare multiple ways of calculating the attention:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pytorch: Running the attention using pure PyTorch primitives (without using FlashAttention)&lt;/li&gt;
  &lt;li&gt;FlashAttention v2&lt;/li&gt;
  &lt;li&gt;FasterTransformer: Uses the FasterTransformer attention kernel&lt;/li&gt;
  &lt;li&gt;Flash-Decoding&lt;/li&gt;
  &lt;li&gt;And an upper bound calculated as the time it takes to read from memory the entire model along with the KV-cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Flash-Decoding unlocks up to 8x speedups in decoding speed for very large sequences, and scales much better than alternative approaches.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/decoding_codellama34b.png&quot; alt=&quot;CodeLlama&quot; style=&quot;width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All approaches perform similarly for small prompts, but scale poorly as the sequence length increases from 512 to 64k, except Flash-Decoding. In this regime (batch size 1) with Flash-Decoding, scaling the sequence length has little impact on generation speed&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;component-level-micro-benchmarks&quot;&gt;Component-level micro-benchmarks&lt;/h2&gt;

&lt;p&gt;We also micro-benchmark the scaled multi-head attention for various sequence lengths and batch sizes on A100 with inputs in f16. We set the batch size to 1, and use 16 query heads of dimension 128, for 2 key/value heads (grouped-query attention), which matches the dimensions used in CodeLLaMa-34b when running on 4 GPUs.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Setting \ Algorithm&lt;/td&gt;
      &lt;td&gt;PyTorch Eager&lt;/td&gt;
      &lt;td&gt;Flash-Attention v2.0.9&lt;/td&gt;
      &lt;td&gt;Flash-Decoding&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=256, seqlen=256&lt;/td&gt;
      &lt;td&gt;3058.6&lt;/td&gt;
      &lt;td&gt;390.5&lt;/td&gt;
      &lt;td&gt;63.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=128, seqlen=512&lt;/td&gt;
      &lt;td&gt;3151.4&lt;/td&gt;
      &lt;td&gt;366.3&lt;/td&gt;
      &lt;td&gt;67.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=64, seqlen=1024&lt;/td&gt;
      &lt;td&gt;3160.4&lt;/td&gt;
      &lt;td&gt;364.8&lt;/td&gt;
      &lt;td&gt;77.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=32, seqlen=2048&lt;/td&gt;
      &lt;td&gt;3158.3&lt;/td&gt;
      &lt;td&gt;352&lt;/td&gt;
      &lt;td&gt;58.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=16, seqlen=4096&lt;/td&gt;
      &lt;td&gt;3157&lt;/td&gt;
      &lt;td&gt;401.7&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=8, seqlen=8192&lt;/td&gt;
      &lt;td&gt;3173.1&lt;/td&gt;
      &lt;td&gt;529.2&lt;/td&gt;
      &lt;td&gt;56.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=4, seqlen=16384&lt;/td&gt;
      &lt;td&gt;3223&lt;/td&gt;
      &lt;td&gt;582.7&lt;/td&gt;
      &lt;td&gt;58.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=2, seqlen=32768&lt;/td&gt;
      &lt;td&gt;3224.1&lt;/td&gt;
      &lt;td&gt;1156.1&lt;/td&gt;
      &lt;td&gt;60.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=1, seqlen=65536&lt;/td&gt;
      &lt;td&gt;1335.6&lt;/td&gt;
      &lt;td&gt;2300.6&lt;/td&gt;
      &lt;td&gt;64.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;B=1, seqlen=131072&lt;/td&gt;
      &lt;td&gt;2664&lt;/td&gt;
      &lt;td&gt;4592.2&lt;/td&gt;
      &lt;td&gt;106.6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Micro-benchmark of the multi-head attention, run-time in us. Flash-Decoding achieves almost constant run-time as the sequence length scales to up to 64k.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The up to 8x speedup end-to-end measured earlier is made possible because the attention itself is up to 50x faster than FlashAttention. Up until sequence length 32k, the attention time is roughly constant, because Flash-Decoding manages to fully utilize the GPU.&lt;/p&gt;

&lt;h2 id=&quot;using-flash-decoding&quot;&gt;Using Flash-Decoding&lt;/h2&gt;

&lt;p&gt;Flash-decoding is available:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention/tree/main&quot;&gt;FlashAttention&lt;/a&gt; package, starting at version 2.2&lt;/li&gt;
  &lt;li&gt;Through &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; starting at version 0.0.22 through `xformers.ops.memory_efficient_attention`. The dispatcher will automatically use either the Flash-Decoding or FlashAttention approaches depending on the problem size. When these approaches are not supported, it can dispatch to an efficient triton kernel that implements the Flash-Decoding algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A full example of decoding with LLaMa v2 / CodeLLaMa is available in the FlashAttention repo &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention/tree/main/examples/inference&quot;&gt;here&lt;/a&gt; and in the xFormers &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;repo&lt;/a&gt; here. We also provide a &lt;a href=&quot;https://github.com/facebookresearch/xformers/tree/main/examples/llama_inference&quot;&gt;minimal example&lt;/a&gt; of an efficient decoding code for LLaMa v1/v2 models, meant to be fast, easy to read, educational and hackable.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Thanks to Erich Elsen, Ashish Vaswani, and Michaël Benesty for suggesting this idea of splitting the KVcache loading. We want to thank Jeremy Reizenstein, Patrick Labatut and Andrew Tulloch for the valuable discussions. We also want to thank Geeta Chauhan and Gregory Chanan for helping with the writing and more broadly contributing to getting this published on the PyTorch blog.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Tri Dao, Daniel Haziza, Francisco Massa, Grigory Sizov</name>
        
        
      </author>

      

      

      
        <summary type="html">Motivation</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ML Model Server Resource Saving - Transition From High-Cost GPUs to Intel CPUs and oneAPI powered Software with performance</title>
      <link href="https://pytorch.org/blog/ml-model-server-resource-saving/" rel="alternate" type="text/html" title="ML Model Server Resource Saving - Transition From High-Cost GPUs to Intel CPUs and oneAPI powered Software with performance" />
      <published>2023-10-11T00:00:00-07:00</published>
      <updated>2023-10-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/ml-model-server-resource-saving</id>
      <content type="html" xml:base="https://pytorch.org/blog/ml-model-server-resource-saving/">&lt;p&gt;Reviewers: &lt;a href=&quot;https://www.linkedin.com/in/yunsang-ju/&quot;&gt;Yunsang Ju&lt;/a&gt;(Naver GplaceAI Leader), Min Jean Cho(Intel), Jing Xu(Intel), Mark Saroufim(Meta)&lt;/p&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;Here, We will be sharing our experience in moving AI workloads from our GPU servers to our Intel CPU servers without any performance or quality degradation, and &lt;strong&gt;saving annual costs of approximately 340 thousand U.S. Dollar&lt;/strong&gt; (refer to the &lt;strong&gt;Conclusion&lt;/strong&gt;) in the process.&lt;/p&gt;

&lt;p&gt;We aim to provide value to our consumers by serving various AI models that enhance the Online to Offline (O2O) experience. With the ongoing growth in the demand for new models and the limited nature of high-cost resource GPUs, we needed to transition relatively lightweight AI models from GPU servers to Intel CPU servers for reducing resource consumption. In the same setting, however, the CPU server had issues where performance of rps, inference time, etc. was reduced by tens of times. We applied various engineering techniques and lightweighted the model to solve this problem, and we were able to successfully transition to the Intel CPU servers with the same performance or better performance as the GPU servers with just a three-fold scale out.&lt;/p&gt;

&lt;p&gt;For a more detailed introduction about our team, please refer to the &lt;a href=&quot;https://medium.com/naver-place-dev/introduction-to-naver-place-ai-development-team-a8b0630e3b23&quot;&gt;Introduction to NAVER Place AI Development Team&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ll mention it again in the middle, but I’ve received a lot of help from &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;Grokking Pytorch Intel CPU Performance From First Principles&lt;/a&gt; written by Intel and PyTorch in the overall work.&lt;/p&gt;

&lt;h2 id=&quot;problem-definition&quot;&gt;Problem Definition&lt;/h2&gt;

&lt;h3 id=&quot;1-service-architecture&quot;&gt;1: Service Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg1.jpg&quot; alt=&quot;Simplified service architecture&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simplified service architecture (Image Source: NAVER GplaceAI)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To facilitate understanding, a brief introduction to our service architecture will be provided. CPU intensive tasks such as preprocessing input to tensor format (then forwarded to the model) and post processing inference results to human readable output (e.g. natural language and image formats) are performed on the App Server(FastAPI) The Model Server(TorchServe) exclusively handles inference operations. For stable operation of the service, the following actions need to be performed with sufficient throughput and low latency.&lt;/p&gt;

&lt;p&gt;The specific processing sequence is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The client submits a request to the app server via the Traefik gateway.&lt;/li&gt;
  &lt;li&gt;The app server pre-processes the input by performing actions such as resizing and transforming, and converting it into a Torch tensor before then requesting the model server.&lt;/li&gt;
  &lt;li&gt;The model server performs inference and returns the feature to the app server&lt;/li&gt;
  &lt;li&gt;The app server converts the feature into a format understandable by humans through post-processing and returns it to the client&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-throughput-and-latency-measurement&quot;&gt;2:  Throughput and Latency Measurement&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg1-1.jpg&quot; alt=&quot;Comparison of Image Scoring Models&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparison of Image Scoring Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With all other conditions remaining the same, deploying on a threefold increase CPU server pod, yet, notably, the RPS (requests per second) and response time deteriorated by more than tenfold. While it was not surprising that CPU inference performance is inferior to GPUs, the challenging situation was evident. Given the goal of maintaining performance within limited resources, achieving an approximate &lt;strong&gt;10 to 20 times performance improvement&lt;/strong&gt; was necessary Barring any additional scaling.&lt;/p&gt;

&lt;h3 id=&quot;3-challenges-from-a-throughput-perspective&quot;&gt;3: Challenges From a Throughput Perspective&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                        37     0(0.00%) |   9031    4043   28985   8200 |    1.00        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                        37     0(0.00%) |   9031    4043   28985   8200 |    1.00        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One of the first steps TorchServer framework users might take in order to improve throughput is to increase the number of workers in TorchServe. This approach is effective on GPU servers Because of parallel workload processing, excluding the linear memory usage increase as workers scale. However, we were experiencing worse performance when increasing the number of workers. Identifying the cause of performance degradation on CPU servers required further investigation.&lt;/p&gt;

&lt;h3 id=&quot;4challenges-from-a-latency-perspective&quot;&gt;4: Challenges From a Latency Perspective&lt;/h3&gt;

&lt;p&gt;Our primary concern was latency. Throughput improvement is normally achievable when a system’s implementation is faithful to scale-out principles, except for perhaps very rare worst-case scenarios. However, in the case of the Image Scoring model example, even performing a single inference took more than 1 second, and as the request volume increased, latency increased to as much as 4 seconds. It was a situation where the timeout criteria to satisfy the client could not be met even with a single inference.&lt;/p&gt;

&lt;h2 id=&quot;proposed-solutions&quot;&gt;Proposed Solutions&lt;/h2&gt;

&lt;p&gt;Improvements were needed from both an ML and an engineering perspective. It was essential to fundamentally reduce the inference time on the CPU and to identify the causes of performance degradation when applying config that generally enhances performance, in order to find the optimal configuration values. To accomplish this, collaboration was established with MLE professionals to concurrently execute tasks encompassing ‘model lightweighting without compromising performance’, and ‘Identify optimal configurations for achieving peak performance’. Using the aforementioned approaches we were able to effectively transition workload handling to our CPU servers.&lt;/p&gt;

&lt;h3 id=&quot;1-resolving-low-rps-from-an-engineering-perspective&quot;&gt;1: Resolving Low RPS from an Engineering Perspective&lt;/h3&gt;

&lt;p&gt;First, the reason for performance degradation even after increasing the worker number was the front-end bound caused by logical threads in GEMM operations. Generally, when increasing the number of workers, the expected improvement effect is the increase in parallelism. Conversely, if performance decreases, one can infer the corresponding trade-off effect.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg2.jpg&quot; alt=&quot;CPU + GPU&quot; style=&quot;width:100%; max-width: 420px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image Source: &lt;a href=&quot;https://blogs.nvidia.com/blog/2018/06/11/what-is-a-virtual-gpu/&quot;&gt;Nvidia&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As many are aware, the reason model inference performance on CPUs is inferior to GPUs lies in the difference in hardware design, particularly in terms of multi-threading capabilities. Diving deeper, model inference is fundamentally a repetition of &lt;strong&gt;GEMM (General Matrix Multiply)&lt;/strong&gt; operations, and these GEMM operations are executed independently in &lt;strong&gt;“fused-multiply-add” (FMA)&lt;/strong&gt; or &lt;strong&gt;“dot-product” (DP)&lt;/strong&gt; execution units. If the GEMM operation becomes a bottleneck on the CPU, increasing parallelism might actually result in decreased performance. While researching the problem we found relevant information within the &lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/advanced/cpu_affinity.html#binding-processes-to-physical-cores&quot;&gt;PyTorch documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;While two logical threads run GEMM at the same time, they will be sharing the same core resources causing front-end bound&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This information highlighted that logical threads could cause a bottleneck in CPU GEMM operations, which helped us intuitively understand why performance decreased when increasing the worker num. This is because the default value of the torch thread corresponds to the physical core value of the CPU.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@test-pod:/# lscpu
  …
Thread(s) per core: 2
Core(s) per socket: 12
  …
root@test-pod:/# python
&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; print(torch.get_num_threads())
24
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the worker_num increases, the total thread count increases by the product of the physical core * worker number. Consequently, logical threads are utilized. In order to improve performance, the total number of threads per worker was adjusted to align with the physical core count. Below, it can be observed that the metric RPS &lt;strong&gt;increased approximately threefold&lt;/strong&gt; to 6.3(from the previous value of 2.1) when the worker_num was increased to 4 and the total thread count was aligned with the number of physical cores.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                       265     0(0.00%) |   3154    1885    4008   3200 |    6.30        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       265     0(0.00%) |   3154    1885    4008   3200 |    6.30        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 1&lt;/strong&gt;: Our team is Using Kubernetes to maintain our deployments. So we are adjusting the which required us to adjust according to the CPU resource limit of the pod, rather than the physical core count of the node that can be checked using the lscpu command. (Setting the torch thread of each worker to 8/4 = 2, or 24/4 = 6 resulted in performance degradation.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 2&lt;/strong&gt;: Since torch thread settings for each worker &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.set_num_threads.html&quot;&gt;can only be configured as integers&lt;/a&gt;, it’s advisable to set the CPU limit divisible by the worker_num in order to adequately utilize CPU usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg3.jpg&quot; alt=&quot;example&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ex) core=8, In the case of worker_num=3: int(8/worker_num) = 2, 2*worker_num/8 = 75%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg4.jpg&quot; alt=&quot;example&quot; style=&quot;width:100%; margin-top: 30px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ex) core=8, In the case of worker_num=4: int(8/worker_num) = 2, 2*worker_num/8 = 100%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We also analyzed the model containers to see why we got a mere threefold improvement in performance despite a four times increase in the number of workers. Various resources were monitored, and among them, the core utilization rate was identified as the underlying cause.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg5.jpg&quot; alt=&quot;threads&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even when the total thread count was adjusted to match the CPU(2nd Generation, Intel(R) Xeon(R) Silver 4214) limit(8 core), there were instances where computations were executed from logical thread to logical core. Due to the presence of 24 physical cores, the cores numbered 25 to 48 are classified as logical cores. The possibility of confining thread execution solely within physical cores seemed to offer the potential for further performance enhancement. The reference to this solution could be found within the source document mentioned in the PyTorch-geometric article that warned about CPU GEMM bottlenecks.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reference Documentation: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;Grokking Pytorch Intel CPU Performance From First Principles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As per the instructions in the document, Intel provides Intel® Extension for PyTorch where we can simply pin cores to specific sockets. The application method is also made very simple, by adding the following settings to the &lt;strong&gt;torchserve config.properties&lt;/strong&gt; file.(used intel_extension_for_pytorch==1.13.0)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ipex_enable=true
CPU_launcher_enable=true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg6.jpg&quot; alt=&quot;two-socket configuration&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image Source: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html#grokking-pytorch-intel-cpu-performance-from-first-principles&quot;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Beyond the removal of logical threads through socket pinning, there is an additional effect of eliminating UPI cache hit overhead. Since the CPU comprises more than one socket when threads scheduled on socket 1 are rescheduled on socket 2, cache hits occur in cases of accessing the cache of socket 1 via Intel Ultra Path Interconnect (UPI). At this point, UPI access to the local cache becomes more than twice as slow as local cache access, resulting in more bottlenecks. With threads being pinned to socket units by oneAPI powered Intel® Extension for PyTorch, We observed rps handling increase of up to &lt;strong&gt;four times than when the bottleneck existed&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/image-scoring                                                       131     0(0.00%) |   3456    1412    6813   3100 |    7.90        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       131     0(0.00%) |   3456    1412    6813   3100 |    7.90        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 1&lt;/strong&gt;: Intel® Extension for PyTorch is specialized in neural network (referred to as “nn” hereafter) inference optimization, so the performance improvement from additional techniques outside nn might be minimal. Indeed, in the instance of the image scoring system highlighted as an example, where svr (support vector regression) is applied post-inference, the performance enhancement was confined to a 4-fold increase. However, for a purely nn inference model such as the food recognition model, &lt;strong&gt;a&lt;/strong&gt; &lt;strong&gt;performance boost of 7-fold (2.5rps -&amp;gt; 17.5rps)&lt;/strong&gt; was detected.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/food-classification                                                 446     0(0.00%) |   1113     249    1804   1200 |   17.50        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                       446     0(0.00%) |   1113     249    1804   1200 |   17.50        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Cautionary Note 2&lt;/strong&gt;: Applying Intel® Extension for PyTorch requires &lt;strong&gt;torchserve version 0.6.1 or higher&lt;/strong&gt;. Since our team was using version 0.6.0, there was an issue where socket pinning was not functioning correctly. Currently, we have made modifications to the guide document, specifying the required version.&lt;/p&gt;

&lt;p&gt;Within &lt;a href=&quot;https://github.com/pytorch/serve/blob/4236a86dc0a018198ecd3fe261e835b416df739e/frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java&quot;&gt;WorkerLifeCycle.java&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;,&lt;/span&gt; multi-worker pinning is not supported in 0.6.0 and below (ninstance is hardcoded to 1)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// 0.6.0 version

public ArrayList&amp;lt;String&amp;gt; launcherArgsToList() {
   ArrayList&amp;lt;String&amp;gt; arrlist = new ArrayList&amp;lt;String&amp;gt;();
   arrlist.add(&quot;-m&quot;);
   arrlist.add(&quot;intel_extension_for_pytorch.cpu.launch&quot;);
   arrlist.add(&quot; — ninstance&quot;);
   arrlist.add(&quot;1&quot;);
   if (launcherArgs != null &amp;amp;&amp;amp; launcherArgs.length() &amp;gt; 1) {
     String[] argarray = launcherArgs.split(&quot; &quot;);
     for (int i = 0; i &amp;lt; argarray.length; i++) {
       arrlist.add(argarray[i]);
     }
   }
   return arrlist;
 }
// master version

if (this.numWorker &amp;gt; 1) {
   argl.add(&quot; — ninstances&quot;);
   argl.add(String.valueOf(this.numWorker));
   argl.add(&quot; — instance_idx&quot;);
   argl.add(String.valueOf(this.currNumRunningWorkers));
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-addressing-slow-latency-through-model-lightweighting&quot;&gt;2: Addressing Slow Latency Through Model Lightweighting&lt;/h3&gt;

&lt;p&gt;We also streamlined our model using &lt;strong&gt;Knowledge Distillation&lt;/strong&gt; (commonly abbreviated as KD) to further reduce latency. As is widely known, kd is a technique where knowledge from a larger network (Teacher network) is conveyed to a smaller, lightweight network (Student network) which is less resource intensive and can be more readily deployed. For more detailed information, please refer to the paper where this concept was initially introduced, titled &lt;span style=&quot;text-decoration:underline;&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg7.jpg&quot; alt=&quot;neural networks&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a variety of KD techniques available and because we were primarily focused on &lt;strong&gt;accuracy loss minimization&lt;/strong&gt;, we adopted the approach from the paper &lt;a href=&quot;https://arxiv.org/pdf/2205.10536.pdf&quot;&gt;Knowledge Distillation from A Stronger Teacher&lt;/a&gt;, which was published in the year 2022. The concept is straightforward. Unlike the conventional method of distillation that utilizes only the model’s prop values, the chosen approach involves having the student network learn the correlations between classes in the teacher network. When put into actual application, We observed effective model weight reduction to observe the effective reduction in the model’s weight while mainting high accuracy. The following are the outcomes of our experimentation with the mentioned knowledge distillation technique on several candidate student models, where selections were made based on the maintained level of accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg8.jpg&quot; alt=&quot;table of services&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the image scoring system, additional measures were taken to reduce the input size. Considering that the prior use of CPU-based ML technique SVR (Support Vector Regression) was used (2-stage: CNN + SVR), even when this was streamlined into a 1-stage model, significant speed advantages were not observed in CPU inference. In order for streamlining to have significance, the input size of the student model during inference needed further reduction. Consequently, experiments were conducted with the size reduced from 384&lt;em&gt;384 to 224&lt;/em&gt;224.&lt;/p&gt;

&lt;p&gt;Further simplifying transformations, the 2-stage (CNN + SVR) approach was unified into a 1-stage model with a larger ConvNext, and then kd was applied using the lightweight EfficientNet to resolve the accuracy trade-off. During the experiments, we encountered a problem where changing Img_resize to 224 led to a performance drop from 0.4007 to 0.4296 in terms of MAE. Due to the reduction in input size, various preprocessing techniques applied to the original training images (such as Affine, RandomRotate90, Blur, OneOf [GridDistortion, OpticalDistortion, ElasticTransform], VerticalFlip) had a counterproductive effect. By adopting these measures, effective training of the student was achieved, and the &lt;strong&gt;MAE value improved by 25% compared to the previous one (.518 to .3876)&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;validation&quot;&gt;Validation&lt;/h2&gt;

&lt;h3 id=&quot;1-final-performance-measurement&quot;&gt;1: Final Performance Measurement&lt;/h3&gt;

&lt;p&gt;The following shows the final performance improvements using CPU servers, on the three models mentioned throughout this article.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Food photo classifier (pod 3): 2.5rps -&amp;gt; 84 rps

 Type Name                                                                           # reqs # fails | Avg Min Max Med | req/s failures/s
 --------|----------------------------------------------------------------------------|------|------------|-------|------|-------|-------|--------|--------- 
POST /predictions/food-classification 2341 0(0.00%) | 208 130 508 200 | 84.50 0.00 
--------|----------------------------------------------------------------------------|--------|-------------|------|-------|--------|------|--------|----------
         Aggregated                                                                      2341     0(0.00%) |    208     130     508    200 |   84.50        0.00

# Image scoring (pod 3): 2.1rps -&amp;gt; 62rps
 Type Name                                                                               #reqs #fails | Avg Min Max Median | req/s failures/s
 --------|---------------------------------------------------------------------------------|--------|-------------|--------|-------|--------|---------|--------|--------- 
  POST /predictions/image-scoring 1298 0 (0.00%) | 323 99 607 370 | 61.90 0.00 
--------|---------------------------------------------------------------------------------|--------|-------------|--------|------|--------|---------|--------|----------
          Aggregated                                                                          1298     0(0.00%)  |     323      99     607     370  |   61.90        0.00

# receipt classifier(pod 3) : 20rps -&amp;gt; 111.8rps
Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
POST     /predictions/receipt-classification                                             4024     0(0.00%) |    266     133    2211    200 |   111.8        0.00
--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------
         Aggregated                                                                      4020     0(0.00%) |    266     133    2211    200 |   111.8        0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-traffic-mirroring&quot;&gt;2:  Traffic Mirroring&lt;/h3&gt;

&lt;p&gt;As previously mentioned, our team’s service architecture employs the tool “traefik” as a gateway in front of the app server, as briefly introduced at the beginning of the article. For final validation, the mirroring feature of this traefik gateway was utilized to mirror traffic from production to staging for a month of validation before applying it to production, which is now operational.&lt;/p&gt;

&lt;p&gt;Details regarding mirroring are beyond the scope of this topic and hence omitted. For those interested, kindly refer to the document at &lt;a href=&quot;https://doc.traefik.io/traefik/routing/services/#mirroring-service&quot;&gt;https://doc.traefik.io/traefik/routing/services/#mirroring-service&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;This concludes the discussion about transitioning from a GPU model server to a CPU server while maintaining service quality. Through this effort, our team &lt;strong&gt;was able to save 15 GPUs each in South Korea and Japan&lt;/strong&gt;, resulting in an &lt;strong&gt;annual cost savings of approximately 340 thousand U.S. Dollar&lt;/strong&gt;. Although we directly purchase and use GPUs within NAVER, we calculated a rough cost reduction &lt;a href=&quot;https://aws.amazon.com/ko/ec2/instance-types/g4/&quot;&gt;based on AWS EC2 instances&lt;/a&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt; &lt;/span&gt;that stably support T4 GPUs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-model-server-resource-saving/fg9.jpg&quot; alt=&quot;instance sizes&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Calculation: 1.306 (1-year reserved instance effective hourly cost) * 24 (hours) * 365 (days) * 15 (number of GPUs) * 2 (KR + JP)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These secured GPUs will be harnessed to further advance and enhance our team’s AI services, delivering exceptional service experiences. We sincerely appreciate your encouragement and anticipation.:)&lt;/p&gt;

&lt;h2 id=&quot;explore-more&quot;&gt;Explore More&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/ecosystem/pytorch-foundation.html&quot;&gt;https://www.intel.com/content/www/us/en/developer/ecosystem/pytorch-foundation.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/advanced/cpu_affinity.html#binding-processes-to-physical-cores&quot;&gt;https://pytorch-geometric.readthedocs.io/en/latest/advanced/CPU_affinity.html#binding-processes-to-physical-cores&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.10536.pdf&quot;&gt;https://arxiv.org/pdf/2205.10536.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Sangjune Park(Naver GplaceAI MLOps), Jooyoung Lee(Naver GplaceAI MLE), Junho Min(Naver GplaceAI MLE)</name>
        
        
      </author>

      

      

      
        <summary type="html">Reviewers: Yunsang Ju(Naver GplaceAI Leader), Min Jean Cho(Intel), Jing Xu(Intel), Mark Saroufim(Meta)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Real-time Audio-visual Speech Recognition</title>
      <link href="https://pytorch.org/blog/real-time-speech-rec/" rel="alternate" type="text/html" title="Real-time Audio-visual Speech Recognition" />
      <published>2023-10-10T00:00:00-07:00</published>
      <updated>2023-10-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/real-time-speech-rec</id>
      <content type="html" xml:base="https://pytorch.org/blog/real-time-speech-rec/">&lt;p&gt;Audio-Visual Speech Recognition (AV-ASR, or AVSR) is the task of transcribing text from audio and visual streams, which has recently attracted a lot of research attention due to its robustness to noise. The vast majority of work to date has focused on developing AV-ASR models for non-streaming recognition; studies on streaming AV-ASR are very limited.&lt;/p&gt;

&lt;p&gt;We have developed a compact real-time speech recognition system based on TorchAudio, a library for audio and signal processing with &lt;a href=&quot;http://pytorch.org&quot;&gt;PyTorch&lt;/a&gt;. It can run locally on a laptop with high accuracy without accessing the cloud. Today, we are releasing &lt;a href=&quot;https://github.com/pytorch/audio/tree/main/examples/avsr&quot;&gt;the real-time AV-ASR recipe&lt;/a&gt; under a permissive open license (BSD-2-Clause license), enabling a broad set of applications and fostering further research on audio-visual models for speech recognition.&lt;/p&gt;

&lt;p&gt;This work is part of our approach to &lt;a href=&quot;https://arxiv.org/abs/2303.14307&quot;&gt;AV-ASR research&lt;/a&gt;. A promising aspect of this approach is its ability to automatically annotate large-scale audio-visual datasets, which enables the training of more accurate and robust speech recognition systems. Furthermore, this technology has the potential to run on smart devices since it achieves the latency and memory efficiency that such devices require for inference.&lt;/p&gt;

&lt;p&gt;In the future, speech recognition systems are expected to power applications in numerous domains. One of the primary applications of AV-ASR is to enhance the performance of ASR in noisy environments. Since visual streams are not affected by acoustic noise, integrating them into an audio-visual speech recognition model can compensate for the performance drop of ASR models. Our AV-ASR system has the potential to serve multiple purposes beyond speech recognition, such as text summarization, translation and even text-to-speech conversion. Moreover, the exclusive use of VSR can be useful in certain scenarios, e.g. where speaking is not allowed, in meetings, and where privacy in public conversations is desired.&lt;/p&gt;

&lt;h1 id=&quot;av-asr&quot;&gt;AV-ASR&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/real-time-speech-rec/pipeline.jpg&quot; alt=&quot;Fig. 1 The pipeline for audio-visual speech recognition system&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: The pipeline for audio-visual speech recognition system&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Our real-time AV-ASR system is presented in Fig. 1. It consists of three components, a data collection module, a pre-processing module and an end-to-end model. The data collection module comprises hardware devices, such as a microphone and camera. Its role is to collect information from the real world. Once the information is collected, the pre-processing module location and crop out face. Next, we feed the raw audio stream and the pre-processed video stream into our end-to-end model for inference.&lt;/p&gt;

&lt;h2 id=&quot;data-collection&quot;&gt;Data collection&lt;/h2&gt;

&lt;p&gt;We use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchaudio.io.StreamReader&lt;/code&gt; to capture audio/video from streaming device input, e.g. microphone and camera on laptop. Once the raw video and audio streams are collected, the pre-processing module locates and crops faces. It should be noted that data is immediately deleted during the streaming process.&lt;/p&gt;

&lt;h2 id=&quot;pre-processing&quot;&gt;Pre-processing&lt;/h2&gt;

&lt;p&gt;Before feeding the raw stream into our model, each video sequence has to undergo a specific pre-processing procedure. This involves three critical steps. The first step is to perform face detection. Following that, each individual frame is aligned to a referenced frame, commonly known as the mean face, in order to normalize rotation and size differences across frames. The final step in the pre-processing module is to crop the face region from the aligned face image. We would like to clearly note that our model is fed with raw audio waveforms and pixels of the face, without any further preprocessing like face parsing or landmark detection. An example of the pre-processing procedure is illustrated in Table 1.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/original.gif&quot; alt=&quot;Original image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img src=&quot;/assets/images/real-time-speech-rec/detected.gif&quot; alt=&quot;Detected image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/transformed.gif&quot; alt=&quot;Transformed image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;
&lt;img src=&quot;/assets/images/real-time-speech-rec/cropped.gif&quot; alt=&quot;Cropped image&quot; style=&quot;width:100%; max-width:200px&quot; /&gt;

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
    0. Original
   &lt;/td&gt;
   &lt;td&gt;
1. Detection
   &lt;/td&gt;
   &lt;td&gt;
2. Alignment
   &lt;/td&gt;
   &lt;td&gt;
3. Crop
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: Preprocessing pipeline.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/real-time-speech-rec/model.jpg&quot; alt=&quot;Fig. 2 The architecture for the audio-visual speech recognition system.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: The architecture for the audio-visual speech recognition system&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;We consider two configurations: Small with 12 Emformer blocks and Large with 28, with 34.9M and 383.3M parameters, respectively. Each AV-ASR model composes front-end encoders, a fusion module, an Emformer encoder, and a transducer model. To be specific, we use convolutional frontends to extract features from raw audio waveforms and facial images. The features are concatenated to form 1024-d features, which are then passed through a two-layer multi-layer perceptron and an Emformer transducer model. The entire network is trained using RNN-T loss. The architecture of the proposed AV-ASR model is illustrated in Fig. 2.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Datasets.&lt;/strong&gt; We follow &lt;a href=&quot;https://arxiv.org/abs/2303.14307&quot;&gt;Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels&lt;/a&gt; to use publicly available audio-visual datasets including &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/lip_reading/&quot;&gt;LRS3&lt;/a&gt;, &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html&quot;&gt;VoxCeleb2&lt;/a&gt; and &lt;a href=&quot;https://looking-to-listen.github.io/avspeech/&quot;&gt;AVSpeech&lt;/a&gt; for training. We do not use mouth ROIs or facial landmarks or attributes during both training and testing stages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparisons with the state-of-the-art.&lt;/strong&gt; Non-streaming evaluation results on LRS3 are presented in Table 2. Our audio-visual model with an algorithmic latency of 800 ms (160ms+1280msx0.5) yields a WER of 1.3%, which is on par with those achieved by state-of-the-art offline models such as AV-HuBERT, RAVEn, and Auto-AVSR.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Method&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Total Hours&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;WER (%)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;ViT3D-CM
   &lt;/td&gt;
   &lt;td&gt;90, 000
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AV-HuBERT
   &lt;/td&gt;
   &lt;td&gt;1, 759
   &lt;/td&gt;
   &lt;td&gt;1.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;RAVEn
   &lt;/td&gt;
   &lt;td&gt;1, 759
   &lt;/td&gt;
   &lt;td&gt;1.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AutoAVSR
   &lt;/td&gt;
   &lt;td&gt;3, 448
   &lt;/td&gt;
   &lt;td&gt;0.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Ours
   &lt;/td&gt;
   &lt;td&gt;3, 068
   &lt;/td&gt;
   &lt;td&gt;1.3
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 2&lt;/strong&gt;: Non-streaming evaluation results for audio-visual models on the LRS3 dataset.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Noisy experiments.&lt;/strong&gt; During training, 16 different noise types are randomly injected to audio waveforms, including 13 types from &lt;a href=&quot;https://zenodo.org/record/1227121&quot;&gt;Demand&lt;/a&gt; database, ‘DLIVING’,’DKITCHEN’, ‘OMEETING’, ‘OOFFICE’, ‘PCAFETER’, ‘PRESTO’, ‘PSTATION’, ‘STRAFFIC’,  ‘SPSQUARE’, ‘SCAFE’, ‘TMETRO’, ‘TBUS’ and ‘TCAR’, two more types of noise from &lt;a href=&quot;https://arxiv.org/abs/1804.03209&quot;&gt;speech commands&lt;/a&gt; database, white and pink and one more type of noise from &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/0167639393900953&quot;&gt;NOISEX-92&lt;/a&gt; database, babble noise. SNR levels in the range of [clean, 7.5dB, 2.5dB, -2.5dB, -7.5dB] are selected from with a uniform distribution. Results of ASR and AV-ASR models, when tested with babble noise, are shown in Table 3. With increasing noise level, the performance advantage of our audio-visual model over our audio-only model grows, indicating that incorporating visual data improves noise robustness.&lt;/p&gt;

&lt;table class=&quot;table&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Type&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;∞&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;10dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;5dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;-5dB&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;-10dB&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;A
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;1.8
   &lt;/td&gt;
   &lt;td&gt;3.2
   &lt;/td&gt;
   &lt;td&gt;10.9
   &lt;/td&gt;
   &lt;td&gt;27.9
   &lt;/td&gt;
   &lt;td&gt;55.5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;A+V
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;1.7
   &lt;/td&gt;
   &lt;td&gt;2.1
   &lt;/td&gt;
   &lt;td&gt;6.2
   &lt;/td&gt;
   &lt;td&gt;11.7
   &lt;/td&gt;
   &lt;td&gt;27.6
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 3&lt;/strong&gt;: Streaming evaluation WER (%) results at various signal-to-noise ratios for our audio-only (A) and audio-visual (A+V) models on the LRS3 dataset under 0.80-second latency constraints.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-time factor&lt;/strong&gt;. The real-time factor (RTF) is an important measure of a system’s ability to process real-time tasks efficiently. An RTF value of less than 1 indicates that the system meets real-time requirements. We measure RTF using a laptop with an Intel® Core™ i7-12700 CPU running at 2.70 GHz and an NVIDIA 3070 GeForce RTX 3070 Ti GPU. To the best of our knowledge, this is the first AV-ASR model that reports RTFs on the LRS3 benchmark. The Small model achieves a WER of 2.6% and an RTF of 0.87 on CPU (Table 4), demonstrating its potential for real-time on-device inference applications.&lt;/p&gt;

&lt;table class=&quot;table table-bordered text-center&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Device&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Streaming WER [%]&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;RTF&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Large
   &lt;/td&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;1.6
   &lt;/td&gt;
   &lt;td&gt;0.35
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Small
   &lt;/td&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;2.6
   &lt;/td&gt;
   &lt;td&gt;0.33
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPU
   &lt;/td&gt;
   &lt;td&gt;0.87
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Table 4&lt;/strong&gt;: Impact of AV-ASR model size and device on WER and RTF. Note that the RTF calculation includes the pre-processing step wherein the Ultra-Lightweight Face Detection Slim 320 model is used to generate face bounding boxes.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Learn more about the system from the published works below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shi, Yangyang, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Mike Seltzer. “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition.” In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6783-6787. IEEE, 2021.&lt;/li&gt;
  &lt;li&gt;Ma, Pingchuan, Alexandros Haliassos, Adriana Fernandez-Lopez, Honglie Chen, Stavros Petridis, and Maja Pantic. “Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels.” In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. IEEE, 2023.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">Audio-Visual Speech Recognition (AV-ASR, or AVSR) is the task of transcribing text from audio and visual streams, which has recently attracted a lot of research attention due to its robustness to noise. The vast majority of work to date has focused on developing AV-ASR models for non-streaming recognition; studies on streaming AV-ASR are very limited.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">High performance Llama 2 deployments with AWS Inferentia2 using TorchServe</title>
      <link href="https://pytorch.org/blog/high-performance-llama/" rel="alternate" type="text/html" title="High performance Llama 2 deployments with AWS Inferentia2 using TorchServe" />
      <published>2023-10-04T00:00:00-07:00</published>
      <updated>2023-10-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/high-performance-llama</id>
      <content type="html" xml:base="https://pytorch.org/blog/high-performance-llama/">&lt;p&gt;Recently, &lt;a href=&quot;https://ai.meta.com/llama/&quot;&gt;Llama 2&lt;/a&gt; was released and has attracted a lot of interest from the machine learning community. &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/inf2/&quot;&gt;Amazon EC2 Inf2 instances&lt;/a&gt;, powered by &lt;a href=&quot;https://aws.amazon.com/machine-learning/inferentia/&quot;&gt;AWS Inferentia2&lt;/a&gt;, now support training and inference of Llama 2 models. In this post, we show low-latency and cost-effective inference of Llama-2 models on Amazon EC2 Inf2 instances using the latest &lt;a href=&quot;https://aws.amazon.com/machine-learning/neuron/&quot;&gt;AWS Neuron SDK&lt;/a&gt; release.  We first introduce how to create, compile and deploy the Llama-2 model and explain the optimization techniques introduced by AWS Neuron SDK to achieve high performance at low cost. We then present our benchmarking results. Lastly, we show how the Llama-2 model can be deployed through Amazon SageMaker using TorchServe on an Inf2 instance. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/software_stack_inf2.jpg&quot; alt=&quot;Llama 2 is an auto-regressive language model that uses an optimized transformer architecture&quot; style=&quot;width:100%; max-width: 420px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-llama-2&quot;&gt;What is Llama 2&lt;/h2&gt;

&lt;p&gt;Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. Llama 2 is intended for commercial and research use in English. It comes in multiple sizes—7 billion, 13 billion, and 70 billion parameters—as well as pre-trained and fine-tuned variations. According to Meta, the tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. Llama 2 was pre-trained on 2 trillion tokens of data from publicly available sources. The tuned models are intended for assistant-like chat, whereas pre-trained models can be adapted for a variety of natural language generation tasks. Regardless of which version of the model a developer uses, the &lt;a href=&quot;https://ai.meta.com/llama/responsible-use-guide/&quot;&gt;responsible use guide from Meta &lt;/a&gt;can assist in guiding additional fine-tuning that may be necessary to customize and optimize the models with appropriate safety mitigations.&lt;/p&gt;

&lt;h2 id=&quot;amazon-ec2-inf2-instances-overview&quot;&gt;Amazon EC2 Inf2 instances Overview&lt;/h2&gt;

&lt;p&gt;Amazon EC2 Inf2 instances, featuring Inferentia2, provide 3x higher compute, 4x more accelerator memory, resulting in up to 4x higher throughput, and up to 10x lower latency, compared to the first generation Inf1 instances.&lt;/p&gt;

&lt;p&gt;Large language model (LLM) inference is a memory bound workload, performance scales up with more accelerator memory bandwidth. Inf2 instances are the only inference optimized instances in Amazon EC2 to provide high speed accelerator interconnect (NeuronLink) enabling high performance large LLM model deployments with cost effective distributed inference. You can now efficiently and cost-effectively deploy billion-scale LLMs across multiple accelerators on Inf2 instances.&lt;/p&gt;

&lt;p&gt;Inferentia2 supports FP32, TF32, BF16, FP16, UINT8, and the new configurable FP8 (cFP8) data type. AWS Neuron can take high-precision FP32 and FP16 models and autocast them to lower-precision data types while optimizing accuracy and performance. Autocasting reduces time to market by removing the need for lower-precision retraining and enabling higher-performance inference with smaller data types.&lt;/p&gt;

&lt;p&gt;To make it flexible and extendable to deploy constantly evolving deep learning models, Inf2 instances have hardware optimizations and software support for dynamic input shapes as well as custom operators written in C++ through the standard PyTorch custom operator programming interfaces.&lt;/p&gt;

&lt;h2 id=&quot;transformers-neuron-transformers-neuronx&quot;&gt;Transformers Neuron (transformers-neuronx)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx&quot;&gt;Transformers Neuron&lt;/a&gt; is a software package that enables PyTorch users to deploy performance optimized LLM inference. It has an optimized version of transformer models implemented with XLA high level operators (HLO), which enables sharding tensors across multiple NeuronCores, a.k.a. tensor parallelism, and performance optimizations such as parallel context encoding and KV caching for Neuron hardware. The Llama 2 source code in XLA HLOs can be found &lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx/blob/main/src/transformers_neuronx/llama/model.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Llama 2 is supported in Transformers Neuron through the &lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx/blob/33fa412447a4028edb252fd06aae9ed93086a450/src/transformers_neuronx/llama/model.py#L29&quot;&gt;LlamaForSampling&lt;/a&gt; class. Transformers Neuron provides a seamless user experience with Hugging Face models to provide optimized inference on Inf2 instances. More details can be found from the &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html#transformers-neuronx-developer-guide&quot;&gt;Transforms Neuron Developer Guide&lt;/a&gt;. In the following section, we will explain how to deploy the Llama-2 13B model using Transformers Neuron. And, this example also applies to other Llama-based models.&lt;/p&gt;

&lt;h2 id=&quot;llama-2-model-inference-with-transformers-neuron&quot;&gt;Llama 2 model inference with Transformers Neuron&lt;/h2&gt;

&lt;h3 id=&quot;create-model-compile-and-deploy&quot;&gt;Create model, compile and deploy&lt;/h3&gt;

&lt;p&gt;We have three simple steps here to create, compile and deploy the model on Inf2 instances.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a CPU model, use this &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/Readme.md?plain=1#L71&quot;&gt;script&lt;/a&gt; or the following code snippet to serialize and save checkpoints in a local directory.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from transformers import AutoModelForCausalLM
from transformers_neuronx.module import save_pretrained_split
model_cpu = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-13b-hf&quot;, low_cpu_mem_usage=True)
model_dir = &quot;./llama-2-13b-split&quot;
save_pretrained_split(model_cpu, model_dir)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Load and compile model from the local directory that you saved serialized checkpoints using the following.
To load the Llama 2 model, we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LlamaForSampling&lt;/code&gt; from Transformers Neuron. Note that the environment variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURON_RT_NUM_CORES&lt;/code&gt; specifies the number of NeuronCores to be used at runtime and it should match the tensor parallelism (TP) degree specified for the model. Also, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURON_CC_FLAGS&lt;/code&gt; enables compiler optimization on decoder-only LLM models.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from transformers_neuronx.llama.model import LlamaForSampling
os.environ['NEURON_RT_NUM_CORES'] = '24'
os.environ['NEURON_CC_FLAGS'] = '--model-type=transformer'
model = LlamaForSampling.from_pretrained(
        model_dir,
        batch_size=1,
        tp_degree=24,
        amp='bf16',
        n_positions=16,
        context_length_estimate=[8]
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p style=&quot;padding-left:6.25rem&quot;&gt;Now let's compile the model and load model weights into device memory with a one liner API.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.to_neuron()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Finally let’s run the inference on the compiled model. Note that both input and output of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sample&lt;/code&gt; function are a sequence of tokens.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;inputs = torch.tensor([[1, 16644, 31844, 312, 31876, 31836, 260, 3067, 2228, 31844]])
seq_len = 16
outputs = model.sample(inputs, seq_len, top_k=1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;inference-optimizations-in-transformers-neuron&quot;&gt;Inference optimizations in Transformers Neuron&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Tensor parallelism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_tp.jpg&quot; alt=&quot;Latency with different TP degrees&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Transformer Neuron implements parallel tensor operations across multiple NeuronCores. We denote the number of cores to be used for inference as TP degree. Larger TP degree provides higher memory bandwidth, leading to lower latency, as LLM token generation is a memory-IO bound workload. With increasing the TP degree, the inference latency has decreased significantly, our results shows, ~4x overall speed up with increased TP degrees from 2 to 24. For the Llama-2 7B model, latency decreases from 30.1 ms/token with 2 cores to 7.9 ms/token with 24 cores; similarly for the Llama-2 13B model, it goes down from 57.3 ms/token  to 11.1 ms/token.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parallel context encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the transformer architecture, tokens are produced in a sequential procedure called autoregressive sampling while input prompt tokens can be processed in parallel with parallel context encoding. This can significantly reduce the latency for input prompt context encoding before token generation through autoregressive sampling. By default, the parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;context_length_estimate&lt;/code&gt; would be set as a list of power-of-2 numbers which aims to cover a wide variety of context lengths. Depending on the use case, it can be set to custom numbers. This can be done when creating the Llama 2 model using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LlamaForSampling.from_pretrained&lt;/code&gt;. We characterize the impact of input token length on end-to-end (E2E) latency. As shown in the figure, latency for text generation with the Llama-2 7B model only slightly increases with bigger input prompts, thanks to parallel context encoding.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_input_token_length.jpg&quot; alt=&quot;E2E latency&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KV caching&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Self-attention block performs the self-attention operation with KV vectors. And, KV vectors are calculated using token embeddings and weights of KV and thus associated with tokens. In naive implementations, for each generated token, the entire KV cache is recalculated, but this reduces performance. Therefore Transformers Neuron library is reusing previously calculated KV vectors to avoid unnecessary computation, also known as KV caching, to reduce latency in the autoregressive sampling phase. &lt;/p&gt;

&lt;h3 id=&quot;benchmarking-results&quot;&gt;Benchmarking results&lt;/h3&gt;

&lt;p&gt;We benchmarked the latency and cost for both Llama-2 7B and 13B models under different conditions, i.e., number of output tokens, instance types. Unless specified, we use data type ‘bf16’ and batch size of 1 as this is a common configuration for real-time applications like chatbot and code assistant.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following graphs shows the per token latency on inf2.48xlarge instance with TP degree 24. Here, the latency per output token is calculated as the end-to-end latency divided by the number of output tokens. Our experiments show Llama-2 7B end-to-end latency to generate 256 tokens is 2x faster compared to other comparable inference-optimized EC2 instances. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_output_token_length.png&quot; alt=&quot;Latency on inf2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We now show the number of tokens generated per second for the Llama-2 7B and 13B models that can be delivered by the inf2.48xlarge instance. With TP degree 24, fully utilizing all the 24 NeuronCores, we can achieve 130 tokens/sec and 90 tokens/sec for the Llama-2 7B and 13B models, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/throughput_vs_output_token_length.jpg&quot; alt=&quot;E2E throughput&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For latency-first applications, we show the cost of hosting Llama-2 models on the inf2.48xlarge instance, &lt;strong&gt;$&lt;/strong&gt;0.011 per 1000 tokens and &lt;strong&gt;$&lt;/strong&gt;0.016 per 1000 tokens for the 7B and 13B models, respectively, which achieve 3x cost saving over other comparable inference-optimized EC2 instances. Note that we report the cost based on &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/inf2/&quot;&gt;3-year reserved instance price&lt;/a&gt; which is what customers use for large production deployments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/cost_vs_output_token_length_7b_13b.jpg&quot; alt=&quot;Cost on inf2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also compare the cost of hosting the Llama-2 7B model on inf2.xlarge and inf2.48xlarge instances. We can see that inf2.xlarge is more than 4x cheaper than inf2.48xlarge but at the expense of longer latency due to smaller TP degree. For example, it takes 7.9 ms for the model to generate 256 output tokens with 256 input tokens on inf2.48xlarge but 30.1 ms on Inf2.xlarge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/cost_vs_output_token_length_xl_48xl.jpg&quot; alt=&quot;Cost on Llama&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;serving-llama2-with-torchserve-on-ec2-inf2-instance&quot;&gt;Serving Llama2 with TorchServe on EC2 Inf2 instance&lt;/h2&gt;

&lt;p&gt;Now, we move on to model deployment. In this section, we show you how to deploy the &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-2-13b-hf&quot;&gt;Llama-2 13B model&lt;/a&gt; through SageMaker using TorchServe, which is the recommended model server for PyTorch, preinstalled in the AWS PyTorch Deep Learning Containers (DLC).&lt;/p&gt;

&lt;p&gt;This section describes the preparation work needed for using TorchServe, particularly, how to configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_config.yaml&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inf2_handler.py&lt;/code&gt; as well as how to generate model artifacts and pre-compile the model for use in later model deployment. Preparing the model artifacts ahead-of-time avoids model compilation during model deployment and thus reduces the model loading time.&lt;/p&gt;

&lt;h3 id=&quot;model-configurationmodel-configyaml&quot;&gt;Model configuration &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/model-config.yaml&quot;&gt;model-config.yaml&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The parameters defined in section &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handler&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;micro_batching&lt;/code&gt; are used in customer handler &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/inf2_handler.py&quot;&gt;inf2_handler.py&lt;/a&gt;. More details about model_config.yaml are &lt;a href=&quot;https://github.com/pytorch/serve/blob/2bf505bae3046b0f7d0900727ec36e611bb5dca3/docs/configuration.md?plain=1#L267&quot;&gt;here&lt;/a&gt;. TorchServe micro-batching is a mechanism to pre-process and post-process a batch of inference requests in parallel. It is able to achieve higher throughput by better utilizing the available accelerator when the backend is steadily fed with incoming data, see &lt;a href=&quot;https://github.com/pytorch/serve/tree/master/examples/micro_batching&quot;&gt;here&lt;/a&gt; for more details. For model inference on Inf2, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;micro_batch_size, amp, tp_degree and max_length&lt;/code&gt; specify the batch size, data type, tensor parallelism degree and max sequence length, respectively.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TorchServe Frontend Parameters
minWorkers: 1
maxWorkers: 1
maxBatchDelay: 100
responseTimeout: 10800
batchSize: 16

# TorchServe Backend Custom Handler Parameters
handler:
    model_checkpoint_dir: &quot;llama-2-13b-split&quot;
    amp: &quot;bf16&quot;
    tp_degree: 12
    max_length: 100

micro_batching:
    # Used by batch_size in function LlamaForSampling.from_pretrained
    micro_batch_size: 1  
    parallelism:
        preprocess: 2
        inference: 1
        postprocess: 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;custom-handlerinf2_handlerpy&quot;&gt;Custom handler &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/inf2_handler.py&quot;&gt;inf2_handler.py&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Custom handler in Torchserve is a simple Python script that lets you define the model initialization, preprocessing, inference and post-processing logic as functions. Here, we create our Inf2 custom handler.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/inf2_handler.py#L33&quot;&gt;initialize&lt;/a&gt; function is used to load the model. Here, Neuron SDK will compile the model for the first time and save the precompiled model in the directory as enabled by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURONX_CACHE&lt;/code&gt; in the directory specified by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURONX_DUMP_TO&lt;/code&gt;. After the first time, subsequent runs will check if there are already pre-compiled model artifacts. If so, it will skip model compilation.
Once the model is loaded, we initiate warm-up inference requests so that the compiled version is cached. When the &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html&quot;&gt;neuron persistent cache &lt;/a&gt;is utilized, it can significantly reduce the model loading latency, ensuring that the subsequent inference runs swiftly.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;os.environ[&quot;NEURONX_CACHE&quot;] = &quot;on&quot;
os.environ[&quot;NEURONX_DUMP_TO&quot;] = f&quot;{model_dir}/neuron_cache&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p style=&quot;padding-left:6.25rem&quot;&gt;TorchServe `TextIteratorStreamerBatch` extends Hugging Face transformers `BaseStreamer` to support response streaming when `batchSize` is larger than 1. &lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.output_streamer = TextIteratorStreamerBatch(
    self.tokenizer,
    batch_size=self.handle.micro_batch_size,
    skip_special_tokens=True,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/inf2_handler.py#L124&quot;&gt;inference&lt;/a&gt; function calls send_intermediate_predict_response to send the streaming response.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for new_text in self.output_streamer:
    logger.debug(&quot;send response stream&quot;)
    send_intermediate_predict_response(
        new_text[: len(micro_batch_req_id_map)],
        micro_batch_req_id_map,
        &quot;Intermediate Prediction success&quot;,
        200,
        self.context,
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;package-model-artifacts&quot;&gt;Package model artifacts&lt;/h3&gt;

&lt;p&gt;Package all the model artifacts into a folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1&lt;/code&gt; using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch-model-archiver&lt;/code&gt;. &lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch-model-archiver --model-name llama-2-13b-neuronx-b1 --version 1.0 --handler inf2_handler.py -r requirements.txt --config-file model-config.yaml --archive-format no-archive
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;serve-the-model&quot;&gt;Serve the model&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export TS_INSTALL_PY_DEP_PER_MODEL=&quot;true&quot;
torchserve --ncs --start --model-store model_store --models llama-2-13b-neuronx-b1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the log shows “&lt;strong&gt;WORKER_MODEL_LOADED&lt;/strong&gt;”, the pre-compiled model should be saved in the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1/neuron_cache&lt;/code&gt;, which is tightly coupled with Neuron SDK version. Then, upload the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1&lt;/code&gt; to your S3 bucket for later use in the product deployment. The Llama-2 13B model artifacts in this blog can be found &lt;a href=&quot;https://torchserve.s3.amazonaws.com/mar_files/sm-neuronx/llama-2-13b-neuronx-b1/&quot;&gt;here&lt;/a&gt;, which is associated with Neuron SDK 2.13.2, in the TorchServe model zoo.&lt;/p&gt;

&lt;h2 id=&quot;deploy-llama-2-13b-model-on-sagemakerinf2-instance-using-torchserve&quot;&gt;Deploy Llama-2 13B model on SageMaker Inf2 instance using TorchServe &lt;/h2&gt;

&lt;p&gt;In this section, we deploy the Llama-2 13B model using a &lt;a href=&quot;https://github.com/aws/deep-learning-containers/blob/master/available_images.md#neuron-containers&quot;&gt;PyTorch Neuronx container&lt;/a&gt; on a SageMaker endpoint with an ml.inf2.24xlarge hosting instance, which has 6 Inferentia2 accelerators corresponding to our model configuration &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_config.yaml&lt;/code&gt; handler’s setting - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tp_degree: 12&lt;/code&gt;. Given that we have packaged all the model artifacts into a folder using &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/model-archiver/README.md&quot;&gt;torch-model-archiver&lt;/a&gt; and uploaded to S3 bucket, we will now use the SageMaker Python SDK to create a SageMaker model and deploy it to a SageMaker real-time endpoint using the deploy &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html&quot;&gt;uncompressed model method&lt;/a&gt;. Speed is the key benefit to deploying in this manner with SageMaker and you get a fully functional production ready endpoint complete with a secure RESTful endpoint without any effort spent on infrastructure. There are 3 steps to deploying the model and running inference on SageMaker. The notebook example can be found &lt;a href=&quot;https://github.com/aws/amazon-sagemaker-examples-community/blob/main/torchserve/inf2/llama2/llama-2-13b.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a SageMaker model&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from datetime import datetime

instance_type = &quot;ml.inf2.24xlarge&quot;
endpoint_name = sagemaker.utils.name_from_base(&quot;ts-inf2-llama2-13b-b1&quot;)

model = Model(
    name=&quot;torchserve-inf2-llama2-13b&quot; + datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;),
    # Enable SageMaker uncompressed model artifacts
    model_data={
        &quot;S3DataSource&quot;: {
                &quot;S3Uri&quot;: s3_uri,
                &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
                &quot;CompressionType&quot;: &quot;None&quot;,
        }
    },
    image_uri=container,
    role=role,
    sagemaker_session=sess,
    env={&quot;TS_INSTALL_PY_DEP_PER_MODEL&quot;: &quot;true&quot;},
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Deploy a SageMaker model&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.deploy(
    initial_instance_count=1,
    instance_type=instance_type,
    endpoint_name=endpoint_name,
    volume_size=512, # increase the size to store large model
    model_data_download_timeout=3600, # increase the timeout to download large model
    container_startup_health_check_timeout=600, # increase the timeout to load large model
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Run streaming response inference on SageMaker
When the endpoint is in service, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invoke_endpoint_with_response_stream&lt;/code&gt; API call to invoke the model. This feature enables the return of each generated token to the user, enhancing the user experience. It’s especially beneficial when generating an entire sequence is time-consuming.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import json

body = &quot;Today the weather is really nice and I am planning on&quot;.encode('utf-8')
resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=body, ContentType=&quot;application/json&quot;)
event_stream = resp['Body']
parser = Parser()
for event in event_stream:
    parser.write(event['PayloadPart']['Bytes'])
    for line in parser.scan_lines():
        print(line.decode(&quot;utf-8&quot;), end=' ')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;sample-inference&quot;&gt;Sample inference:&lt;/h3&gt;

&lt;p&gt;Input&lt;/p&gt;

&lt;p&gt;“Today the weather is really nice and I am planning on”&lt;/p&gt;

&lt;p&gt;Output&lt;/p&gt;

&lt;p&gt;“Today the weather is really nice and I am planning on going to the beach. I am going to take my camera and take some pictures of the beach. I am going to take pictures of the sand, the water, and the people. I am also going to take pictures of the sunset. I am really excited to go to the beach and take pictures.&lt;/p&gt;

&lt;p&gt;The beach is a great place to take pictures. The sand, the water, and the people are all great subjects for pictures. The sunset is also a great subject for pictures.”&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we showcased how to run Llama 2 model inference using Transformers Neuron and deploy Llama 2 model serving using TorchServe through Amazon SageMaker on an EC2 Inf2 instance. We demonstrated the benefits of using Inferentia2—low latency and low cost—enabled by optimizations in AWS Neuron SDK including tensor parallelism, parallel context encoding and KV caching, particularly for LLM inference. To stay up to date, please follow &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html&quot;&gt;AWS Neuron’s latest release&lt;/a&gt; for new features.&lt;/p&gt;

&lt;p&gt;Get started today with Llama 2 examples on &lt;a href=&quot;https://github.com/aws-neuron/aws-neuron-samples/blob/master/torch-neuronx/transformers-neuronx/inference/meta-llama-2-13b-sampling.ipynb&quot;&gt;EC2&lt;/a&gt; and through &lt;a href=&quot;https://github.com/aws/amazon-sagemaker-examples-community/blob/main/torchserve/inf2/llama2/llama-2-13b.ipynb&quot;&gt;SageMaker&lt;/a&gt; and stay tuned for how to optimize Llama 70B on Inf2!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Mike Zhang, Li Ning, Sergey Ivanov, Naman Nandan, Hamid Shojanazeri, Geeta Chauhan, Abhi Shivaditya, Michael Nguyen, Pinak Panigrahi</name>
        
        
      </author>

      

      

      
        <summary type="html">Recently, Llama 2 was released and has attracted a lot of interest from the machine learning community. Amazon EC2 Inf2 instances, powered by AWS Inferentia2, now support training and inference of Llama 2 models. In this post, we show low-latency and cost-effective inference of Llama-2 models on Amazon EC2 Inf2 instances using the latest AWS Neuron SDK release.  We first introduce how to create, compile and deploy the Llama-2 model and explain the optimization techniques introduced by AWS Neuron SDK to achieve high performance at low cost. We then present our benchmarking results. Lastly, we show how the Llama-2 model can be deployed through Amazon SageMaker using TorchServe on an Inf2 instance. </summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">New Library Updates in PyTorch 2.1</title>
      <link href="https://pytorch.org/blog/new-library-updates/" rel="alternate" type="text/html" title="New Library Updates in PyTorch 2.1" />
      <published>2023-10-04T00:00:00-07:00</published>
      <updated>2023-10-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/new-library-updates</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-library-updates/">&lt;h2 id=&quot;summary&quot;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 2.1 release. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch. &lt;/p&gt;

&lt;p&gt;Along with 2.1, we are also releasing a series of beta updates to the PyTorch domain libraries including TorchAudio and TorchVision. Please find the list of the latest stable versions and updates below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Latest Stable Library Versions&lt;/th&gt;
      &lt;th&gt;(&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;Full List&lt;/a&gt;)*&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;TorchArrow 0.1.0&lt;/td&gt;
      &lt;td&gt;TorchRec 0.5.0&lt;/td&gt;
      &lt;td&gt;TorchVision 0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TorchAudio 2.1&lt;/td&gt;
      &lt;td&gt;TorchServe 0.8.2&lt;/td&gt;
      &lt;td&gt;TorchX 0.5.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TorchData 0.7.0&lt;/td&gt;
      &lt;td&gt;TorchText 0.16.0&lt;/td&gt;
      &lt;td&gt;PyTorch on XLA Devices 1.14&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;*To see &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;prior versions&lt;/a&gt; or (unstable) nightlies, click on versions in the top left menu above ‘Search Docs’.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio&quot;&gt;&lt;strong&gt;TorchAudio&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;TorchAudio v2.1 introduces the following new features and backward-incompatible changes:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] A new API to apply filter, effects and codec&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;`torchaudio.io.AudioEffector` can apply filters, effects and encodings to waveforms in online/offline fashion. You can use it as a form of augmentation.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/effector_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/effector_tutorial.html&lt;/a&gt; for the usage and examples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] Tools for Forced alignment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;New functions and a pre-trained model for forced alignment were added. `torchaudio.functional.forced_align` computes alignment from an emission and `torchaudio.pipelines.MMS_FA` provides access to the model trained for multilingual forced alignment in &lt;a href=&quot;https://ai.meta.com/blog/multilingual-model-speech-recognition/&quot;&gt;MMS: Scaling Speech Technology to 1000+ languages&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/ctc_forced_alignment_api_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/ctc_forced_alignment_api_tutorial.html&lt;/a&gt; for the usage of `forced_align` function, and &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/forced_alignment_for_multilingual_data_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/forced_alignment_for_multilingual_data_tutorial.html&lt;/a&gt; for how one can use `MMS_FA` to align transcript in multiple languages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] TorchAudio-Squim : Models for reference-free speech assessment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Model architectures and pre-trained models from the paper &lt;a href=&quot;https://arxiv.org/abs/2304.01448&quot;&gt;TorchAudio-Sequim: Reference-less Speech Quality and Intelligibility measures in TorchAudio&lt;/a&gt; were added.&lt;/p&gt;

&lt;p&gt;You can use the pre-trained models `torchaudio.pipelines.SQUIM_SUBJECTIVE` and `torchaudio.pipelines.SQUIM_OBJECTIVE`. They can estimate the various speech quality and intelligibility metrics (e.g. STOI, wideband PESQ, Si-SDR, and MOS). This is helpful when evaluating the quality of speech generation models, such as Text-to-Speech (TTS).&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/squim_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/squim_tutorial.html&lt;/a&gt; for the details.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] CUDA-based CTC decoder&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;`torchaudio.models.decoder.CUCTCDecoder` performs CTC beam search in CUDA devices. The beam search is fast. It eliminates the need to move data from CUDA device to CPU when performing automatic speech recognition. With PyTorch’s CUDA support, it is now possible to perform the entire speech recognition pipeline in CUDA.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/tutorials/asr_inference_with_cuda_ctc_decoder_tutorial.html&quot;&gt;https://pytorch.org/audio/2.1/tutorials/asr_inference_with_cuda_ctc_decoder_tutorial.html&lt;/a&gt; for the detail.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] Utilities for AI music generation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are working to add utilities that are relevant to music AI. Since the last release, the following APIs were added to the prototype.&lt;/p&gt;

&lt;p&gt;Please refer to respective documentation for the usage.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/main/generated/torchaudio.prototype.functional.chroma_filterbank.html&quot;&gt;torchaudio.prototype.chroma_filterbank&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/main/generated/torchaudio.prototype.transforms.ChromaScale.html&quot;&gt;torchaudio.prototype.transforms.ChromaScale&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/main/generated/torchaudio.prototype.transforms.ChromaSpectrogram.html&quot;&gt;torchaudio.prototype.transforms.ChromaSpectrogram&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/main/generated/torchaudio.prototype.pipelines.VGGISH.html&quot;&gt;torchaudio.prototype.pipelines.VGGISH&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;New recipes for training models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recipes for Audio-visual ASR, multi-channel DNN beamforming and TCPGen context-biasing were added.&lt;/p&gt;

&lt;p&gt;Please refer to the recipes&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/audio/tree/release/2.1/examples/avsr&quot;&gt;https://github.com/pytorch/audio/tree/release/2.1/examples/avsr&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/audio/tree/release/2.1/examples/dnn_beamformer&quot;&gt;https://github.com/pytorch/audio/tree/release/2.1/examples/dnn_beamformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/audio/tree/release/2.1/examples/asr/librispeech_conformer_rnnt_biasing&quot;&gt;https://github.com/pytorch/audio/tree/release/2.1/examples/asr/librispeech_conformer_rnnt_biasing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Update to FFmpeg support&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The version of supported FFmpeg libraries was updated. TorchAudio v2.1 works with FFmpeg 6, 5 and 4.4. The support for 4.3, 4.2 and 4.1 are dropped.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/installation.html#optional-dependencies&quot;&gt;https://pytorch.org/audio/2.1/installation.html#optional-dependencies&lt;/a&gt; for the detail of the new FFmpeg integration mechanism.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update to libsox integration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TorchAudio now depends on libsox installed separately from torchaudio. Sox I/O backend no longer supports file-like objects. (This is supported by FFmpeg backend and soundfile.)&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://pytorch.org/audio/2.1/installation.html#optional-dependencies&quot;&gt;https://pytorch.org/audio/2.1/installation.html#optional-dependencies&lt;/a&gt; for the details.&lt;/p&gt;

&lt;h2 id=&quot;torchrl&quot;&gt;TorchRL&lt;/h2&gt;

&lt;p&gt;Our RLHF components make it easy to build an RLHF training loop with limited RL knowledge. TensorDict enables an easy interaction between datasets (eg, HF datasets) and RL models. The new algorithms we provide deliver a wide range of solutions for offline RL training, which is more data efficient.&lt;/p&gt;

&lt;p&gt;Through RoboHive and IsaacGym, TorchRL now provides a built-in interface with hardware (robots), tying training at scale with policy deployment on device. Thanks to SMAC, VMAS, and PettingZoo and related MARL-oriented losses, TorchRL is now fully capable of training complex policies in multi-agent settings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;[BETA] We integrate some RLHF components and examples: we provide building blocks for data formatting in RL frameworks, reward model design, specific transforms that enable efficient learning (eg. KL correction) and training scripts&lt;/li&gt;
  &lt;li&gt;[Stable] New algorithms include Decision transformers, CQL, multi-agent losses such as MAPPO and QMixer.&lt;strong&gt;New features&lt;/strong&gt;- [Stable] New transforms such as Visual Cortex 1 (VC1), a foundational model for RL. &lt;/li&gt;
  &lt;li&gt;We widened the panel of library covered by TorchRL: 
    &lt;ul&gt;
      &lt;li&gt;[Beta] IsaacGym, a powerful GPU-based simulator that allows interaction and rendering of thousands of vectorized environments by NVIDIA.&lt;/li&gt;
      &lt;li&gt;[Stable] PettingZoo, a multi-agent library by the Farama Foundation.&lt;/li&gt;
      &lt;li&gt;[Stable] SMAC-v2, the new Starcraft Multi-agent simulator&lt;/li&gt;
      &lt;li&gt;[Stable] RoboHive, a collection of environments/tasks simulated with the MuJoCo physics engine.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Performance improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We provide faster data collection through refactoring and integration of SB3 and Gym asynchronous environments execution. We also made our value functions faster to execute.&lt;/p&gt;

&lt;h2 id=&quot;torchrec&quot;&gt;TorchRec&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] Zero Collision / Managed Collision Embedding Bags&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A common constraint in Recommender Systems is the sparse id input range is larger than the number of embeddings the model can learn for a given parameter size.   To resolve this issue, the conventional solution is to hash sparse ids into the same size range as the embedding table.  This will ultimately lead to hash collisions, with multiple sparse ids sharing the same embedding space.   We have developed a performant alternative algorithm that attempts to address this problem by tracking the &lt;em&gt;N&lt;/em&gt; most common sparse ids and ensuring that they have a unique embedding representation. The module is defined &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/b992eebd80e8ccfc3b96a7fd39cb072c17e8907d/torchrec/modules/mc_embedding_modules.py#L26&quot;&gt;here&lt;/a&gt; and an example can be found &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/b992eebd80e8ccfc3b96a7fd39cb072c17e8907d/torchrec/modules/mc_embedding_modules.py#L26&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] UVM Caching - Prefetch Training Pipeline&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For tables where on-device memory is insufficient to hold the entire embedding table, it is common to leverage a caching architecture where part of the embedding table is cached on device and the full embedding table is on host memory (typically DDR SDRAM).   However, in practice, caching misses are common, and hurt performance due to relatively high latency of going to host memory.   Building on TorchRec’s existing data pipelining, we developed a new &lt;a href=&quot;https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.train_pipeline.PrefetchPipelinedForward&quot;&gt;&lt;em&gt;Prefetch Training Pipeline&lt;/em&gt;&lt;/a&gt; to avoid these cache misses by prefetching the relevant embeddings for upcoming batch from host memory, effectively eliminating cache misses in the forward path.&lt;/p&gt;

&lt;h2 id=&quot;torchvision&quot;&gt;TorchVision &lt;/h2&gt;
&lt;h3 id=&quot;transforms-and-augmentations&quot;&gt;&lt;strong&gt;Transforms and augmentations&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Major speedups&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The new transforms in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.transforms.v2&lt;/code&gt; are now&lt;a href=&quot;https://github.com/pytorch/vision/issues/7497#issuecomment-1557478635&quot;&gt; 10%-40% faster&lt;/a&gt; than before! This is mostly achieved thanks to 2X-4X improvements made to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v2.Resize()&lt;/code&gt;, which now supports native &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8&lt;/code&gt; tensors for Bilinear and Bicubic mode. Output results are also now closer to PIL’s! Check out our&lt;a href=&quot;https://pytorch.org/vision/stable/transforms.html#performance-considerations&quot;&gt; performance recommendations&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;p&gt;Additionally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision&lt;/code&gt; now ships with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libjpeg-turbo&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libjpeg&lt;/code&gt;, which should significantly speed-up the jpeg decoding utilities (&lt;a href=&quot;https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_image&lt;/code&gt;&lt;/a&gt;,&lt;a href=&quot;https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.decode_jpeg&quot;&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;decode_jpeg&lt;/code&gt;&lt;/a&gt;), and avoid compatibility issues with PIL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CutMix and MixUp&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Long-awaited support for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CutMix&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MixUp&lt;/code&gt; augmentations is now here! Check&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py&quot;&gt; our tutorial&lt;/a&gt; to learn how to use them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Towards stable V2 transforms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the&lt;a href=&quot;https://github.com/pytorch/vision/releases/tag/v0.15.1&quot;&gt; previous release 0.15&lt;/a&gt; we BETA-released a new set of transforms in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.transforms.v2&lt;/code&gt; with native support for tasks like segmentation, detection, or videos. We have now stabilized the design decisions of these transforms and made further improvements in terms of speedups, usability, new transforms support, etc.&lt;/p&gt;

&lt;p&gt;We’re keeping the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.transforms.v2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.tv_tensors&lt;/code&gt; namespaces as BETA until 0.17 out of precaution, but we do not expect disruptive API changes in the future.&lt;/p&gt;

&lt;p&gt;Whether you’re new to Torchvision transforms, or you’re already experienced with them, we encourage you to start with&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py&quot;&gt; Getting started with transforms v2&lt;/a&gt; in order to learn more about what can be done with the new v2 transforms.&lt;/p&gt;

&lt;p&gt;Browse our&lt;a href=&quot;https://pytorch.org/vision/stable/transforms.html#&quot;&gt; main docs&lt;/a&gt; for general information and performance tips. The available transforms and functionals are listed in the&lt;a href=&quot;https://pytorch.org/vision/stable/transforms.html#v2-api-ref&quot;&gt; API reference&lt;/a&gt;. Additional information and tutorials can also be found in our&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/index.html#gallery&quot;&gt; example gallery&lt;/a&gt;, e.g.&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_e2e.html#sphx-glr-auto-examples-transforms-plot-transforms-e2e-py&quot;&gt; Transforms v2: End-to-end object detection/segmentation example&lt;/a&gt; or&lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_custom_transforms.html#sphx-glr-auto-examples-transforms-plot-custom-transforms-py&quot;&gt; How to write your own v2 transforms&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-mps-support&quot;&gt;[BETA] MPS support&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nms&lt;/code&gt; and roi-align kernels (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roi_align&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roi_pool&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps_roi_align&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps_roi_pool&lt;/code&gt;) now support MPS. Thanks to&lt;a href=&quot;https://github.com/qqaatw&quot;&gt; Li-Huai (Allan) Lin&lt;/a&gt; for this contribution!&lt;/p&gt;

&lt;h2 id=&quot;torchx&quot;&gt;TorchX&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Schedulers&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;[Prototype] Kubernetes MCAD Scheduler: Integration for easily scheduling jobs on Multi-Cluster-Application-Dispatcher (MCAD)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AWS Batch &lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Add privileged option to enable running containers on EFA enabled instances with elevated networking permissions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;torchx-tracker&quot;&gt;&lt;strong&gt;TorchX Tracker&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;[Prototype] MLFlow backend for TorchX Tracker: in addition to &lt;em&gt;fsspec&lt;/em&gt; based tracker, TorchX can use MLFlow instance to track metadata/experiments &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Components&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;dist.spmd&lt;/em&gt; component to support Single-Process-Multiple-Data style applications&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Workspace&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Add ability to access image and workspace path from Dockerfile while building docker workspace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Release includes number of other bugfixes.&lt;/p&gt;

&lt;p&gt;To learn more about Torchx visit &lt;a href=&quot;https://pytorch.org/torchx/latest/&quot;&gt;https://pytorch.org/torchx/latest/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;torchtext-and-torchdata&quot;&gt;TorchText and TorchData&lt;/h2&gt;

&lt;p&gt;As of September 2023 we have paused active development of TorchText and TorchData as we re-evaluate how we want to serve the needs of the community in this space.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing</title>
      <link href="https://pytorch.org/blog/pytorch-2-1/" rel="alternate" type="text/html" title="PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing" />
      <published>2023-10-04T00:00:00-07:00</published>
      <updated>2023-10-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-1</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-1/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.1 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.1.0&quot;&gt;release note&lt;/a&gt;)! PyTorch 2.1 offers automatic dynamic shape support in &lt;em&gt;torch.compile&lt;/em&gt;, &lt;em&gt;torch.distributed.checkpoint&lt;/em&gt; for saving/loading distributed training jobs on multiple ranks in parallel, and &lt;em&gt;torch.compile&lt;/em&gt; support for the NumPy API.&lt;/p&gt;

&lt;p&gt;In addition, this release offers numerous performance improvements (e.g. CPU inductor improvements, AVX512 support, scaled-dot-product-attention support) as well as a prototype release of &lt;em&gt;torch.export&lt;/em&gt;, a sound full-graph capture mechanism, and &lt;em&gt;torch.export&lt;/em&gt;-based quantization.&lt;/p&gt;

&lt;p&gt;Along with 2.1, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog. &lt;/p&gt;

&lt;p&gt;This release is composed of 6,682 commits and 784 contributors since 2.0. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.1.  More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;Summary: &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; now includes automatic support for detecting and minimizing recompilations due to tensor shape changes using &lt;em&gt;automatic dynamic shapes.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.distributed.checkpoint&lt;/em&gt; enables saving and loading models from multiple ranks in parallel, as well as resharding due to changes in cluster topology.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; can now compile NumPy operations via translating them into PyTorch-equivalent operations.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.compile&lt;/em&gt; now includes improved support for Python 3.11.&lt;/li&gt;
  &lt;li&gt;New CPU performance features include inductor improvements (e.g. bfloat16 support and dynamic shapes), AVX512 kernel support, and scaled-dot-product-attention kernels.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.export&lt;/em&gt;, a sound full-graph capture mechanism is introduced as a prototype feature, as well as &lt;em&gt;torch.export&lt;/em&gt;-based quantization.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.sparse&lt;/em&gt; now includes prototype support for semi-structured (2:4) sparsity on NVIDIA® GPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Stable&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Beta&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Prototype&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Automatic Dynamic Shapes&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.export()&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;AVX512 kernel support&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.distributed.checkpoint&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;Torch.export-based Quantization&lt;/td&gt;
      &lt;td&gt;CPU optimizations for scaled-dot-product-attention (SPDA)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.compile&lt;/em&gt; + NumPy&lt;/td&gt;
      &lt;td&gt;semi-structed (2:4) sparsity&lt;/td&gt;
      &lt;td&gt;CPU optimizations for bfloat16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.compile&lt;/em&gt; + Python 3.11&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;cpp_wrapper&lt;/em&gt; for torchinductor&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;torch.compile + autograd.Function&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;third-party device integration: &lt;em&gt;PrivateUse1&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public 2.1, 2.0, and 1.13 feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;&lt;strong&gt;Beta Features&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;(Beta) Automatic Dynamic Shapes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dynamic shapes is functionality built into &lt;em&gt;torch.compile&lt;/em&gt; that can minimize recompilations by tracking and generating code based on the symbolic shape of a tensor rather than the static shape (e.g. &lt;em&gt;[B, 128, 4]&lt;/em&gt; rather than &lt;em&gt;[64, 128, 4]&lt;/em&gt;). This allows &lt;em&gt;torch.compile&lt;/em&gt; to generate a single kernel that can work for many sizes, at only a modest cost to efficiency. Dynamic shapes has been greatly stabilized in PyTorch 2.1, and is now automatically enabled if &lt;em&gt;torch.compile&lt;/em&gt; notices recompilation due to varying input shapes. You can disable automatic dynamic by passing &lt;em&gt;dynamic=False&lt;/em&gt; to torch.compile, or by setting &lt;em&gt;torch._dynamo.config.automatic_dynamic_shapes = False&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In PyTorch 2.1, we have shown good performance with dynamic shapes enabled on a variety of model types, including large language models, on both CUDA and CPU.&lt;/p&gt;

&lt;p&gt;For more information on dynamic shapes, see &lt;a href=&quot;https://pytorch.org/docs/2.1/torch.compiler_dynamic_shapes.html&quot;&gt;this documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] &lt;em&gt;torch.distributed.checkpoint&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.distributed.checkpoint&lt;/em&gt; enables saving and loading models from multiple ranks in parallel. In addition, checkpointing automatically handles fully-qualified-name (FQN) mappings across models and optimizers, enabling load-time resharding across differing cluster topologies.&lt;/p&gt;

&lt;p&gt;For more information, see &lt;em&gt;torch.distributed.checkpoint&lt;/em&gt; &lt;a href=&quot;https://pytorch.org/docs/2.1/distributed.checkpoint.html&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] &lt;em&gt;torch.compile&lt;/em&gt; + &lt;em&gt;NumPy&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.compile&lt;/em&gt; now understands how to compile NumPy operations via translating them into PyTorch-equivalent operations.  Because this integration operates in a device-agnostic manner, you can now GPU-accelerate NumPy programs – or even mixed NumPy/PyTorch programs – just by using &lt;em&gt;torch.compile&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Please see &lt;a href=&quot;https://pytorch.org/docs/2.1/torch.compiler_faq.html#does-numpy-work-with-torch-compile&quot;&gt;this section&lt;/a&gt; in the &lt;em&gt;torch.compile&lt;/em&gt; FAQ for more information about &lt;em&gt;torch.compile + NumPy interaction&lt;/em&gt;, and follow the &lt;a href=&quot;https://pytorch.org/blog/&quot;&gt;PyTorch Blog&lt;/a&gt; for a forthcoming blog about this feature.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] &lt;em&gt;torch.compile&lt;/em&gt; + Python 3.11&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.compile&lt;/em&gt; previously only supported Python versions 3.8-3.10. Users can now optimize models with &lt;em&gt;torch.compile&lt;/em&gt; in Python 3.11.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] &lt;em&gt;torch.compile&lt;/em&gt; + &lt;em&gt;autograd.Function&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.compile&lt;/em&gt; can now trace and optimize the backward function of user-defined &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#function&quot;&gt;autograd Functions&lt;/a&gt;, which unlocks training optimizations for models that make heavier use of extensions mechanisms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Beta] Improved third-party device support: &lt;em&gt;PrivateUse1&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Third-party device types can now be registered to PyTorch using the privateuse1 dispatch key.  This allows device extensions to register new kernels to PyTorch and to associate them with the new key, allowing user code to work equivalently to built-in device types.  For example, to register &lt;em&gt;“my_hardware_device&lt;/em&gt;”, one can do the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.rename_privateuse1_backend(&quot;my_hardware_device&quot;)
torch.utils.generate_methods_for_privateuse1_backend()
x = torch.randn((2, 3), device='my_hardware_device')
y = x + x # run add kernel on 'my_hardware_device'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To validate this feature, the OSS team from &lt;em&gt;Ascend NPU&lt;/em&gt; has successfully integrated &lt;a href=&quot;https://github.com/Ascend/pytorch&quot;&gt;&lt;strong&gt;torch_npu&lt;/strong&gt;&lt;/a&gt; into pytorch as a plug-in through the &lt;em&gt;PrivateUse1&lt;/em&gt; functionality.&lt;/p&gt;

&lt;p&gt;For more information, please see the PrivateUse1 tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/privateuseone.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;&lt;strong&gt;Prototype Features&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] &lt;em&gt;torch.export()&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.export()&lt;/em&gt; provides a sound tracing mechanism to capture a full graph from a PyTorch program based on new technologies provided by PT2.0.&lt;/p&gt;

&lt;p&gt;Users can extract a clean representation (Export IR) of a PyTorch program in the form of a dataflow graph, consisting of mostly straight-line calls to PyTorch operators. Export IR can then be transformed, serialized, saved to file, transferred, loaded back for execution in an environment with or without Python.&lt;/p&gt;

&lt;p&gt;For more information, please see the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] &lt;em&gt;torch.export&lt;/em&gt;-based Quantization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.ao.quantization&lt;/em&gt; now supports quantization on PyTorch 2 &lt;em&gt;torch.export&lt;/em&gt;-based flows.  This includes support for built-in &lt;em&gt;XNNPACK&lt;/em&gt; and &lt;em&gt;X64Inductor&lt;/em&gt; &lt;em&gt;Quantizer&lt;/em&gt;, as well as the ability to specify one’s own &lt;em&gt;Quantizer&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For an explanation on post-training static quantization with torch.export, see &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html&quot;&gt;this tutorial&lt;/a&gt;, for quantization-aware training for static quantization with torch.export, see &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For an explanation on how to write one’s own Quantizer, see &lt;a href=&quot;https://pytorch.org/tutorials/prototype/pt2e_quantizer.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] semi-structured (2:4) sparsity for NVIDIA® GPUs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;torch.sparse&lt;/em&gt; now supports creating and accelerating compute over semi-structured sparse (2:4) tensors.  For more information on the format, see &lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/&quot;&gt;this&lt;/a&gt; blog from NVIDIA.A minimal example introducing semi-structured sparsity is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch.sparse import to_sparse_semi_structured
 
x = torch.rand(64, 64).half().cuda()
mask = torch.tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()
linear = nn.Linear(64, 64).half().cuda()

linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))
linear(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To learn more, please see the &lt;a href=&quot;https://pytorch.org/docs/2.1/sparse.html#sparse-semi-structured-tensors&quot;&gt;documentation&lt;/a&gt; and accompanying &lt;a href=&quot;https://pytorch.org/tutorials/prototype/semi_structured_sparse.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Prototype] &lt;em&gt;cpp_wrapper&lt;/em&gt; for &lt;em&gt;torchinductor&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;cpp_wrapper&lt;/em&gt; can reduce the Python overhead for invoking kernels in torchinductor by generating the kernel wrapper code in C++. This feature is still in the prototype phase; it does not support all programs that successfully compile in PT2 today. Please file issues if you discover limitations for your use case to help us prioritize.&lt;/p&gt;

&lt;p&gt;The API to turn this feature on is:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torch._inductor.config as config
config.cpp_wrapper = True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more information, please see the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;AVX512 kernel support&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In PyTorch 2.0, AVX2 kernels would be used even if the CPU supported AVX512 instructions.  Now, PyTorch defaults to using AVX512 CPU kernels if the CPU supports those instructions, equivalent to setting &lt;em&gt;ATEN_CPU_CAPABILITY=avx512&lt;/em&gt; in previous releases.  The previous behavior can be enabled by setting &lt;em&gt;ATEN_CPU_CAPABILITY=avx2.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CPU optimizations for scaled-dot-product-attention (SDPA)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Previous versions of PyTorch provided optimized CUDA implementations for transformer primitives via &lt;em&gt;torch.nn.functiona.scaled_dot_product_attention&lt;/em&gt;.  PyTorch 2.1 includes optimized FlashAttention-based CPU routines.&lt;/p&gt;

&lt;p&gt;See the documentation &lt;a href=&quot;https://pytorch.org/docs/2.1/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CPU optimizations for bfloat16&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.1 includes CPU optimizations for bfloat16, including improved vectorization support and &lt;em&gt;torchinductor&lt;/em&gt; codegen.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.1 (release note)! PyTorch 2.1 offers automatic dynamic shape support in torch.compile, torch.distributed.checkpoint for saving/loading distributed training jobs on multiple ranks in parallel, and torch.compile support for the NumPy API.</summary>
      

      
      
    </entry>
  
</feed>


