<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-08-03T17:57:35-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Hugging Face Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/hugging-face-joins/" rel="alternate" type="text/html" title="Hugging Face Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-08-03T00:00:00-07:00</published>
      <updated>2023-08-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hugging-face-joins</id>
      <content type="html" xml:base="https://pytorch.org/blog/hugging-face-joins/">&lt;p&gt;&lt;img src=&quot;/assets/images/huggingface-joins-1.jpg&quot; alt=&quot;Smiling hugging face&quot; style=&quot;max-width:250px;float:right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Hugging Face has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Hugging Face has been a long time supporter and contributor to the PyTorch Ecosystem by providing powerful models and resources that accelerate research, development, and adoption of AI technologies, particularly in the field of natural language processing.&lt;/p&gt;

&lt;p&gt;“Our mission has always been to democratize AI and make it accessible to everyone. We’re truly aligned with PyTorch’s objective of reducing the barrier of entry to practitioners. By joining the PyTorch Foundation, we can further amplify that impact and support this very important framework of the ecosystem that is PyTorch,” said Lysandre Debut, Head of Open Source at Hugging Face. “We believe the two ecosystems have significant overlap, and collaborating with the foundation will allow us to bridge the gap to provide the best software, the best tools to the machine learning community at large.”&lt;/p&gt;

&lt;p&gt;Hugging Face’s Model Hub and open source libraries promote collaboration and knowledge sharing within the AI open source community, making Hugging Face a great match to the growing PyTorch Foundation. They continue to drive industry adoption and collaboration by creating user-friendly tools and resources and providing accessible and well-documented libraries.&lt;/p&gt;

&lt;p&gt;“Hugging Face’s commitment to open source development and their exceptional contributions to the PyTorch ecosystem have truly impressed us. With their help, we will drive innovation, foster collaboration, and empower the global AI community to create transformative solutions for the AI community,” said PyTorch Foundation Executive Director Ibrahim Haddad. “We welcome Hugging Face to the PyTorch Foundation and look forward to the achievements that lie ahead.”&lt;/p&gt;

&lt;p&gt;As a premier member, Hugging Face is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/huggingface-joins-2.jpg&quot; alt=&quot;Lysandre Debut&quot; style=&quot;max-width:250px;float:right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Lysandre Debut, Head of Open Source at Hugging Face to our board.  Lysandre has been at Hugging Face since the company’s pivot to open-source, and was the first engineer to focus entirely on the open-source mission. Now leading the open-source part of the organization, Lysandre remains technically involved by being a core maintainer of the Transformers library.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-hugging-face&quot;&gt;About Hugging Face&lt;/h2&gt;

&lt;p&gt;Hugging Face is a community and company dedicated to lowering the barrier of entry to Machine Learning and Deep Learning. Strong advocates for open-source and open-science, their model Hub hosts more than 250,000 public models and 50,000 public datasets that are very simple to use. Transformers, Diffusers, PEFT, Accelerate, and Datasets are some of the open-source tools made available by Hugging Face.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its trademark usage page: www.linuxfoundation.org/trademark-usage. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AMD’s Journey to Openness and Performance</title>
      <link href="https://pytorch.org/blog/amd-journey/" rel="alternate" type="text/html" title="AMD's Journey to Openness and Performance" />
      <published>2023-08-01T00:00:00-07:00</published>
      <updated>2023-08-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/amd-journey</id>
      <content type="html" xml:base="https://pytorch.org/blog/amd-journey/">&lt;p&gt;AMD has gained progress in building a robust software stack that supports an open ecosystem of models, libraries, frameworks, and tools. With proven platforms gaining momentum, there is significance of a leadership software stack and an optimized ecosystem for achieving application performance. PyTorch is a key part of AMD’s AI  journey, and AMD’s Victor Peng, AMD President and Soumith Chintala, founder of PyTorch discussed the latest progress at the DC &amp;amp; AI Keynote on June 12.&lt;/p&gt;

&lt;h2 id=&quot;building-a-powerful-sw-stack-with-rocm&quot;&gt;Building a Powerful SW Stack with ROCm&lt;/h2&gt;

&lt;p&gt;Victor introduced ROCm, AMD’s SW stack for Instinct Data Center GPUs. It offers a comprehensive set of open-source libraries, runtime, compilers, and tools for developing, running, and fine-tuning AI models. The fifth generation ROCm incorporates optimizations for AI and high-performance computing workloads, including tailored kernels for low-latency memory systems, support for new data types, and integration with OpenAI Triton. With tools for porting AI software to AMD Instinct platforms, ROCm ensures quality and robustness, tested extensively and compliant with PyTorch and TensorFlow frameworks.&lt;/p&gt;

&lt;h2 id=&quot;collaboration-with-pytorch&quot;&gt;Collaboration with PyTorch&lt;/h2&gt;

&lt;p&gt;To shed light on the partnership between AMD and PyTorch, Victor invited &lt;a href=&quot;https://www.linkedin.com/in/soumith/&quot;&gt;Soumith Chintala&lt;/a&gt;, the founder of PyTorch, to discuss the advancements and integration between the two. PyTorch, the industry’s most famous AI framework, boasts a vibrant developer community and is known for its continuous innovation and incorporation of cutting-edge research.&lt;/p&gt;

&lt;p&gt;To highlight the AMD and PyTorch partnership, Victor hosted a discussion with Soumith Chintala, the founder of PyTorch. PyTorch, renowned for its innovation and community, is the industry’s leading AI framework. The latest version, PyTorch 2.0, integrates with hardware-agnostic software compilers like OpenAI Triton, enabling efficient training and deployment of AI models. With optimized techniques, PyTorch 2.0 enhances productivity and offers remarkable speed improvements. The collaboration between AMD and the PyTorch Foundation ensures seamless utilization of AMD GPUs, expanding AI accelerator accessibility worldwide and paving the way for future optimizations and broader hardware support.&lt;/p&gt;

&lt;h2 id=&quot;empowering-the-developer-community&quot;&gt;Empowering the Developer Community&lt;/h2&gt;

&lt;p&gt;The partnership between AMD and PyTorch benefits the developer community by democratizing access to AI accelerators. Support for AMD GPUs in PyTorch allows developers to train and deploy models across various platforms, including CPUs like EPYC and Ryzen, GPUs like Instinct and Radeon, and embedded devices like Versal SoCs. By ensuring immediate compatibility of new models on AMD platforms, the collaboration streamlines the development process and empowers developers to leverage the full potential of AMD’s hardware. This increased accessibility and flexibility enable developers worldwide to push the boundaries of AI innovation.&lt;/p&gt;

&lt;h2 id=&quot;hugging-face-and-ai-model-innovation&quot;&gt;Hugging Face and AI Model Innovation&lt;/h2&gt;

&lt;p&gt;Victor praised Hugging Face as the leading force behind open-source AI model innovation, empowering generative AI with transformative transformers. AMD’s optimized software enables a high-performing development stack, supporting groundbreaking AI advancements for customers and developers through scalable real-world deployments.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At the DC &amp;amp; AI Keynote, AMD demonstrated its dedication to openness, performance, and collaboration. The ROCm SW stack, PyTorch integration, and support for Hugging Face exemplify AMD’s commitment to empowering developers and researchers to achieve AI breakthroughs. By offering accessible, high-performing solutions, AMD fuels the future of AI as a leading GPU platform integrated with PyTorch.&lt;/p&gt;

&lt;p&gt;To listen to the full keynote visit the &lt;a href=&quot;https://www.youtube.com/watch?v=l3pe_qx95E0&quot;&gt;AMD Youtube&lt;/a&gt; channel&lt;/p&gt;

&lt;p&gt;To listen to Soumith Chintala’s section of the &lt;a href=&quot;https://www.youtube.com/watch?v=RgQEG2G1iaY&quot;&gt;keynote&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">AMD has gained progress in building a robust software stack that supports an open ecosystem of models, libraries, frameworks, and tools. With proven platforms gaining momentum, there is significance of a leadership software stack and an optimized ecosystem for achieving application performance. PyTorch is a key part of AMD’s AI journey, and AMD’s Victor Peng, AMD President and Soumith Chintala, founder of PyTorch discussed the latest progress at the DC &amp;amp; AI Keynote on June 12.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Performant Distributed checkpointing in Production with IBM</title>
      <link href="https://pytorch.org/blog/performant-distributed-checkpointing/" rel="alternate" type="text/html" title="Performant Distributed checkpointing in Production with IBM" />
      <published>2023-07-31T00:00:00-07:00</published>
      <updated>2023-07-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/performant-distributed-checkpointing</id>
      <content type="html" xml:base="https://pytorch.org/blog/performant-distributed-checkpointing/">&lt;p&gt;&lt;img src=&quot;/assets/images/2023-07-31-performant-distributed-checkpointing-1.png&quot; alt=&quot;Params saved per minute&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Last year, IBM Research began collaborating with us to onboard Fully Sharded Data Parallelism (FSDP) for their large foundation models. They became interested as FSDP is a PyTorch native offering for scaling their distributed training efforts on IBM Cloud.&lt;/p&gt;

&lt;p&gt;We are pleased to share that, in collaboration with IBM, we have achieved substantial checkpointing speedups for large models (72x vs the original PyTorch 1.13 save speed), proven model and optimizer checkpoint scaling to 30B parameters, and enabled cloud first training using FSDP + Distributed Checkpoint on S3 backends.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-distributed-checkpoint&quot;&gt;What is a Distributed Checkpoint?&lt;/h2&gt;

&lt;p&gt;Distributed checkpointing is the PyTorch native solution for saving and loading PyTorch models and optimizer states from multiple ranks, as well as supporting dynamically changing world sizes between reloads.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-07-31-performant-distributed-checkpointing-2.png&quot; alt=&quot;Checkpoint time vs model params&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Distributed Checkpoint (DCP) APIs were introduced in PyTorch 1.13, and are included as an official prototype feature in PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;Distributed checkpoint is different from torch.save() and torch.load() in a few significant ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;DCP produces multiples files per checkpoint, with at least one file per rank,&lt;/li&gt;
  &lt;li&gt;DCP operates in place, meaning that the model should allocate its data first and the Distributed Checkpoint will then use the storage.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A major improvement from 1.13 to 2.0 includes adding sharded_state_dict support for checkpointing FSDP models. This allows checkpointing for larger sized models, as well as adding support for load-time resharding. Load time resharding enables saving in one cluster topology, and loading into another.  This feature was highly requested as it allows training jobs to be run on one cluster, saved, and then continued on a different cluster with different world size.&lt;/p&gt;

&lt;p&gt;Another major change is that we decouple the storage layer from the checkpoint planning layer and separate implementation from the interface for both layers. With this change, users can now specify how their state_dict should be chunked or transformed during the checkpoint planning phase. Additionally, the customizable storage layer can easily accommodate different backends.&lt;/p&gt;

&lt;p&gt;More information on the Distributed Checkpoint package can be found &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.checkpoint.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performant-distributed-checkpointing-in-production-with-ibm&quot;&gt;Performant Distributed checkpointing in Production with IBM&lt;/h2&gt;

&lt;p&gt;IBM at Think 2023 announced its &lt;a href=&quot;https://www.ibm.com/products/watsonx-ai&quot;&gt;watsonx.ai&lt;/a&gt; platform for development and deployment of foundation models for the enterprise. Built on Hybrid Cloud, the platform enables use cases across multiple modalities such as NLP, timeseries, weather, chemistry, tabular data, and cybersecurity, with model sizes from 100s of millions to 10s of billions of parameters. Model architectures range from vision transformers, to multi-modal RoBERTa-style feature extractors, to large-scale generative language models similar to T5, GPT and Llama.&lt;/p&gt;

&lt;p&gt;As of today, IBM has now enabled checkpointing for T5-style architectures up to 11B parameters, and decoder architectures (GPT style) up to 30B.&lt;/p&gt;

&lt;p&gt;IBM helped us identify that this limits the scaling power of DCP from both memory and performance standpoints. With their suggestion, we enhanced our FileSystemWriter to produce single checkpoint per rank to reduce read write overhead.&lt;/p&gt;

&lt;p&gt;With this option as the new default, DCP now creates a single file per rank during checkpoint saving, which would then be sliced when reading parameters at load time.&lt;/p&gt;

&lt;p&gt;By combining sharded_state_dict support with single filer per rank writer, distributed checkpoint was able to accelerate checkpoint saving time over 72x vs the original PyTorch 1.13 save speed, and enable rapid checkpointing for models sizes over 15B which would previously simply time out.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Looking back, it’s really astounding the speedups we’ve seen, handling training for many of these models. We went from taking almost half an hour to write a single 11B checkpoint in PyTorch 1.13, to being able to handle a 30B parameter model, with optimizer and dataloader state - so that’s over eight times the raw data - in just over 3 minutes. That’s done wonders for both the stability and efficiency of our jobs, as we scale up training to hundreds of gpus.”  – &lt;strong&gt;Davis Wertheimer, IBM Research&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;IBM’s adoption has also helped us validate and improve our solutions in a real world, large-scale training environment. As an example, IBM discovered that DCP was working well for them on a single node with multiple GPUs, but erred out when used on multiple nodes.&lt;/p&gt;

&lt;p&gt;Upon investigating the issue, we realized that we were assuming writing to a NFS-like shared file system, which assumes strong read-after-write consistencies. Object stores with file system APIs such as S3FS provide eventual consistency semantics, thus causing the distributed checkpoint in such a setting to fail. Working together with IBM, we identified this issue and fixed it by making &lt;a href=&quot;https://research.ibm.com/blog/ibm-pytorch-ai-training&quot;&gt;one line code change&lt;/a&gt; and enabled object storage backend for DCP! Such storage approaches are typically an order of magnitude cheaper than shared file systems thus enabling finer grained checkpointing.&lt;/p&gt;

&lt;h2 id=&quot;looking-for-collaboration&quot;&gt;Looking for Collaboration&lt;/h2&gt;

&lt;p&gt;If you are interested in trying Distributed Checkpoint, feel free to reach out to us!&lt;/p&gt;

&lt;p&gt;If you run into any issue when trying it, you can open an &lt;a href=&quot;https://github.com/pytorch/pytorch/labels/module%3A%20distributed_checkpoint&quot;&gt;issue&lt;/a&gt; at our Github repo.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;This project would not have been possible without the assistance from many collaborators. We would like to thank Yanli Zhao, Andrew Gu, Rohan Varma for their support of FSDP. Thanks to Pritam Damania, Junjie Zhao, and Wanchao Liang for their support of ShardedTensor.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta:  Iris Zhang, Less Wright, Rodrigo Kumpera, Chien-Chin Huang, IBM: Davis Wertheimer, Supriyo Chakraboty, Sophia Wen, Raghu Ganti, Mudhakar Srivatsa, Seethrami Seelam</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">IBM Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/ibm-joins-pytorch/" rel="alternate" type="text/html" title="IBM Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-07-27T00:00:00-07:00</published>
      <updated>2023-07-27T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/ibm-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/ibm-joins-pytorch/">&lt;p&gt;The PyTorch Foundation, part of The Linux Foundation, is pleased to announce that IBM has joined as a premier member.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-ibm-logo.png&quot; alt=&quot;IBM Logo&quot; style=&quot;max-width:250px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The foundation serves as a neutral space for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. With its extensive industry expertise and leadership in open source and AI, IBM is committed to actively contributing to the PyTorch community.&lt;/p&gt;

&lt;p&gt;IBM offers a comprehensive portfolio of enterprise AI solutions and recently released watsonx, its next-generation data and AI platform. IBM’s watsonx platform leverages PyTorch to offer an enterprise-grade software stack for end-to-end training and fine-tuning of AI foundation models.&lt;/p&gt;

&lt;p&gt;“By joining the PyTorch Foundation, we aim to contribute our expertise and resources to further advance PyTorch’s capabilities and make AI more accessible in hybrid cloud environments with flexible hardware options,” said Priya Nagpurkar, Vice President, Hybrid Cloud Platform and Developer Productivity, IBM Research. “We intend for our collaboration with PyTorch to bring the power of foundation models and generative AI to enterprises using the watsonx platform to drive business transformation.”&lt;/p&gt;

&lt;p&gt;IBM and PyTorch have already collaborated on two projects. The first enables foundation models with billions of parameters to train efficiently on standard cloud networking infrastructure, such as Ethernet networking. Together, IBM and PyTorch have also worked on ways to make checkpointing for AI training considerably more cost-effective, by fixing the distributed checkpointing within PyTorch to support certain types of object storage.&lt;/p&gt;

&lt;p&gt;“We’re happy to welcome IBM as a premier member. IBM’s expertise and dedication to advancing the field of artificial intelligence align perfectly with the mission of the PyTorch community,” said PyTorch Foundation Executive Director Ibrahim Haddad. “Their commitment to open collaboration and innovation will strengthen our collective efforts to empower developers and researchers worldwide.”&lt;/p&gt;

&lt;p&gt;As a premier member, IBM  is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-ibm-headshot.png&quot; alt=&quot;Raghu Ganti Headshot&quot; style=&quot;max-width:250px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Raghu Ganti, Principal Research Scientist at IBM Research, to our board.  Raghu co-leads IBM Research’s foundation model training and validation platform, built on Red Hat OpenShift. His team primarily contributes to the PyTorch training components, with the mission of democratizing training and validation of foundation models.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, part of The Linux Foundation, is pleased to announce that IBM has joined as a premier member.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing CPP-based S3 IO DataPipes</title>
      <link href="https://pytorch.org/blog/announcing-cpp/" rel="alternate" type="text/html" title="Announcing CPP-based S3 IO DataPipes" />
      <published>2023-07-25T00:00:00-07:00</published>
      <updated>2023-07-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/announcing-cpp</id>
      <content type="html" xml:base="https://pytorch.org/blog/announcing-cpp/">&lt;p&gt;Training large deep learning models requires large datasets. &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) is a scalable cloud object store service used for storing large training datasets. Machine learning (ML) practitioners need an efficient data pipe that can download data from Amazon S3, transform the data, and feed the data to GPUs for training models with high throughput and low latency.&lt;/p&gt;

&lt;p&gt;In this post, we introduce the new S3 IO DataPipes for PyTorch, &lt;a href=&quot;hhttps://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/load/s3io.py#L19&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLister&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/load/s3io.py#L106&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt;&lt;/a&gt;. For memory efficiency and fast runs, the new DataPipes use the C++ extension to access Amazon S3. Benchmarking shows that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; is 59.8% faster than &lt;a href=&quot;https://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/load/fsspec.py#L125&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FSSpecFileOpener&lt;/code&gt;&lt;/a&gt; for downloading a natural language processing (NLP) dataset from Amazon S3. You can build &lt;a href=&quot;https://pytorch.org/data/beta/torchdata.datapipes.iter.html&quot;&gt;IterDataPipe&lt;/a&gt; training pipelines with the new DataPipes. We also demonstrate that the new DataPipe can reduce overall Bert and ResNet50 training time by 7%. The new DataPipes have been upstreamed to the open-source &lt;a href=&quot;https://github.com/pytorch/data/releases/tag/v0.4.0&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TorchData 0.4.0&lt;/code&gt;&lt;/a&gt; with &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v1.12.0&quot;&gt;PyTorch 1.12.0&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Amazon S3 is a scalable cloud storage service with no limit on data volume. Loading data from Amazon S3 and feeding the data to high-performance GPUs such as NVIDIA A100 can be challenging. It requires an efficient data pipeline that can meet the data processing speed of GPUs. To help with this, we released a new high performance tool for PyTorch: S3 IO DataPipes. DataPipes are subclassed from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchdata.datapipes.iter.IterDataPipe&lt;/code&gt;, so they can interact with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IterableDataPipe&lt;/code&gt; interface. Developers can quickly build their DataPipe DAGs to access, transform, and manipulate data with shuffle, sharding, and batch features.&lt;/p&gt;

&lt;p&gt;The new DataPipes are designed to be file format agnostic and Amazon S3 data is downloaded as binary large objects (BLOBs). It can be used as a composable building block to assemble a DataPipe graph that can load tabular, NLP, and computer vision (CV) data into your training pipelines.&lt;/p&gt;

&lt;p&gt;Under the hood, the new S3 IO DataPipes employ a C++ S3 handler with the AWS C++ SDK. In general, a C++ implementation is more memory efficient and has better CPU core usage (no Global Interpreter Lock) in threading compared to Python. The new C++ S3 IO DataPipes are recommended for high throughput, low latency data loading in training large deep learning models.&lt;/p&gt;

&lt;p&gt;The new S3 IO DataPipes provide two first-class citizen APIs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;S3FileLister&lt;/strong&gt; – Iterable that lists S3 file URLs within the given S3 prefixes. The functional name for this API is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;list_files_by_s3&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;S3FileLoader&lt;/strong&gt; – Iterable that loads S3 files from the given S3 prefixes. The functional name for this API is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_files_by_s3&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;usage&quot;&gt;Usage&lt;/h2&gt;

&lt;p&gt;In this section, we provide instructions for using the new S3 IO DataPipes. We also provide a code snippet for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_files_by_s3()&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;build-from-source&quot;&gt;Build from source&lt;/h3&gt;
&lt;p&gt;The new S3 IO DataPipes use the C++ extension. It is built into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchdata&lt;/code&gt; package by default. However, if the new DataPipes are not available within the environment, for example Windows on Conda, you need to build from the source. For more information, refer to &lt;a href=&quot;https://github.com/pytorch/data/tree/main/torchdata/datapipes/iter/load#s3-io-datapipe-documentation&quot;&gt;Iterable Datapipes&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;configuration&quot;&gt;Configuration&lt;/h3&gt;
&lt;p&gt;Amazon S3 supports global buckets. However, a bucket is created within a Region. You can pass a Region to the DataPipes by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__init__()&lt;/code&gt;. Alternatively, you can either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;export AWS_REGION=us-west-2&lt;/code&gt; into your shell or set an environment variable with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;os.environ['AWS_REGION'] = 'us-east-1'&lt;/code&gt; in your code.&lt;/p&gt;

&lt;p&gt;To read objects in a bucket that aren’t publicly accessible, you must provide AWS credentials through one of the following methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html&quot;&gt;Install and configure&lt;/a&gt; the &lt;a href=&quot;aws.amazon.com/cli&quot;&gt;AWS Command Line Interface&lt;/a&gt; (AWS CLI) with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS configure&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Set credentials in the AWS credentials profile file on the local system, located at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.aws/credentials&lt;/code&gt; on Linux, macOS, or Unix&lt;/li&gt;
  &lt;li&gt;Set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; environment variables&lt;/li&gt;
  &lt;li&gt;If you’re using this library on an &lt;a href=&quot;aws.amazon.com/ec2&quot;&gt;Amazon Elastic Compute Cloud&lt;/a&gt; (Amazon EC2) instance, specify an &lt;a href=&quot;aws.amazon.com/iam&quot;&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) role and then give the EC2 instance access to that role&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;example-code&quot;&gt;Example code&lt;/h3&gt;
&lt;p&gt;The following code snippet provides a typical usage of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_files_by_s3()&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch.utils.data import DataLoader 
from torchdata.datapipes.iter import IterableWrapper  

s3_shard_urls = IterableWrapper([&quot;s3://bucket/prefix/&quot;,]) 
s3_shards = s3_shard_urls.load_files_by_s3() 
# text data 
training_data = s3_shards.readlines(return_path=False) 
data_loader = DataLoader(
      training_data,
      batch_size=batch_size,
      num_workers=num_workers, 
) # training loop 
for epoch in range(epochs):     
      # training step     
      for bach_data in data_loader:         
         # forward pass, backward pass, model update  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;benchmark&quot;&gt;Benchmark&lt;/h2&gt;

&lt;p&gt;In this section, we demonstrate how the new DataPipe can reduce overall Bert and ResNet50 training time.&lt;/p&gt;

&lt;h3 id=&quot;isolated-dataloader-performance-evaluation-against-fsspec&quot;&gt;Isolated DataLoader performance evaluation against FSSpec&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FSSpecFileOpener&lt;/code&gt; is another PyTorch S3 DataPipe. It uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;botocore&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aiohttp/asyncio&lt;/code&gt; to access S3 data. The following is the performance test setup and result (quoted from &lt;a href=&quot;https://github.com/pytorch/data/issues/500&quot;&gt;Performance Comparison between native AWSSDK and FSSpec (boto3) based DataPipes&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The S3 data in the test is a sharded text dataset. Each shard has about 100,000 lines and each line is around 1.6 KB, making each shard about 156 MB. The measurements in this benchmark are averaged over 1,000 batches. No shuffling, sampling, or transforms were performed.&lt;/p&gt;

&lt;p&gt;The following chart reports the throughput comparison for various batch sizes for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_workers=0&lt;/code&gt;, the data loader runs in the main process. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; has higher queries per second (QPS). It is 90% higher than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsspec&lt;/code&gt; at batch size 512.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-1.png&quot; alt=&quot;Batch Sizes 1&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following chart reports the results for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_workers=4&lt;/code&gt;, the data loaders runs in the main process. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; is 59.8% higher than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsspec&lt;/code&gt; at batch size 512.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-5.png&quot; alt=&quot;Batch Sizes 2&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-resnet50-model-against-boto3&quot;&gt;Training ResNet50 Model against Boto3&lt;/h3&gt;
&lt;p&gt;For the following chart, we trained a ResNet50 model on a cluster of 4 p3.16xlarge instances with a total 32 GPUs. The training dataset is ImageNet with 1.2 million images organized into 1,000-image shards. The training batch size is 64. The training time is measured in seconds. For eight epochs, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; is 7.5% faster than Boto3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-2.png&quot; alt=&quot;Boto3&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-a-bert-model-against-boto3&quot;&gt;Training a Bert model against Boto3&lt;/h3&gt;
&lt;p&gt;For the following cart, we trained a Bert model on a cluster of 4 p3.16xlarge instances with a total 32 GPUs. The training corpus has 1474 files. Each file has around 150,000 samples. To run a shorter epoch, we use 0.05% (approximately 75 samples) per file. The batch size is 2,048. The training time is measured in seconds. For one epoch, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; is 7% faster than Boto3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-3.png&quot; alt=&quot;Boto3 2&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;comparison-against-the-original-pytorch-s3-plugin&quot;&gt;Comparison against the original PyTorch S3 plugin&lt;/h3&gt;
&lt;p&gt;The new PyTorch S3 DataPipes perform substantially better than the original &lt;a href=&quot;https://github.com/aws/amazon-s3-plugin-for-pytorch&quot;&gt;PyTorch S3 plugin&lt;/a&gt;. We have tuned the internal buffer size for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt;. The loading time is measured in seconds.&lt;/p&gt;

&lt;p&gt;For the 10 sharded charades files (approximately 1.5 GiB each), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; was 3.5 times faster in our experiments.&lt;/p&gt;

&lt;h3 id=&quot;best-practices&quot;&gt;Best practices&lt;/h3&gt;
&lt;p&gt;Training large deep learning models may require a massive compute cluster with tens or even hundreds of nodes. Each node in the cluster may generate a large number of data loading requests that hit a specific S3 shard. To avoid throttle, we recommend sharding training data across S3 buckets and S3 folders.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-4.png&quot; alt=&quot;Best Practices&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To achieve good performance, it helps to have file sizes that are big enough to parallelize across a given file, but not so big that we hit the limits of throughput on that object on Amazon S3 depending on the training job. The optimal size can be between 50–200 MB.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-next-steps&quot;&gt;Conclusion and next steps&lt;/h2&gt;

&lt;p&gt;In this post, we introduced you to the new PyTorch IO DataPipes. The new DataPipes use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws-sdk-cpp&lt;/code&gt; and show better performance against Boto3-based data loaders.&lt;/p&gt;

&lt;p&gt;For next steps, we plan to improve on usability, performance, and functionality by focusing on the following features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;S3 authorization with IAM roles&lt;/strong&gt; – Currently, the S3 DataPipes support explicit access credentials, instance profiles, and S3 bucket policies. However, there are use cases where IAM roles are preferred.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Double buffering&lt;/strong&gt; – We plan to offer double buffering to support multi-worker downloading.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Local caching&lt;/strong&gt; – We plan on making model training able to traverse the training dataset for multiple passes. Local caching after the first epoch can cut out time of flight delays from Amazon S3, which can substantially accelerate data retrieval time for subsequent epochs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Customizable configuration&lt;/strong&gt; – We plan to expose more parameters such as internal buffer size, multi-part chunk size, and executor count and allow users to further tune data loading efficiency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Amazon S3 upload&lt;/strong&gt; – We plan to expand the S3 DataPipes to support upload for checkpointing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Merge with fsspec&lt;/strong&gt; – &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsspec&lt;/code&gt; is used in other systems such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.save()&lt;/code&gt;. We can integrate the new S3 DataPipes with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsspec&lt;/code&gt; so they can have more use cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h3&gt;

&lt;p&gt;We would like to thank Vijay Rajakumar and Kiuk Chung from Amazon for providing their guidance for S3 Common RunTime and PyTorch DataLoader. We also want to thank Erjia Guan, Kevin Tse, Vitaly Fedyunin , Mark Saroufim, Hamid Shojanazeri, Matthias Reso, and Geeta Chauhan from Meta AI/ML, and Joe Evans from AWS for reviewing the blog and the GitHub PRs.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/announcing-the-amazon-s3-plugin-for-pytorch/&quot;&gt;Announcing the Amazon S3 plugin for PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/data/issues/500&quot;&gt;Performance Comparison between native AWSSDK and FSSpec (boto3) based DataPipes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>John He, Khaled ElGalaind Roshani Nagmote, Daiming Yang</name>
        
        
      </author>

      

      

      
        <summary type="html">Training large deep learning models requires large datasets. Amazon Simple Storage Service (Amazon S3) is a scalable cloud object store service used for storing large training datasets. Machine learning (ML) practitioners need an efficient data pipe that can download data from Amazon S3, transform the data, and feed the data to GPUs for training models with high throughput and low latency. In this post, we introduce the new S3 IO DataPipes for PyTorch, S3FileLister and S3FileLoader. For memory efficiency and fast runs, the new DataPipes use the C++ extension to access Amazon S3. Benchmarking shows that S3FileLoader is 59.8% faster than FSSpecFileOpener for downloading a natural language processing (NLP) dataset from Amazon S3. You can build IterDataPipe training pipelines with the new DataPipes. We also demonstrate that the new DataPipe can reduce overall Bert and ResNet50 training time by 7%. The new DataPipes have been upstreamed to the open-source TorchData 0.4.0 with PyTorch 1.12.0.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">How to Accelerate PyTorch Geometric on Intel® CPUs</title>
      <link href="https://pytorch.org/blog/how-to-accelerate/" rel="alternate" type="text/html" title="How to Accelerate PyTorch Geometric on Intel® CPUs" />
      <published>2023-07-10T00:00:00-07:00</published>
      <updated>2023-07-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/how-to-accelerate</id>
      <content type="html" xml:base="https://pytorch.org/blog/how-to-accelerate/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The Intel PyTorch team has been collaborating with the PyTorch Geometric (PyG) community to provide CPU performance optimizations for Graph Neural Network (GNN) and PyG workloads. In the PyTorch 2.0 release, several critical optimizations were introduced to improve GNN training and inference performance on CPU. Developers and researchers can now take advantage of &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Intel’s AI/ML Framework optimizations&lt;/a&gt; for significantly faster model training and inference, which unlocks the ability for GNN workflows directly using PyG.&lt;/p&gt;

&lt;p&gt;In this blog, we will perform a deep dive on how to optimize PyG performance for both training and inference while using the PyTorch 2.0 flagship torch.compile feature to speed up PyG models.&lt;/p&gt;

&lt;h2 id=&quot;message-passing-paradigm&quot;&gt;Message Passing Paradigm&lt;/h2&gt;

&lt;p&gt;Message passing refers to the process of nodes exchanging information with their respective neighbors by sending messages to one another. In PyG, the process of message passing can be generalized into three steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Gather&lt;/strong&gt;: Collect edge-level information of adjacent nodes and edges.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Apply&lt;/strong&gt;: Update the collected information with user-defined functions (UDFs).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scatter&lt;/strong&gt;: Aggregate to node-level information, e.g., via a particular reduce function such as sum, mean, or max.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-to-accelerate/f1-pyg-message-passing-paradigm.png&quot; alt=&quot;Figure 1: The message passing paradigm&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: The message passing paradigm (Source: &lt;a href=&quot;http://github.com/rusty1s&quot;&gt;Matthias Fey&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Message passing performance is highly related to the storage format of the adjacency matrix of the graph, which records how pairs of nodes are connected. Two methods for the storage format are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Adjacency matrix in COO (Coordinate Format):&lt;/strong&gt; The graph data is physically stored in a two-dimensional tensor shape of &lt;strong&gt;[2, num_edges]&lt;/strong&gt;, which maps each connection of source and destination nodes. The performance hotspot is scatter-reduce.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Adjacency matrix in CSR (Compressed Sparse Row):&lt;/strong&gt; Similar format to COO, but compressed on the row indices. This format allows for more efficient row access and faster sparse matrix-matrix multiplication (SpMM). The performance hotspot is sparse matrix related reduction ops.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;scatter-reduce&quot;&gt;Scatter-Reduce&lt;/h2&gt;

&lt;p&gt;The pattern of scatter-reduce is parallel in nature, which updates values of a &lt;strong&gt;self&lt;/strong&gt; tensor using values from a &lt;strong&gt;src&lt;/strong&gt; tensor at the entries specified by &lt;strong&gt;index&lt;/strong&gt;. Ideally, parallelizing on the outer dimension would be most performant. However, direct parallelization leads to write conflicts, as different threads might try to update the same entry simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-to-accelerate/f2-scatter-reduce-scheme.png&quot; alt=&quot;Figure 2: Scatter-reduce and its optimization scheme&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Scatter-reduce and its optimization scheme (Source: Mingfei Ma)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;To optimize this kernel, we use sorting followed by a reduction:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sorting:&lt;/strong&gt; Sort the &lt;strong&gt;index&lt;/strong&gt; tensor in ascending order with parallel radix sort, such that indices pointing to the same entry in the &lt;strong&gt;self&lt;/strong&gt; tensor are managed in the same thread.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reduction:&lt;/strong&gt; Paralleled on the outer dimension of &lt;strong&gt;self&lt;/strong&gt;, and do vectorized reduction for each indexed &lt;strong&gt;src&lt;/strong&gt; entry.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For its backward path during the training process (i.e., gather), sorting is not needed because its memory access pattern will not lead to any write conflicts.&lt;/p&gt;

&lt;h2 id=&quot;spmm-reduce&quot;&gt;SpMM-Reduce&lt;/h2&gt;

&lt;p&gt;Sparse matrix-matrix reduction is a fundamental operator in GNNs, where &lt;strong&gt;A&lt;/strong&gt; is sparse adjacency matrix in CSR format and &lt;strong&gt;B&lt;/strong&gt; is a dense feature matrix where the reduction type could be &lt;em&gt;sum&lt;/em&gt;, &lt;em&gt;mean&lt;/em&gt; or &lt;em&gt;max&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-to-accelerate/f3-spmm-optimization-scheme.png&quot; alt=&quot;Figure 3: SpMM optimization scheme&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: SpMM optimization scheme (Source: Mingfei Ma)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The biggest challenge when optimizing this kernel is how to balance thread payload when parallelizing along rows of the sparse matrix &lt;strong&gt;A&lt;/strong&gt;. Each row in &lt;strong&gt;A&lt;/strong&gt; corresponds to a node, and its number of connections may vary vastly from one to another; this results in thread payload imbalance. One technique to address such issues is to do payload scanning before thread partition. Aside from that, other techniques are also introduced to further exploit CPU performance such as vectorization and unrolling and blocking.&lt;/p&gt;

&lt;p&gt;These optimizations are done via &lt;strong&gt;torch.sparse.mm&lt;/strong&gt; using the reduce flags of &lt;em&gt;amax&lt;/em&gt;, &lt;em&gt;amin&lt;/em&gt;, &lt;em&gt;mean&lt;/em&gt;, &lt;em&gt;sum&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-gains-up-to-41x-speedup&quot;&gt;Performance Gains: Up to 4.1x Speedup&lt;/h2&gt;

&lt;p&gt;We collected benchmark performance for both inference and training in &lt;a href=&quot;http://github.com/pyg-team/pytorch_geometric/tree/master/benchmark&quot;&gt;pytorch_geometric/benchmark&lt;/a&gt; and in the &lt;a href=&quot;http://github.com/snap-stanford/ogb&quot;&gt;Open Graph Benchmark (OGB)&lt;/a&gt; to demonstrate the performance improvement from the above-mentioned methods on Intel® Xeon® Platinum 8380 Processor.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;

  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model – Dataset&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Option&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Speedup ratio&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;4&quot;&gt; 
        GCN-Reddit (inference)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.22x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.25x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.31x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.68x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;4&quot;&gt; 
        GraphSage-ogbn-products (inference)
   &lt;/td&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.15x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.33x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;full-batch-sparse
   &lt;/td&gt;
   &lt;td&gt;4.07x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-PROTEINS (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-REDDIT-BINARY (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;GCN-Reddit (training)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.12x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: Performance Speedup on PyG Benchmark&lt;sup&gt;1&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;From the benchmark results, we can see that our optimizations in PyTorch and PyG achieved &lt;strong&gt;1.1x-4.1x speed-up&lt;/strong&gt; for inference and training.&lt;/p&gt;

&lt;h2 id=&quot;torchcompile-for-pyg&quot;&gt;torch.compile for PyG&lt;/h2&gt;

&lt;p&gt;The PyTorch2.0 flagship feature torch.compile is fully compatible with PyG 2.3 release, bringing additional speed-up in PyG model inference/training over imperative mode, thanks to TorchInductor C++/OpenMP backend for CPUs. In particular, &lt;strong&gt;a 3.0x – 5.4x performance speed-up&lt;/strong&gt; is measured on &lt;a href=&quot;http://github.com/pyg-team/pytorch_geometric/blob/master/test/nn/models/test_basic_gnn.py&quot;&gt;basic GNN models&lt;/a&gt; with Intel Xeon Platinum 8380 Processor on model training&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-to-accelerate/f4-torch-compile-performance-speedup.png&quot; alt=&quot;Figure 4: Performance Speedup with Torch Compile&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Performance Speedup with Torch Compile&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Torch.compile can fuse the multiple stages of message passing into a single kernel, which provides significant speedup due to the saved memory bandwidth. Refer to this &lt;a href=&quot;http://pytorch-geometric.readthedocs.io/en/latest/tutorial/compile.html&quot;&gt;pytorch geometric tutorial&lt;/a&gt; for additional support.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt; that torch.compile within PyG is in beta mode and under active development. Currently, some features do not yet work together seamlessly such as torch.compile(model, dynamic=True), but fixes are on the way from Intel.&lt;/p&gt;

&lt;h2 id=&quot;conclusion--future-work&quot;&gt;Conclusion &amp;amp; Future Work&lt;/h2&gt;

&lt;p&gt;In this blog, we introduced the GNN performance optimizations included in PyTorch 2.0 on CPU. We are closely collaborating with the PyG community for future optimization work, which will focus on in-depth optimizations from torch.compile, sparse optimization, and distributed training.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h3&gt;

&lt;p&gt;The results presented in this blog is a joint effort of Intel PyTorch team and Kumo. Special thanks to &lt;a href=&quot;http://github.com/rusty1s&quot;&gt;Matthias Fey&lt;/a&gt; (Kumo), &lt;a href=&quot;http://github.com/pearu&quot;&gt;Pearu Peterson&lt;/a&gt; (Quansight) and &lt;a href=&quot;http://www.linkedin.com/in/christianpuhrsch/&quot;&gt;Christian Puhrsch&lt;/a&gt; (Meta) who spent precious time and gave substantial assistance! Together, we made one more step forward on the path of improving the PyTorch CPU ecosystem.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;Accelerating PyG on Intel CPUs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/pyg-team/pytorch_geometric/releases/tag/2.3.0&quot;&gt;PyG 2.3.0&lt;/a&gt;: PyTorch 2.0 support, native sparse tensor support, explainability and accelerations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;h4 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h4&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Platinum 8380: 1-node, 2x Intel Xeon Platinum 8380 processor with 256GB (16 slots/ 16GB/3200) total DDR4 memory, uCode 0xd000389, HT on, Turbo on, Ubuntu 20.04.5 LTS,  5.4.0-146-generic, INTEL SSDPE2KE016T8 1.5T; GCN + Reddit FP32 inference, GCN+Reddit FP32 training, GraphSAGE + ogbn-products FP32 inference, GCN-PROTAIN, GCN-REDDIT-BINARY FP32 training; Software: PyTorch 2.1.0.dev20230302+cpu, pytorch_geometric 2.3.0, torch-scatter 2.1.0, torch-sparse 0.6.16, test by Intel on 3/02/2023.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt;Platinum 8380: 1-node, 2x Intel Xeon Platinum 8380 processor with 256GB (16 slots/ 16GB/3200) total DDR4 memory, uCode 0xd000389, HT on, Turbo on, Ubuntu 20.04.5 LTS,  5.4.0-146-generic, INTEL SSDPE2KE016T8 1.5T; GCN, GraphSAGE, GIN and EdgeCNN, FP32; Software: PyTorch 2.1.0.dev20230411+cpu, pytorch_geometric 2.4.0, torch-scatter 2.1.1+pt20cpu, torch-sparse 0.6.17+pt20cpu, test by Intel on 4/11/2023.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt;Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Optimizing LibTorch-based inference engine memory usage and thread-pooling</title>
      <link href="https://pytorch.org/blog/optimizing-libtorch/" rel="alternate" type="text/html" title="Optimizing LibTorch-based inference engine memory usage and thread-pooling" />
      <published>2023-06-29T00:00:00-07:00</published>
      <updated>2023-06-29T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/optimizing-libtorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/optimizing-libtorch/">&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;In this blog post we show how to optimize LibTorch-based inference engine to maximize throughput by reducing memory usage and optimizing the thread-pooling strategy. We apply these optimizations to Pattern Recognition engines for audio data, for example, music and speech recognition or acoustic fingerprinting. The optimizations discussed in this blog post allow for memory usage reduction by 50% and reduction in end-to-end latency for Inference by 37.5%. These optimizations are applicable to computer vision and natural language processing.&lt;/p&gt;

&lt;h2 id=&quot;audio-recognition-inferencing&quot;&gt;Audio Recognition Inferencing&lt;/h2&gt;

&lt;p&gt;Audio Recognition (AR) engines can be used to recognize and identify sound patterns. As an example, identifying the type and species of a bird from audio recordings, distinguishing music from the singer’s voice, or detecting an abnormal sound indicating a breach in a building. To identify sounds of interest, AR engines process audio through 4 stages:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;File Validation&lt;/strong&gt;: The AR engine validates the input audio file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feature Extraction&lt;/strong&gt;: Features are extracted from each segment within the audio file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: LibTorch performs inference using CPUs or accelerators. In our case Intel processors on an Elastic Cloud Compute (EC2) instance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Post-processing&lt;/strong&gt;: A post-processing model decodes the results and calculates scores that are used to convert inference output into tags or transcripts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Of these 4 steps, inference is the most computationally intensive and can take up to 50% of the pipeline processing time depending on the model complexity. This means that any optimization at this stage has a significant impact on the overall pipeline. &lt;/p&gt;

&lt;h2 id=&quot;optimizing-the-audio-recognition-engine-with-concurrencyis-not-so-simple&quot;&gt;Optimizing the Audio Recognition engine with concurrency…is not so simple&lt;/h2&gt;

&lt;p&gt;Our objective for this processing pipeline is to extract audio segments into tags or transcripts through a processing. The input data is an audio file composed of several short sound segments (S1 to S6 in Figure 1). The output data corresponds to tags or transcripts ordered by timestamps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im1.jpg&quot; alt=&quot;Figure 1: Example audio file with segment boundaries&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Example audio file with segment boundaries&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Each segment can be processed independently and in an out-of-order fashion. This offers the opportunity to process segments concurrently and in parallel to optimize the overall inference throughput as well as maximize the usage of the resources.&lt;/p&gt;

&lt;p&gt;Parallelization on an instance can be achieved through multi-threading (pThreads, std::threads, OpenMP) or multi-processing. The advantage of multi-threading over multi-processing is the ability to use shared memory. It enables developers to minimize data duplication across threads by sharing data across threads; the AR models in our case (&lt;em&gt;Figure 2&lt;/em&gt;). Furthermore, a reduction in memory allows us to run more pipelines in parallel by increasing the number of engine threads in order to utilize all vCPUs on our Amazon EC2 instance (&lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/c5/&quot;&gt;c5.4xlarge&lt;/a&gt; in our case, it offers 16 vCPUs). In theory, we expect to see higher hardware utilization and higher throughput for our AR engine as a result.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im2.jpg&quot; alt=&quot;Figure 2: Multi-threaded AR Engine&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Multi-threaded AR Engine&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;But we found these assumptions to be wrong. Indeed, we found that increasing the number of threads of the application led to an increase of the end-to-end latency for each audio segment and to a decrease of the engine throughput. For example, increasing the concurrency from 1 to 5 threads led to an increase of the latency by 4x which had a proportional effect on decreasing the throughput. In fact, metrics showed that within the pipeline, the latency of the inference stage alone was 3x higher than it’s single thread baseline. &lt;/p&gt;

&lt;p&gt;Using a profiler, we found that the CPU &lt;a href=&quot;https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/reference/cpu-metrics-reference.html#cpu-metrics-reference_SPIN-AND-OVERHEAD-TIME&quot;&gt;Spin Time&lt;/a&gt; increased, potentially due to CPU oversubscription which impacts system and application performance. Given our control over the application’s multi-thread implementation, we chose to dive deeper into the stack and identify potential conflicts with LibTorch’s default settings.&lt;/p&gt;

&lt;h3 id=&quot;diving-deeper-on-libtorchs-multi-threading-and-its-impact-on-concurrency&quot;&gt;Diving deeper on LibTorch’s multi-threading and its impact on concurrency&lt;/h3&gt;

&lt;p&gt;LibTorch’s parallel implementations on CPU for inference are based on  &lt;a href=&quot;https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#cpu-threading-and-torchscript-inference&quot;&gt;global thread pools&lt;/a&gt;. Examples of implementations are Inter-op and intra-op parallelism, which can be chosen depending on the model’s properties. In both cases, it is possible to set &lt;a href=&quot;https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#tuning-the-number-of-threads&quot;&gt;the number of threads&lt;/a&gt; in each thread-poll to optimize the latency and throughput. &lt;/p&gt;

&lt;p&gt;To test if LibTorch’s parallel default implementation settings had a counter effect on our inference latency, we ran an experiment on a 16 vCPus machine with a 35-minute audio file, keeping the LibTorch inter-threads constant at 1 (because our models didn’t utilize the inter-op thread pool). We collected the following data as shown in Figure 3 and 4. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im3.jpg&quot; alt=&quot;Figure 3: CPU Utilization for different number of engine threads&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: CPU Utilization for different number of engine threads&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im4.jpg&quot; alt=&quot;Figure 4: Processing times for different number of engine threads&quot; style=&quot;max-height:800px; width:100%; margin-top: 4rem;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Processing times for different number of engine threads&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Execution time in Figure 4 is the end-to-end processing time for processing all the segments of the given audio file. We have 4 different configurations of LibTorch intra-threads which are 1, 4, 8, 16 and we change the number of engine threads from 1 to 16 for each intra-thread LibTorch configuration. As we see in Figure 3, CPU utilization increases with an increase in the number of engine threads for all LibTorch intra-thread configurations. But as we see in Figure 4, an increase in CPU utilization doesn’t translate into lower execution time. We found out that in all but one case, as the number of engine threads shot up, so did execution time. The one exception was the case where the intra-thread pool size was 1.&lt;/p&gt;

&lt;h3 id=&quot;resolving-the-global-thread-pool-issue&quot;&gt;Resolving the global thread pool issue&lt;/h3&gt;

&lt;p&gt;Using too many threads with a global thread pool led to performance degradation and caused an over-subscription problem. Without disabling&lt;a href=&quot;https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html&quot;&gt; LibTorch global thread pools&lt;/a&gt;, it was difficult to match the performance of the multi-process engine.&lt;/p&gt;

&lt;p&gt;Disabling the LibTorch global thread pool is as simple as setting the intra-op/inter-op parallelism threads to 1, as shown here:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;at::set_num_threads(1)           // Disables the intraop thread pool.
at::set_num_interop_threads(1). // Disables the interop thread pool.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As shown in Figure 4, the lowest processing time was measured when the LibTorch global thread pool was disabled.&lt;/p&gt;

&lt;p&gt;This solution improved AR engine throughput in several cases. However, when evaluating long datasets (audio files longer than 2 hours in load test), we found that the memory footprint of the engine gradually started to increase.&lt;/p&gt;

&lt;h3 id=&quot;optimizing-memory-usage&quot;&gt;Optimizing memory usage&lt;/h3&gt;

&lt;p&gt;We ran a load-test on the system with two hours long audio files and found out that the observed memory increase was the result of memory fragmentation within a multi-threaded LibTorch inference. We resolved this using&lt;a href=&quot;https://github.com/jemalloc/jemalloc&quot;&gt; jemalloc&lt;/a&gt;, which is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support. &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#switch-memory-allocator&quot;&gt;Using jemalloc&lt;/a&gt;, our peak memory usage decreased by an average of 34% and average memory usage decreased by 53%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimizing-libtorch/im5.jpg&quot; alt=&quot;Figure 5: Memory usage over time using the same input file with and without jemalloc&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: Memory usage over time using the same input file with and without jemalloc&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;To optimize the performance of multi-threaded LibTorch-based inference engines, we recommend verifying that there is no oversubscription problem in LibTorch. In our case, all threads in the multi-threaded engine were sharing the LibTorch global thread pool, which caused an oversubscription problem. This was remedied by disabling the global thread pool: we disabled the interop and intraop global thread pool by setting threads to 1. To optimize the memory of a multi-threaded engine, we recommend using Jemalloc as a memory allocator tool rather than the default malloc function.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Himalay Mohanlal Joriwal, Pierre-Yves Aquilanti, Vivek Govindan, Hamid Shojanazeri, Ankith Gunapal, Tristan Rice</name>
        
        
      </author>

      

      

      
        <summary type="html">Outline</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA</title>
      <link href="https://pytorch.org/blog/path-achieve-low-inference-latency/" rel="alternate" type="text/html" title="The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA" />
      <published>2023-06-28T00:00:00-07:00</published>
      <updated>2023-06-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/path-achieve-low-inference-latency</id>
      <content type="html" xml:base="https://pytorch.org/blog/path-achieve-low-inference-latency/">&lt;h2 id=&quot;background--state-of-the-art&quot;&gt;Background &amp;amp; State of the Art&lt;/h2&gt;

&lt;p&gt;In the natural language processing (NLP) space, language models are designed to generate a token (e.g. word) using a sequence of past input tokens. Large Language Models (LLMs) are the latest deep learning innovation in this space built to generate text in a human-like fashion. These models generally use &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;transformers&lt;/a&gt; to improve their attention over a large sequence of input tokens.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;LLaMA&lt;/a&gt;, open sourced by &lt;a href=&quot;https://ai.facebook.com/&quot;&gt;Meta AI&lt;/a&gt;, is a powerful foundation LLM trained on over 1T tokens. LLaMA is competitive with many best-in-class models such as &lt;a href=&quot;https://openai.com/blog/gpt-3-apps&quot;&gt;GPT-3&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2203.15556.pdf&quot;&gt;Chinchilla&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM&lt;/a&gt;. &lt;a href=&quot;https://arxiv.org/pdf/2302.13971.pdf&quot;&gt;LLaMA (13B) outperforms GPT-3 (175B)&lt;/a&gt; highlighting its ability to extract more compute from each model parameter.&lt;/p&gt;

&lt;p&gt;In this blog post, we use LLaMA as an example model to demonstrate the capabilities of PyTorch/XLA for LLM inference. We discuss how the computation techniques and optimizations discussed here improve inference latency by 6.4x on 65B parameter LLaMA models powered by Google Cloud TPU v4 (v4-16).&lt;/p&gt;

&lt;h2 id=&quot;model-overview&quot;&gt;Model Overview&lt;/h2&gt;

&lt;p&gt;We demonstrate the performance capabilities of PyTorch/XLA on &lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;LLaMA&lt;/a&gt;, the latest LLM from Meta. We showcase performance optimizations on a series of common LLaMA configurations. Notice the 175B parameter model configuration is absent in the public domain. For the 175B parameter model mentioned below, we apply &lt;a href=&quot;https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/models/opt/modeling_opt.py#L804&quot;&gt;OPT 175B model configuration&lt;/a&gt; to the LLaMA code base. Unless stated otherwise, in all configurations, we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_len=256&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dtype=bfloat16&lt;/code&gt; for weights and activations.&lt;/p&gt;

&lt;h4 id=&quot;table-1-model-configurations-explored-in-this-article&quot;&gt;Table 1: Model Configurations Explored in this article&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;LLaMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;Model Hyper Parameters&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;# Parameters&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Dimensions&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;N Heads&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;N Layers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Max Seq Len&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;7B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;4,096
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;33B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;6,656
   &lt;/td&gt;
   &lt;td&gt;52
   &lt;/td&gt;
   &lt;td&gt;60
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;65B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;8,192
   &lt;/td&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;80
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;175B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;12,288
   &lt;/td&gt;
   &lt;td&gt;96
   &lt;/td&gt;
   &lt;td&gt;96
   &lt;/td&gt;
   &lt;td&gt;256
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;performance-challenges-of-llms&quot;&gt;Performance Challenges of LLMs&lt;/h2&gt;

&lt;p&gt;LLMs have a few properties that make them challenging for compiler optimizations. (a) LLMs use autoregressive decoding to generate the next token baked on the previous ones; this means prompt tensors and coaches have a dynamic shape. (b) LLMs must work with variable input prompt lengths without triggering recompilation due to input tensor shape changes; input tensors must be properly bucketized and padded to avoid recompilation. (c) LLMs often require more memory than a single TPU (or GPU) device can support. A model-sharding scheme is required to fit the model across a distributed compute architecture. For instance, a LLaMA model with 65B parameters can fit on a v4-16 Cloud TPU, which is comparable to 8 A100 GPUs. (d) running LLMs in production can be expensive; one way to improve performance per total cost of ownership (Perf/TCO) is via quantization; quantization can potentially reduce hardware requirements.&lt;/p&gt;

&lt;h2 id=&quot;inference-tech-stack-in-pytorchxla&quot;&gt;Inference Tech Stack in PyTorch/XLA&lt;/h2&gt;

&lt;p&gt;Our goal is to offer the AI community a high performance inference stack. PyTorch/XLA integrates with &lt;a href=&quot;https://pytorch.org/docs/stable/dynamo/index.html&quot;&gt;TorchDynamo&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla/#pjrt-runtime-beta&quot;&gt;PjRt&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla-path-forward/&quot;&gt;OpenXLA&lt;/a&gt;, and various model parallelism schemes. TorchDynamo eliminates tracing overhead at runtime, PjRt enables efficient host-device communication; PyTorch/XLA traceable collectives enable model and data parallelism on LLaMA via &lt;a href=&quot;https://pytorch.org/docs/stable/dynamo/index.html&quot;&gt;TorchDynamo&lt;/a&gt;. To try our results, please use our custom &lt;a href=&quot;https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly+20230422-cp38-cp38-linux_x86_64.whl&quot;&gt;torch&lt;/a&gt;, &lt;a href=&quot;https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly+20230422-cp38-cp38-linux_x86_64.whl&quot;&gt;torch-xla&lt;/a&gt; wheels to reproduce our &lt;a href=&quot;https://github.com/pytorch-tpu/llama/tree/blog&quot;&gt;LLaMA inference solution&lt;/a&gt;. PyTorch/XLA 2.1 will support the features discussed in this post by default.&lt;/p&gt;

&lt;h2 id=&quot;parallel-computing&quot;&gt;Parallel Computing&lt;/h2&gt;

&lt;h3 id=&quot;fairscale-sharding&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/fairscale&quot;&gt;FairScale&lt;/a&gt; Sharding&lt;/h3&gt;

&lt;p&gt;LLaMA uses FairScale model sharding API (&lt;a href=&quot;https://github.com/facebookresearch/llama/blob/main/llama/model.py#L13-L17&quot;&gt;fairscale.nn.model_parallel.layers&lt;/a&gt;). We built an equivalent representation of this API using PyTorch/XLA communication collective (CC) ops such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all-reduce&lt;/code&gt; to communicate program state (e.g. activations) between accelerators. TorchDynamo does not fully support capturing CC ops currently (a.k.a. &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/93173&quot;&gt;traceable collectives&lt;/a&gt;). Without this support, a TorchDynamo FX graph would be cut at every device communication, meaning at every model layer. Graph cuts lead to performance loss as the underlying XLA compiler loses full graph optimization opportunities. To resolve this, we offer PyTorch/XLA traceable collectives by integrating the dispatcher collectives into our existing CC APIs. The difference is we don’t need to insert &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c10d.wait()&lt;/code&gt; ops after collectives, given the lazy execution nature of PyTorch/XLA. With support for traceable collectives, PyTorch/XLA allows singular FX graph generation in TorchDynamo.&lt;/p&gt;

&lt;h2 id=&quot;autoregressive-decoding-on-pytorchxla&quot;&gt;Autoregressive Decoding on PyTorch/XLA&lt;/h2&gt;

&lt;p&gt;LLMs need autoregressive decoding to feed the previous word as a prompt to predict the next token. Autoregressive decoding leads to unbounded dynamic shape problems, which in turn causes recompilation of every prompt. We optimized the LLaMA autoregressive decoder to operate with fixed shapes that in-place updates the KV-cache, output sequences, and attention masks during every token generation. With a combination of padding, masking, and index ops, we avoided excessive graph recompilation, thereby achieving efficient autoregressive decoding.&lt;/p&gt;

&lt;h3 id=&quot;kv-cache-optimization&quot;&gt;KV-Cache Optimization&lt;/h3&gt;

&lt;p&gt;LLaMA implements autoregressive decoding with KV-cache. For every generated token, the KV-cache stores the attention key/value activations of each Transformer layer. Thus, upon decoding a new token, the key/values of prior tokens no longer need recomputation.&lt;/p&gt;

&lt;p&gt;In LLaMA, the KV-cache tensor slices are updated in-place; this leads to recompilation events every time a token is generated. To address this issue, we use index tensors and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.index_copy()&lt;/code&gt; ops to replace the in-place slice updates. Attention masks and output sequences also benefit from the same optimization.&lt;/p&gt;

&lt;h2 id=&quot;input-prompt-optimization&quot;&gt;Input Prompt Optimization&lt;/h2&gt;

&lt;p&gt;Variable length input prompts are common in LLM applications. This property causes input tensor shape dynamism and in turn recompilation events. When processing a prompt to fill the KV-cache, we either (a) process the input prompt token-by-token, or (b) process the whole prompt in one iteration. The pros and cons of each method are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pre-compile 1 graph and process a prompt token-by-token
    &lt;ul&gt;
      &lt;li&gt;Practical: 1 graph is compiled during warm-up&lt;/li&gt;
      &lt;li&gt;Slow: &lt;em&gt;O(L)&lt;/em&gt; to process an input prompt length &lt;em&gt;L&lt;/em&gt; - a disadvantage for long prompts&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pre-compile all graphs with input lengths ranging from 1 to max_seq_len (e.g. 2,048)
    &lt;ul&gt;
      &lt;li&gt;Impractical: pre-compile and cache &lt;em&gt;max_seq_len&lt;/em&gt; graphs during warm-up time&lt;/li&gt;
      &lt;li&gt;Fast: 1 graph execution to process the full prompt&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We introduce prompt length bucketization, an optimization to strike a balance between the two alternatives. We define a set of ascending bucket sizes, &lt;em&gt;(b&lt;sub&gt;0&lt;/sub&gt;,b&lt;sub&gt;1&lt;/sub&gt;,b&lt;sub&gt;2&lt;/sub&gt;,…,b&lt;sub&gt;B-1&lt;/sub&gt;)&lt;/em&gt;, and then pre-compile program graphs with input sizes according to these bucket values, &lt;em&gt;(G&lt;sub&gt;0&lt;/sub&gt;,G&lt;sub&gt;1&lt;/sub&gt;,G&lt;sub&gt;2&lt;/sub&gt;,…,G&lt;sub&gt;B-1&lt;/sub&gt;)&lt;/em&gt;; &lt;em&gt;B&lt;/em&gt; is the number of buckets. For a given input prompt, we round up the prompt length to the closest bucket value &lt;em&gt;b&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt;, pad the sequence, and use &lt;em&gt;G&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt; to process the prompt in one iteration. The computation on the padding tokens is discarded. For prompts larger than the largest bucket size, we process them section-by-section.&lt;/p&gt;

&lt;p&gt;The optimal bucket sizes should be determined by prompt length distribution in a target application. Here, we adopt bucket lengths: 128, 256, 384, 512. Any input prompt with up to 2,047 tokens requires up to 4 graph executions. For example, a 1,500 input prompt with generation length of 256 requires 260 graph executions - 4 to process the input, and 256 to generate the output.&lt;/p&gt;

&lt;h2 id=&quot;quantization&quot;&gt;Quantization&lt;/h2&gt;

&lt;p&gt;Quantization reduces the number of bits necessary to represent a value; it reduces the bandwidth to communicate data across multiple accelerator nodes (via collectives) and lowers the hardware requirements to serve a specific model size.&lt;/p&gt;

&lt;p&gt;Normally, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BF16&lt;/code&gt; weights, a 175B parameter model would consume about 351GB of memory, and therefore require a v4-32 instance to accommodate the model. By quantizing the weights to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;INT8&lt;/code&gt;, we reduced the model size by roughly 50%, allowing it to run on a smaller v4-16 instance. Because LLaMA shards model activations, quantization offers negligible communication gain.&lt;/p&gt;

&lt;p&gt;In our experiments, we quantized the linear layer. Since LLaMA model checkpoints are unavailable publicly, and our goal is to evaluate performance, the quantized model is initialized with random weights.Recent literature such as &lt;a href=&quot;https://arxiv.org/pdf/2306.00978.pdf&quot;&gt;AWQ&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/2305.12356.pdf&quot;&gt;Integer or Floating Point?&lt;/a&gt; offer insights into performance properties of LLaMA under various low-bit quantization schemes.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-batch-size-on-quantization-performance&quot;&gt;Effect of Batch Size on Quantization Performance&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2304.01433.pdf&quot;&gt;TPU v4&lt;/a&gt; is programmed to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul&lt;/code&gt; on the Matrix Multiply Unit (MXU) when the model batch size (BS) &amp;gt; 1. For BS = 1, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul&lt;/code&gt; runs on the Vector Processor Unit (VPU). Since MXU is more efficient than VPU, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;INT8&lt;/code&gt; quantization gains performance at BS&amp;gt;1. See &lt;a href=&quot;#heading=h.4xqv3t16rl42&quot;&gt;Performance Analysis&lt;/a&gt; section for details.&lt;/p&gt;

&lt;h2 id=&quot;op-support&quot;&gt;Op Support&lt;/h2&gt;

&lt;p&gt;Occasionally, new models introduce new mathematical operations that require PyTorch/XLA to extend its supported op set for compilation. For LLaMA, we supported: &lt;a href=&quot;https://github.com/pytorch/xla/issues/4839&quot;&gt;multinomial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;LLaMA works on PyTorch/XLA out of the box on LazyTensorCore. We use this configuration as a baseline for our follow up analysis. All experiments assume 256-long input prompts. In the absence of a publicly available model checkpoint, we used random tensor initialization for this inference stack optimization effort. A model checkpoint is not expected to change latency results discussed here.&lt;/p&gt;

&lt;h3 id=&quot;model-sizing&quot;&gt;Model Sizing&lt;/h3&gt;

&lt;p&gt;Assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; is the number of parameters, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dimensions&lt;/code&gt; is the hidden size, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_layers&lt;/code&gt; is the number of layers, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_heads&lt;/code&gt; is the number of attention heads, the equation below can be used to approximate the model size. See the &lt;a href=&quot;#heading=h.tehlvi942ssk&quot;&gt;Model Overview&lt;/a&gt; section for details.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N = (dimensions)^2 * n_layers * 12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_heads&lt;/code&gt; doesn’t affect &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;, but the following equation holds for the open sourced model configs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dim = 128 * n_heads
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;cache-sizing&quot;&gt;Cache Sizing&lt;/h4&gt;

&lt;p&gt;Both model parameters and the cache layers in the Attention block contribute to memory consumption. Since the default LLaMA model uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BF16&lt;/code&gt; weights, the memory consumption calculation in this section is based on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BF16&lt;/code&gt; weights.&lt;/p&gt;

&lt;p&gt;The size of the cache layer is calculated by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cache_size = max_batch_size * max_seq_len * dimensions&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_batch_size = 1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_len = 256 &lt;/code&gt;are used as an example configuration in the following calculations. There are 2 cache layers in each Attention block. So, the total LLaMA cache size (in Bytes) is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;total_cache_size = n_layers * 2 * cache_size * (2 bytes)&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;tpu-v4-hardware-sizing&quot;&gt;TPU v4 Hardware Sizing&lt;/h4&gt;

&lt;p&gt;Each TPU v4 chip has 32GB of available High-Bandwidth Memory (HBM). Table 2 has the details on memory consumption and the number of required TPU chips to hold a LLaMA model.&lt;/p&gt;

&lt;h4 id=&quot;table-2-llama-tpu-v4-hbm-requirements-ie-tpu-v4-chip-requirements&quot;&gt;Table 2: LLaMA TPU v4 HBM requirements (i.e. TPU v4 chip requirements)&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;# Parameters&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Parameter (MB)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Cache (MB)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Total (GB)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Min # of TPU v4 Chips&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;7B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;14,000
   &lt;/td&gt;
   &lt;td&gt;134
   &lt;/td&gt;
   &lt;td&gt;14.128
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;33B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;66,000
   &lt;/td&gt;
   &lt;td&gt;408
   &lt;/td&gt;
   &lt;td&gt;66.41
   &lt;/td&gt;
   &lt;td&gt;3
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;65B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;130,000
   &lt;/td&gt;
   &lt;td&gt;671
   &lt;/td&gt;
   &lt;td&gt;130.67
   &lt;/td&gt;
   &lt;td&gt;5
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;175B&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;350,000
   &lt;/td&gt;
   &lt;td&gt;1,208
   &lt;/td&gt;
   &lt;td&gt;351.21
   &lt;/td&gt;
   &lt;td&gt;11
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;Below are useful metrics to measure inference speed. Assuming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;/code&gt; is the total time, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is the batch size, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; is the decoded sequence length.&lt;/p&gt;

&lt;h4 id=&quot;latency-definition&quot;&gt;Latency Definition&lt;/h4&gt;

&lt;p&gt;Latency is the time it takes to get the decoded result at target length &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt;, regardless of the batch size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt;. Latency represents how long the user should wait to get the response from the generation model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Latency = T (s)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;per-token-latency&quot;&gt;Per-token latency&lt;/h4&gt;

&lt;p&gt;One step of autoregressive decoding generates a token for each sample in the batch. Per-token latency is the average time for that one step.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Per-token latency = T / L (s/token)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;throughput&quot;&gt;Throughput&lt;/h4&gt;

&lt;p&gt;Throughput measures how many tokens are generated per unit time. While it’s not a useful metric for evaluating online serving it is useful to measure the speed of batch processing.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Throughput = B * L / T (tokens/s)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To minimize confusion and misinterpretation, it’s better to avoid metrics like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T / (B * L)&lt;/code&gt;, which mixes latency and throughput.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;Figure 1 shows latency / token results for LLaMA 7B to 175B models. In each case, the model is run on a range of TPU v4 configurations. For instance, LLaMA 7B shows 4.7ms/token and 3.8ms/token on v4-8 and v4-16 respectively. For more comparison, visit the HuggingFace &lt;a href=&quot;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&quot;&gt;LLM performance leaderboard&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the absence of the features discussed in this blog post, the LLaMA 65B running on v4-32 delivers 120ms/token instead of 14.5ms/token obtained here, leading to &lt;strong&gt;8.3x&lt;/strong&gt; speedup. As discussed earlier, developers are encouraged to try our custom &lt;a href=&quot;https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch-nightly+20230422-cp38-cp38-linux_x86_64.whl&quot;&gt;torch&lt;/a&gt;, &lt;a href=&quot;https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-nightly+20230422-cp38-cp38-linux_x86_64.whl&quot;&gt;torch-xla&lt;/a&gt; wheels that unlock the repro of &lt;a href=&quot;https://github.com/pytorch-tpu/llama/tree/blog&quot;&gt;LLaMA inference&lt;/a&gt; results shared here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im1.svg&quot; alt=&quot;Figure 1: LLaMA Inference Performance on TPU v4 hardware&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: LLaMA Inference Performance on TPU v4 hardware&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;PyTorch/XLA:GPU performance is better than PyTorch:GPU eager and similar to PyTorch Inductor. PyTorch/XLA:TPU performance is superior to PyTorch/XLA:GPU. In the near future, XLA:GPU will deliver optimizations that bring parity with XLA:TPU. The single A100 configuration only fits LLaMA 7B, and the 8-A100 doesn’t fit LLaMA 175B.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im2.svg&quot; alt=&quot;Figure 2: LLaMA Inference Performance on GPU A100 hardware&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: LLaMA Inference Performance on GPU A100 hardware&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;As the batch size increases, we observe a sublinear increase in per-token latency highlighting the tradeoff between hardware utilization and latency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im3.svg&quot; alt=&quot;Figure 3: LLaMA Inference Performance across different batch sizes&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: LLaMA Inference Performance across different batch sizes&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Our studies suggest the impact of maximum sequence input length (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_len&lt;/code&gt;) on inference latency is relatively minimal. We attribute this to the sequential and iterative nature of token generation. The small difference in performance can be due to KV cache access latency changes as the storage size increases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im4.svg&quot; alt=&quot;Figure 4: LLaMA Inference Performance across different prompt lengths&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: LLaMA Inference Performance across different prompt lengths&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;LLMs are often memory bound applications; thus, by quantizing model parameters we enable loading and executing a larger tensor on MXUs per unit time (i.e. HBM ⇒ CMEM and CMEM ⇒ MXU data moevment). Figure 5 shows &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;INT8&lt;/code&gt; weight-only quantization offers 1.6x-1.9x speedup allowing running a larger model on a given hardware.&lt;/p&gt;

&lt;p&gt;When BS=1, INT8 tensors are dispatched to VPU which is smaller than MXU (see the &lt;a href=&quot;https://arxiv.org/pdf/2304.01433.pdf&quot;&gt;TPU v4 paper&lt;/a&gt;); otherwise, MXU is used. As a result, when BS=1, quantization memory bandwidth gains are offset by lack of MXU utilization. When BS&amp;gt;1, however, memory gains deliver superior latency on the quantized model. For example, in the case of 175B parameters LLaMA, v4-16 with quantiztion and v4-32 without quantiztion deliver similar performance. Note we do not provied &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FP8&lt;/code&gt; comparisons because PyTorch is yet to offer this data type.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im5.svg&quot; alt=&quot;Figure 5: LLaMA Inference Performance vs. weight-only quantization. The missing blue bars suggest the model size doesn’t fit in the specified TPU hardware.&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: LLaMA Inference Performance vs. weight-only quantization. The missing blue bars suggest the model size doesn’t fit in the specified TPU hardware.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Figure 6 demonstrates the steady performance advantage of PyTorch/XLA as the input prompt length grows from 10 tokens to 1,500 tokens. This strong scaling capability suggests minimal PyTorch/XLA recompilation events enabling a wide range of real-world applications. In this experiment, the maximum length is 2,048 and maximum generation length is 256.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/low-latency/im6.svg&quot; alt=&quot;Figure 6: LLaMA Inference Performance vs. Input Prompt Length&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: LLaMA Inference Performance vs. Input Prompt Length&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; so that we can openly collaborate. You can also &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;try out&lt;/a&gt; PyTorch/XLA for yourself on various XLA devices including TPUs and GPUs.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br /&gt;
The PyTorch/XLA Team at Google&lt;br /&gt;
#PoweredByPyTorch&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Milad Mohammadi, Jiewen Tan, Liyang Lu, Siyuan Liu, Yeounoh Chung,  Wonjoo Lee, Manfei Bai, Steven Krawczyk, Shauheen Zahirazami, Alex Wertheim, Meghan Cowan, Jack Cao,  Joe Spisak</name>
        
        
      </author>

      

      

      
        <summary type="html">Background &amp;amp; State of the Art</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Optimized PyTorch 2.0 Inference with AWS Graviton processors</title>
      <link href="https://pytorch.org/blog/optimized-pytorch-w-graviton/" rel="alternate" type="text/html" title="Optimized PyTorch 2.0 Inference with AWS Graviton processors" />
      <published>2023-06-22T00:00:00-07:00</published>
      <updated>2023-06-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/optimized-pytorch-w-graviton</id>
      <content type="html" xml:base="https://pytorch.org/blog/optimized-pytorch-w-graviton/">&lt;p&gt;New generations of CPUs offer significant performance improvement in machine learning (ML) inference due to specialized built-in instructions. Combined with their flexibility, high speed of development, and low operating cost, these general-purpose processors offer an alternative ML inference solution to other existing hardware solutions.&lt;/p&gt;

&lt;p&gt;AWS, Arm, Meta, and others helped optimize the performance of PyTorch 2.0 inference for Arm-based processors. As a result, we are delighted to announce that Arm-based AWS Graviton instance inference performance for PyTorch 2.0 is up to 3.5 times the speed for ResNet-50 compared to the previous PyTorch release, and up to 1.4 times the speed for BERT, making Graviton-based instances the fastest compute optimized instances on AWS for these models (see the following graph).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im1.png&quot; alt=&quot;Relative speed improvement achieved by upgrading PyTorch to 2.0&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Relative speed improvement achieved by upgrading from PyTorch version 1.13 to 2.0 (higher is better). The performance is measured on c7g.4xlarge instances.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;As shown in the next graph, we measured up to 50% cost savings for PyTorch inference with Graviton3-based c7g instances across Torch Hub ResNet-50 and multiple Hugging Face models compared to comparable x86-based compute optimized Amazon EC2 instances. For that graph, we first measured the cost per million inference for the five instance types. Then, we normalized the cost per million inference results to a c5.4xlarge instance, which is the baseline measure of “1” on the Y-axis of the chart.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im2.png&quot; alt=&quot;Relative cost of PyTorch inference running on different AWS instances&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Relative cost of PyTorch inference running on different AWS instances (lower is better). &lt;br /&gt;Source: AWS ML Blog on &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/optimized-pytorch-2-0-inference-with-aws-graviton-processors/&quot;&gt;Graviton PyTorch2.0 inference performance&lt;/a&gt;.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Similar to the preceding inference cost comparison graph, the following graph shows the model p90 latency for the same five instance types. We normalized the latency results to the c5.4xlarge instance, which is the baseline measure of “1” on the Y-axis of the chart. The c7g.4xlarge (AWS Graviton3) model inference latency is up to 50% better than the latencies measured on c5.4xlarge, c6i.4xlarge, and c6a.4xlarge. \&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im3.png&quot; alt=&quot;Relative latency (p90) of PyTorch inference running on different AWS instances&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: Relative latency (p90) of PyTorch inference running on different AWS instances (lower is better). &lt;br /&gt;Source: AWS ML Blog on &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/optimized-pytorch-2-0-inference-with-aws-graviton-processors/&quot;&gt;Graviton PyTorch2.0 inference performance&lt;/a&gt;.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimization-details&quot;&gt;Optimization details&lt;/h2&gt;

&lt;p&gt;PyTorch supports Compute Library for the Arm® Architecture (ACL) GEMM kernels via the oneDNN backend (previously called “MKL-DNN”) for AArch64 platforms. The optimizations are primarily for PyTorch ATen CPU BLAS, ACL kernels for fp32 and bfloat16, and oneDNN primitive caching. There are no frontend API changes, so no changes are required at the application level to get these optimizations working on Graviton3-based instances.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-level-optimizations&quot;&gt;PyTorch level optimizations&lt;/h3&gt;

&lt;p&gt;We extended the ATen CPU BLAS interface to accelerate more operators and tensor configurations via oneDNN backend for aarch64 platform. The following diagram highlights (in orange) the optimized components that improved the PyTorch inference performance on aarch64 platform.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im4.png&quot; alt=&quot;PyTorch software stack highlighting (in orange) the components optimized for inference performance improvement on AArch64 platform&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 4&lt;/strong&gt;: PyTorch software stack highlighting (in orange) the components optimized for inference performance improvement on AArch64 platform&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;acl-kernels-and-bfloat16-fpmath-mode&quot;&gt;ACL kernels and BFloat16 FPmath mode&lt;/h3&gt;

&lt;p&gt;The ACL library provides Neon and SVE optimized GEMM kernels for both fp32 and bfloat16 formats: These kernels improve the SIMD hardware utilization and reduce the end to end inference latencies. The bfloat16 support in Graviton3 allows efficient deployment of models trained using bfloat16, fp32 and Automatic Mixed Precision (AMP). The standard fp32 models use bfloat16 kernels via oneDNN FPmath mode without model quantization. They provide up to two times faster performance compared to existing fp32 model inference without bfloat16 FPmath support. For more details on ACL GEMM kernel support, refer to &lt;a href=&quot;https://github.com/ARM-software/ComputeLibrary&quot;&gt;Arm Compute Library github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;primitive-caching&quot;&gt;Primitive Caching&lt;/h3&gt;

&lt;p&gt;The following call sequence diagram shows how ACL operators are integrated into oneDNN backend. As shown in the diagram, ACL objects are handled as oneDNN resources instead of the primitive objects. This is because the ACL objects are stateful and mutable. Since the ACL objects are handled as resource objects, they are not cacheable with the default primitive caching feature supported in oneDNN. We implemented primitive caching at ideep operator level for “convolution”, “matmul” and “inner product” operators to avoid redundant GEMM kernel initialization and tensor allocation overhead.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im5.png&quot; alt=&quot;Call sequence diagram showing how the Compute Library for the Arm® Architecture (ACL) GEMM kernels are integrated into oneDNN backend&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 5&lt;/strong&gt;: Call sequence diagram showing how the Compute Library for the Arm® Architecture (ACL) GEMM kernels are integrated into oneDNN backend&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-take-advantage-of-the-optimizations&quot;&gt;How to take advantage of the optimizations&lt;/h2&gt;

&lt;p&gt;Install the PyTorch 2.0 wheel from the official repo and set environment variables to enable the additional optimizations.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Install Python
sudo apt-get update
sudo apt-get install -y python3 python3-pip

# Upgrade pip3 to the latest version
python3 -m pip install --upgrade pip

# Install PyTorch and extensions
python3 -m pip install torch
python3 -m pip install torchvision torchaudio torchtext

# Turn on Graviton3 optimization
export DNNL_DEFAULT_FPMATH_MODE=BF16
export LRU_CACHE_CAPACITY=1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;running-an-inference&quot;&gt;Running an inference&lt;/h2&gt;

&lt;p&gt;You can use PyTorch &lt;a href=&quot;https://github.com/pytorch/benchmark&quot;&gt;torchbench&lt;/a&gt; to measure the CPU inference performance improvements, or to compare different instance types.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Pre-requisite:
# pip install PyTorch2.0 wheels and set the above mentioned environment variables

# Clone PyTorch benchmark repo
git clone https://github.com/pytorch/benchmark.git

# Setup ResNet-50 benchmark
cd benchmark
python3 install.py resnet50

# Install the dependent wheels
python3 -m pip install numba

# Run ResNet-50 inference in jit mode. On successful completion of the inference runs,
# the script prints the inference latency and accuracy results
python3 run.py resnet50 -d cpu -m jit -t eval --use_cosine_similarity
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;performance-analysis&quot;&gt;Performance Analysis&lt;/h2&gt;

&lt;p&gt;Now, we will analyze the inference performance of ResNet-50 on Graviton3-based c7g instance using PyTorch profiler. We run the code below with PyTorch 1.13 and PyTorch 2.0 and run the inference for a few iterations as a warmup before measuring the performance.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Turn on Graviton3 optimization
export DNNL_DEFAULT_FPMATH_MODE=BF16
export LRU_CACHE_CAPACITY=1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torchvision import models
sample_input = [torch.rand(1, 3, 224, 224)]
eager_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
model = torch.jit.script(eager_model, example_inputs=[sample_input, ])

model = model.eval()
model = torch.jit.optimize_for_inference(model)

with torch.no_grad():
    # warmup runs
    for i in range(10):
        model(*sample_input)
    prof = torch.profiler.profile(
      on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'), record_shapes=True, with_stack=True)
    # profile after warmup
    prof.start()
    model(*sample_input)
    prof.stop()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We use tensorboard to view results of the profiler and analyze model performance.&lt;/p&gt;

&lt;p&gt;Install PyTorch Profiler Tensorboard plugin as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install torch_tb_profiler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Launch the tensorboard using&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensorboard --logdir=./logs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Launch the following in the browser to view the profiler output. The profiler supports ‘Overview’, ‘Operator’, ‘Trace’ and ‘Module’ views to get insight into the inference execution.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://localhost:6006/#pytorch_profiler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following diagram is the profiler ‘Trace’ view which shows the call stack along with the execution time of each function. In the profiler, we selected the forward() function to get the overall inference time. As shown in the diagram, the inference time for the ResNet-50 model on Graviton3-based c7g instance is around 3 times faster in PyTorch 2.0 compared to PyTorch 1.13.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im6.png&quot; alt=&quot;Profiler Trace view: Forward pass wall duration on PyTorch 1.13 and PyTorch 2.0&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 6&lt;/strong&gt;: Profiler Trace view: Forward pass wall duration on PyTorch 1.13 and PyTorch 2.0&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The next diagram is the ‘Operator’ view which shows the list of PyTorch operators and their execution time. Similar to the preceding Trace view, the Operator view shows that the operator host duration for the ResNet-50 model on Graviton3-based c7g instance is around 3 times faster in PyTorch 2.0 compared to PyTorch 1.13.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optimized/im7.png&quot; alt=&quot;Profiler Operator view: Forward operator Host duration on PyTorch 1.13 and PyTorch 2.0&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Image 7&lt;/strong&gt;: Profiler Operator view: Forward operator Host duration on PyTorch 1.13 and PyTorch 2.0&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-hugging-face-models&quot;&gt;Benchmarking Hugging Face models&lt;/h2&gt;

&lt;p&gt;You can use the&lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html&quot;&gt; Amazon SageMaker Inference Recommender&lt;/a&gt; utility to automate performance benchmarking across different instances. With Inference Recommender, you can find the real-time inference endpoint that delivers the best performance at the lowest cost for a given ML model. We collected the preceding data using the Inference Recommender notebooks by deploying the models on production endpoints. For more details on Inference Recommender, refer to the&lt;a href=&quot;https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-inference-recommender/huggingface-inference-recommender/huggingface-inference-recommender.ipynb&quot;&gt; amazon-sagemaker-examples&lt;/a&gt; GitHub repo. We benchmarked the following models for this post:&lt;a href=&quot;https://pytorch.org/hub/pytorch_vision_resnet/&quot;&gt; ResNet50 image classification&lt;/a&gt;,&lt;a href=&quot;https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english&quot;&gt; DistilBERT sentiment analysis&lt;/a&gt;,&lt;a href=&quot;https://huggingface.co/roberta-base&quot;&gt; RoBERTa fill mask&lt;/a&gt;, and&lt;a href=&quot;https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment&quot;&gt; RoBERTa sentiment analysis&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;For PyTorch 2.0, the Graviton3-based C7g instance is the most cost-effective compute optimized Amazon EC2 instance for inference. These instances are available on&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-adds-new-graviton-based-instances-model-deployment/&quot;&gt; SageMaker&lt;/a&gt; and&lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/c7g/&quot;&gt; Amazon EC2&lt;/a&gt;. The&lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt; AWS Graviton Technical Guide&lt;/a&gt; provides the list of optimized libraries and best practices that will help you achieve cost benefit with Graviton instances across different workloads.&lt;/p&gt;

&lt;p&gt;If you find use cases where similar performance gains are not observed on Graviton, please open an issue on the &lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt;aws-graviton-getting-started&lt;/a&gt; github to let us know about it. We will continue to add more performance improvements to make AWS Graviton-based instances the most cost-effective and efficient general purpose processor for inference using PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;We would like to thank Ali Saidi (Sr. Principal Engineer) and Csaba Csoma (Sr. Manager, Software Development) from AWS, Ashok Bhat (Sr. Product Manager), Nathan Sircombe (Sr. Engineering Manager) and Milos Puzovic (Principal Software Engineer) from Arm for their support during the Graviton PyTorch inference optimization work. We would also like to thank Geeta Chauhan (Engineering Leader, Applied AI) from Meta for her guidance on this blog.&lt;/p&gt;

&lt;h2 id=&quot;about-the-authors&quot;&gt;About the authors&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Sunita Nadampalli&lt;/strong&gt; is a ML Engineer and Software Development Manager at AWS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ankith Gunapal&lt;/strong&gt; is an AI Partner Engineer at Meta(PyTorch).&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sunita Nadampalli from AWS &amp; Ankith Gunapal from Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">New generations of CPUs offer significant performance improvement in machine learning (ML) inference due to specialized built-in instructions. Combined with their flexibility, high speed of development, and low operating cost, these general-purpose processors offer an alternative ML inference solution to other existing hardware solutions.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">🎉 PyTorch Docathon H1 2023 Wrap-up 🎉</title>
      <link href="https://pytorch.org/blog/docathon-h1-2023-wrap-up/" rel="alternate" type="text/html" title="🎉 PyTorch Docathon H1 2023 Wrap-up 🎉" />
      <published>2023-06-16T00:00:00-07:00</published>
      <updated>2023-06-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/docathon-h1-2023-wrap-up</id>
      <content type="html" xml:base="https://pytorch.org/blog/docathon-h1-2023-wrap-up/">&lt;p&gt;Thank you to all who participated in our first ever PyTorch Docathon, the results have been nothing short of amazing! We want to extend our sincerest gratitude to all the participants who made this event a resounding success. Your passion, talent, and hard work have left an indelible mark on the PyTorch documentation.&lt;/p&gt;

&lt;p&gt;The virtual Docathon ran from May 31 through June 15 with more than 230 registrants and more than 110 participants joining the Docathon Slack channel, the energy and enthusiasm were palpable. Entrants were judged on the difficulty of submissions that resulted in over 40 merged pull requests and the publication of four new tutorials and addition of one new example.&lt;/p&gt;

&lt;p&gt;We want to give a special shout-out to our top contributors, who went above and beyond during this event. Your dedication and expertise have been invaluable in enhancing the PyTorch documentation and empowering developers worldwide. See the full list of contributors &lt;a href=&quot;https://github.com/pytorch/tutorials/blob/main/docathon-leaderboard.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Meet the top contributors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First place: &lt;a href=&quot;https://github.com/JoseLuisC99&quot;&gt;JoseLuisC99&lt;/a&gt;, &lt;a href=&quot;https://github.com/QasimKhan5x&quot;&gt;QasimKhan5x&lt;/a&gt;, &lt;a href=&quot;https://github.com/bjhargrave&quot;&gt;bjhargrave&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Second place: &lt;a href=&quot;https://github.com/Aidyn-A&quot;&gt;Aidyn-A&lt;/a&gt;, &lt;a href=&quot;https://github.com/CaoE&quot;&gt;CaoE&lt;/a&gt;, &lt;a href=&quot;https://github.com/HemanthSai7&quot;&gt;HemanthSai7&lt;/a&gt;, &lt;a href=&quot;https://github.com/leslie-fang-intel&quot;&gt;leslie-fang-intel&lt;/a&gt;, 	&lt;a href=&quot;https://github.com/Valentine233&quot;&gt;Valentine233&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Third place: &lt;a href=&quot;https://github.com/TheMemoryDealer&quot;&gt;TheMemoryDealer&lt;/a&gt;, &lt;a href=&quot;https://github.com/arunppsg&quot;&gt;arunppsg&lt;/a&gt;, &lt;a href=&quot;https://github.com/noqqaqq&quot;&gt;noqqaqq&lt;/a&gt;, &lt;a href=&quot;https://github.com/zabboud&quot;&gt;zabboud&lt;/a&gt;, &lt;a href=&quot;https://github.com/kiersten-stokes&quot;&gt;kiersten-stokes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Honorable mentions: &lt;a href=&quot;https://github.com/frasertajima&quot;&gt;frasertajima&lt;/a&gt;, &lt;a href=&quot;https://github.com/nairbv&quot;&gt;nairbv&lt;/a&gt;, &lt;a href=&quot;https://github.com/mikebrow&quot;&gt;mikebrow&lt;/a&gt;, &lt;a href=&quot;https://github.com/NeoKish&quot;&gt;NeoKish&lt;/a&gt;, &lt;a href=&quot;https://github.com/fabiogomez11c&quot;&gt;fabiogomez11c&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we bring this Docathon to a close, we encourage each and every one of you to stay inspired and keep  contributing to PyTorch &lt;a href=&quot;https://github.com/pytorch/tutorials#contributing&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md&quot;&gt;code&lt;/a&gt;, and pushing the boundaries of what’s possible with PyTorch. Your collective efforts are shaping the landscape of deep learning and fostering innovation in the AI community.&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Thank you to all who participated in our first ever PyTorch Docathon, the results have been nothing short of amazing! We want to extend our sincerest gratitude to all the participants who made this event a resounding success. Your passion, talent, and hard work have left an indelible mark on the PyTorch documentation.</summary>
      

      
      
    </entry>
  
</feed>


