<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-09-10T17:58:48-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Graphcore Joins the PyTorch Foundation as a General Member</title>
      <link href="https://pytorch.org/blog/graphcore-joins-pytorch/" rel="alternate" type="text/html" title="Graphcore Joins the PyTorch Foundation as a General Member" />
      <published>2023-09-06T00:00:00-07:00</published>
      <updated>2023-09-06T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/graphcore-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/graphcore-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/graphcore-logo.jpg&quot; alt=&quot;Graphcore logo&quot; style=&quot;max-width:350px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Graphcore has joined as a general member.&lt;/p&gt;

&lt;p&gt;Graphcore is a UK-based company that specializes in designing and manufacturing AI accelerators, hardware and software specifically tailored for artificial intelligence and machine learning workloads.&lt;/p&gt;

&lt;p&gt;“We’re thrilled that PyTorch is the leading framework for development on the Graphcore  platform,” said Executive Director of the PyTorch Foundation Ibrahim Haddad. “Graphcore has played  an important role in the hardware and open source space, and we look forward to their continued contributions to PyTorch.”&lt;/p&gt;

&lt;p&gt;Graphcore has contributed to the PyTorch ecosystem by developing integrations to run on their IPU hardware. These integrations enable researchers and practitioners to use their preferred frameworks while taking advantage of Graphcore’s specialized hardware.&lt;/p&gt;

&lt;p&gt;“At Graphcore we’re truly aligned with PyTorch’s objective of reducing the barrier of entry to AI practitioners. By supporting a native PyTorch software environment for IPUs we are giving developers access to new underlying hardware, designed from the ground up for AI, to help unlock new AI techniques to improve efficiency or performance and to drive breakthroughs in AI research and applications, with the same user-friendly PyTorch framework they know and expect. We look forward to contributing to and growing the global AI community as an active member of the PyTorch Foundation and are proud to be the first general member.” Anthony Barbier, Software Frameworks Lead at Graphcore.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-graphcore&quot;&gt;About Graphcore&lt;/h2&gt;

&lt;p&gt;Graphcore compute systems are accelerating the AI revolution.  Powered by the groundbreaking Intelligence Processing Unit (IPU), Graphcore delivers leading-edge AI performance with unprecedented efficiency. IPUs are used around the world by organisations building their intelligent compute capabilities, including AI-centric startups, large multinational corporations and both public and private research institutions.  Graphcore is backed by some of the world’s leading investors and has attracted more than $700m of funding. The company is based in Bristol, UK, with offices across Europe, Asia and North America.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Automated trace collection and analysis</title>
      <link href="https://pytorch.org/blog/automated-trace-collection/" rel="alternate" type="text/html" title="Automated trace collection and analysis" />
      <published>2023-09-05T00:00:00-07:00</published>
      <updated>2023-09-05T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/automated-trace-collection</id>
      <content type="html" xml:base="https://pytorch.org/blog/automated-trace-collection/">&lt;p&gt;In this blog, we share how we enabled the collection and analysis of PyTorch Profiler traces for training workloads &lt;strong&gt;without any user side code instrumentation&lt;/strong&gt;. We leveraged Dynolog - an open source daemon for CPU and GPU telemetry to collect PyTorch Profiler traces, and analyzed the collected traces using Holistic Trace Analysis - an open source library for analyzing PyTorch Profiler traces. This toolchain has allowed engineers at Meta to accelerate their performance optimization workflows. The keystone to our solution was implementing pre and post hooks for the base Optimizer class in PyTorch. We demo PyTorch trace collection using Dynolog in a short video.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;Software developers at Meta run a large number of distributed training runs daily. In order to ensure that GPUs are being used effectively it is necessary to measure and analyze GPU performance for all jobs. Moreover, developers need the capability to introspect models and understand how CPUs and GPUs interact to debug performance issues. Developers build initial prototypes using a handful of GPUs and the production versions scale out to hundreds or thousands of GPUs, serving numerous business use cases such as generative AI, recommendation systems, ad ranking etc.&lt;/p&gt;

&lt;p&gt;Given the scale at Meta, it is necessary to have toolchains for performance measurement and monitoring which have low overhead and operate seamlessly with each other, to maintain high developer efficiency.&lt;/p&gt;

&lt;p&gt;In this blog, we describe how we use the PyTorch Profiler, Dynolog (a telemetry daemon) and Holistic Trace Analysis (a performance debugging library) to collect traces without any user side code instrumentation and analyze them to identify jobs with low GPU utilization.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;The diagram below shares an overview of how the toolchain works together.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;User launches a PyTorch application.&lt;/li&gt;
  &lt;li&gt;A training service or user triggers a profiling session using the Dynolog CLI which sends a request over the network to the Dynolog daemon.&lt;/li&gt;
  &lt;li&gt;Dynolog daemon relays the profiling configuration to the PyTorch application, setting it temporarily in a profiling mode.&lt;/li&gt;
  &lt;li&gt;PyTorch Profiler collects a trace and stores it to the database (e.g., network file system or S3 bucket).&lt;/li&gt;
  &lt;li&gt;The collected traces are then analyzed using Holistic Trace Analysis (HTA).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dyno_hta.png&quot; alt=&quot;Figure 1: Dynolog, PyTorch Profiler and HTA toolchain workflow&quot; style=&quot;width:100%; max-width: 662px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;mb-3&quot; style=&quot;text-align: center&quot;&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Dynolog, PyTorch Profiler and HTA toolchain workflow&lt;/em&gt;&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Let’s dig a bit deeper in each of the components.&lt;/p&gt;

&lt;h3 id=&quot;dynolog&quot;&gt;Dynolog&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://developers.facebook.com/blog/post/2022/11/16/dynolog-open-source-system-observability/&quot;&gt;Dynolog&lt;/a&gt; is a lightweight monitoring daemon for heterogeneous CPU-GPU systems. It supports continuous monitoring of &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/docs/Metrics.md&quot;&gt;performance metrics&lt;/a&gt; from the CPU (utilization, network bandwidth, instructions/second) and GPU (SM Occupancy, DRAM bandwidth, GPU power draw). Additionally, dynolog exports APIs to collect deep-dive profiling data that can be accessed via the dyno CLI.&lt;/p&gt;

&lt;p&gt;One of the chief integrations Dynolog offers is interfacing with the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html&quot;&gt;PyTorch Profiler&lt;/a&gt;. This enables &lt;a href=&quot;https://pytorch.org/blog/performance-debugging-of-production-pytorch-models-at-meta/&quot;&gt;on-demand remote tracing&lt;/a&gt; using a single command to trace thousands of servers. This can be accomplished by using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; command.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-profiler&quot;&gt;PyTorch Profiler&lt;/h3&gt;

&lt;p&gt;GPU kernels execute asynchronously, and GPU-side support is needed to create the trace. NVIDIA provides this visibility via the CUPTI library. Kineto is the subsystem within Profiler that interfaces with CUPTI. The &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/&quot;&gt;PyTorch Profiler&lt;/a&gt; leverages the &lt;a href=&quot;https://github.com/pytorch/kineto&quot;&gt;Kineto library&lt;/a&gt; to collect GPU traces. To enable automated profiling of training workloads at scale &lt;strong&gt;without any user side code instrumentation&lt;/strong&gt; we made a few fundamental changes to PyTorch. These changes enable trace collection without any user intervention.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Registration:** **First, we modified PyTorch to register with the Dynolog daemon on start up. This feature is switched on by setting the environment variable KINETO_USE_DAEMON=True. With this environment variable set to True, the PyTorch Profiler periodically polls Dynolog to check for on-demand tracing requests.&lt;/li&gt;
  &lt;li&gt;Iteration hooks: Then, we &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/89176&quot;&gt;implemented pre and post hooks for the base Optimizer class&lt;/a&gt;. This allowed us to annotate start/end of training iterations. The profiler is then aware of the iteration count and can safely capture a fixed number of iterations in the trace.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;holistic-trace-analysis-hta&quot;&gt;Holistic Trace Analysis (HTA)&lt;/h3&gt;

&lt;p&gt;ML researchers and engineers often struggle to computationally scale up their models as they are unaware of the performance bottlenecks in their workloads. Large distributed training jobs could generate thousands of traces, containing way too much data for a human to inspect. This is where &lt;a href=&quot;https://pytorch.org/blog/trace-analysis-for-masses/&quot;&gt;Holistic Trace Analysis&lt;/a&gt; comes in. HTA is an open source library for performance analysis - it takes as input PyTorch Profiler traces and up-levels the performance information contained in them. Its goal is to help researchers and engineers achieve the best performance from the hardware stack. To aid performance debugging HTA provides the following features (partial list):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/temporal_breakdown.html&quot;&gt;Temporal Breakdown&lt;/a&gt;: Breakdown of GPU time in terms of time spent in computation, communication, memory events, and idle time on a single node and across all ranks.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/idle_time_breakdown.html&quot;&gt;Idle Time Breakdown&lt;/a&gt;: Breakdown of GPU idle time into waiting for the host, waiting for another kernel or attributed to an unknown cause.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/kernel_breakdown.html&quot;&gt;Kernel Breakdown&lt;/a&gt;: Find kernels with the longest duration on each rank.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/kernel_breakdown.html#kernel-duration-distribution&quot;&gt;Kernel Duration Distribution&lt;/a&gt;: Distribution of average time taken by longest kernels across different ranks.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/comm_comp_overlap.html&quot;&gt;Communication Computation Overlap&lt;/a&gt;: Calculate the percentage of time when communication overlaps computation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We invite you to check out these &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/tree/main/examples&quot;&gt;Jupyter notebooks&lt;/a&gt; to see what HTA can do for you. If you are a first time user we recommend starting with the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/trace_analysis_demo.ipynb&quot;&gt;trace_analysis_demo&lt;/a&gt; notebook.&lt;/p&gt;

&lt;p&gt;To summarize, Dynolog allows us to collect PyTorch Profiler traces on-the-fly in a scalable manner. Furthermore, by leveraging HTA we can automate performance analysis and identify bottlenecks. At Meta, we use the Dynolog, PyTorch Profiler and HTA toolchain to accelerate our performance optimization workflows.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;We share a screencast showcasing trace collection without any user side code instrumentation for a toy PyTorch program. The demo runs in a docker container and the trace collection is triggered using Dynolog. HTA can be used to subsequently analyze the collected trace.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FjmHYMJLIdw?si=xahelamoBIja94Ox&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;faqs&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Q. What else can &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; do for me?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; command supports several custom PyTorch Profiler options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;capturing python stacks&lt;/li&gt;
  &lt;li&gt;memory profiling&lt;/li&gt;
  &lt;li&gt;record input shapes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace --help&lt;/code&gt; for all the options.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Q. Does Dynolog collect hardware performance metrics?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Dynolog can also be used for always-on monitoring:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It incorporates out-of-box &lt;a href=&quot;https://github.com/facebookincubator/dynolog/tree/main#gpu-monitoring&quot;&gt;GPU performance monitoring&lt;/a&gt; for NVIDIA GPUs using &lt;a href=&quot;https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/index.html#&quot;&gt;DCGM&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Dynolog provides basic Linux kernel &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/docs/Metrics.md&quot;&gt;performance metrics&lt;/a&gt; including CPU, network and IO resource usage.&lt;/li&gt;
  &lt;li&gt;Dynolog manages hardware performance counters for micro-architecture specific events related to CPU Cache, TLBs etc on Intel and AMD CPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Q: How can I build the Docker image used in the demo?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The dockerfile is available &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/dynolog_hta.dockerfile&quot;&gt;here&lt;/a&gt;. Use the command below to build the Docker image.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -f /path/to/dynolog_repo/dynolog_hta.dockerfile -t &amp;lt;image_name:tag&amp;gt; .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Q. How can I run the docker image?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You can refer to this &lt;a href=&quot;https://gist.github.com/anupambhatnagar/07ebff374bc45e4b63eb42893cca7e87&quot;&gt;cheat sheet&lt;/a&gt; to run the Docker image.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Anupam Bhatnagar, Brian Coutinho</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we share how we enabled the collection and analysis of PyTorch Profiler traces for training workloads without any user side code instrumentation. We leveraged Dynolog - an open source daemon for CPU and GPU telemetry to collect PyTorch Profiler traces, and analyzed the collected traces using Holistic Trace Analysis - an open source library for analyzing PyTorch Profiler traces. This toolchain has allowed engineers at Meta to accelerate their performance optimization workflows. The keystone to our solution was implementing pre and post hooks for the base Optimizer class in PyTorch. We demo PyTorch trace collection using Dynolog in a short video.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization</title>
      <link href="https://pytorch.org/blog/pytorch-xla-spmd/" rel="alternate" type="text/html" title="PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization" />
      <published>2023-08-31T00:00:00-07:00</published>
      <updated>2023-08-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-xla-spmd</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-xla-spmd/">&lt;p&gt;Today, we are delighted to announce PyTorch/XLA SPMD: the integration of &lt;a href=&quot;https://arxiv.org/pdf/2105.04663.pdf&quot;&gt;GSPMD&lt;/a&gt; into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; is an automatic parallelization system for ML workloads. The XLA compiler transforms the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. This allows developers to write PyTorch programs as if they are on a single large device without any custom sharded computation and/or collective communication ops to scale models.&lt;/p&gt;

&lt;p&gt;PyTorch/XLA SPMD allows PyTorch users to parallelize their ML workloads with GSPMD with less effort and with better performance. Some of the key highlights are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Better developer experience. Everything happens with a few &lt;a href=&quot;#simple-example-with-sharding-annotation&quot;&gt;sharding annotations&lt;/a&gt; from the user, and PyTorch/XLA SPMD achieves comparable performance to the most efficient PyTorch sharding implementation (see the Examples and Results section below). PyTorch/XLA SPMD separates the task of programming an ML model from the challenge of parallelization. Its automated approach to model sharding frees up the user from implementing the sharded version of ops with proper collectives in place.&lt;/li&gt;
  &lt;li&gt;A single API that enables a large variety of parallelism algorithms (including data parallelism, fully sharded data parallelism, spatial partitioning tensor and pipeline parallelism, as well as combinations of these algorithms) for different ML workloads and model architectures.&lt;/li&gt;
  &lt;li&gt;Industry-leading performance in large model training. PyTorch/XLA SPMD brings the powerful XLA GSPMD to PyTorch, enabling users to harness the full power of Google Cloud TPUs.&lt;/li&gt;
  &lt;li&gt;Enabling PyTorch and JAX developers take advantage of the same underlying XLA API to scale models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-concepts&quot;&gt;Key Concepts&lt;/h2&gt;

&lt;p&gt;The key concepts behind the sharding annotation API are: 1) Mesh, 2) Partition Spec, and 3) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API to express sharding intent using Mesh and Partition Spec. A more detailed design overview is available as a user guide &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/spmd.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mesh&quot;&gt;Mesh&lt;/h3&gt;

&lt;p&gt;For a given cluster of devices, a physical mesh is a representation of the interconnect topology.&lt;/p&gt;

&lt;p&gt;We derive a logical mesh based on this topology to create sub-groups of devices which can be used for partitioning different axes of tensors in a model. We apply sharding annotations to map the program across the logical mesh; this automatically inserts communication collectives in the program graph to support functional correctness (see the figure below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig1.png&quot; alt=&quot;SPMD on PyTorch/XLA&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We abstract logical mesh with &lt;a href=&quot;https://github.com/pytorch/xla/blob/028df4da388468fa9a41b1f98ea08bfce13b4c63/torch_xla/experimental/xla_sharding.py#L16&quot;&gt;Mesh API&lt;/a&gt;. The axes of the logical Mesh can be named. Here is an example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch_xla.runtime as xr
import torch_xla.experimental.xla_sharding as xs
from torch_xla.experimental.xla_sharding import Mesh

# Enable XLA SPMD execution mode.
xr.use_spmd()

# Assuming you are running on a TPU host that has 8 devices attached
num_devices = xr.global_runtime_device_count()
# mesh shape will be (4,2) in this example
mesh_shape = (num_devices // 2, 2)
device_ids = np.array(range(num_devices))
# axis_names 'x' nad 'y' are optional
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))

mesh.get_logical_mesh()
&amp;gt;&amp;gt; array([[0, 1],
          [2, 3],
          [4, 5],
          [6, 7]])
mesh.shape()
&amp;gt;&amp;gt; OrderedDict([('x', 4), ('y', 2)])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;partition-spec&quot;&gt;Partition Spec&lt;/h3&gt;

&lt;p&gt;partition_spec has the same rank as the input tensor. Each dimension describes how the corresponding input tensor dimension is sharded across the device mesh (logically defined by mesh_shape). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition_spec&lt;/code&gt; is a tuple of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device_mesh&lt;/code&gt; dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt;, None, or a tuple of mesh dimension indices. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt; can be an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;str&lt;/code&gt; if the corresponding mesh dimension is named. This specifies how each input rank is sharded (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mesh_shape&lt;/code&gt;) or replicated (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Provide optional mesh axis names and use them in the partition spec
mesh = Mesh(device_ids, (4, 2), ('data', 'model'))
partition_spec = ('model', 'data')
xs.mark_sharding(input_tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We support all three types of sharding described in the original &lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; paper. For instance, one can specify partial replication like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Provide optional mesh axis names and use them in the partition spec
mesh = Mesh(device_ids, (2, 2, 2), ('x', 'y', 'z'))

# evenly shard across x and z and replicate among y
partition_spec = ('x', 'z')  # equivalent to ('x', None, 'z')
xs.mark_sharding(input_tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;simple-example-with-sharding-annotation&quot;&gt;Simple Example With Sharding Annotation&lt;/h3&gt;

&lt;p&gt;Users can annotate native PyTorch tensors using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API (&lt;a href=&quot;https://github.com/pytorch/xla/blob/9a5fdf3920c18275cf7dba785193636f1b39ced9/torch_xla/experimental/xla_sharding.py#L388&quot;&gt;src&lt;/a&gt;). This takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt; as input and returns a &lt;a href=&quot;https://github.com/pytorch/xla/blob/03991d44a0a0297ced3ba9fc10ba451a4b6c94ab/torch_xla/experimental/xla_sharded_tensor.py#L55-L62&quot;&gt;XLAShardedTensor&lt;/a&gt; as output.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def mark_sharding(t: Union[torch.Tensor, XLAShardedTensor], mesh: Mesh, partition_spec: Tuple[Union[int, None]]) -&amp;gt; XLAShardedTensor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Invoking &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API takes a user defined logical &lt;a href=&quot;#mesh&quot;&gt;mesh&lt;/a&gt; and &lt;a href=&quot;#partition-spec&quot;&gt;partition_spec&lt;/a&gt; and generates a sharding annotation for the XLA compiler. The sharding specification is attached to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XLATensor&lt;/code&gt;, as well as the original input tensor. Here is a simple usage example from the [&lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt;], to illustrate how the sharding annotation API works:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch
import torch_xla.core.xla_model as xm
import torch_xla.runtime as xr
import torch_xla.experimental.xla_sharding as xs
from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
from torch_xla.experimental.xla_sharding import Mesh

# Enable XLA SPMD execution mode.
xr.use_spmd()

# Device mesh, this and partition spec as well as the input tensor shape define the individual shard shape.
num_devices = xr.global_runtime_device_count()
mesh_shape = (2, num_devicese // 2)  # 2x4 on v3-8, 2x2 on v4-8  
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))

t = torch.randn(8, 4).to(xm.xla_device())

# Mesh partitioning, each device holds 1/8-th of the input
partition_spec = (0, 1)
m1_sharded = xs.mark_sharding(t, mesh, partition_spec)
assert isinstance(m1_sharded, XLAShardedTensor) == True
# Note that the sharding annotation is also in-placed updated to t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can annotate different tensors in the PyTorch program to enable different parallelism techniques, as described in the comment below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Sharding annotate the linear layer weights. SimpleLinear() is a nn.Module.
model = SimpleLinear().to(xm.xla_device())
xs.mark_sharding(model.fc1.weight, mesh, partition_spec)

# Training loop
model.train()
for step, (data, target) in enumerate(loader):
  # Assumes `loader` returns data, target on XLA device
  optimizer.zero_grad()
  # Sharding annotate input data, we can shard any input
  # dimensions. Sharding the batch dimension enables 
  # data parallelism, sharding the feature dimension enables
  # spatial partitioning.
  xs.mark_sharding(data, mesh, partition_spec)
  ouput = model(data)
  loss = loss_fn(output, target)
  optimizer.step()
  xm.mark_step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More complete unit test cases and integration test examples are available in the PyTorch/XLA &lt;a href=&quot;https://github.com/pytorch/xla/tree/r2.0/test/spmd&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;We measured the performance of PyTorch/XLA SPMD using a GPT-2 model (&lt;a href=&quot;https://github.com/pytorch-tpu/transformers/tree/yeounoh_gpt2_spmd&quot;&gt;src&lt;/a&gt;) and compared it with &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;user-mode FSDP&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here, SPMD applies the same sharding scheme as the FSDP plot (i.e. 1D sharding). Users are expected to achieve better MFU results by exploring more advanced SPMD sharding schemes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig2.png&quot; alt=&quot;SPMD vs. FSDP&quot; style=&quot;width:100%; max-width: 600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We use Model FLOPS Utilization (MFU) as a metric for comparison. MFU is “the ratio of the observed throughput relative to the theoretical maximum throughput of a system operating at peak FLOPs” (&lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM paper&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;flops_per_step = 6 * global_batch_size * seq_len * num_params
model_flops_utilization = flops_per_step / step_time(s) / chip_count / flops_per_chip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This estimation assumes that the input dimensionality is much larger than the input sequence length (d_model » seq_len). If this assumption is violated the self-attention FLOPs start to be significant enough and this expression will underestimate the true MFU.&lt;/p&gt;

&lt;h3 id=&quot;scalability&quot;&gt;Scalability&lt;/h3&gt;

&lt;p&gt;One of the core benefits of SPMD is the flexible partitioning which can be used to save accelerator memory (HBM) usage and improve scalability. For scalability analysis, we present two studies: 1) we examine the peak HBM across 4 model sizes using Hugging Face transformers (GPT-2) as the base implementation; 2) we examine the peak HBM usage with &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus&quot;&gt;spatial partitioning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig3.png&quot; alt=&quot;Peak HBM Utilization&quot; style=&quot;width:100%; max-width: 600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above figure illustrates the unsharded 2B parameters model peak memory footprint stands at 26GB (red dashed line). harding model weights (model parallelism) reduces the peak memory footprint, and thus, enables larger model training with a given TPU pod slice. In  these experiments, we achieved up to 39.75% MFU on a 4B parameters model on Google Cloud TPU v4-16.&lt;/p&gt;

&lt;p&gt;We also ran an input batch scalability test using &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus&quot;&gt;spatial partitioning&lt;/a&gt; and a simple ResNet50 example (&lt;a href=&quot;https://github.com/pytorch/xla/blob/master/test/spmd/test_train_spmd_imagenet.py&quot;&gt;src&lt;/a&gt;) on Cloud TPU v4-8. Input batch is commonly sharded across the batch dimension for data parallelism (DDP, FSDP), but PyTorch/XLA SPMD enables input sharding across input feature dimensions for spatial sharding. As shown in the below figure, one can push the per-device batch size to 512 with spatial partitioning which is not possible with other data parallelism techniques.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig4.png&quot; alt=&quot;Batch size scaling with spatial partitioning&quot; style=&quot;width:100%; max-width: 741px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-road-forward-for-pytorchxla-spmd&quot;&gt;The Road Forward for PyTorch/XLA SPMD&lt;/h2&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. SPMD is still experimental, and we continuously add new features to it. In future releases, we plan to address async dataloading, partially replicated sharding, and other improvements. We’d love to &lt;a href=&quot;https://github.com/pytorch/xla#providing-feedback&quot;&gt;hear from you&lt;/a&gt;, answer your questions about PyTorch/XLA SPMD, and learn how you use SPMD.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yeounoh Chung, Jon Bolin, Milad Mohammadi, Jiewen Tan, Jack Cao, Joe Spisak, Alex Spiridonov, Shauheen Zahirazami, Steven Krawczyk, Wonjoo Lee Mohit Khatwani, Wanchao Liang, Vaibhav Singh</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we are delighted to announce PyTorch/XLA SPMD: the integration of GSPMD into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP</title>
      <link href="https://pytorch.org/blog/large-scale-training-hugging-face/" rel="alternate" type="text/html" title="Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP" />
      <published>2023-08-24T00:00:00-07:00</published>
      <updated>2023-08-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/large-scale-training-hugging-face</id>
      <content type="html" xml:base="https://pytorch.org/blog/large-scale-training-hugging-face/">&lt;p&gt;AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.&lt;/p&gt;

&lt;p&gt;To meet the growing demands of AI training at scale, last year we introduced &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;Fully Sharded Data Parallel (FSDP)&lt;/a&gt; in PyTorch/XLA. FSDP is a model parallelism architecture that unlocks the ability to easily and efficiently scale AI models into hundreds of billions of parameters. With &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/fsdp.md&quot;&gt;PyTorch/XLA FSDP&lt;/a&gt;, during distributed training, each device can store a specific model shard, and all-gather the full model weights when it is time to perform the forward pass. Nested FSDP further optimizes performance by only using a given layer’s full parameters during its forward pass.&lt;/p&gt;

&lt;p&gt;We are excited to announce that PyTorch/XLA FSDP has &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v4.27.0&quot;&gt;landed&lt;/a&gt; in &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Hugging Face Transformers&lt;/a&gt;. Now, Hugging Face users can train PyTorch models with up to 20 times more parameters using the same amount of computing power as before.&lt;/p&gt;

&lt;p&gt;We built PyTorch/XLA FSDP support directly into the Hugging Face Trainer class, so that any model using Trainer can leverage FSDP. And with the &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla/#fsdp-beta&quot;&gt;addition of automatic wrapping to PyTorch/XLA FSDP&lt;/a&gt;, nested FSDP wrapping is both flexible and simple to apply. These new features make it easy to train a wide range of Hugging Face models at large scales. In this guide, we demonstrate training GPT-2 models with up to 128B parameters on Google Cloud TPUs. PyTorch/XLA FSDP training on TPUs is highly efficient, achieving up to 45.1% model FLOPS utilization (MFU) for GPT-2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hugging_face_transformers.svg&quot; alt=&quot;Figure 1: Model FLOPS utilization for Hugging Face GPT-2 on Google Cloud TPU v4&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Model FLOPS utilization for Hugging Face GPT-2 on Google Cloud TPU v4&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;configuring-pytorchxla-fsdp-in-the-hugging-face-trainer&quot;&gt;Configuring PyTorch/XLA FSDP in the Hugging Face Trainer&lt;/h2&gt;

&lt;p&gt;First, follow your preferred method to create your TPU(s) and install PyTorch and PyTorch/XLA. You need versions &amp;gt;= 2.0 for PyTorch and PyTorch/XLA.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    pip3 install https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torc h-2.0-cp38-cp38-linux_x86_64.whl --user

    pip3 install https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torc h_xla-2.0-cp38-cp38-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, clone and install the Hugging Face Transformers repo. Install all necessary dependencies (e.g., datasets, evaluate, scikit-learn, accelerate).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    cd $HOME
    git clone https://github.com/huggingface/transformers.git cd transformers
    git checkout v4.31-release
    pip3 install -e .
    pip3 install datasets evaluate scikit-learn
    pip3 install accelerate==0.21.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/transformers&lt;/code&gt;, create any model-specific configuration files you might need. Here is an example of a configuration file for a GPT-2 model with 2B parameters, which we later refer to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt2_config.json&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;activation_function&quot;: &quot;gelu_new&quot;, 
    &quot;architectures&quot;: [
        &quot;GPT2LMHeadModel&quot;
    ],
    &quot;attn_pdrop&quot;: 0.1,
    &quot;bos_token_id&quot;: 50256, &quot;embd_pdrop&quot;: 0.1, &quot;eos_token_id&quot;: 50256, &quot;initializer_range&quot;: 0.02, &quot;layer_norm_epsilon&quot;: 1e-05, &quot;model_type&quot;: &quot;gpt2&quot;,
    &quot;n_embd&quot;: 3072,
    &quot;n_head&quot;: 24,
    &quot;n_layer&quot;: 18,
    &quot;n_positions&quot;: 1024,
    &quot;resid_pdrop&quot;: 0.1,
    &quot;summary_activation&quot;: null,
    &quot;summary_first_dropout&quot;: 0.1,
    &quot;summary_proj_to_labels&quot;: true,
    &quot;summary_type&quot;: &quot;cls_index&quot;,
    &quot;summary_use_proj&quot;: true,
    &quot;task_specific_params&quot;: {
        &quot;text-generation&quot;: {
            &quot;do_sample&quot;: true,
            &quot;max_length&quot;: 50
        }
    },
    &quot;vocab_size&quot;: 50257
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With PyTorch/XLA FSDP, it is possible to train model sizes much bigger than this on large accelerator slices. We have trained GPT-2 models as large as 128B parameters with these techniques; for expert tips on how to replicate this scale, see the appendix.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/transformers&lt;/code&gt;, create your FSDP configuration file, a JSON file containing all of the configurable aspects of your XLA FSDP wrapping stored as a dictionary. Following the &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;official Hugging Face Transformers XLA FSDP documentation&lt;/a&gt;, the following arguments are available to set:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla (bool, \*optional\*, defaults to False)&lt;/code&gt;: This is a boolean which determines whether or not you use XLA FSDP. Make sure to set this to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_fsdp_settings (dict, \*optional\*)&lt;/code&gt;: This is a dictionary which stores all of the XLA FSDP wrapping parameters you want to set; note that you do not have to specify settings for parameters where you are using the default value. For a complete list of settings, see &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_dtype&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buffer_dtype&lt;/code&gt;, enter these as strings which contain the corresponding torch data type, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bfloat16&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params (int, \*optional\*, defaults to 0)&lt;/code&gt;: An integer which sets the minimum number of parameters for size-based auto wrapping. Every module with at least as many parameters as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; will be XLA FSDP wrapped.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap (List[str], \*optional\*)&lt;/code&gt;: A list of (case-sensitive) transformer layer class names to wrap. Note that this is mutually exclusive with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt;. Example: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[&quot;GPT2Block&quot;, &quot;GPT2MLP&quot;]&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_fsdp_grad_ckpt (bool, \*optional\*, defaults to False)&lt;/code&gt;: This is a boolean which determines whether to use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be used when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla&lt;/code&gt; flag is set to true, and an auto wrapping policy is specified through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For transformer-based models, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; when performing automatic nested FSDP wrapping. Layers which share weights should not belong to separate FSDP wrapped units, and the input and output embedding layers in transformer-based models share weights.&lt;/p&gt;

&lt;p&gt;For this GPT-2 example, here is what the corresponding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_config.json&lt;/code&gt; file looks like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    {
        &quot;fsdp_transformer_layer_cls_to_wrap&quot;: [
            &quot;GPT2Block&quot;
        ],
        &quot;xla&quot;: true,
        &quot;xla_fsdp_settings&quot;: {
            &quot;compute_dtype&quot;: &quot;bfloat16&quot;,
            &quot;shard_param_on_dim_0&quot;: true,
            &quot;pin_layout_in_collective_ops&quot;: true
        },
       &quot;xla_fsdp_grad_ckpt&quot;: true
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Now, it’s time to train your model! First, ensure that you have your PyTorch/XLA runtime set up appropriately by setting&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    export PJRT_DEVICE=TPU
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When running training, the key flags to pass are:&lt;/p&gt;

&lt;p&gt;a) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fsdp &quot;full_shard&quot;&lt;/code&gt;
b) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fsdp_config fsdp_config.json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where you should replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_config.json&lt;/code&gt; with whatever you named your FSDP configuration file. Here is a sample command to train our example 2B GPT-2 model, where training is started by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_spawn.py&lt;/code&gt;, a &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/examples/pytorch/xla_spawn.py&quot;&gt;launcher script for&lt;/a&gt; distributed TPU training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    python3 -u examples/pytorch/xla_spawn.py --num_cores 4 examples/pytorch/language-modeling/run_clm.py \
    --num_train_epochs 1 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \ --per_device_train_batch_size 32 \ --per_device_eval_batch_size 32 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm \
    --overwrite_output_dir \
    --config_name gpt2_config.json \
    --cache_dir /tmp \
    --tokenizer_name gpt2 \
    --block_size 1024 \
    --optim adafactor \
    --adafactor true \
    --save_strategy no \
    --logging_strategy no \
    --fsdp &quot;full_shard&quot; \
    --fsdp_config fsdp_config.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;measuring-model-flops-utilization-mfu-for-gpt-2&quot;&gt;Measuring Model FLOPS Utilization (MFU) for GPT-2&lt;/h2&gt;

&lt;p&gt;Model FLOPS are the floating point operations required to perform a single forward and backward pass. Model FLOPS are hardware- and implementation- independent, and only depend on the underlying model. In each step, the number of FLOPS is computed via the following formulas:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tokens_per_batch = global_batch_size \* seq_len

FLOPS_per_step = 6 \* tokens_per_batch \* num_params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq_len&lt;/code&gt; is the sequence length and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_params&lt;/code&gt; is the number of parameters in the model. We note that this estimation assumes that the input dimensionality is much larger than the input sequence length (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d_model &amp;gt;&amp;gt; seq_len&lt;/code&gt;). If this assumption is violated the self-attention FLOPs start to be significant enough and this expression will underestimate the true MFU.&lt;/p&gt;

&lt;p&gt;Based on the step time and the hardware details (numbers of chips and the peak FLOPS per chip), we can compute Model FLOPS Utilization (MFU), which measures how effectively our implementation is using the underlying hardware. Achieving 100% MFU means that the hardware is being used perfectly by that model. We calculate MFU using the following formula:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model_FLOPS_utilization = FLOPS_per_step / step_time(s) / chip_count / FLOPS_per_chip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When training a GPT-2 model with 2B parameters with the XLA FSDP configuration file above on a Cloud TPU v4-8, we measure a step time of 4.191s. Using the above formula, we calculate 35.7% MFU on a v4-8. For further details on calculating MFU, refer to the &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The table below presents MFU for GPT-2 models with sizes between 2B and 128B, with a sequence length of 1024.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;TPU NumCores&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-8&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-64&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-128&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-128&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-256&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-512&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;# of Tokens / Batch&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;131,072&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;1,048,576&lt;/td&gt;
      &lt;td&gt;1,048,576&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;# of Parameters&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;2B&lt;/td&gt;
      &lt;td&gt;16B&lt;/td&gt;
      &lt;td&gt;20B&lt;/td&gt;
      &lt;td&gt;32B&lt;/td&gt;
      &lt;td&gt;64B&lt;/td&gt;
      &lt;td&gt;128B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Step Time (ms)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;4,191&lt;/td&gt;
      &lt;td&gt;14,592&lt;/td&gt;
      &lt;td&gt;7,824&lt;/td&gt;
      &lt;td&gt;12,970&lt;/td&gt;
      &lt;td&gt;25,653&lt;/td&gt;
      &lt;td&gt;30,460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PFLOPS / Step&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;404&lt;/td&gt;
      &lt;td&gt;809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;MFU&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;35.7%&lt;/td&gt;
      &lt;td&gt;38.8%&lt;/td&gt;
      &lt;td&gt;45.1%&lt;/td&gt;
      &lt;td&gt;44.4%&lt;/td&gt;
      &lt;td&gt;44.7%&lt;/td&gt;
      &lt;td&gt;37.7%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1: GPT-2 model FLOPS utilization calculation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Among these configurations, MFU peaks at 45.1% for the 20B parameter model on v4-128. This result compares favorably to, for example, 41.5% MFU for &lt;a href=&quot;https://arxiv.org/pdf/2205.05198.pdf&quot;&gt;a 22B Megatron-like model&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are two actionable insights from these experiments:&lt;/p&gt;

&lt;p&gt;First, simply increasing the number of chips without increasing the batch size generally means lower FLOPS utilization, because more time is spent on sharing the model shards. FSDP uses all-reduce communication collectives which are not asynchronous, which means that chip-to-chip communication cannot be overlapped with computation. As the number of chips increases, the number of model shards that must be communicated increases, and so we should expect the portion of the step time spent on communication to increase with the number of chips.&lt;/p&gt;

&lt;p&gt;Second, increasing the batch size generally means better FLOPS utilization. As the number of chips increases, the memory footprint of the model decreases, which often frees up high bandwidth memory (HBM) to scale up the global batch size. With a larger global batch size, the number of tokens processed in each step increases, and thus, so does the FLOPS per step. As long as the step time does not increase proportionally, we expect a larger global batch size to improve MFU.&lt;/p&gt;

&lt;p&gt;Therefore, to maximize the MFU, we recommend training with the largest global batch size possible that can fit in the HBM of the TPU slice, using FSDP to reduce memory required for the model parameters.&lt;/p&gt;

&lt;h2 id=&quot;training-very-large-models-tested-to-128b-parameters&quot;&gt;Training Very Large Models (tested to 128B parameters)&lt;/h2&gt;

&lt;p&gt;When using PyTorch/XLA, tensors must be initialized on the CPU before being moved to the XLA device. This means one may encounter host-side out-of-memory errors if the model is sufficiently large, even though the model can fit in the device HBM after sharding. To avoid this, we must defer each submodule’s initialization until it is FSDP wrapped, which ensures that submodules are sharded as soon as their values are populated, avoiding host-side limitations.&lt;/p&gt;

&lt;p&gt;Below, we explain how to modify a local copy of the Hugging Face transformers repository to train a GPT-2 model with up to 128B parameters using this technique.&lt;/p&gt;

&lt;p&gt;First, using the commands below, install torchdistX, which is a library containing experimental PyTorch Distributed features. This is the engine behind deferred initialization, and allows you to create tensors that don’t require immediate storage and can be materialized later. You also need to install a specific PyTorch/XLA 2.0 version that takes advantage of this package; note that you must uninstall PyTorch and PyTorch/XLA first, if you installed them earlier.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install torch==2.0 --index-url [https://download.pytorch.org/whl/test/cpu](https://download.pytorch.org/whl/test/cpu) --user
pip3 install torch_xla[torchdistx] -f https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/experimen tal/torch_xla-2.0-cp38-cp38-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, apply the following changes to your local copy of Hugging Face Transformers:&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src/transformers/trainer.py&lt;/code&gt;, add the following function in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_wrap_model&lt;/code&gt; on the line immediately prior to PyTorch/XLA FSDP wrapping:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchdistx import deferred_init

def _init_with_torchdistX(module):
    def check_fn(k):
        return not isinstance(k, FSDP)
    deferred_init.materialize_module(module, check_fn=check_fn)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;materialize_module&lt;/code&gt; will initialize the model tensors if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_fn&lt;/code&gt; returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt;. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_fn&lt;/code&gt; checks whether the module has been FSDP wrapped.&lt;/p&gt;

&lt;p&gt;Within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_wrap_model&lt;/code&gt;, modify your FSDP wrapping to accept the additional argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;param_init_fn=_init_with_torchdistX&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model = model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        auto_wrapper_callable=auto_wrapper_callable,
        param_init_fn=_init_with_torchdistX,
        \*\*fsdp_kwargs,
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;examples/pytorch/language-modeling/run_clm.py&lt;/code&gt;, add the following import statement at the beginning of the file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchdistx import deferred_init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Edit the model initialization so that the model is wrapped with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deferred_init.deferred_init&lt;/code&gt; by replacing the line&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = AutoModelForCausalLM.from_config(config)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = deferred_init.deferred_init(AutoModelForCausalLM.from_config, config)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that this assumes you are supplying your own model configuration file. Otherwise, you should modify your model initialization statement accordingly.&lt;/p&gt;

&lt;p&gt;You should also comment out these two lines which immediately follow the line above:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values()) logger.info(f&quot;Training new model from scratch - Total size={n_params/2\*\*20:.2f}M params&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;They will cause an error if left unmodified, since the model tensors do not actually have storage when these lines are executed.&lt;/p&gt;

&lt;p&gt;With these changes, you can now run GPT-2 models with as many as 128B parameters, provided the accelerator size is suitably large.&lt;/p&gt;

&lt;h2 id=&quot;next-steps--acknowledgements&quot;&gt;Next Steps &amp;amp; Acknowledgements&lt;/h2&gt;

&lt;p&gt;To learn more, the docs can be found &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;here&lt;/a&gt;. We’d love to &lt;a href=&quot;https://github.com/pytorch/xla#providing-feedback&quot;&gt;hear from you&lt;/a&gt; if you run into any issues with FSDP in PyTorch/XLA, or just want to tell us about how you are using it.&lt;/p&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; so that we can openly collaborate.&lt;/p&gt;

&lt;p&gt;We’d like to thank Ronghang Hu and Ross Girshick at Meta AI and Lysandre Debut, Sourab Mangrulkar, Sylvain Gugger and Arthur Zucker for all the support and collaboration. We’d also like to thank Jiewen Tan, Liyang Lu, Will Cromar, Vaibhav Singh, and Chandra Devarakonda for their assistance in preparing this post.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Alex Wertheim, Milad Mohammadi, Jack Cao, Alex Spiridonov, Joe Spisak, Lysandre Debut, Sylvain Gugger, Sourab Mangrulkar</name>
        
        
      </author>

      

      

      
        <summary type="html">AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Intel Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/intel-joins-pytorch/" rel="alternate" type="text/html" title="Intel Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-08-10T00:00:00-07:00</published>
      <updated>2023-08-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/intel-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/intel-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/intel-new-logo.svg&quot; alt=&quot;Intel logo&quot; style=&quot;max-width:250px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Intel has joined as a premier member.&lt;/p&gt;

&lt;p&gt;“The PyTorch Foundation is thrilled to welcome Intel as a premier member, marking a significant milestone in our mission to empower the global AI community. Intel’s extensive expertise and commitment to advancing cutting-edge technologies align perfectly with our vision of fostering open-source innovation,” said PyTorch Foundation Executive Director Ibrahim Haddad. “Together, we will accelerate the development and democratization of PyTorch, and use the collaboration to shape a vibrant future of AI for all.”&lt;/p&gt;

&lt;p&gt;Intel has developed and released several PyTorch-based tools and libraries to enable developers to accelerate their AI workflows, and is actively working on optimizing PyTorch to leverage Intel hardware capabilities.&lt;/p&gt;

&lt;p&gt;“At Intel, we believe in the power of collaboration and open-source innovation to propel the ecosystem towards an AI Everywhere future. Joining the Governing Board of the PyTorch Foundation is a testament to Intel’s commitment to advancing and democratizing AI,” said Wei Li, Vice President and General Manager of Artificial Intelligence and Analytics (AIA) at Intel. “By harnessing the collective expertise and resources within the deep learning community, we aim to accelerate the development of PyTorch and continue to drive breakthroughs in AI research and applications.”&lt;/p&gt;

&lt;p&gt;Intel fosters industry collaboration, co-engineering, and open source contributions to accelerate software innovation and develop new technologies that bring benefits to the open source community. By working together with other member companies and under the guidance of the PyTorch Foundation, Intel remains committed to actively contributing to and advocating for the community.&lt;/p&gt;

&lt;p&gt;As a premier member, Intel is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/wei-li.jpg&quot; alt=&quot;Wei Li&quot; style=&quot;max-width:250px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Wei Li, Vice President and General Manager of Artificial Intelligence and Analytics (AIA)  at Intel, to our board.  Dr. Wei Li is Vice President and General Manager of Artificial Intelligence and Analytics (AIA) at Intel, where he leads a world-wide team of engineering “magicians” who make AI Everywhere a reality by supercharging machine performance and developer productivity.  Wei and his team have been instrumental in Intel’s recent multi-billion-dollar AI revenue growth by delivering 10-100X software acceleration, across deep learning, statistical machine learning and big data analytics, to complement Intel’s AI-optimized hardware portfolio.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read more about Intel’s commitment to the PyTorch Community &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/ai-everywhere-intel-joins-pytorch-foundation.html#gs.4984sj&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-intel&quot;&gt;About Intel&lt;/h2&gt;

&lt;p&gt;Intel (Nasdaq: INTC) is an industry leader, creating world-changing technology that enables global progress and enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and manufacturing of semiconductors to help address our customers’ greatest challenges. By embedding intelligence in the cloud, network, edge and every kind of computing device, we unleash the potential of data to transform business and society for the better. To learn more about Intel’s innovations, go to&lt;a href=&quot;https://newsroom.intel.com/&quot;&gt; newsroom.intel.com&lt;/a&gt; and &lt;a href=&quot;https://intel.com/&quot;&gt;intel.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;© Intel Corporation. Intel, the Intel logo and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/legal/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">INT8 Quantization for x86 CPU in PyTorch</title>
      <link href="https://pytorch.org/blog/int8-quantization/" rel="alternate" type="text/html" title="INT8 Quantization for x86 CPU in PyTorch" />
      <published>2023-08-07T00:00:00-07:00</published>
      <updated>2023-08-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/int8-quantization</id>
      <content type="html" xml:base="https://pytorch.org/blog/int8-quantization/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;INT8 quantization is a powerful technique for speeding up deep learning inference on x86 CPU platforms. By reducing the precision of the model’s weights and activations from 32-bit floating-point (FP32) to 8-bit integer (INT8), INT8 quantization can significantly improve the inference speed and reduce memory requirements without sacrificing accuracy.&lt;/p&gt;

&lt;p&gt;In this blog, we will discuss the recent progress on INT8 quantization for x86 CPU in PyTorch, focusing on the new x86 quantization backend. We will also briefly look at the new quantization path with PyTorch 2.0 Export (PT2E) and TorchInductor.&lt;/p&gt;

&lt;h2 id=&quot;x86-quantization-backend&quot;&gt;X86 Quantization Backend&lt;/h2&gt;

&lt;p&gt;The current recommended way of quantization in PyTorch is &lt;a href=&quot;http://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html?highlight=fx&quot;&gt;FX&lt;/a&gt;. Before PyTorch 2.0, the default quantization backend (a.k.a. QEngine) on x86 CPUs was FBGEMM, which leveraged the FBGEMM performance library to achieve the performance speedup. In the PyTorch 2.0 release, a new quantization backend called X86 was introduced to replace FBGEMM. The x86 quantization backend offers improved INT8 inference performance when compared to the original FBGEMM backend by leveraging the strengths of both FBGEMM and the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/onednn.html&quot;&gt;Intel® oneAPI Deep Neural Network Library (oneDNN)&lt;/a&gt; kernel libraries.&lt;/p&gt;

&lt;h2 id=&quot;performance-benefit-from-x86-backend&quot;&gt;Performance Benefit from X86 Backend&lt;/h2&gt;

&lt;p&gt;To measure the performance benefits of the new X86 backend, we ran INT8 inference on 69 popular deep learning models (shown in &lt;strong&gt;Figures 1-3&lt;/strong&gt; below) using &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;4th Gen Intel® Xeon® Scalable processors&lt;/a&gt;. The results showed a 2.97X geomean performance speedup compared to FP32 inference performance, while the speedup was 1.43X with the FBGEMM backend. The charts below show the per-model performance speedup comparing the x86 backend and the FBGEMM backend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_1.jpg&quot; alt=&quot;Figure 1: Models with less than 2x performance boost with x86 backend1&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Models with less than 2x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_2.jpg&quot; alt=&quot;Figure 2: Models with 2x-4x performance boost with x86 backend1&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Models with 2x-4x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_3.jpg&quot; alt=&quot;Figure 3: Models with larger than 4x performance boost with x86 backend1&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Models with larger than 4x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;usage-of-x86-backend&quot;&gt;Usage of x86 Backend&lt;/h2&gt;

&lt;p&gt;By default in 2.0, users on x86 platforms will use the x86 quantization backend and their PyTorch programs will remain unchanged when using the default backend. Alternatively, users can specify x86 as the quantization backend explicitly. &lt;br /&gt;
Below is an example code snippet of PyTorch static post-training quantization with x86 quantization backend.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torch.ao.quantization import get_default_qconfig_mapping
from torch.quantization.quantize_fx import prepare_fx, convert_fx

qconfig_mapping = get_default_qconfig_mapping()
# Or explicity specify the qengine
# qengine = 'x86'
# torch.backends.quantized.engine = qengine
# qconfig_mapping = get_default_qconfig_mapping(qengine)

model_fp32 = MyModel().eval()
x = torch.randn((1, 3, 224, 224), dtype=torch.float)
x = x.to(memory_format=torch.channels_last)

# Insert observers according to qconfig and backend config
prepared_model = prepare_fx(model_fp32, qconfig_mapping, example_inputs=x)

# Calibration code not shown

# Convert to quantized model
quantized_model = convert_fx(prepared_model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;technical-details-of-x86-backend&quot;&gt;Technical Details of x86 Backend&lt;/h2&gt;

&lt;p&gt;We devised heuristic dispatching rules according to the performance numbers from the models we benchmarked to decide whether to invoke oneDNN or FBGEMM performance library to execute the convolution or matrix multiplication operations. The rules are a combination of operation kinds, shapes, CPU architecture information, etc. Detailed logic is available &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/93ff71ec37e3c946603600a46edef70b42f81213/aten/src/ATen/native/quantized/cpu/OnednnUtils.h#L396&quot;&gt;here&lt;/a&gt;. For more design and technical discussion, please refer to the &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;Request for Comments&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps-with-a-new-quantization-path-pytorch-20-export&quot;&gt;Next Steps With a New Quantization Path PyTorch 2.0 Export&lt;/h2&gt;

&lt;p&gt;Although still far from finalized, a new quantization path, PyTorch 2.0 Export (PT2E), is in early design and PoC stage. The new approach is slated to replace the FX quantization path in the future. It is built upon the capabilities of TorchDynamo Export, a feature introduced in the PyTorch 2.0 release for FX graph capturing. This graph is then quantized and lowered to different backends. TorchInductor, the new DL compiler of PyTorch, has shown promising results in terms of FP32 inference speedup on x86 CPU. We are working actively to enable it as one of the quantization backends of PT2E. We believe the new path will lead to further improvements in INT8 inference performance due to more flexibility of fusion at different levels.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The x86 backend introduced in PyTorch 2.0 release has demonstrated a remarkable improvement in INT8 inference speed on x86 CPU platforms. It offers a 1.43X speedup compared to the original FBGEMM backend while maintaining backward compatibility. This enhancement can benefit end users with minimal or no modifications to their programs. Furthermore, a new quantization path, PT2E, is currently in development and is expected to provide even more possibilities in the future.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;Special thanks to Nikita Shulga, Vasiliy Kuznetsov, Supriya Rao, and Jongsoo Park. Together, we made one more step forward on the path of improving the PyTorch CPU ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; AWS EC2 r7iz.metal-16xl instance (Intel(R) Xeon(R) Gold 6455B, 32-core/64-thread, Turbo Boost On, Hyper-Threading On, Memory: 8x64GB, Storage: 192GB); OS: Ubuntu 22.04.1 LTS; Kernel: 5.15.0-1028-aws; Batch Size: 1; Core per Instance: 4; PyTorch 2.0 RC3; TorchVision 0.15.0+cpu, test by Intel on 3/77/2023. May not reflect all publicly available security updates.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Hugging Face Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/hugging-face-joins/" rel="alternate" type="text/html" title="Hugging Face Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-08-03T00:00:00-07:00</published>
      <updated>2023-08-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hugging-face-joins</id>
      <content type="html" xml:base="https://pytorch.org/blog/hugging-face-joins/">&lt;p&gt;&lt;img src=&quot;/assets/images/huggingface-joins-1.jpg&quot; alt=&quot;Smiling hugging face&quot; style=&quot;max-width:250px;float:right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Hugging Face has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Hugging Face has been a long time supporter and contributor to the PyTorch Ecosystem by providing powerful models and resources that accelerate research, development, and adoption of AI technologies, particularly in the field of natural language processing.&lt;/p&gt;

&lt;p&gt;“Our mission has always been to democratize AI and make it accessible to everyone. We’re truly aligned with PyTorch’s objective of reducing the barrier of entry to practitioners. By joining the PyTorch Foundation, we can further amplify that impact and support this very important framework of the ecosystem that is PyTorch,” said Lysandre Debut, Head of Open Source at Hugging Face. “We believe the two ecosystems have significant overlap, and collaborating with the foundation will allow us to bridge the gap to provide the best software, the best tools to the machine learning community at large.”&lt;/p&gt;

&lt;p&gt;Hugging Face’s Model Hub and open source libraries promote collaboration and knowledge sharing within the AI open source community, making Hugging Face a great match to the growing PyTorch Foundation. They continue to drive industry adoption and collaboration by creating user-friendly tools and resources and providing accessible and well-documented libraries.&lt;/p&gt;

&lt;p&gt;“Hugging Face’s commitment to open source development and their exceptional contributions to the PyTorch ecosystem have truly impressed us. With their help, we will drive innovation, foster collaboration, and empower the global AI community to create transformative solutions for the AI community,” said PyTorch Foundation Executive Director Ibrahim Haddad. “We welcome Hugging Face to the PyTorch Foundation and look forward to the achievements that lie ahead.”&lt;/p&gt;

&lt;p&gt;As a premier member, Hugging Face is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/huggingface-joins-2.jpg&quot; alt=&quot;Lysandre Debut&quot; style=&quot;max-width:250px;float:right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Lysandre Debut, Head of Open Source at Hugging Face to our board.  Lysandre has been at Hugging Face since the company’s pivot to open-source, and was the first engineer to focus entirely on the open-source mission. Now leading the open-source part of the organization, Lysandre remains technically involved by being a core maintainer of the Transformers library.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-hugging-face&quot;&gt;About Hugging Face&lt;/h2&gt;

&lt;p&gt;Hugging Face is a community and company dedicated to lowering the barrier of entry to Machine Learning and Deep Learning. Strong advocates for open-source and open-science, their model Hub hosts more than 250,000 public models and 50,000 public datasets that are very simple to use. Transformers, Diffusers, PEFT, Accelerate, and Datasets are some of the open-source tools made available by Hugging Face.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its trademark usage page: www.linuxfoundation.org/trademark-usage. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AMD’s Journey to Openness and Performance</title>
      <link href="https://pytorch.org/blog/amd-journey/" rel="alternate" type="text/html" title="AMD's Journey to Openness and Performance" />
      <published>2023-08-01T00:00:00-07:00</published>
      <updated>2023-08-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/amd-journey</id>
      <content type="html" xml:base="https://pytorch.org/blog/amd-journey/">&lt;p&gt;AMD has gained progress in building a robust software stack that supports an open ecosystem of models, libraries, frameworks, and tools. With proven platforms gaining momentum, there is significance of a leadership software stack and an optimized ecosystem for achieving application performance. PyTorch is a key part of AMD’s AI  journey, and AMD’s Victor Peng, AMD President and Soumith Chintala, founder of PyTorch discussed the latest progress at the DC &amp;amp; AI Keynote on June 12.&lt;/p&gt;

&lt;h2 id=&quot;building-a-powerful-sw-stack-with-rocm&quot;&gt;Building a Powerful SW Stack with ROCm&lt;/h2&gt;

&lt;p&gt;Victor introduced ROCm, AMD’s SW stack for Instinct Data Center GPUs. It offers a comprehensive set of open-source libraries, runtime, compilers, and tools for developing, running, and fine-tuning AI models. The fifth generation ROCm incorporates optimizations for AI and high-performance computing workloads, including tailored kernels for low-latency memory systems, support for new data types, and integration with OpenAI Triton. With tools for porting AI software to AMD Instinct platforms, ROCm ensures quality and robustness, tested extensively and compliant with PyTorch and TensorFlow frameworks.&lt;/p&gt;

&lt;h2 id=&quot;collaboration-with-pytorch&quot;&gt;Collaboration with PyTorch&lt;/h2&gt;

&lt;p&gt;To shed light on the partnership between AMD and PyTorch, Victor invited &lt;a href=&quot;https://www.linkedin.com/in/soumith/&quot;&gt;Soumith Chintala&lt;/a&gt;, the founder of PyTorch, to discuss the advancements and integration between the two. PyTorch, the industry’s most famous AI framework, boasts a vibrant developer community and is known for its continuous innovation and incorporation of cutting-edge research.&lt;/p&gt;

&lt;p&gt;To highlight the AMD and PyTorch partnership, Victor hosted a discussion with Soumith Chintala, the founder of PyTorch. PyTorch, renowned for its innovation and community, is the industry’s leading AI framework. The latest version, PyTorch 2.0, integrates with hardware-agnostic software compilers like OpenAI Triton, enabling efficient training and deployment of AI models. With optimized techniques, PyTorch 2.0 enhances productivity and offers remarkable speed improvements. The collaboration between AMD and the PyTorch Foundation ensures seamless utilization of AMD GPUs, expanding AI accelerator accessibility worldwide and paving the way for future optimizations and broader hardware support.&lt;/p&gt;

&lt;h2 id=&quot;empowering-the-developer-community&quot;&gt;Empowering the Developer Community&lt;/h2&gt;

&lt;p&gt;The partnership between AMD and PyTorch benefits the developer community by democratizing access to AI accelerators. Support for AMD GPUs in PyTorch allows developers to train and deploy models across various platforms, including CPUs like EPYC and Ryzen, GPUs like Instinct and Radeon, and embedded devices like Versal SoCs. By ensuring immediate compatibility of new models on AMD platforms, the collaboration streamlines the development process and empowers developers to leverage the full potential of AMD’s hardware. This increased accessibility and flexibility enable developers worldwide to push the boundaries of AI innovation.&lt;/p&gt;

&lt;h2 id=&quot;hugging-face-and-ai-model-innovation&quot;&gt;Hugging Face and AI Model Innovation&lt;/h2&gt;

&lt;p&gt;Victor praised Hugging Face as the leading force behind open-source AI model innovation, empowering generative AI with transformative transformers. AMD’s optimized software enables a high-performing development stack, supporting groundbreaking AI advancements for customers and developers through scalable real-world deployments.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At the DC &amp;amp; AI Keynote, AMD demonstrated its dedication to openness, performance, and collaboration. The ROCm SW stack, PyTorch integration, and support for Hugging Face exemplify AMD’s commitment to empowering developers and researchers to achieve AI breakthroughs. By offering accessible, high-performing solutions, AMD fuels the future of AI as a leading GPU platform integrated with PyTorch.&lt;/p&gt;

&lt;p&gt;To listen to the full keynote visit the &lt;a href=&quot;https://www.youtube.com/watch?v=l3pe_qx95E0&quot;&gt;AMD Youtube&lt;/a&gt; channel&lt;/p&gt;

&lt;p&gt;To listen to Soumith Chintala’s section of the &lt;a href=&quot;https://www.youtube.com/watch?v=RgQEG2G1iaY&quot;&gt;keynote&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">AMD has gained progress in building a robust software stack that supports an open ecosystem of models, libraries, frameworks, and tools. With proven platforms gaining momentum, there is significance of a leadership software stack and an optimized ecosystem for achieving application performance. PyTorch is a key part of AMD’s AI journey, and AMD’s Victor Peng, AMD President and Soumith Chintala, founder of PyTorch discussed the latest progress at the DC &amp;amp; AI Keynote on June 12.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Performant Distributed checkpointing in Production with IBM</title>
      <link href="https://pytorch.org/blog/performant-distributed-checkpointing/" rel="alternate" type="text/html" title="Performant Distributed checkpointing in Production with IBM" />
      <published>2023-07-31T00:00:00-07:00</published>
      <updated>2023-07-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/performant-distributed-checkpointing</id>
      <content type="html" xml:base="https://pytorch.org/blog/performant-distributed-checkpointing/">&lt;p&gt;&lt;img src=&quot;/assets/images/2023-07-31-performant-distributed-checkpointing-1.png&quot; alt=&quot;Params saved per minute&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Last year, IBM Research began collaborating with us to onboard Fully Sharded Data Parallelism (FSDP) for their large foundation models. They became interested as FSDP is a PyTorch native offering for scaling their distributed training efforts on IBM Cloud.&lt;/p&gt;

&lt;p&gt;We are pleased to share that, in collaboration with IBM, we have achieved substantial checkpointing speedups for large models (72x vs the original PyTorch 1.13 save speed), proven model and optimizer checkpoint scaling to 30B parameters, and enabled cloud first training using FSDP + Distributed Checkpoint on S3 backends.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-distributed-checkpoint&quot;&gt;What is a Distributed Checkpoint?&lt;/h2&gt;

&lt;p&gt;Distributed checkpointing is the PyTorch native solution for saving and loading PyTorch models and optimizer states from multiple ranks, as well as supporting dynamically changing world sizes between reloads.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-07-31-performant-distributed-checkpointing-2.png&quot; alt=&quot;Checkpoint time vs model params&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Distributed Checkpoint (DCP) APIs were introduced in PyTorch 1.13, and are included as an official prototype feature in PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;Distributed checkpoint is different from torch.save() and torch.load() in a few significant ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;DCP produces multiples files per checkpoint, with at least one file per rank,&lt;/li&gt;
  &lt;li&gt;DCP operates in place, meaning that the model should allocate its data first and the Distributed Checkpoint will then use the storage.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A major improvement from 1.13 to 2.0 includes adding sharded_state_dict support for checkpointing FSDP models. This allows checkpointing for larger sized models, as well as adding support for load-time resharding. Load time resharding enables saving in one cluster topology, and loading into another.  This feature was highly requested as it allows training jobs to be run on one cluster, saved, and then continued on a different cluster with different world size.&lt;/p&gt;

&lt;p&gt;Another major change is that we decouple the storage layer from the checkpoint planning layer and separate implementation from the interface for both layers. With this change, users can now specify how their state_dict should be chunked or transformed during the checkpoint planning phase. Additionally, the customizable storage layer can easily accommodate different backends.&lt;/p&gt;

&lt;p&gt;More information on the Distributed Checkpoint package can be found &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.checkpoint.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performant-distributed-checkpointing-in-production-with-ibm&quot;&gt;Performant Distributed checkpointing in Production with IBM&lt;/h2&gt;

&lt;p&gt;IBM at Think 2023 announced its &lt;a href=&quot;https://www.ibm.com/products/watsonx-ai&quot;&gt;watsonx.ai&lt;/a&gt; platform for development and deployment of foundation models for the enterprise. Built on Hybrid Cloud, the platform enables use cases across multiple modalities such as NLP, timeseries, weather, chemistry, tabular data, and cybersecurity, with model sizes from 100s of millions to 10s of billions of parameters. Model architectures range from vision transformers, to multi-modal RoBERTa-style feature extractors, to large-scale generative language models similar to T5, GPT and Llama.&lt;/p&gt;

&lt;p&gt;As of today, IBM has now enabled checkpointing for T5-style architectures up to 11B parameters, and decoder architectures (GPT style) up to 30B.&lt;/p&gt;

&lt;p&gt;IBM helped us identify that this limits the scaling power of DCP from both memory and performance standpoints. With their suggestion, we enhanced our FileSystemWriter to produce single checkpoint per rank to reduce read write overhead.&lt;/p&gt;

&lt;p&gt;With this option as the new default, DCP now creates a single file per rank during checkpoint saving, which would then be sliced when reading parameters at load time.&lt;/p&gt;

&lt;p&gt;By combining sharded_state_dict support with single filer per rank writer, distributed checkpoint was able to accelerate checkpoint saving time over 72x vs the original PyTorch 1.13 save speed, and enable rapid checkpointing for models sizes over 15B which would previously simply time out.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Looking back, it’s really astounding the speedups we’ve seen, handling training for many of these models. We went from taking almost half an hour to write a single 11B checkpoint in PyTorch 1.13, to being able to handle a 30B parameter model, with optimizer and dataloader state - so that’s over eight times the raw data - in just over 3 minutes. That’s done wonders for both the stability and efficiency of our jobs, as we scale up training to hundreds of gpus.”  – &lt;strong&gt;Davis Wertheimer, IBM Research&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;IBM’s adoption has also helped us validate and improve our solutions in a real world, large-scale training environment. As an example, IBM discovered that DCP was working well for them on a single node with multiple GPUs, but erred out when used on multiple nodes.&lt;/p&gt;

&lt;p&gt;Upon investigating the issue, we realized that we were assuming writing to a NFS-like shared file system, which assumes strong read-after-write consistencies. Object stores with file system APIs such as S3FS provide eventual consistency semantics, thus causing the distributed checkpoint in such a setting to fail. Working together with IBM, we identified this issue and fixed it by making &lt;a href=&quot;https://research.ibm.com/blog/ibm-pytorch-ai-training&quot;&gt;one line code change&lt;/a&gt; and enabled object storage backend for DCP! Such storage approaches are typically an order of magnitude cheaper than shared file systems thus enabling finer grained checkpointing.&lt;/p&gt;

&lt;h2 id=&quot;looking-for-collaboration&quot;&gt;Looking for Collaboration&lt;/h2&gt;

&lt;p&gt;If you are interested in trying Distributed Checkpoint, feel free to reach out to us!&lt;/p&gt;

&lt;p&gt;If you run into any issue when trying it, you can open an &lt;a href=&quot;https://github.com/pytorch/pytorch/labels/module%3A%20distributed_checkpoint&quot;&gt;issue&lt;/a&gt; at our Github repo.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;This project would not have been possible without the assistance from many collaborators. We would like to thank Yanli Zhao, Andrew Gu, Rohan Varma for their support of FSDP. Thanks to Pritam Damania, Junjie Zhao, and Wanchao Liang for their support of ShardedTensor.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta:  Iris Zhang, Less Wright, Rodrigo Kumpera, Chien-Chin Huang, IBM: Davis Wertheimer, Supriyo Chakraboty, Sophia Wen, Raghu Ganti, Mudhakar Srivatsa, Seethrami Seelam</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">IBM Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/ibm-joins-pytorch/" rel="alternate" type="text/html" title="IBM Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-07-27T00:00:00-07:00</published>
      <updated>2023-07-27T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/ibm-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/ibm-joins-pytorch/">&lt;p&gt;The PyTorch Foundation, part of The Linux Foundation, is pleased to announce that IBM has joined as a premier member.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-ibm-logo.png&quot; alt=&quot;IBM Logo&quot; style=&quot;max-width:250px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The foundation serves as a neutral space for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. With its extensive industry expertise and leadership in open source and AI, IBM is committed to actively contributing to the PyTorch community.&lt;/p&gt;

&lt;p&gt;IBM offers a comprehensive portfolio of enterprise AI solutions and recently released watsonx, its next-generation data and AI platform. IBM’s watsonx platform leverages PyTorch to offer an enterprise-grade software stack for end-to-end training and fine-tuning of AI foundation models.&lt;/p&gt;

&lt;p&gt;“By joining the PyTorch Foundation, we aim to contribute our expertise and resources to further advance PyTorch’s capabilities and make AI more accessible in hybrid cloud environments with flexible hardware options,” said Priya Nagpurkar, Vice President, Hybrid Cloud Platform and Developer Productivity, IBM Research. “We intend for our collaboration with PyTorch to bring the power of foundation models and generative AI to enterprises using the watsonx platform to drive business transformation.”&lt;/p&gt;

&lt;p&gt;IBM and PyTorch have already collaborated on two projects. The first enables foundation models with billions of parameters to train efficiently on standard cloud networking infrastructure, such as Ethernet networking. Together, IBM and PyTorch have also worked on ways to make checkpointing for AI training considerably more cost-effective, by fixing the distributed checkpointing within PyTorch to support certain types of object storage.&lt;/p&gt;

&lt;p&gt;“We’re happy to welcome IBM as a premier member. IBM’s expertise and dedication to advancing the field of artificial intelligence align perfectly with the mission of the PyTorch community,” said PyTorch Foundation Executive Director Ibrahim Haddad. “Their commitment to open collaboration and innovation will strengthen our collective efforts to empower developers and researchers worldwide.”&lt;/p&gt;

&lt;p&gt;As a premier member, IBM  is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-ibm-headshot.png&quot; alt=&quot;Raghu Ganti Headshot&quot; style=&quot;max-width:250px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Raghu Ganti, Principal Research Scientist at IBM Research, to our board.  Raghu co-leads IBM Research’s foundation model training and validation platform, built on Red Hat OpenShift. His team primarily contributes to the PyTorch training components, with the mission of democratizing training and validation of foundation models.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, part of The Linux Foundation, is pleased to announce that IBM has joined as a premier member.</summary>
      

      
      
    </entry>
  
</feed>


