<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-10-04T10:40:29-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">High performance Llama 2 deployments with AWS Inferentia2 using TorchServe</title>
      <link href="https://pytorch.org/blog/high-performance-llama/" rel="alternate" type="text/html" title="High performance Llama 2 deployments with AWS Inferentia2 using TorchServe" />
      <published>2023-10-04T00:00:00-07:00</published>
      <updated>2023-10-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/high-performance-llama</id>
      <content type="html" xml:base="https://pytorch.org/blog/high-performance-llama/">&lt;p&gt;Recently, &lt;a href=&quot;https://ai.meta.com/llama/&quot;&gt;Llama 2&lt;/a&gt; was released and has attracted a lot of interest from the machine learning community. &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/inf2/&quot;&gt;Amazon EC2 Inf2 instances&lt;/a&gt;, powered by &lt;a href=&quot;https://aws.amazon.com/machine-learning/inferentia/&quot;&gt;AWS Inferentia2&lt;/a&gt;, now support training and inference of Llama 2 models. In this post, we show low-latency and cost-effective inference of Llama-2 models on Amazon EC2 Inf2 instances using the latest &lt;a href=&quot;https://aws.amazon.com/machine-learning/neuron/&quot;&gt;AWS Neuron SDK&lt;/a&gt; release.  We first introduce how to create, compile and deploy the Llama-2 model and explain the optimization techniques introduced by AWS Neuron SDK to achieve high performance at low cost. We then present our benchmarking results. Lastly, we show how the Llama-2 model can be deployed through Amazon SageMaker using TorchServe on an Inf2 instance. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/software_stack_inf2.jpg&quot; alt=&quot;Llama 2 is an auto-regressive language model that uses an optimized transformer architecture&quot; style=&quot;width:100%; max-width: 420px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-llama-2&quot;&gt;What is Llama 2&lt;/h2&gt;

&lt;p&gt;Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. Llama 2 is intended for commercial and research use in English. It comes in multiple sizes—7 billion, 13 billion, and 70 billion parameters—as well as pre-trained and fine-tuned variations. According to Meta, the tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. Llama 2 was pre-trained on 2 trillion tokens of data from publicly available sources. The tuned models are intended for assistant-like chat, whereas pre-trained models can be adapted for a variety of natural language generation tasks. Regardless of which version of the model a developer uses, the &lt;a href=&quot;https://ai.meta.com/llama/responsible-use-guide/&quot;&gt;responsible use guide from Meta &lt;/a&gt;can assist in guiding additional fine-tuning that may be necessary to customize and optimize the models with appropriate safety mitigations.&lt;/p&gt;

&lt;h2 id=&quot;amazon-ec2-inf2-instances-overview&quot;&gt;Amazon EC2 Inf2 instances Overview&lt;/h2&gt;

&lt;p&gt;Amazon EC2 Inf2 instances, featuring Inferentia2, provide 3x higher compute, 4x more accelerator memory, resulting in up to 4x higher throughput, and up to 10x lower latency, compared to the first generation Inf1 instances.&lt;/p&gt;

&lt;p&gt;Large language model (LLM) inference is a memory bound workload, performance scales up with more accelerator memory bandwidth. Inf2 instances are the only inference optimized instances in Amazon EC2 to provide high speed accelerator interconnect (NeuronLink) enabling high performance large LLM model deployments with cost effective distributed inference. You can now efficiently and cost-effectively deploy billion-scale LLMs across multiple accelerators on Inf2 instances.&lt;/p&gt;

&lt;p&gt;Inferentia2 supports FP32, TF32, BF16, FP16, UINT8, and the new configurable FP8 (cFP8) data type. AWS Neuron can take high-precision FP32 and FP16 models and autocast them to lower-precision data types while optimizing accuracy and performance. Autocasting reduces time to market by removing the need for lower-precision retraining and enabling higher-performance inference with smaller data types.&lt;/p&gt;

&lt;p&gt;To make it flexible and extendable to deploy constantly evolving deep learning models, Inf2 instances have hardware optimizations and software support for dynamic input shapes as well as custom operators written in C++ through the standard PyTorch custom operator programming interfaces.&lt;/p&gt;

&lt;h2 id=&quot;transformers-neuron-transformers-neuronx&quot;&gt;Transformers Neuron (transformers-neuronx)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx&quot;&gt;Transformers Neuron&lt;/a&gt; is a software package that enables PyTorch users to deploy performance optimized LLM inference. It has an optimized version of transformer models implemented with XLA high level operators (HLO), which enables sharding tensors across multiple NeuronCores, a.k.a. tensor parallelism, and performance optimizations such as parallel context encoding and KV caching for Neuron hardware. The Llama 2 source code in XLA HLOs can be found &lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx/blob/main/src/transformers_neuronx/llama/model.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Llama 2 is supported in Transformers Neuron through the &lt;a href=&quot;https://github.com/aws-neuron/transformers-neuronx/blob/33fa412447a4028edb252fd06aae9ed93086a450/src/transformers_neuronx/llama/model.py#L29&quot;&gt;LlamaForSampling&lt;/a&gt; class. Transformers Neuron provides a seamless user experience with Hugging Face models to provide optimized inference on Inf2 instances. More details can be found from the &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html#transformers-neuronx-developer-guide&quot;&gt;Transforms Neuron Developer Guide&lt;/a&gt;. In the following section, we will explain how to deploy the Llama-2 13B model using Transformers Neuron. And, this example also applies to other Llama-based models.&lt;/p&gt;

&lt;h2 id=&quot;llama-2-model-inference-with-transformers-neuron&quot;&gt;Llama 2 model inference with Transformers Neuron&lt;/h2&gt;

&lt;h3 id=&quot;create-model-compile-and-deploy&quot;&gt;Create model, compile and deploy&lt;/h3&gt;

&lt;p&gt;We have three simple steps here to create, compile and deploy the model on Inf2 instances.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a CPU model, use this &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/Readme.md?plain=1#L71&quot;&gt;script&lt;/a&gt; or the following code snippet to serialize and save checkpoints in a local directory.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from transformers import AutoModelForCausalLM
from transformers_neuronx.module import save_pretrained_split
model_cpu = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-13b-hf&quot;, low_cpu_mem_usage=True)
model_dir = &quot;./llama-2-13b-split&quot;
save_pretrained_split(model_cpu, model_dir)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Load and compile model from the local directory that you saved serialized checkpoints using the following.
To load the Llama 2 model, we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LlamaForSampling&lt;/code&gt; from Transformers Neuron. Note that the environment variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURON_RT_NUM_CORES&lt;/code&gt; specifies the number of NeuronCores to be used at runtime and it should match the tensor parallelism (TP) degree specified for the model. Also, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURON_CC_FLAGS&lt;/code&gt; enables compiler optimization on decoder-only LLM models.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from transformers_neuronx.llama.model import LlamaForSampling
os.environ['NEURON_RT_NUM_CORES'] = '24'
os.environ['NEURON_CC_FLAGS'] = '--model-type=transformer'
model = LlamaForSampling.from_pretrained(
        model_dir,
        batch_size=1,
        tp_degree=24,
        amp='bf16',
        n_positions=16,
        context_length_estimate=[8]
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p style=&quot;padding-left:6.25rem&quot;&gt;Now let's compile the model and load model weights into device memory with a one liner API.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.to_neuron()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Finally let’s run the inference on the compiled model. Note that both input and output of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sample&lt;/code&gt; function are a sequence of tokens.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;inputs = torch.tensor([[1, 16644, 31844, 312, 31876, 31836, 260, 3067, 2228, 31844]])
seq_len = 16
outputs = model.sample(inputs, seq_len, top_k=1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;inference-optimizations-in-transformers-neuron&quot;&gt;Inference optimizations in Transformers Neuron&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Tensor parallelism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_tp.jpg&quot; alt=&quot;Latency with different TP degrees&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Transformer Neuron implements parallel tensor operations across multiple NeuronCores. We denote the number of cores to be used for inference as TP degree. Larger TP degree provides higher memory bandwidth, leading to lower latency, as LLM token generation is a memory-IO bound workload. With increasing the TP degree, the inference latency has decreased significantly, our results shows, ~4x overall speed up with increased TP degrees from 2 to 24. For the Llama-2 7B model, latency decreases from 30.1 ms/token with 2 cores to 7.9 ms/token with 24 cores; similarly for the Llama-2 13B model, it goes down from 57.3 ms/token  to 11.1 ms/token.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parallel context encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the transformer architecture, tokens are produced in a sequential procedure called autoregressive sampling while input prompt tokens can be processed in parallel with parallel context encoding. This can significantly reduce the latency for input prompt context encoding before token generation through autoregressive sampling. By default, the parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;context_length_estimate&lt;/code&gt; would be set as a list of power-of-2 numbers which aims to cover a wide variety of context lengths. Depending on the use case, it can be set to custom numbers. This can be done when creating the Llama 2 model using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LlamaForSampling.from_pretrained&lt;/code&gt;. We characterize the impact of input token length on end-to-end (E2E) latency. As shown in the figure, latency for text generation with the Llama-2 7B model only slightly increases with bigger input prompts, thanks to parallel context encoding.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_input_token_length.jpg&quot; alt=&quot;E2E latency&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KV caching&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Self-attention block performs the self-attention operation with KV vectors. And, KV vectors are calculated using token embeddings and weights of KV and thus associated with tokens. In naive implementations, for each generated token, the entire KV cache is recalculated, but this reduces performance. Therefore Transformers Neuron library is reusing previously calculated KV vectors to avoid unnecessary computation, also known as KV caching, to reduce latency in the autoregressive sampling phase. &lt;/p&gt;

&lt;h3 id=&quot;benchmarking-results&quot;&gt;Benchmarking results&lt;/h3&gt;

&lt;p&gt;We benchmarked the latency and cost for both Llama-2 7B and 13B models under different conditions, i.e., number of output tokens, instance types. Unless specified, we use data type ‘bf16’ and batch size of 1 as this is a common configuration for real-time applications like chatbot and code assistant.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following graphs shows the per token latency on inf2.48xlarge instance with TP degree 24. Here, the latency per output token is calculated as the end-to-end latency divided by the number of output tokens. Our experiments show Llama-2 7B end-to-end latency to generate 256 tokens is 2x faster compared to other comparable inference-optimized EC2 instances. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/latency_vs_output_token_length.png&quot; alt=&quot;Latency on inf2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We now show the number of tokens generated per second for the Llama-2 7B and 13B models that can be delivered by the inf2.48xlarge instance. With TP degree 24, fully utilizing all the 24 NeuronCores, we can achieve 130 tokens/sec and 90 tokens/sec for the Llama-2 7B and 13B models, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/throughput_vs_output_token_length.jpg&quot; alt=&quot;E2E throughput&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For latency-first applications, we show the cost of hosting Llama-2 models on the inf2.48xlarge instance, &lt;strong&gt;$&lt;/strong&gt;0.011 per 1000 tokens and &lt;strong&gt;$&lt;/strong&gt;0.016 per 1000 tokens for the 7B and 13B models, respectively, which achieve 3x cost saving over other comparable inference-optimized EC2 instances. Note that we report the cost based on &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/inf2/&quot;&gt;3-year reserved instance price&lt;/a&gt; which is what customers use for large production deployments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/cost_vs_output_token_length_7b_13b.jpg&quot; alt=&quot;Cost on inf2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also compare the cost of hosting the Llama-2 7B model on inf2.xlarge and inf2.48xlarge instances. We can see that inf2.xlarge is more than 4x cheaper than inf2.48xlarge but at the expense of longer latency due to smaller TP degree. For example, it takes 7.9 ms for the model to generate 256 output tokens with 256 input tokens on inf2.48xlarge but 30.1 ms on Inf2.xlarge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/high-performance-llama/cost_vs_output_token_length_xl_48xl.jpg&quot; alt=&quot;Cost on Llama&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;serving-llama2-with-torchserve-on-ec2-inf2-instance&quot;&gt;Serving Llama2 with TorchServe on EC2 Inf2 instance&lt;/h2&gt;

&lt;p&gt;Now, we move on to model deployment. In this section, we show you how to deploy the &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-2-13b-hf&quot;&gt;Llama-2 13B model&lt;/a&gt; through SageMaker using TorchServe, which is the recommended model server for PyTorch, preinstalled in the AWS PyTorch Deep Learning Containers (DLC).&lt;/p&gt;

&lt;p&gt;This section describes the preparation work needed for using TorchServe, particularly, how to configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_config.yaml&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inf2_handler.py&lt;/code&gt; as well as how to generate model artifacts and pre-compile the model for use in later model deployment. Preparing the model artifacts ahead-of-time avoids model compilation during model deployment and thus reduces the model loading time.&lt;/p&gt;

&lt;h3 id=&quot;model-configurationmodel-configyaml&quot;&gt;Model configuration &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/model-config.yaml&quot;&gt;model-config.yaml&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The parameters defined in section &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handler&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;micro_batching&lt;/code&gt; are used in customer handler &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/inf2_handler.py&quot;&gt;inf2_handler.py&lt;/a&gt;. More details about model_config.yaml are &lt;a href=&quot;https://github.com/pytorch/serve/blob/2bf505bae3046b0f7d0900727ec36e611bb5dca3/docs/configuration.md?plain=1#L267&quot;&gt;here&lt;/a&gt;. TorchServe micro-batching is a mechanism to pre-process and post-process a batch of inference requests in parallel. It is able to achieve higher throughput by better utilizing the available accelerator when the backend is steadily fed with incoming data, see &lt;a href=&quot;https://github.com/pytorch/serve/tree/master/examples/micro_batching&quot;&gt;here&lt;/a&gt; for more details. For model inference on Inf2, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;micro_batch_size, amp, tp_degree and max_length&lt;/code&gt; specify the batch size, data type, tensor parallelism degree and max sequence length, respectively.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TorchServe Frontend Parameters
minWorkers: 1
maxWorkers: 1
maxBatchDelay: 100
responseTimeout: 10800
batchSize: 16

# TorchServe Backend Custom Handler Parameters
handler:
    model_checkpoint_dir: &quot;llama-2-13b-split&quot;
    amp: &quot;bf16&quot;
    tp_degree: 12
    max_length: 100

micro_batching:
    # Used by batch_size in function LlamaForSampling.from_pretrained
    micro_batch_size: 1  
    parallelism:
        preprocess: 2
        inference: 1
        postprocess: 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;custom-handlerinf2_handlerpy&quot;&gt;Custom handler &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/inferentia2/llama2/inf2_handler.py&quot;&gt;inf2_handler.py&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Custom handler in Torchserve is a simple Python script that lets you define the model initialization, preprocessing, inference and post-processing logic as functions. Here, we create our Inf2 custom handler.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/inf2_handler.py#L33&quot;&gt;initialize&lt;/a&gt; function is used to load the model. Here, Neuron SDK will compile the model for the first time and save the precompiled model in the directory as enabled by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURONX_CACHE&lt;/code&gt; in the directory specified by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NEURONX_DUMP_TO&lt;/code&gt;. After the first time, subsequent runs will check if there are already pre-compiled model artifacts. If so, it will skip model compilation.
Once the model is loaded, we initiate warm-up inference requests so that the compiled version is cached. When the &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html&quot;&gt;neuron persistent cache &lt;/a&gt;is utilized, it can significantly reduce the model loading latency, ensuring that the subsequent inference runs swiftly.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;os.environ[&quot;NEURONX_CACHE&quot;] = &quot;on&quot;
os.environ[&quot;NEURONX_DUMP_TO&quot;] = f&quot;{model_dir}/neuron_cache&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p style=&quot;padding-left:6.25rem&quot;&gt;TorchServe `TextIteratorStreamerBatch` extends Hugging Face transformers `BaseStreamer` to support response streaming when `batchSize` is larger than 1. &lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.output_streamer = TextIteratorStreamerBatch(
    self.tokenizer,
    batch_size=self.handle.micro_batch_size,
    skip_special_tokens=True,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/pytorch/serve/blob/d0ae857abfe6d36813c88e531316149a5a354a93/examples/large_models/inferentia2/llama2/inf2_handler.py#L124&quot;&gt;inference&lt;/a&gt; function calls send_intermediate_predict_response to send the streaming response.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for new_text in self.output_streamer:
    logger.debug(&quot;send response stream&quot;)
    send_intermediate_predict_response(
        new_text[: len(micro_batch_req_id_map)],
        micro_batch_req_id_map,
        &quot;Intermediate Prediction success&quot;,
        200,
        self.context,
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;package-model-artifacts&quot;&gt;Package model artifacts&lt;/h3&gt;

&lt;p&gt;Package all the model artifacts into a folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1&lt;/code&gt; using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch-model-archiver&lt;/code&gt;. &lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch-model-archiver --model-name llama-2-13b-neuronx-b1 --version 1.0 --handler inf2_handler.py -r requirements.txt --config-file model-config.yaml --archive-format no-archive
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;serve-the-model&quot;&gt;Serve the model&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export TS_INSTALL_PY_DEP_PER_MODEL=&quot;true&quot;
torchserve --ncs --start --model-store model_store --models llama-2-13b-neuronx-b1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the log shows “&lt;strong&gt;WORKER_MODEL_LOADED&lt;/strong&gt;”, the pre-compiled model should be saved in the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1/neuron_cache&lt;/code&gt;, which is tightly coupled with Neuron SDK version. Then, upload the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llama-2-13b-neuronx-b1&lt;/code&gt; to your S3 bucket for later use in the product deployment. The Llama-2 13B model artifacts in this blog can be found &lt;a href=&quot;https://torchserve.s3.amazonaws.com/mar_files/sm-neuronx/llama-2-13b-neuronx-b1/&quot;&gt;here&lt;/a&gt;, which is associated with Neuron SDK 2.13.2, in the TorchServe model zoo.&lt;/p&gt;

&lt;h2 id=&quot;deploy-llama-2-13b-model-on-sagemakerinf2-instance-using-torchserve&quot;&gt;Deploy Llama-2 13B model on SageMaker Inf2 instance using TorchServe &lt;/h2&gt;

&lt;p&gt;In this section, we deploy the Llama-2 13B model using a &lt;a href=&quot;https://github.com/aws/deep-learning-containers/blob/master/available_images.md#neuron-containers&quot;&gt;PyTorch Neuronx container&lt;/a&gt; on a SageMaker endpoint with an ml.inf2.24xlarge hosting instance, which has 6 Inferentia2 accelerators corresponding to our model configuration &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_config.yaml&lt;/code&gt; handler’s setting - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tp_degree: 12&lt;/code&gt;. Given that we have packaged all the model artifacts into a folder using &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/model-archiver/README.md&quot;&gt;torch-model-archiver&lt;/a&gt; and uploaded to S3 bucket, we will now use the SageMaker Python SDK to create a SageMaker model and deploy it to a SageMaker real-time endpoint using the deploy &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html&quot;&gt;uncompressed model method&lt;/a&gt;. Speed is the key benefit to deploying in this manner with SageMaker and you get a fully functional production ready endpoint complete with a secure RESTful endpoint without any effort spent on infrastructure. There are 3 steps to deploying the model and running inference on SageMaker. The notebook example can be found &lt;a href=&quot;https://github.com/aws/amazon-sagemaker-examples-community/blob/main/torchserve/inf2/llama2/llama-2-13b.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a SageMaker model&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from datetime import datetime

instance_type = &quot;ml.inf2.24xlarge&quot;
endpoint_name = sagemaker.utils.name_from_base(&quot;ts-inf2-llama2-13b-b1&quot;)

model = Model(
    name=&quot;torchserve-inf2-llama2-13b&quot; + datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;),
    # Enable SageMaker uncompressed model artifacts
    model_data={
        &quot;S3DataSource&quot;: {
                &quot;S3Uri&quot;: s3_uri,
                &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
                &quot;CompressionType&quot;: &quot;None&quot;,
        }
    },
    image_uri=container,
    role=role,
    sagemaker_session=sess,
    env={&quot;TS_INSTALL_PY_DEP_PER_MODEL&quot;: &quot;true&quot;},
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Deploy a SageMaker model&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.deploy(
    initial_instance_count=1,
    instance_type=instance_type,
    endpoint_name=endpoint_name,
    volume_size=512, # increase the size to store large model
    model_data_download_timeout=3600, # increase the timeout to download large model
    container_startup_health_check_timeout=600, # increase the timeout to load large model
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Run streaming response inference on SageMaker
When the endpoint is in service, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invoke_endpoint_with_response_stream&lt;/code&gt; API call to invoke the model. This feature enables the return of each generated token to the user, enhancing the user experience. It’s especially beneficial when generating an entire sequence is time-consuming.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import json

body = &quot;Today the weather is really nice and I am planning on&quot;.encode('utf-8')
resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=body, ContentType=&quot;application/json&quot;)
event_stream = resp['Body']
parser = Parser()
for event in event_stream:
    parser.write(event['PayloadPart']['Bytes'])
    for line in parser.scan_lines():
        print(line.decode(&quot;utf-8&quot;), end=' ')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;sample-inference&quot;&gt;Sample inference:&lt;/h3&gt;

&lt;p&gt;Input&lt;/p&gt;

&lt;p&gt;“Today the weather is really nice and I am planning on”&lt;/p&gt;

&lt;p&gt;Output&lt;/p&gt;

&lt;p&gt;“Today the weather is really nice and I am planning on going to the beach. I am going to take my camera and take some pictures of the beach. I am going to take pictures of the sand, the water, and the people. I am also going to take pictures of the sunset. I am really excited to go to the beach and take pictures.&lt;/p&gt;

&lt;p&gt;The beach is a great place to take pictures. The sand, the water, and the people are all great subjects for pictures. The sunset is also a great subject for pictures.”&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we showcased how to run Llama 2 model inference using Transformers Neuron and deploy Llama 2 model serving using TorchServe through Amazon SageMaker on an EC2 Inf2 instance. We demonstrated the benefits of using Inferentia2—low latency and low cost—enabled by optimizations in AWS Neuron SDK including tensor parallelism, parallel context encoding and KV caching, particularly for LLM inference. To stay up to date, please follow &lt;a href=&quot;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html&quot;&gt;AWS Neuron’s latest release&lt;/a&gt; for new features.&lt;/p&gt;

&lt;p&gt;Get started today with Llama 2 examples on &lt;a href=&quot;https://github.com/aws-neuron/aws-neuron-samples/blob/master/torch-neuronx/transformers-neuronx/inference/meta-llama-2-13b-sampling.ipynb&quot;&gt;EC2&lt;/a&gt; and through &lt;a href=&quot;https://github.com/aws/amazon-sagemaker-examples-community/blob/main/torchserve/inf2/llama2/llama-2-13b.ipynb&quot;&gt;SageMaker&lt;/a&gt; and stay tuned for how to optimize Llama 70B on Inf2!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Mike Zhang, Li Ning, Sergey Ivanov, Naman Nandan, Hamid Shojanazeri, Geeta Chauhan, Abhi Shivaditya, Michael Nguyen, Pinak Panigrahi</name>
        
        
      </author>

      

      

      
        <summary type="html">Recently, Llama 2 was released and has attracted a lot of interest from the machine learning community. Amazon EC2 Inf2 instances, powered by AWS Inferentia2, now support training and inference of Llama 2 models. In this post, we show low-latency and cost-effective inference of Llama-2 models on Amazon EC2 Inf2 instances using the latest AWS Neuron SDK release.  We first introduce how to create, compile and deploy the Llama-2 model and explain the optimization techniques introduced by AWS Neuron SDK to achieve high performance at low cost. We then present our benchmarking results. Lastly, we show how the Llama-2 model can be deployed through Amazon SageMaker using TorchServe on an Inf2 instance. </summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">How to Build an Interactive Chat-Generation Model using DialoGPT and PyTorch</title>
      <link href="https://pytorch.org/blog/interactive-chat-gen-model/" rel="alternate" type="text/html" title="How to Build an Interactive Chat-Generation Model using DialoGPT and PyTorch" />
      <published>2023-10-03T00:00:00-07:00</published>
      <updated>2023-10-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/interactive-chat-gen-model</id>
      <content type="html" xml:base="https://pytorch.org/blog/interactive-chat-gen-model/">&lt;p&gt;The focus on interactive chat-generation (or conversational response-generation) models has greatly increased in the past several months. Conversational response-generation models such as ChatGPT and Google Bard have taken the AI world by storm. The purpose of interactive chat generation is to answer various questions posed by humans, and these AI based models use natural language processing (NLP) to generate conversations almost indistinguishable from those generated by humans.&lt;/p&gt;

&lt;p&gt;This article showcases a &lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/blob/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Interactive_Chat_Quantization/IntelPytorch_Interactive_Chat_Quantization.ipynb&quot;&gt;code sample&lt;/a&gt; on how to create interactive chats based on a pre-trained DialoGPT model from Hugging Face with the addition of the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; to perform dynamic quantization on the model.&lt;/p&gt;

&lt;h2 id=&quot;get-started&quot;&gt;Get Started&lt;/h2&gt;

&lt;h3 id=&quot;why-dialogpt&quot;&gt;Why DialoGPT?&lt;/h3&gt;

&lt;p&gt;DialoGPT (&lt;strong&gt;Dialo&lt;/strong&gt;gue &lt;strong&gt;G&lt;/strong&gt;enerative &lt;strong&gt;P&lt;/strong&gt;re-trained &lt;strong&gt;T&lt;/strong&gt;ransformer) is a large-scale, pre-trained dialogue-response-generation model trained on 147M conversation-like exchanges pulled out from Reddit comment chains and discussion threads. &lt;a href=&quot;http://github.com/microsoft/DialoGPT&quot;&gt;DialoGPT&lt;/a&gt; was proposed by Microsoft in 2019. The main goal was to create open-domain chatbots capable of producing natural responses to a variety of conversational topics. The conversational response-generation systems that leverage DialoGPT generate more applicable, resourceful, diverse, and context-specific replies.&lt;/p&gt;

&lt;h3 id=&quot;dialogpt-architecture&quot;&gt;DialoGPT Architecture&lt;/h3&gt;

&lt;p&gt;DialoGPT architecture is based on the GPT-2 model. It is formulated as an autoregressive language model and uses a multi-layer transformer as the model architecture. GPT-2 was proposed by OpenAI. GPT-2 models are trained on general text data whereas DialoGPT is trained on Reddit discussion threads.&lt;/p&gt;

&lt;p&gt;Let’s look at the GPT-2 architecture. There are two types of blocks in general transformer architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Encoder - contains self-attention layer and feed-forward neural network&lt;/li&gt;
  &lt;li&gt;Decoder - similar to encoder, but the self-attention layer is masked&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The self-attention layer allows a position to peak at tokens to the right of the current word (the successive words in text), whereas masked self-attention layer prevents that from happening.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-self-attention-vs-masked.png&quot; alt=&quot;self-attention layer vs masked self-attention layer&quot; style=&quot;width:100%; max-width: 845px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GPT-2 is built using transformer decoder blocks. This means that the following layers are used in the architecture:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Embedding Layer – responsible for converting input text into embeddings (each word is converted to a fixed-length vector representation)&lt;/li&gt;
  &lt;li&gt;Transformer Decoder – includes multiple decoder blocks with masked self-attention and feed forward neural network layers&lt;/li&gt;
  &lt;li&gt;Output Layer – responsible for converting embeddings obtained from the decoder into words&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;GPT-2 architecture (and DialoGPT architecture) is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-dialogpt-article.png&quot; alt=&quot;GPT-2 architecture&quot; style=&quot;width:100%; max-width: 651px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the model is based on transformers architecture, it has the issue of repetition and copying the inputs. To avoid repetition, we can use Top-K sampling and Top-p sampling.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Top-K sampling - filters the K most likely next words and redistributes the probability mass among only those K next words.&lt;/li&gt;
  &lt;li&gt;Top-p sampling - rather than selecting only the most likely K words, selects the smallest possible set of words whose cumulative probability exceeds the probability p.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The probability mass is then redistributed among the words in the set. As a result, the size of the set of words can be dynamically increased and decreased based on the probability distribution of the next word.&lt;/p&gt;

&lt;h3 id=&quot;quantization-using-intel-extension-for-pytorch&quot;&gt;Quantization using Intel® Extension for PyTorch&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;What is Quantization?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Quantization is a systematic reduction of the precision of all or several layers within the model. This means a higher-precision type, such as the single-precision floating-point (FP32) mostly used in deep learning, is converted into a lower-precision type such as FP16 (16 bits) or INT8 (8 bits).&lt;/p&gt;

&lt;p&gt;This helps in achieving,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;lower memory bandwidth&lt;/li&gt;
  &lt;li&gt;lower storage&lt;/li&gt;
  &lt;li&gt;higher performance with minimum-to-zero accuracy loss&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantization is especially important with large models such as those based on the Transformer architecture like BERT or GPT.&lt;/p&gt;

&lt;p&gt;There are two types of quantization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Static – Static quantization quantizes the weights and activations of the model. This quantization is used when both memory bandwidth and compute savings are important.&lt;/li&gt;
  &lt;li&gt;Dynamic – In dynamic quantization, the weights are quantized ahead of time, but the activations are dynamically quantized during inference.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Intel Extension for PyTorch:&lt;/strong&gt; The Intel Extension extends PyTorch with up-to-date features and optimizations for an extra performance boost on Intel® hardware. Learn how to &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch#installation&quot;&gt;install it standalone&lt;/a&gt; or get it a part of the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html&quot;&gt;Intel® AI Analytics Toolkit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The extension can be loaded as a Python* module or linked as a C++ library. Python users can enable it dynamically by importing intel_extension_for_pytorch.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/cpu/latest/&quot;&gt;CPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel CPUs. Source code is available at the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/master&quot;&gt;master branch&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;This &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/xpu/latest/&quot;&gt;GPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel GPUs. Source code is available at the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/xpu-master&quot;&gt;xpu-master branch&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How to perform dynamic quantization using Intel Extension for PyTorch?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here are the steps to quantize the existing FP32 model to INT8 model using dynamic quantization:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prepare quantization configuration - We can use default dynamic quantization configuration with &lt;strong&gt;ipex.quantization.default_dynamic_qconfig&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Prepare the FP32 model by using the** ipex.quantization.prepare **method (provide the input parameters such as FP32 model to quantize, the prepared configuration, example inputs and information if the quantization should be in place).&lt;/li&gt;
  &lt;li&gt;Convert the model from FP32 to INT8 - Use &lt;strong&gt;ipex.quantization.convert&lt;/strong&gt; method for conversion. The input model will be the model prepared in step 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We also encourage you to check out the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&quot;&gt;Intel® Neural Compressor&lt;/a&gt; tool that automates popular model-compression technologies such as quantization, pruning, and knowledge distillation across multiple &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;deep learning frameworks&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;code-sample&quot;&gt;Code Sample&lt;/h2&gt;

&lt;p&gt;The following steps are implemented in the &lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/blob/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Interactive_Chat_Quantization/IntelPytorch_Interactive_Chat_Quantization.ipynb&quot;&gt;code sample&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Load model and tokenizer:&lt;/strong&gt; &lt;a href=&quot;http://huggingface.co/docs/transformers/index&quot;&gt;Transformers library&lt;/a&gt; (check out &lt;a href=&quot;http://github.com/intel/intel-extension-for-transformers&quot;&gt;Intel® Extension for Transformers&lt;/a&gt;) and &lt;a href=&quot;http://huggingface.co/docs/transformers/model_doc/auto&quot;&gt;Auto Classes available in the Hugging Face Main Classes&lt;/a&gt; are used in this step. These allow us to automatically find the relevant model by the given name. It also allows to easily change the model without major changes in the code on the developer’s side as shown below:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;The model parameter is specified as an input for the tokenizer, and model initialization is just the path to the pre-trained DialoGPT model. In this sample, we are using ‘microsoft/DialoGPT-large.’ If you have limited resources, you can use ‘microsoft/DialoGPT-medium’ or ‘microsoft/DialoGPT-small’ models and receive comparable results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Perform dynamic quantization of the model:&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;Create the configuration using the default dynamic quantization configuration from Intel Extension for PyTorch library.&lt;/li&gt;
      &lt;li&gt;Prepare the model.&lt;/li&gt;
      &lt;li&gt;Convert the model from FP32 to INT8. &lt;br /&gt;
The steps are explained in detail in the above section.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Response generation:&lt;/strong&gt; The first step in response generation is to encode the input sentence as shown in the code below:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;new_input_ids = tokenizer.encode(input(&quot;&amp;gt;&amp;gt; You:&quot;) + tokenizer.eos_token, return_tensors='pt')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;In this sample, we want our model to save history, so we are adding input sentences in the form of tokens to the chat history:&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_round &amp;gt; 0 else new_input_ids
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;The text generation can be done by the model.generate function, where we can specify all important parameters like saved chat history, length of the response in tokens, and usage of both Top-K and Top-p sampling.&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chat_history_ids = model.generate(bot_input_ids, do_sample=True, max_length=2000, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;The last step is to decode and print the response:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Preparation for interactive conversation:&lt;/strong&gt; After response generation, the last step is to add interaction. This can be done by using a simple for loop. Based on the initialized tokenizer, model, and empty chat history, responses are generated for a number of rounds:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for chat_round in range(n):
chat_history_ids = generate_response(
tokenizer,
model,
chat_round,
chat_history_ids
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;An example of interactive chat generation will look like the one shown in the picture below.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f3-dialogpt-interaction.png&quot; alt=&quot;An example of interactive chat generation&quot; style=&quot;width:100%; max-width: 981px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;Get started with interactive chat-generation models using Intel Extension for PyTorch and DialoGPT. Download and try the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html&quot;&gt;Intel AI Analytics Toolkit&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;Intel Extension for PyTorch&lt;/a&gt; for yourself to build various end-to-end AI applications.&lt;/p&gt;

&lt;p&gt;We encourage you to also check out and incorporate Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;AI/ML Framework optimizations&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;end-to-end portfolio of tools&lt;/a&gt; into your AI workflow and learn about the unified, open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; programming model that forms the foundation of Intel’s &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI Software Portfolio&lt;/a&gt; to help you prepare, build, deploy, and scale your AI solutions.&lt;/p&gt;

&lt;p&gt;For more details about the new 4th Gen Intel® Xeon® Scalable processors, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;Intel’s AI Solution Platform portal&lt;/a&gt; where you can learn how Intel is empowering developers to run end-to-end AI pipelines on these powerful CPUs.&lt;/p&gt;

&lt;h3 id=&quot;useful-resources&quot;&gt;Useful resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;Intel AI Developer Tools and resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI unified programming model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;Official documentation - PyTorch Optimizations from Intel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/&quot;&gt;Intel® Extension for PyTorch - Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;explore-more-ai-code-samples&quot;&gt;Explore more AI code samples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification&quot;&gt;Language Identification: Building an End-to-End AI Solution using PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;Optimize PyTorch Models using Intel® Extension for PyTorch (IPEX) Quantization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPyTorch_TrainingOptimizations_AMX_BF16&quot;&gt;PyTorch Training Optimizations with Advanced Matrix Extensions Bfloat16&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/code-samples.html&quot;&gt;See all code samples&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">The focus on interactive chat-generation (or conversational response-generation) models has greatly increased in the past several months. Conversational response-generation models such as ChatGPT and Google Bard have taken the AI world by storm. The purpose of interactive chat generation is to answer various questions posed by humans, and these AI based models use natural language processing (NLP) to generate conversations almost indistinguishable from those generated by humans.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing PyTorch Docathon H2 2023</title>
      <link href="https://pytorch.org/blog/announcing-docathon-h2-2023/" rel="alternate" type="text/html" title="Announcing PyTorch Docathon H2 2023" />
      <published>2023-10-02T00:00:00-07:00</published>
      <updated>2023-10-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/announcing-docathon-h2-2023</id>
      <content type="html" xml:base="https://pytorch.org/blog/announcing-docathon-h2-2023/">&lt;p&gt;We are excited to announce that we will be holding a Docathon for PyTorch on November 1, 2023! This event is an opportunity for our community to come together and improve the quality of our documentation.&lt;/p&gt;

&lt;p&gt;During the Docathon, we will focus on updating and improving existing content, as well as adding new tutorials and docstrings. We encourage all members of the community to participate and contribute their expertise to make our documentation even better. This is a great opportunity to learn and collaborate together.&lt;/p&gt;

&lt;p&gt;Check out our previous docathon success story &lt;a href=&quot;https://pytorch.org/blog/docathon-h1-2023-wrap-up/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-participate&quot;&gt;Why Participate&lt;/h2&gt;

&lt;p&gt;One of the best things about the Docathon is that you can make a tangible, positive impact on the quality of documentation in real time. This collaborative event brings together diverse team members from various companies, backgrounds, and roles, united to work towards a common goal. This event not only fosters team building and knowledge sharing but also presents an opportunity for individuals to acquire new skills, such as writing, editing, and utilizing documentation tools. Participating in a docathon can be particularly beneficial for team members who may lack experience in these areas.&lt;/p&gt;

&lt;p&gt;And of course all participants will be recognized for their contributions. Top participants will receive special awards.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;Event Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Nov 1: Kick-off&lt;/li&gt;
  &lt;li&gt;Nov 1- Nov 12:  Submissions and Feedback&lt;/li&gt;
  &lt;li&gt;Nov 13 - Nov 15: Final Reviews&lt;/li&gt;
  &lt;li&gt;Nov 15: Winner Announcements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Details for the Docathon to be announced at the kick-off call on November 1.&lt;/p&gt;

&lt;p&gt;To participate in the Docathon and receive updates about the event, register here: &lt;a href=&quot;https://community.linuxfoundation.org/events/details/lfhq-pytorch-foundation-presents-fall-pytorch-docathon-nov-1st-rsvp/&quot;&gt;RSVP&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are excited to see the improvements that will come out of this Docathon, and we look forward to your participation!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce that we will be holding a Docathon for PyTorch on November 1, 2023! This event is an opportunity for our community to come together and improve the quality of our documentation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond</title>
      <link href="https://pytorch.org/blog/inside-the-matrix/" rel="alternate" type="text/html" title="Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond" />
      <published>2023-09-25T00:00:00-07:00</published>
      <updated>2023-09-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/inside-the-matrix</id>
      <content type="html" xml:base="https://pytorch.org/blog/inside-the-matrix/">&lt;p&gt;&lt;em&gt;Use 3D to visualize matrix multiplication expressions, attention heads with real weights, and more.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Matrix multiplications (matmuls) are the building blocks of today’s ML models. This note presents &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;mm&lt;/a&gt;, a visualization tool for matmuls and compositions of matmuls.&lt;/p&gt;

&lt;p&gt;Because mm uses all three spatial dimensions, it helps build intuition and spark ideas with less cognitive overhead than the usual squares-on-paper idioms, especially (though not only) for visual/spatial thinkers.&lt;/p&gt;

&lt;p&gt;And with three dimensions available for &lt;em&gt;composing&lt;/em&gt; matmuls, along with the ability to load trained weights, we can visualize big, compound expressions like attention heads and observe how they actually behave, using im.&lt;/p&gt;

&lt;p&gt;mm is fully interactive, runs &lt;a href=&quot;https://bhosmer.github.io/mm/&quot;&gt;in the browser&lt;/a&gt; or &lt;a href=&quot;https://colab.research.google.com/drive/1wZIoU20eRWKtRNCW7e5Iugm3MhfaE1f7&quot;&gt;notebook iframes&lt;/a&gt; and keeps its complete state in the URL, so links are shareable sessions (the screenshots and videos in this note all have links that open the visualizations in the tool). This &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;reference guide&lt;/a&gt; describes all of the available functionality.&lt;/p&gt;

&lt;p&gt;We’ll first introduce the visualization approach, build intuition by visualizing some simple matmuls and expressions, then dive into some more extended examples:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Pitch&lt;/strong&gt; - why is this way of visualizing better?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Warmup - animations&lt;/strong&gt; - watching the canonical matmul decompositions in action&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Warmup - expressions&lt;/strong&gt; - a quick tour of some fundamental expression building blocks&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inside an attention head&lt;/strong&gt; - an in-depth look at the structure, values and computation behavior of a couple of attention heads from GPT2 via &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;NanoGPT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Parallelizing attention&lt;/strong&gt; - visualizing attention head parallelization with examples from the recent &lt;a href=&quot;https://arxiv.org/pdf/2305.19370.pdf&quot;&gt;Blockwise Parallel Transformer&lt;/a&gt; paper&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sizes in an attention layer&lt;/strong&gt; - what do the MHA and FFA halves of an attention layer look like together, when we visualize a whole layer as a single structure? How does the picture change during autoregressive decoding?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt; - a visual explanation of this elaboration of the attention head architecture&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Wrapup&lt;/strong&gt; - next steps and call for feedback&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-pitch&quot;&gt;1 Pitch&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;mm&lt;/a&gt;’s visualization approach is based on the premise that &lt;em&gt;matrix multiplication is fundamentally a three-dimensional operation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In other words this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/matmul3.jpg&quot; alt=&quot;matrix multiplication is fundamentally a three-dimensional operation&quot; style=&quot;width:100%; max-width: 478px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;is a sheet of paper trying to be this (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A20%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22none%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A3%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22closed%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A0.8227%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-48.763575165818956%2C%22y%22%3A43.72517618222101%2C%22z%22%3A33.70077275818966%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/initial.jpg&quot; alt=&quot;wrap the matmul around a cube&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we wrap the matmul around a cube this way, the correct relationships between argument shapes, result shape and shared dimensions all fall into place.&lt;/p&gt;

&lt;p&gt;Now the computation makes &lt;em&gt;geometric sense&lt;/em&gt;: each location &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i, j&lt;/code&gt; in the result matrix anchors a vector running along the depth dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; in the cube’s interior, where the horizontal plane extending from row &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; and a vertical plane extending from column &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; intersect. Along this vector, pairs of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(i, k)&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(k, j)&lt;/code&gt; elements from the left and right arguments meet and are multiplied, and the resulting products are summed along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; and the result is deposited in location &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i, j&lt;/code&gt; of the result.&lt;/p&gt;

&lt;p&gt;(Jumping ahead momentarily, &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A48%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22dotprod%20(row%20major)%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;here’s an animation&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;This is the &lt;em&gt;intuitive&lt;/em&gt; meaning of matrix multiplication:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;project&lt;/strong&gt; two orthogonal matrices into the interior of a cube&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;multiply&lt;/strong&gt; the pair of values at each intersection, forming a grid of products&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;sum&lt;/strong&gt; along the third orthogonal dimension to produce a result matrix.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For orientation, the tool displays an arrow in the cube’s interior that points towards the result matrix, with a blue vane coming from the left argument and a &lt;strong&gt;r&lt;/strong&gt;ed vane coming from the &lt;strong&gt;r&lt;/strong&gt;ight argument. The tool also displays white guidelines to indicate the row axis of each matrix, though they’re faint in this screenshot.&lt;/p&gt;

&lt;p&gt;The layout constraints are straightforward:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;left argument and result must be adjoined along their shared &lt;strong&gt;height&lt;/strong&gt; (i) dimension&lt;/li&gt;
  &lt;li&gt;right argument and result must be adjoined along their shared &lt;strong&gt;width&lt;/strong&gt; (j) dimension&lt;/li&gt;
  &lt;li&gt;left and right arguments must be adjoined along their shared (left width/right height) dimension, which becomes the matmul’s &lt;strong&gt;depth&lt;/strong&gt; (k) dimension&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This geometry gives us a solid foundation for visualizing all the standard matmul decompositions, and an intuitive basis for exploring nontrivially complex &lt;em&gt;compositions&lt;/em&gt; of matmuls, as we’ll see below.&lt;/p&gt;

&lt;h2 id=&quot;2-warmup---animations&quot;&gt;2 Warmup - animations&lt;/h2&gt;

&lt;p&gt;Before diving into some more complex examples, we’ll run through a few intuition builders to get a feel for how things look and feel in this style of visualization.&lt;/p&gt;

&lt;h3 id=&quot;2a-dot-product&quot;&gt;2a Dot product&lt;/h3&gt;

&lt;p&gt;First, the canonical algorithm - computing each result element by taking the dot product of the corresponding left row and right column. What we see in the animation is the sweep of multiplied value vectors through the cube’s interior, each delivering a summed result at the corresponding position.&lt;/p&gt;

&lt;p&gt;Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; has blocks of rows filled with 1 (blue) or -1 (red); &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; has column blocks filled similarly. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; is 24 here, so the result matrix (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L @ R&lt;/code&gt;) has blue values of 24 and red values of -24 (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A48%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22dotprod%20(row%20major)%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt; - long click or control-click to inspect values):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/dotprod1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h3 id=&quot;2b-matrix-vector-products&quot;&gt;2b Matrix-vector products&lt;/h3&gt;

&lt;p&gt;A matmul decomposed into matrix-vector products looks like a vertical plane (a product of the left argument with each column of the right argument) painting columns onto the result as it sweeps horizontally through the cube’s interior (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A12%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22mvprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/mvprod1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;Observing the intermediate values of a decomposition can be quite interesting, even in simple examples.&lt;/p&gt;

&lt;p&gt;For instance, note the prominent vertical patterns in the intermediate matrix-vector products when we use randomly initialized arguments- reflecting the fact that each intermediate is a column-scaled replica of the left argument (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(i%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(j%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A6%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22mvprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22open%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22local%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.14559417241423%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768186%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/mvprod2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h3 id=&quot;2c-vector-matrix-products&quot;&gt;2c Vector-matrix products&lt;/h3&gt;

&lt;p&gt;A matmul decomposed into vector-matrix products looks like a horizontal plane painting rows onto the result as it descends through the cube’s interior (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A12%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vmprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vmprod_check.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;Switching to randomly initialized arguments, we see patterns analogous to those we saw with matrix-vector products - only this time the patterns are horizontal, corresponding to the fact that each intermediate vector-matrix product is a row-scaled replica of the right argument.&lt;/p&gt;

&lt;p&gt;When thinking about how matmuls express the rank and structure of their arguments, it’s useful to envision both of these patterns happening simultaneously in the computation (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(i%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(j%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A6%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vmprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22open%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22local%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.14559417241423%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768186%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vmprod3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;Here’s one more intuition builder using vector-matrix products, showing how the identity matrix functions exactly like a mirror set at a 45deg angle to both its counterargument and the result (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A24%2C%22init%22%3A%22eye%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(i%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22row%20major%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(j%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22open%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A12%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vmprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22open%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22local%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-50.560896320538845%2C%22y%22%3A45.336792719337595%2C%22z%22%3A34.94291121097398%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vmprod_id.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h3 id=&quot;2d-summed-outer-products&quot;&gt;2d Summed outer products&lt;/h3&gt;

&lt;p&gt;The third planar decomposition is along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; axis, computing the matmul result by a pointwise summation of vector outer products. Here we see the plane of outer products sweeping the cube “from back to front”, accumulating into the result (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22expr%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201%22%2C%22folder%22%3A%22closed%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A12%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vvprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0.5%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22closed%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22semilocal%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.145594172414235%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768185%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vvprod_check.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;Using randomly initialized matrices with this decomposition, we can see not just values but &lt;em&gt;rank&lt;/em&gt; accumulate in the result, as each rank-1 outer product is added to it.&lt;/p&gt;

&lt;p&gt;Among other things this builds intuition for why “low-rank factorization” - i.e. approximating a matrix by constructing a matmul whose arguments are small in the depth dimension - works best when the matrix being approximated is low rank. &lt;a href=&quot;https://arxiv.org/pdf/2106.09685.pdf&quot;&gt;LoRA&lt;/a&gt; in a later section (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?params=%7B%22expr%22%3A%22L%20%40%20R%22%2C%22name%22%3A%22L%20%40%20R%22%2C%22epilog%22%3A%22none%22%2C%22left%22%3A%7B%22name%22%3A%22L%22%2C%22matmul%22%3Afalse%2C%22h%22%3A32%2C%22w%22%3A24%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(i%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22closed%22%7D%2C%22right%22%3A%7B%22name%22%3A%22R%22%2C%22matmul%22%3Afalse%2C%22h%22%3A24%2C%22w%22%3A32%2C%22init%22%3A%22gaussian%22%2C%22url%22%3A%22%22%2C%22min%22%3A-1%2C%22max%22%3A1%2C%22dropout%22%3A0%2C%22expr%22%3A%22(-Math.trunc(j%20%2F%208)%20%25%202)%20%2B%20.5%22%2C%22folder%22%3A%22closed%22%7D%2C%22anim%22%3A%7B%22fuse%22%3A%22none%22%2C%22speed%22%3A6%2C%22hide%20inputs%22%3Afalse%2C%22alg%22%3A%22vvprod%22%2C%22spin%22%3A0%2C%22folder%22%3A%22open%22%7D%2C%22block%22%3A%7B%22i%20blocks%22%3A1%2C%22j%20blocks%22%3A1%2C%22k%20blocks%22%3A1%7D%2C%22layout%22%3A%7B%22scheme%22%3A%22blocks%22%2C%22gap%22%3A5%2C%22scatter%22%3A0%2C%22molecule%22%3A1%2C%22blast%22%3A0%2C%22polarity%22%3A%22negative%22%2C%22left%20placement%22%3A%22left%22%2C%22right%20placement%22%3A%22top%22%2C%22result%20placement%22%3A%22front%22%2C%22folder%22%3A%22open%22%7D%2C%22deco%22%3A%7B%22legends%22%3A6%2C%22shape%22%3Atrue%2C%22spotlight%22%3A2%2C%22row%20guides%22%3A1%2C%22flow%20guides%22%3A0%2C%22lens%20size%22%3A0.5%2C%22magnification%22%3A10%2C%22interior%20spotlight%22%3Afalse%2C%22axes%22%3Afalse%2C%22folder%22%3A%22open%22%7D%2C%22viz%22%3A%7B%22sensitivity%22%3A%22local%22%2C%22min%20size%22%3A0.196%2C%22min%20light%22%3A0.4%2C%22max%20light%22%3A0.6%2C%22elem%20scale%22%3A1%2C%22zero%20hue%22%3A0.77%2C%22hue%20gap%22%3A0.74%2C%22hue%20spread%22%3A0.04%2C%22folder%22%3A%22open%22%7D%2C%22diag%22%3A%7B%22url%22%3A%22%22%7D%2C%22cam%22%3A%7B%22x%22%3A-54.14559417241423%2C%22y%22%3A48.55110882721702%2C%22z%22%3A37.42031544768186%2C%22target%22%3A%7B%22x%22%3A0%2C%22y%22%3A0%2C%22z%22%3A0%7D%7D%2C%22folder%22%3A%22closed%22%2C%22compress%22%3Afalse%7D&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/vvprod_random_fast.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h2 id=&quot;3-warmup---expressions&quot;&gt;3 Warmup - expressions&lt;/h2&gt;

&lt;p&gt;How can we extend this visualization approach to &lt;em&gt;compositions&lt;/em&gt; of matmuls? Our examples so far have all visualized a single matmul &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L @ R&lt;/code&gt; of some matrices &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; - what about when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L&lt;/code&gt; and/or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; are themselves matmuls, and so on transitively?&lt;/p&gt;

&lt;p&gt;It turns out we can extend the approach nicely to compound expressions. The key rules are simple: the subexpression (child) matmul is another cube, subject to the same layout constraints as the parent, and the result face of the child is &lt;em&gt;simultaneously&lt;/em&gt; the corresponding argument face of the parent, like a covalently shared electron.&lt;/p&gt;

&lt;p&gt;Within these constraints, we’re free to arrange the faces of a child matmul however we like. Here we use the tool’s default scheme, which generates alternating convex and concave cubes - this layout works well in practice to maximize use of space and minimize occlusion. (Layouts are completely customizable, however - see the &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;reference&lt;/a&gt; for details.)&lt;/p&gt;

&lt;p&gt;In this section we’ll visualize some of the key building blocks we find in ML models, to gain fluency in the visual idiom and to see what intuitions even simple examples can give us.&lt;/p&gt;

&lt;h3 id=&quot;3a-left-associative-expressions&quot;&gt;3a Left-associative expressions&lt;/h3&gt;

&lt;p&gt;We’ll look at two expressions of the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(A @ B) @ C&lt;/code&gt;, each with its own distinctive shape and character. (Note: mm adheres to the convention that matrix multiplication is left-associative and writes this simply as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A @ B @ C&lt;/code&gt;.)&lt;/p&gt;

&lt;p&gt;First we’ll give &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A @ B @ C&lt;/code&gt; the characteristic FFN shape, in which the “hidden dimension” is wider than the “input” or “output” dimensions. (Concretely in the context of this example, this means that the width of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is greater than the widths of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt;.)&lt;/p&gt;

&lt;p&gt;As in the single matmul examples, the floating arrows point towards the result matrix, blue vane coming from the left argument and red vane from right argument (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20C&amp;amp;1=A%20%40%20B%20%40%20C&amp;amp;2=none&amp;amp;12=closed&amp;amp;64=true&amp;amp;3.1=A%20%40%20B&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.12=open&amp;amp;3.2=none&amp;amp;13.1=A&amp;amp;13.4=false&amp;amp;13.5=64&amp;amp;13.6=32&amp;amp;13.7=expr&amp;amp;13.8=&amp;amp;13.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.12=open&amp;amp;14.1=B&amp;amp;14.4=false&amp;amp;14.5=32&amp;amp;14.6=96&amp;amp;14.7=row%20major&amp;amp;14.8=&amp;amp;14.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;14.9=-1&amp;amp;14.10=1&amp;amp;14.11=0&amp;amp;14.12=open&amp;amp;15.16=inherit&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.23=1&amp;amp;24.1=C&amp;amp;24.4=false&amp;amp;24.5=96&amp;amp;24.6=32&amp;amp;24.7=col%20major&amp;amp;24.8=&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.12=open&amp;amp;25.26=none&amp;amp;25.27=12&amp;amp;25.28=false&amp;amp;25.16=none&amp;amp;25.29=0&amp;amp;25.12=closed&amp;amp;30.31=1&amp;amp;30.32=1&amp;amp;30.23=1&amp;amp;33.34=blocks&amp;amp;33.35=5&amp;amp;33.36=0&amp;amp;33.37=1&amp;amp;33.38=0&amp;amp;33.18=negative&amp;amp;33.19=left&amp;amp;33.20=top&amp;amp;33.21=front&amp;amp;33.12=closed&amp;amp;39.40=6&amp;amp;39.41=true&amp;amp;39.42=2&amp;amp;39.43=1&amp;amp;39.44=0.5&amp;amp;39.45=0.5&amp;amp;39.46=10&amp;amp;39.47=false&amp;amp;39.48=false&amp;amp;39.12=open&amp;amp;49.50=semilocal&amp;amp;49.51=0.2&amp;amp;49.52=0.4&amp;amp;49.53=0.6&amp;amp;49.54=1.25&amp;amp;49.55=0.77&amp;amp;49.56=0.74&amp;amp;49.57=0.04&amp;amp;49.12=open&amp;amp;58.8=&amp;amp;59.60=-102.42301073851515&amp;amp;59.61=96.27580041479706&amp;amp;59.62=112.34410815468306&amp;amp;63.60=-4.617417891034972&amp;amp;63.61=-3.695553245058398&amp;amp;63.62=-1.8863985145585351&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;left.left=13&amp;amp;left.right=14&amp;amp;left.anim=15&amp;amp;alg=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.block=22&amp;amp;k%20blocks=23&amp;amp;right=24&amp;amp;anim=25&amp;amp;fuse=26&amp;amp;speed=27&amp;amp;hide%20inputs=28&amp;amp;spin=29&amp;amp;block=30&amp;amp;i%20blocks=31&amp;amp;j%20blocks=32&amp;amp;layout=33&amp;amp;scheme=34&amp;amp;gap=35&amp;amp;scatter=36&amp;amp;molecule=37&amp;amp;blast=38&amp;amp;deco=39&amp;amp;legends=40&amp;amp;shape=41&amp;amp;spotlight=42&amp;amp;row%20guides=43&amp;amp;flow%20guides=44&amp;amp;lens%20size=45&amp;amp;magnification=46&amp;amp;interior%20spotlight=47&amp;amp;axes=48&amp;amp;viz=49&amp;amp;sensitivity=50&amp;amp;min%20size=51&amp;amp;min%20light=52&amp;amp;max%20light=53&amp;amp;elem%20scale=54&amp;amp;zero%20hue=55&amp;amp;hue%20gap=56&amp;amp;hue%20spread=57&amp;amp;diag=58&amp;amp;cam=59&amp;amp;x=60&amp;amp;y=61&amp;amp;z=62&amp;amp;cam.target=63&amp;amp;compress=64&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/la2still.jpg&quot; alt=&quot;As in the single matmul examples, the floating arrows point towards the result matrix, blue vane coming from the left argument and red vane from right argument&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next we’ll visualize &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A @ B @ C&lt;/code&gt; with the width of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; &lt;em&gt;narrower&lt;/em&gt; than that of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt;, giving it a bottleneck or “autoencoder” shape (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20C&amp;amp;1=A%20%40%20B%20%40%20C&amp;amp;2=none&amp;amp;12=closed&amp;amp;64=true&amp;amp;3.1=A%20%40%20B&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.12=open&amp;amp;3.2=none&amp;amp;13.1=A&amp;amp;13.4=false&amp;amp;13.5=64&amp;amp;13.6=96&amp;amp;13.7=expr&amp;amp;13.8=&amp;amp;13.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.12=open&amp;amp;14.1=B&amp;amp;14.4=false&amp;amp;14.5=96&amp;amp;14.6=32&amp;amp;14.7=row%20major&amp;amp;14.8=&amp;amp;14.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;14.9=-1&amp;amp;14.10=1&amp;amp;14.11=0&amp;amp;14.12=open&amp;amp;15.16=inherit&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.23=1&amp;amp;24.1=C&amp;amp;24.4=false&amp;amp;24.5=32&amp;amp;24.6=96&amp;amp;24.7=col%20major&amp;amp;24.8=&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.12=open&amp;amp;25.26=none&amp;amp;25.27=12&amp;amp;25.28=false&amp;amp;25.16=none&amp;amp;25.29=0&amp;amp;25.12=closed&amp;amp;30.31=1&amp;amp;30.32=1&amp;amp;30.23=1&amp;amp;33.34=blocks&amp;amp;33.35=5&amp;amp;33.36=0&amp;amp;33.37=1&amp;amp;33.38=0&amp;amp;33.18=negative&amp;amp;33.19=left&amp;amp;33.20=top&amp;amp;33.21=front&amp;amp;33.12=closed&amp;amp;39.40=6&amp;amp;39.41=true&amp;amp;39.42=2&amp;amp;39.43=1&amp;amp;39.44=0.5&amp;amp;39.45=0.5&amp;amp;39.46=10&amp;amp;39.47=false&amp;amp;39.48=false&amp;amp;39.12=open&amp;amp;49.50=semilocal&amp;amp;49.51=0.2&amp;amp;49.52=0.4&amp;amp;49.53=0.6&amp;amp;49.54=1.25&amp;amp;49.55=0.77&amp;amp;49.56=0.74&amp;amp;49.57=0.04&amp;amp;49.12=open&amp;amp;58.8=&amp;amp;59.60=-125.71162036288077&amp;amp;59.61=101.84279252909485&amp;amp;59.62=122.50425255743914&amp;amp;63.60=-14.817097084822203&amp;amp;63.61=-9.723209466639396&amp;amp;63.62=-5.4699873376955646&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;left.left=13&amp;amp;left.right=14&amp;amp;left.anim=15&amp;amp;alg=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.block=22&amp;amp;k%20blocks=23&amp;amp;right=24&amp;amp;anim=25&amp;amp;fuse=26&amp;amp;speed=27&amp;amp;hide%20inputs=28&amp;amp;spin=29&amp;amp;block=30&amp;amp;i%20blocks=31&amp;amp;j%20blocks=32&amp;amp;layout=33&amp;amp;scheme=34&amp;amp;gap=35&amp;amp;scatter=36&amp;amp;molecule=37&amp;amp;blast=38&amp;amp;deco=39&amp;amp;legends=40&amp;amp;shape=41&amp;amp;spotlight=42&amp;amp;row%20guides=43&amp;amp;flow%20guides=44&amp;amp;lens%20size=45&amp;amp;magnification=46&amp;amp;interior%20spotlight=47&amp;amp;axes=48&amp;amp;viz=49&amp;amp;sensitivity=50&amp;amp;min%20size=51&amp;amp;min%20light=52&amp;amp;max%20light=53&amp;amp;elem%20scale=54&amp;amp;zero%20hue=55&amp;amp;hue%20gap=56&amp;amp;hue%20spread=57&amp;amp;diag=58&amp;amp;cam=59&amp;amp;x=60&amp;amp;y=61&amp;amp;z=62&amp;amp;cam.target=63&amp;amp;compress=64&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/lacontract.jpg&quot; alt=&quot;visualize A @ B @ C with the width of B narrower than that of A or C&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This pattern of alternating convex and concave blocks extends to chains of arbitrary length: for example this multilayer bottleneck (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;1=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;2=none&amp;amp;23=closed&amp;amp;63=true&amp;amp;3.2=none&amp;amp;4.5=inherit&amp;amp;6.7=1&amp;amp;8.9=positive&amp;amp;8.10=left&amp;amp;8.11=bottom&amp;amp;8.12=back&amp;amp;13.0=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;13.1=A%20%40%20B%20%40%20C&amp;amp;13.2=none&amp;amp;14.1=A%20%40%20B&amp;amp;14.15=true&amp;amp;14.16=32&amp;amp;14.17=32&amp;amp;14.18=row%20major&amp;amp;14.19=&amp;amp;14.20=-1&amp;amp;14.21=1&amp;amp;14.22=0&amp;amp;14.23=open&amp;amp;14.2=none&amp;amp;24.1=A&amp;amp;24.15=false&amp;amp;24.16=64&amp;amp;24.17=96&amp;amp;24.18=expr&amp;amp;24.19=&amp;amp;24.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.20=-1&amp;amp;24.21=1&amp;amp;24.22=0&amp;amp;24.23=open&amp;amp;25.1=B&amp;amp;25.15=false&amp;amp;25.16=96&amp;amp;25.17=64&amp;amp;25.18=row%20major&amp;amp;25.19=&amp;amp;25.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;25.20=-1&amp;amp;25.21=1&amp;amp;25.22=0&amp;amp;25.23=open&amp;amp;26.5=inherit&amp;amp;27.9=positive&amp;amp;27.10=left&amp;amp;27.11=bottom&amp;amp;27.12=back&amp;amp;28.7=1&amp;amp;29.1=C&amp;amp;29.15=false&amp;amp;29.16=64&amp;amp;29.17=32&amp;amp;29.18=col%20major&amp;amp;29.19=&amp;amp;29.20=-1&amp;amp;29.21=1&amp;amp;29.22=0&amp;amp;29.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;29.23=open&amp;amp;30.31=none&amp;amp;30.32=12&amp;amp;30.33=false&amp;amp;30.5=none&amp;amp;30.34=0&amp;amp;30.23=closed&amp;amp;35.36=1&amp;amp;35.7=1&amp;amp;37.9=negative&amp;amp;37.10=left&amp;amp;37.11=top&amp;amp;37.12=front&amp;amp;38.39=6&amp;amp;38.40=true&amp;amp;38.41=2&amp;amp;38.42=1&amp;amp;38.43=0.5&amp;amp;38.44=0.5&amp;amp;38.45=10&amp;amp;38.46=false&amp;amp;38.47=false&amp;amp;38.23=open&amp;amp;48.49=semilocal&amp;amp;48.50=0.2&amp;amp;48.51=0.4&amp;amp;48.52=0.6&amp;amp;48.53=1.25&amp;amp;48.54=0.77&amp;amp;48.55=0.74&amp;amp;48.56=0.04&amp;amp;48.23=open&amp;amp;57.19=&amp;amp;58.59=-125.71162036288077&amp;amp;58.60=101.84279252909485&amp;amp;58.61=122.50425255743914&amp;amp;62.59=-14.817097084822203&amp;amp;62.60=-9.723209466639396&amp;amp;62.61=-5.4699873376955646&amp;amp;13.23=open&amp;amp;13.63=true&amp;amp;13.15=true&amp;amp;64.1=D&amp;amp;64.15=false&amp;amp;64.16=32&amp;amp;64.17=64&amp;amp;64.18=col%20major&amp;amp;64.19=&amp;amp;64.20=-1&amp;amp;64.21=1&amp;amp;64.22=0&amp;amp;64.0=&amp;amp;64.23=open&amp;amp;3.1=A%20%40%20B%20%40%20C%20%40%20D&amp;amp;3.15=true&amp;amp;65.1=E&amp;amp;65.15=false&amp;amp;65.16=64&amp;amp;65.17=96&amp;amp;65.18=col%20major&amp;amp;65.19=&amp;amp;65.20=-1&amp;amp;65.21=1&amp;amp;65.22=0&amp;amp;65.0=&amp;amp;66.31=none&amp;amp;66.32=12&amp;amp;66.33=false&amp;amp;66.5=none&amp;amp;66.34=0&amp;amp;66.23=closed&amp;amp;67.36=1&amp;amp;67.68=1&amp;amp;67.7=1&amp;amp;69.70=blocks&amp;amp;69.71=5&amp;amp;69.72=0&amp;amp;69.73=1&amp;amp;69.74=0&amp;amp;69.9=negative&amp;amp;69.10=left&amp;amp;69.11=top&amp;amp;69.12=front&amp;amp;69.23=closed&amp;amp;75.39=5.28&amp;amp;75.40=true&amp;amp;75.41=2&amp;amp;75.42=1&amp;amp;75.43=0.5&amp;amp;75.44=0.5&amp;amp;75.45=10&amp;amp;75.46=false&amp;amp;75.47=false&amp;amp;75.23=open&amp;amp;76.49=semilocal&amp;amp;76.50=0.2&amp;amp;76.51=0.4&amp;amp;76.52=0.6&amp;amp;76.53=1.25&amp;amp;76.54=0.77&amp;amp;76.55=0.74&amp;amp;76.56=0.04&amp;amp;76.23=open&amp;amp;77.19=&amp;amp;78.59=-163.23429720087873&amp;amp;78.60=132.20892347209139&amp;amp;78.61=159.04014894666057&amp;amp;79.59=-14.817097084822203&amp;amp;79.60=-9.723209466639396&amp;amp;79.61=-5.4699873376955646&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;left.anim=4&amp;amp;alg=5&amp;amp;left.block=6&amp;amp;k%20blocks=7&amp;amp;left.layout=8&amp;amp;polarity=9&amp;amp;left%20placement=10&amp;amp;right%20placement=11&amp;amp;result%20placement=12&amp;amp;left.left=13&amp;amp;left.left.left=14&amp;amp;matmul=15&amp;amp;h=16&amp;amp;w=17&amp;amp;init=18&amp;amp;url=19&amp;amp;min=20&amp;amp;max=21&amp;amp;dropout=22&amp;amp;folder=23&amp;amp;left.left.left.left=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.left.block=28&amp;amp;left.left.right=29&amp;amp;left.left.anim=30&amp;amp;fuse=31&amp;amp;speed=32&amp;amp;hide%20inputs=33&amp;amp;spin=34&amp;amp;left.left.block=35&amp;amp;i%20blocks=36&amp;amp;left.left.layout=37&amp;amp;left.left.deco=38&amp;amp;legends=39&amp;amp;shape=40&amp;amp;spotlight=41&amp;amp;row%20guides=42&amp;amp;flow%20guides=43&amp;amp;lens%20size=44&amp;amp;magnification=45&amp;amp;interior%20spotlight=46&amp;amp;axes=47&amp;amp;left.left.viz=48&amp;amp;sensitivity=49&amp;amp;min%20size=50&amp;amp;min%20light=51&amp;amp;max%20light=52&amp;amp;elem%20scale=53&amp;amp;zero%20hue=54&amp;amp;hue%20gap=55&amp;amp;hue%20spread=56&amp;amp;left.left.diag=57&amp;amp;left.left.cam=58&amp;amp;x=59&amp;amp;y=60&amp;amp;z=61&amp;amp;left.left.cam.target=62&amp;amp;compress=63&amp;amp;left.right=64&amp;amp;right=65&amp;amp;anim=66&amp;amp;block=67&amp;amp;j%20blocks=68&amp;amp;layout=69&amp;amp;scheme=70&amp;amp;gap=71&amp;amp;scatter=72&amp;amp;molecule=73&amp;amp;blast=74&amp;amp;deco=75&amp;amp;viz=76&amp;amp;diag=77&amp;amp;cam=78&amp;amp;cam.target=79&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/nlayerbottleneck.jpg&quot; alt=&quot;pattern of alternating convex and concave blocks extends to chains of arbitrary length&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3b-right-associative-expressions&quot;&gt;3b Right associative expressions&lt;/h3&gt;

&lt;p&gt;Next we’ll visualize a right-associative expression &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A @ (B @ C)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In the same way left-associative expressions extend horizontally - sprouting from the left argument of the root expression, so to speak - right-associative chains extend vertically, sprouting from the root’s right argument.&lt;/p&gt;

&lt;p&gt;One sometimes sees an MLP formulated right-associatively, i.e. with columnar input on the right and weight layers running right to left. Using the matrices from the 2-layer FFN example pictured above - suitably transposed - here’s what that looks like, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; now playing the role of the input, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; the first layer and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt; the second layer (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20(B%20%40%20C)&amp;amp;1=A%20%40%20(B%20%40%20C)&amp;amp;2=none&amp;amp;12=closed&amp;amp;64=true&amp;amp;3.1=A&amp;amp;3.4=false&amp;amp;3.5=32&amp;amp;3.6=96&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;3.12=open&amp;amp;13.1=B%20%40%20C&amp;amp;13.4=true&amp;amp;13.5=32&amp;amp;13.6=32&amp;amp;13.7=col%20major&amp;amp;13.8=&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.2=none&amp;amp;14.15=inherit&amp;amp;16.17=1&amp;amp;18.19=positive&amp;amp;18.20=right&amp;amp;18.21=top&amp;amp;18.22=back&amp;amp;23.1=B&amp;amp;23.4=false&amp;amp;23.5=96&amp;amp;23.6=32&amp;amp;23.7=col%20major&amp;amp;23.8=&amp;amp;23.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.12=open&amp;amp;24.1=C&amp;amp;24.4=false&amp;amp;24.5=32&amp;amp;24.6=64&amp;amp;24.7=expr&amp;amp;24.8=&amp;amp;24.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.12=open&amp;amp;13.12=open&amp;amp;25.26=none&amp;amp;25.27=12&amp;amp;25.28=false&amp;amp;25.15=none&amp;amp;25.29=0&amp;amp;25.12=closed&amp;amp;30.31=1&amp;amp;30.32=1&amp;amp;30.17=1&amp;amp;33.34=blocks&amp;amp;33.35=5&amp;amp;33.36=0&amp;amp;33.37=1&amp;amp;33.38=0&amp;amp;33.19=negative&amp;amp;33.20=left&amp;amp;33.21=top&amp;amp;33.22=front&amp;amp;33.12=closed&amp;amp;39.40=6&amp;amp;39.41=true&amp;amp;39.42=2&amp;amp;39.43=1&amp;amp;39.44=0.5&amp;amp;39.45=0.5&amp;amp;39.46=10&amp;amp;39.47=false&amp;amp;39.48=false&amp;amp;39.12=closed&amp;amp;49.50=semilocal&amp;amp;49.51=0.2&amp;amp;49.52=0.4&amp;amp;49.53=0.6&amp;amp;49.54=1.25&amp;amp;49.55=0.77&amp;amp;49.56=0.74&amp;amp;49.57=0.04&amp;amp;49.12=closed&amp;amp;58.8=&amp;amp;58.12=open&amp;amp;59.60=-105.78213185291946&amp;amp;59.61=96.67420268229331&amp;amp;59.62=113.6419504179439&amp;amp;63.60=-4.617417891034972&amp;amp;63.61=-3.695553245058398&amp;amp;63.62=-1.8863985145585351&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;right=13&amp;amp;right.anim=14&amp;amp;alg=15&amp;amp;right.block=16&amp;amp;k%20blocks=17&amp;amp;right.layout=18&amp;amp;polarity=19&amp;amp;left%20placement=20&amp;amp;right%20placement=21&amp;amp;result%20placement=22&amp;amp;right.left=23&amp;amp;right.right=24&amp;amp;anim=25&amp;amp;fuse=26&amp;amp;speed=27&amp;amp;hide%20inputs=28&amp;amp;spin=29&amp;amp;block=30&amp;amp;i%20blocks=31&amp;amp;j%20blocks=32&amp;amp;layout=33&amp;amp;scheme=34&amp;amp;gap=35&amp;amp;scatter=36&amp;amp;molecule=37&amp;amp;blast=38&amp;amp;deco=39&amp;amp;legends=40&amp;amp;shape=41&amp;amp;spotlight=42&amp;amp;row%20guides=43&amp;amp;flow%20guides=44&amp;amp;lens%20size=45&amp;amp;magnification=46&amp;amp;interior%20spotlight=47&amp;amp;axes=48&amp;amp;viz=49&amp;amp;sensitivity=50&amp;amp;min%20size=51&amp;amp;min%20light=52&amp;amp;max%20light=53&amp;amp;elem%20scale=54&amp;amp;zero%20hue=55&amp;amp;hue%20gap=56&amp;amp;hue%20spread=57&amp;amp;diag=58&amp;amp;cam=59&amp;amp;x=60&amp;amp;y=61&amp;amp;z=62&amp;amp;cam.target=63&amp;amp;compress=64&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/raffn.jpg&quot; alt=&quot;an MLP formulated right-associatively&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aside: in addition to the color of the arrow vanes (blue for left, red for right), a second visual cue for distinguishing left and right arguments is their &lt;em&gt;orientation&lt;/em&gt;: the rows of the left argument are coplanar with those of the result - they stack along the same axis (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt;). Both cues tell us for example that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is the left argument to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B @ C)&lt;/code&gt; above.&lt;/p&gt;

&lt;h3 id=&quot;3c-binary-expressions&quot;&gt;3c Binary expressions&lt;/h3&gt;

&lt;p&gt;For a visualization tool to be useful beyond simple didactic examples, visualizations need to remain legible as expressions get more complicated. A key structural component in real-world use cases is binary expressions - matmuls with subexpressions on both the left and right.&lt;/p&gt;

&lt;p&gt;Here we’ll visualize the simplest such expression shape, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(A @ B) @ (C @ D)&lt;/code&gt; (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20(C%20%40%20D)&amp;amp;1=A%20%40%20B%20%40%20(C%20%40%20D)&amp;amp;2=none&amp;amp;12=closed&amp;amp;69=true&amp;amp;3.1=A%20%40%20B&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.12=open&amp;amp;3.2=none&amp;amp;13.1=A&amp;amp;13.4=false&amp;amp;13.5=64&amp;amp;13.6=64&amp;amp;13.7=expr&amp;amp;13.8=&amp;amp;13.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.12=closed&amp;amp;14.1=B&amp;amp;14.4=false&amp;amp;14.5=64&amp;amp;14.6=64&amp;amp;14.7=row%20major&amp;amp;14.8=&amp;amp;14.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;14.9=-1&amp;amp;14.10=1&amp;amp;14.11=0&amp;amp;14.12=closed&amp;amp;15.16=inherit&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.23=1&amp;amp;24.1=C%20%40%20D&amp;amp;24.4=true&amp;amp;24.5=32&amp;amp;24.6=32&amp;amp;24.7=col%20major&amp;amp;24.8=&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.2=none&amp;amp;25.16=inherit&amp;amp;26.23=1&amp;amp;27.18=positive&amp;amp;27.19=right&amp;amp;27.20=top&amp;amp;27.21=back&amp;amp;28.1=C&amp;amp;28.4=false&amp;amp;28.5=64&amp;amp;28.6=64&amp;amp;28.7=col%20major&amp;amp;28.8=&amp;amp;28.9=-1&amp;amp;28.10=1&amp;amp;28.11=0&amp;amp;28.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;28.12=open&amp;amp;29.1=D&amp;amp;29.4=false&amp;amp;29.5=64&amp;amp;29.6=64&amp;amp;29.7=expr&amp;amp;29.8=&amp;amp;29.9=-1&amp;amp;29.10=1&amp;amp;29.11=0&amp;amp;29.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;29.12=open&amp;amp;30.31=none&amp;amp;30.32=12&amp;amp;30.33=false&amp;amp;30.16=none&amp;amp;30.34=0&amp;amp;30.12=closed&amp;amp;35.36=1&amp;amp;35.37=1&amp;amp;35.23=1&amp;amp;38.39=blocks&amp;amp;38.40=5&amp;amp;38.41=0&amp;amp;38.42=1&amp;amp;38.43=0&amp;amp;38.18=negative&amp;amp;38.19=left&amp;amp;38.20=top&amp;amp;38.21=front&amp;amp;38.12=closed&amp;amp;44.45=6&amp;amp;44.46=true&amp;amp;44.47=2&amp;amp;44.48=1&amp;amp;44.49=0.5&amp;amp;44.50=0.5&amp;amp;44.51=10&amp;amp;44.52=false&amp;amp;44.53=false&amp;amp;44.12=closed&amp;amp;54.55=semilocal&amp;amp;54.56=0.4&amp;amp;54.57=0.4&amp;amp;54.58=0.6&amp;amp;54.59=1.5&amp;amp;54.60=0.77&amp;amp;54.61=0.74&amp;amp;54.62=0.04&amp;amp;54.12=open&amp;amp;63.8=&amp;amp;64.65=-149.45958189074523&amp;amp;64.66=140.76437147298853&amp;amp;64.67=162.13832534246401&amp;amp;68.65=-4.044017278625395&amp;amp;68.66=-2.123834827920271&amp;amp;68.67=-2.551083969824457&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;left.left=13&amp;amp;left.right=14&amp;amp;left.anim=15&amp;amp;alg=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.block=22&amp;amp;k%20blocks=23&amp;amp;right=24&amp;amp;right.anim=25&amp;amp;right.block=26&amp;amp;right.layout=27&amp;amp;right.left=28&amp;amp;right.right=29&amp;amp;anim=30&amp;amp;fuse=31&amp;amp;speed=32&amp;amp;hide%20inputs=33&amp;amp;spin=34&amp;amp;block=35&amp;amp;i%20blocks=36&amp;amp;j%20blocks=37&amp;amp;layout=38&amp;amp;scheme=39&amp;amp;gap=40&amp;amp;scatter=41&amp;amp;molecule=42&amp;amp;blast=43&amp;amp;deco=44&amp;amp;legends=45&amp;amp;shape=46&amp;amp;spotlight=47&amp;amp;row%20guides=48&amp;amp;flow%20guides=49&amp;amp;lens%20size=50&amp;amp;magnification=51&amp;amp;interior%20spotlight=52&amp;amp;axes=53&amp;amp;viz=54&amp;amp;sensitivity=55&amp;amp;min%20size=56&amp;amp;min%20light=57&amp;amp;max%20light=58&amp;amp;elem%20scale=59&amp;amp;zero%20hue=60&amp;amp;hue%20gap=61&amp;amp;hue%20spread=62&amp;amp;diag=63&amp;amp;cam=64&amp;amp;x=65&amp;amp;y=66&amp;amp;z=67&amp;amp;cam.target=68&amp;amp;compress=69&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/binary4.jpg&quot; alt=&quot;binary expressions - matmuls with subexpressions on both the left and right&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3d-quick-aside-partitioning-and-parallelism&quot;&gt;3d Quick aside: partitioning and parallelism&lt;/h3&gt;

&lt;p&gt;A full presentation of this topic is out of scope for this note, though we’ll see it in action later in the context of attention heads. But as a warmup, two quick examples should give a sense of how this style of visualization makes reasoning about parallelizing compound expressions very intuitive, via the simple geometry of partitioning.&lt;/p&gt;

&lt;p&gt;In the first example we’ll apply the canonical “data parallel” partitioning to the left-associative multilayer bottleneck example above. We partition along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt;, segmenting the initial left argument (“batch”) and all intermediate results (“activations”), but none of the subsequent arguments (“weights”) - the geometry making it obvious which participants in the expression are segmented and which remain whole (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;1=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;2=none&amp;amp;23=closed&amp;amp;63=true&amp;amp;3.1=A%20%40%20B%20%40%20C%20%40%20D&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.0=A%20%40%20B%20%40%20C%20%40%20D%20%40%20E&amp;amp;21.1=A%20%40%20B%20%40%20C&amp;amp;21.2=none&amp;amp;22.1=A%20%40%20B&amp;amp;22.4=true&amp;amp;22.5=32&amp;amp;22.6=32&amp;amp;22.7=row%20major&amp;amp;22.8=&amp;amp;22.9=-1&amp;amp;22.10=1&amp;amp;22.11=0&amp;amp;22.23=open&amp;amp;22.2=none&amp;amp;24.1=A&amp;amp;24.4=false&amp;amp;24.5=64&amp;amp;24.6=96&amp;amp;24.7=expr&amp;amp;24.8=&amp;amp;24.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.23=open&amp;amp;25.1=B&amp;amp;25.4=false&amp;amp;25.5=96&amp;amp;25.6=64&amp;amp;25.7=row%20major&amp;amp;25.8=&amp;amp;25.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.23=open&amp;amp;26.13=inherit&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.15=1&amp;amp;29.1=C&amp;amp;29.4=false&amp;amp;29.5=64&amp;amp;29.6=32&amp;amp;29.7=col%20major&amp;amp;29.8=&amp;amp;29.9=-1&amp;amp;29.10=1&amp;amp;29.11=0&amp;amp;29.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;29.23=open&amp;amp;30.31=none&amp;amp;30.32=12&amp;amp;30.33=false&amp;amp;30.13=none&amp;amp;30.34=0&amp;amp;30.23=closed&amp;amp;35.36=1&amp;amp;35.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;38.39=6&amp;amp;38.40=true&amp;amp;38.41=2&amp;amp;38.42=1&amp;amp;38.43=0.5&amp;amp;38.44=0.5&amp;amp;38.45=10&amp;amp;38.46=false&amp;amp;38.47=false&amp;amp;38.23=open&amp;amp;48.49=semilocal&amp;amp;48.50=0.2&amp;amp;48.51=0.4&amp;amp;48.52=0.6&amp;amp;48.53=1.25&amp;amp;48.54=0.77&amp;amp;48.55=0.74&amp;amp;48.56=0.04&amp;amp;48.23=open&amp;amp;57.8=&amp;amp;58.59=-125.71162036288077&amp;amp;58.60=101.84279252909485&amp;amp;58.61=122.50425255743914&amp;amp;62.59=-14.817097084822203&amp;amp;62.60=-9.723209466639396&amp;amp;62.61=-5.4699873376955646&amp;amp;21.23=open&amp;amp;21.63=true&amp;amp;21.4=true&amp;amp;64.1=D&amp;amp;64.4=false&amp;amp;64.5=32&amp;amp;64.6=64&amp;amp;64.7=col%20major&amp;amp;64.8=&amp;amp;64.9=-1&amp;amp;64.10=1&amp;amp;64.11=0&amp;amp;64.0=&amp;amp;64.23=open&amp;amp;65.1=E&amp;amp;65.4=false&amp;amp;65.5=64&amp;amp;65.6=96&amp;amp;65.7=col%20major&amp;amp;65.8=&amp;amp;65.9=-1&amp;amp;65.10=1&amp;amp;65.11=0&amp;amp;65.0=&amp;amp;66.31=none&amp;amp;66.32=12&amp;amp;66.33=false&amp;amp;66.13=none&amp;amp;66.34=0&amp;amp;66.23=closed&amp;amp;67.36=8&amp;amp;67.68=1&amp;amp;67.15=1&amp;amp;67.23=open&amp;amp;69.70=blocks&amp;amp;69.71=5&amp;amp;69.72=0&amp;amp;69.73=1&amp;amp;69.74=0&amp;amp;69.17=negative&amp;amp;69.18=left&amp;amp;69.19=top&amp;amp;69.20=front&amp;amp;69.23=closed&amp;amp;75.39=5.28&amp;amp;75.40=true&amp;amp;75.41=2&amp;amp;75.42=1&amp;amp;75.43=0.5&amp;amp;75.44=0.5&amp;amp;75.45=10&amp;amp;75.46=false&amp;amp;75.47=false&amp;amp;75.23=closed&amp;amp;76.49=semilocal&amp;amp;76.50=0.3&amp;amp;76.51=0.4&amp;amp;76.52=0.6&amp;amp;76.53=1.5&amp;amp;76.54=0.77&amp;amp;76.55=0.74&amp;amp;76.56=0.04&amp;amp;76.23=closed&amp;amp;77.8=&amp;amp;78.59=-174.76129648411032&amp;amp;78.60=141.54502619212317&amp;amp;78.61=170.2709730709386&amp;amp;79.59=-14.817097084822203&amp;amp;79.60=-9.723209466639396&amp;amp;79.61=-5.4699873376955646&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;folder=23&amp;amp;left.left.left.left=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.left.block=28&amp;amp;left.left.right=29&amp;amp;left.left.anim=30&amp;amp;fuse=31&amp;amp;speed=32&amp;amp;hide%20inputs=33&amp;amp;spin=34&amp;amp;left.left.block=35&amp;amp;i%20blocks=36&amp;amp;left.left.layout=37&amp;amp;left.left.deco=38&amp;amp;legends=39&amp;amp;shape=40&amp;amp;spotlight=41&amp;amp;row%20guides=42&amp;amp;flow%20guides=43&amp;amp;lens%20size=44&amp;amp;magnification=45&amp;amp;interior%20spotlight=46&amp;amp;axes=47&amp;amp;left.left.viz=48&amp;amp;sensitivity=49&amp;amp;min%20size=50&amp;amp;min%20light=51&amp;amp;max%20light=52&amp;amp;elem%20scale=53&amp;amp;zero%20hue=54&amp;amp;hue%20gap=55&amp;amp;hue%20spread=56&amp;amp;left.left.diag=57&amp;amp;left.left.cam=58&amp;amp;x=59&amp;amp;y=60&amp;amp;z=61&amp;amp;left.left.cam.target=62&amp;amp;compress=63&amp;amp;left.right=64&amp;amp;right=65&amp;amp;anim=66&amp;amp;block=67&amp;amp;j%20blocks=68&amp;amp;layout=69&amp;amp;scheme=70&amp;amp;gap=71&amp;amp;scatter=72&amp;amp;molecule=73&amp;amp;blast=74&amp;amp;deco=75&amp;amp;viz=76&amp;amp;diag=77&amp;amp;cam=78&amp;amp;cam.target=79&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/bottleneck_part.jpg&quot; alt=&quot;the canonical &amp;quot;data parallel&amp;quot; partitioning to the left-associative multilayer bottleneck example&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second example would (for me, anyway) be much harder to build intuition about without clear geometry to support it: it shows how a binary expression can be parallelized by partitioning the left subexpression along its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; axis, the right subexpression along its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; axis, and the parent expression along its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; axis (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=A%20%40%20B%20%40%20(C%20%40%20D)&amp;amp;1=A%20%40%20B%20%40%20(C%20%40%20D)&amp;amp;2=none&amp;amp;12=closed&amp;amp;69=true&amp;amp;3.1=A%20%40%20B&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.12=open&amp;amp;3.2=none&amp;amp;13.1=A&amp;amp;13.4=false&amp;amp;13.5=64&amp;amp;13.6=64&amp;amp;13.7=expr&amp;amp;13.8=&amp;amp;13.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.12=closed&amp;amp;14.1=B&amp;amp;14.4=false&amp;amp;14.5=64&amp;amp;14.6=64&amp;amp;14.7=row%20major&amp;amp;14.8=&amp;amp;14.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;14.9=-1&amp;amp;14.10=1&amp;amp;14.11=0&amp;amp;14.12=closed&amp;amp;15.16=inherit&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.23=1&amp;amp;24.1=C%20%40%20D&amp;amp;24.4=true&amp;amp;24.5=32&amp;amp;24.6=32&amp;amp;24.7=col%20major&amp;amp;24.8=&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.2=none&amp;amp;25.16=inherit&amp;amp;26.23=1&amp;amp;27.18=positive&amp;amp;27.19=right&amp;amp;27.20=top&amp;amp;27.21=back&amp;amp;28.1=C&amp;amp;28.4=false&amp;amp;28.5=64&amp;amp;28.6=64&amp;amp;28.7=col%20major&amp;amp;28.8=&amp;amp;28.9=-1&amp;amp;28.10=1&amp;amp;28.11=0&amp;amp;28.0=-2%20*%20(Math.trunc(i%20%2F%208)%20%25%202)%20%2B%201&amp;amp;28.12=open&amp;amp;29.1=D&amp;amp;29.4=false&amp;amp;29.5=64&amp;amp;29.6=64&amp;amp;29.7=expr&amp;amp;29.8=&amp;amp;29.9=-1&amp;amp;29.10=1&amp;amp;29.11=0&amp;amp;29.0=-2%20*%20(Math.trunc(j%20%2F%208)%20%25%202)%20%2B%201&amp;amp;29.12=open&amp;amp;30.31=none&amp;amp;30.32=12&amp;amp;30.33=false&amp;amp;30.16=none&amp;amp;30.34=0&amp;amp;30.12=closed&amp;amp;35.36=1&amp;amp;35.37=1&amp;amp;35.23=8&amp;amp;35.12=open&amp;amp;38.39=blocks&amp;amp;38.40=5&amp;amp;38.41=0&amp;amp;38.42=1&amp;amp;38.43=0&amp;amp;38.18=negative&amp;amp;38.19=left&amp;amp;38.20=top&amp;amp;38.21=front&amp;amp;38.12=closed&amp;amp;44.45=6&amp;amp;44.46=true&amp;amp;44.47=2&amp;amp;44.48=1&amp;amp;44.49=0.5&amp;amp;44.50=0.5&amp;amp;44.51=10&amp;amp;44.52=false&amp;amp;44.53=false&amp;amp;44.12=closed&amp;amp;54.55=semilocal&amp;amp;54.56=0.4&amp;amp;54.57=0.4&amp;amp;54.58=0.6&amp;amp;54.59=1.5&amp;amp;54.60=0.77&amp;amp;54.61=0.74&amp;amp;54.62=0.04&amp;amp;54.12=open&amp;amp;63.8=&amp;amp;64.65=-163.0431410622342&amp;amp;64.66=153.55767080483412&amp;amp;64.67=176.87418575632128&amp;amp;68.65=-4.044017278625395&amp;amp;68.66=-2.123834827920271&amp;amp;68.67=-2.551083969824457&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;left.left=13&amp;amp;left.right=14&amp;amp;left.anim=15&amp;amp;alg=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.block=22&amp;amp;k%20blocks=23&amp;amp;right=24&amp;amp;right.anim=25&amp;amp;right.block=26&amp;amp;right.layout=27&amp;amp;right.left=28&amp;amp;right.right=29&amp;amp;anim=30&amp;amp;fuse=31&amp;amp;speed=32&amp;amp;hide%20inputs=33&amp;amp;spin=34&amp;amp;block=35&amp;amp;i%20blocks=36&amp;amp;j%20blocks=37&amp;amp;layout=38&amp;amp;scheme=39&amp;amp;gap=40&amp;amp;scatter=41&amp;amp;molecule=42&amp;amp;blast=43&amp;amp;deco=44&amp;amp;legends=45&amp;amp;shape=46&amp;amp;spotlight=47&amp;amp;row%20guides=48&amp;amp;flow%20guides=49&amp;amp;lens%20size=50&amp;amp;magnification=51&amp;amp;interior%20spotlight=52&amp;amp;axes=53&amp;amp;viz=54&amp;amp;sensitivity=55&amp;amp;min%20size=56&amp;amp;min%20light=57&amp;amp;max%20light=58&amp;amp;elem%20scale=59&amp;amp;zero%20hue=60&amp;amp;hue%20gap=61&amp;amp;hue%20spread=62&amp;amp;diag=63&amp;amp;cam=64&amp;amp;x=65&amp;amp;y=66&amp;amp;z=67&amp;amp;cam.target=68&amp;amp;compress=69&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/binary_part.jpg&quot; alt=&quot;a binary expression can be parallelized by partitioning the left subexpression along its j axis, the right subexpression along its i axis, and the parent expression along its k axis&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-inside-an-attention-head&quot;&gt;4 Inside an Attention Head&lt;/h2&gt;

&lt;p&gt;Let’s look at a GPT2 attention head - specifically layer 5, head 4 of the “gpt2” (small) configuration (layers=12, heads=12, embed=768) from &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;NanoGPT&lt;/a&gt;, using OpenAI weights via HuggingFace. Input activations are taken from a forward pass on an OpenWebText training sample of 256 tokens.&lt;/p&gt;

&lt;p&gt;There’s nothing particularly unusual about this particular head; I chose it mainly because it computes a fairly common attention pattern and lives in the middle of the model, where activations have become structured and show some interesting texture. (Aside: in a subsequent note I’ll present an attention head explorer that lets you visualize all layers and heads of this model, along with some travel notes.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=10&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;53.49=open&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0.655&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=closed&amp;amp;78.8=&amp;amp;78.49=closed&amp;amp;79.80=-1149.3128801149742&amp;amp;79.81=1143.004532598807&amp;amp;79.82=1754.3660479535383&amp;amp;83.80=-6.708919569777563&amp;amp;83.81=75.05036284609801&amp;amp;83.82=-216.66743330111652&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;Open in mm&lt;/a&gt; (may take a few seconds to fetch model weights)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/mha1.jpg&quot; alt=&quot;There's nothing particularly unusual about this particular head&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4a-structure&quot;&gt;4a Structure&lt;/h3&gt;

&lt;p&gt;The entire attention head is visualized as a single compound expression, starting with input and ending with projected output. (Note: to keep things self-contained we do per-head output projection as described in &lt;a href=&quot;https://arxiv.org/pdf/1909.08053.pdf&quot;&gt;Megatron-LM&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The computation contains six matmuls:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Q = input @ wQ        // 1
K_t = wK_t @ input_t  // 2
V = input @ wV        // 3
attn = sdpa(Q @ K_t)  // 4
head_out = attn @ V   // 5
out = head_out @ wO   // 6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A thumbnail description of what we’re looking at:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the blades of the windmill are matmuls 1, 2, 3 and 6: the former group are the in-projections from input to Q, K and V; the latter is the out-projection from attn @ V back to the embedding dimension.&lt;/li&gt;
  &lt;li&gt;at the hub is the double matmul that first calculates attention scores (convex cube in back), then uses them to produce output tokens from the values vector (concave cube in front). Causality means that the attention scores form a lower triangle.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But I’d encourage &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=closed&amp;amp;69.70=local&amp;amp;69.71=0&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=-1212.5184472916683&amp;amp;79.81=1205.8631771144878&amp;amp;79.82=1850.8460431010271&amp;amp;83.80=-6.708919569777563&amp;amp;83.81=75.05036284609801&amp;amp;83.82=-216.66743330111652&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;exploring this example in the tool itself&lt;/a&gt;, rather than relying on the screenshot or the video below to convey just how much signal can be absorbed from it - both about its structure and the actual values flowing through the computation.&lt;/p&gt;

&lt;h3 id=&quot;4b-computation-and-values&quot;&gt;4b Computation and Values&lt;/h3&gt;

&lt;p&gt;Here’s an animation of the attention head computation. Specifically, we’re watching&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sdpa(input @ wQ @ K_t) @ V @ wO
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(i.e., matmuls 1, 4 , 5 and 6 above, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K_t&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; precomputed) being computed as a fused chain of vector-matrix products: each item in the sequence goes all the way from input through attention to output in one step. More on this animation choice in the later section on parallelization, but first let’s look at what the values being computed tell us.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_5_4&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_5&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_5&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_5_4&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_5_4&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=vmprod&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-382.8684269325278&amp;amp;79.81=293.7591554956184&amp;amp;79.82=395.95878922315694&amp;amp;83.80=-14.023727291338966&amp;amp;83.81=-38.22974037070054&amp;amp;83.82=-84.10726407282482&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;Open in mm&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/gpt2_big2b.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;There’s a lot of interesting stuff going on here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Before we even get to the attention calculation, it’s quite striking how low-rank &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K_t&lt;/code&gt; are. &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;24=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;23.24=closed&amp;amp;25.1=wQ_5_4&amp;amp;25.4=false&amp;amp;25.5=768&amp;amp;25.6=64&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;25.0=&amp;amp;26.13=none&amp;amp;26.24=open&amp;amp;27.15=1&amp;amp;28.17=positive&amp;amp;28.18=left&amp;amp;28.19=bottom&amp;amp;28.20=back&amp;amp;22.24=open&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.17=positive&amp;amp;32.18=right&amp;amp;32.19=top&amp;amp;32.20=back&amp;amp;33.1=wK_t_5_4&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t_0_5&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=vmprod&amp;amp;36.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;21.24=closed&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input_0_5&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV_5_4&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.17=negative&amp;amp;43.18=right&amp;amp;43.19=top&amp;amp;43.20=back&amp;amp;3.24=closed&amp;amp;44.1=wO_5_4&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=4&amp;amp;45.48=false&amp;amp;45.13=vmprod&amp;amp;45.49=0&amp;amp;45.24=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.24=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-0.30816774330149777&amp;amp;79.81=333.6054152134701&amp;amp;79.82=155.72856559616935&amp;amp;83.80=-0.11764216999897817&amp;amp;83.81=-38.43510027180947&amp;amp;83.82=-78.52287109278605&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;folder=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;Zooming in on the Q @ K_t vector-matrix product animation&lt;/a&gt;, the situation is even more vivid: a significant number of channels (embedding positions) in &lt;em&gt;both&lt;/em&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; look more or less constant across the sequence, implying that the useful attention signal is potentially driven by a only smallish subset of the embedding. Understanding and exploiting this phenomenon is one of the threads we’re pulling on as part of the SysML ATOM transformer efficiency project.&lt;/li&gt;
  &lt;li&gt;Perhaps most familiar is the strong-but-not-perfect diagonal that emerges in the attention matrix. This is a common pattern, showing up in many of the attention heads of this model (and those of many transformers). It produces &lt;em&gt;localized&lt;/em&gt; attention: the value tokens in the small neighborhood immediately preceding an output token’s position largely determine that output token’s content pattern.&lt;/li&gt;
  &lt;li&gt;However, the size of this neighborhood and the influence of individual tokens within it vary nontrivially - this can be seen both in the off-diagonal frost in the attention grid, and in the &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;26=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;25.26=open&amp;amp;27.15=1&amp;amp;28.17=positive&amp;amp;28.18=left&amp;amp;28.19=bottom&amp;amp;28.20=back&amp;amp;22.26=closed&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.17=positive&amp;amp;32.18=right&amp;amp;32.19=top&amp;amp;32.20=back&amp;amp;33.1=wK_t_5_4&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t_0_5&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=none&amp;amp;35.26=open&amp;amp;36.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;21.26=open&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input_0_5&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV_5_4&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.17=negative&amp;amp;43.18=right&amp;amp;43.19=top&amp;amp;43.20=back&amp;amp;3.26=open&amp;amp;44.1=wO_5_4&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=vmprod&amp;amp;45.49=0&amp;amp;45.26=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.26=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-12.838747258760423&amp;amp;79.81=224.62765397316576&amp;amp;79.82=274.71626756027933&amp;amp;83.80=-13.049253781233714&amp;amp;83.81=-55.16215322834755&amp;amp;83.82=-70.26525235295296&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;folder=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;fluctuating patterns of the attn[i] @ V vector-matrix product plane&lt;/a&gt; as it descends the attention matrix on its way through the sequence.&lt;/li&gt;
  &lt;li&gt;But note that the local neighborhood isn’t the only thing that’s attracting attention: the leftmost column of the attention grid, corresponding to the first token of the sequence, is entirely filled with nonzero (but fluctuating) values, meaning every output token will be influenced to some degree by the first value token.&lt;/li&gt;
  &lt;li&gt;Moreover there’s an &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_5_4&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_5&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_5&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_5_4&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_5_4&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-328.8286059935543&amp;amp;79.81=-64.64788859858083&amp;amp;79.82=156.66189435044396&amp;amp;83.80=-6.5479856531724625&amp;amp;83.81=-27.630477427688977&amp;amp;83.82=-64.70186279804427&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;inexact but discernible oscillation in attention score dominance&lt;/a&gt; between the current token neighborhood and the initial token. The period of the oscillation varies, but broadly speaking starts short and then lengthens as one travels down the sequence (evocatively correlated with the quantity of candidate attention tokens for each row, given causality).&lt;/li&gt;
  &lt;li&gt;To get a feel for how (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V)&lt;/code&gt; is formed, it’s important not to focus on attention in isolation - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; is an equal player. Each output item is a weighted average of the entire &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; vector: at the limit when attention is a perfect diagonal, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; is simply an exact copy of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;. Here we see &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out+%3D+%28attn+%3D+%28Q+%3D+input_0_5+%40+wQ_5_4%29+%40+%28K_t+%3D+wK_t_5_4+%40+input_t_0_5%29%29+%40+%28V+%3D+input_0_5+%40+wV_5_4%29+%40+wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;51=closed&amp;amp;84=true&amp;amp;3.1=attn+%40+V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row+major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;14.16=1&amp;amp;14.17=1&amp;amp;18.19=positive&amp;amp;18.20=left&amp;amp;18.21=bottom&amp;amp;18.22=back&amp;amp;23.1=attn&amp;amp;23.4=true&amp;amp;23.2=softmax%28tril%28x%2Fsqrt%28k%29%29%29&amp;amp;24.1=Q&amp;amp;24.4=true&amp;amp;24.2=none&amp;amp;25.1=input_0_5&amp;amp;25.4=false&amp;amp;25.5=256&amp;amp;25.6=768&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;25.0=&amp;amp;26.1=wQ_5_4&amp;amp;26.4=false&amp;amp;26.5=768&amp;amp;26.6=64&amp;amp;26.7=url&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;26.0=&amp;amp;27.13=vmprod&amp;amp;28.15=1&amp;amp;28.16=1&amp;amp;28.17=1&amp;amp;29.19=positive&amp;amp;29.20=left&amp;amp;29.21=bottom&amp;amp;29.22=back&amp;amp;30.2=none&amp;amp;31.13=none&amp;amp;32.15=1&amp;amp;32.16=1&amp;amp;32.17=1&amp;amp;33.19=positive&amp;amp;33.20=right&amp;amp;33.21=top&amp;amp;33.22=back&amp;amp;34.1=wK_t_5_4&amp;amp;34.4=false&amp;amp;34.5=64&amp;amp;34.6=768&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;35.1=input_t_0_5&amp;amp;35.4=false&amp;amp;35.5=768&amp;amp;35.6=256&amp;amp;35.7=url&amp;amp;35.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;35.9=-1&amp;amp;35.10=1&amp;amp;35.11=0&amp;amp;35.0=&amp;amp;30.1=K_t&amp;amp;30.4=true&amp;amp;36.13=vmprod&amp;amp;37.15=1&amp;amp;37.16=1&amp;amp;37.17=1&amp;amp;38.19=negative&amp;amp;38.20=left&amp;amp;38.21=top&amp;amp;38.22=front&amp;amp;39.1=V&amp;amp;39.4=true&amp;amp;39.2=none&amp;amp;40.1=input_0_5&amp;amp;40.4=false&amp;amp;40.5=256&amp;amp;40.6=768&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;40.0=&amp;amp;41.1=wV_5_4&amp;amp;41.4=false&amp;amp;41.5=768&amp;amp;41.6=64&amp;amp;41.7=url&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;41.0=&amp;amp;42.13=none&amp;amp;43.15=1&amp;amp;43.16=1&amp;amp;43.17=1&amp;amp;44.19=negative&amp;amp;44.20=right&amp;amp;44.21=top&amp;amp;44.22=back&amp;amp;45.1=wO_5_4&amp;amp;45.4=false&amp;amp;45.5=64&amp;amp;45.6=768&amp;amp;45.7=url&amp;amp;45.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;45.9=-1&amp;amp;45.10=1&amp;amp;45.11=0&amp;amp;45.0=&amp;amp;46.47=sync&amp;amp;46.48=16&amp;amp;46.49=false&amp;amp;46.13=none&amp;amp;46.50=0&amp;amp;46.51=open&amp;amp;52.16=1&amp;amp;52.15=1&amp;amp;52.17=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.19=negative&amp;amp;53.20=left&amp;amp;53.21=top&amp;amp;53.22=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=20&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.51=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-164.2339403949366&amp;amp;79.81=18.940074323234473&amp;amp;79.82=173.55325640245638&amp;amp;83.80=117.76774477612946&amp;amp;83.81=2.623526996843087&amp;amp;83.82=53.25986191913323&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k+blocks=15&amp;amp;i+blocks=16&amp;amp;j+blocks=17&amp;amp;left.layout=18&amp;amp;polarity=19&amp;amp;left+placement=20&amp;amp;right+placement=21&amp;amp;result+placement=22&amp;amp;left.left=23&amp;amp;left.left.left=24&amp;amp;left.left.left.left=25&amp;amp;left.left.left.right=26&amp;amp;left.left.left.anim=27&amp;amp;left.left.left.block=28&amp;amp;left.left.left.layout=29&amp;amp;left.left.right=30&amp;amp;left.left.right.anim=31&amp;amp;left.left.right.block=32&amp;amp;left.left.right.layout=33&amp;amp;left.left.right.left=34&amp;amp;left.left.right.right=35&amp;amp;left.left.anim=36&amp;amp;left.left.block=37&amp;amp;left.left.layout=38&amp;amp;left.right=39&amp;amp;left.right.left=40&amp;amp;left.right.right=41&amp;amp;left.right.anim=42&amp;amp;left.right.block=43&amp;amp;left.right.layout=44&amp;amp;right=45&amp;amp;anim=46&amp;amp;fuse=47&amp;amp;speed=48&amp;amp;hide+inputs=49&amp;amp;spin=50&amp;amp;folder=51&amp;amp;block=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row+guides=63&amp;amp;flow+guides=64&amp;amp;lens+size=65&amp;amp;magnification=66&amp;amp;interior+spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min+size=71&amp;amp;min+light=72&amp;amp;max+light=73&amp;amp;elem+scale=74&amp;amp;zero+hue=75&amp;amp;hue+gap=76&amp;amp;hue+spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;something more textured&lt;/a&gt;: visible banding where particular tokens have scored high over a contiguous subsequence of attention rows, superimposed on a matrix visibly similar to to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; but with some vertical smearing due to the fat diagonal. (Aside: per the &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;mm reference guide&lt;/a&gt;, long-clicking or control-clicking will reveal the actual numeric values of visualized elements.)&lt;/li&gt;
  &lt;li&gt;Bear in mind that since we’re in a middle layer (5), the input to this attention head is an intermediate representation, not the original tokenized text. So the &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out+%3D+%28attn+%3D+%28Q+%3D+input+%40+wQ%29+%40+%28K_t+%3D+wK_t+%40+input_t%29%29+%40+%28V+%3D+input+%40+wV%29+%40+wO&amp;amp;1=out&amp;amp;2=none&amp;amp;26=closed&amp;amp;84=true&amp;amp;3.1=attn+%40+V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row+major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;14.16=1&amp;amp;14.17=1&amp;amp;18.19=positive&amp;amp;18.20=left&amp;amp;18.21=bottom&amp;amp;18.22=back&amp;amp;23.1=attn&amp;amp;23.4=true&amp;amp;23.2=softmax%28tril%28x%2Fsqrt%28k%29%29%29&amp;amp;24.1=Q&amp;amp;24.4=true&amp;amp;24.2=none&amp;amp;25.1=input&amp;amp;25.4=false&amp;amp;25.5=256&amp;amp;25.6=768&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;25.0=&amp;amp;25.26=open&amp;amp;27.1=wQ&amp;amp;27.4=false&amp;amp;27.5=768&amp;amp;27.6=64&amp;amp;27.7=url&amp;amp;27.9=-1&amp;amp;27.10=1&amp;amp;27.11=0&amp;amp;27.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;27.0=&amp;amp;28.13=vmprod&amp;amp;29.15=1&amp;amp;29.16=1&amp;amp;29.17=1&amp;amp;30.19=positive&amp;amp;30.20=left&amp;amp;30.21=bottom&amp;amp;30.22=back&amp;amp;24.26=open&amp;amp;31.2=none&amp;amp;32.13=none&amp;amp;33.15=1&amp;amp;33.16=1&amp;amp;33.17=1&amp;amp;34.19=positive&amp;amp;34.20=right&amp;amp;34.21=top&amp;amp;34.22=back&amp;amp;35.1=wK_t&amp;amp;35.4=false&amp;amp;35.5=64&amp;amp;35.6=768&amp;amp;35.7=url&amp;amp;35.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;35.9=-1&amp;amp;35.10=1&amp;amp;35.11=0&amp;amp;35.0=&amp;amp;36.1=input_t&amp;amp;36.4=false&amp;amp;36.5=768&amp;amp;36.6=256&amp;amp;36.7=url&amp;amp;36.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;36.9=-1&amp;amp;36.10=1&amp;amp;36.11=0&amp;amp;36.0=&amp;amp;31.1=K_t&amp;amp;31.4=true&amp;amp;37.13=vmprod&amp;amp;38.15=1&amp;amp;38.16=1&amp;amp;38.17=1&amp;amp;39.19=negative&amp;amp;39.20=left&amp;amp;39.21=top&amp;amp;39.22=front&amp;amp;23.26=open&amp;amp;40.1=V&amp;amp;40.4=true&amp;amp;40.2=none&amp;amp;41.1=input&amp;amp;41.4=false&amp;amp;41.5=256&amp;amp;41.6=768&amp;amp;41.7=url&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;41.0=&amp;amp;42.1=wV&amp;amp;42.4=false&amp;amp;42.5=768&amp;amp;42.6=64&amp;amp;42.7=url&amp;amp;42.9=-1&amp;amp;42.10=1&amp;amp;42.11=0&amp;amp;42.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;42.0=&amp;amp;43.13=none&amp;amp;44.15=1&amp;amp;44.16=1&amp;amp;44.17=1&amp;amp;45.19=negative&amp;amp;45.20=right&amp;amp;45.21=top&amp;amp;45.22=back&amp;amp;3.26=open&amp;amp;46.1=wO&amp;amp;46.4=false&amp;amp;46.5=64&amp;amp;46.6=768&amp;amp;46.7=url&amp;amp;46.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;46.9=-1&amp;amp;46.10=1&amp;amp;46.11=0&amp;amp;46.0=&amp;amp;47.48=sync&amp;amp;47.49=16&amp;amp;47.50=false&amp;amp;47.13=none&amp;amp;47.51=0&amp;amp;47.26=open&amp;amp;52.16=1&amp;amp;52.15=1&amp;amp;52.17=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.19=negative&amp;amp;53.20=left&amp;amp;53.21=top&amp;amp;53.22=front&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=5&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.25&amp;amp;59.66=4.632&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.26=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.4&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.26=open&amp;amp;78.8=&amp;amp;78.26=open&amp;amp;79.80=-1126.8641673236093&amp;amp;79.81=-4.707283693510895&amp;amp;79.82=168.0669807860928&amp;amp;83.80=-692.9006907132649&amp;amp;83.81=4.068470706235418&amp;amp;83.82=-171.27561837707958&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k+blocks=15&amp;amp;i+blocks=16&amp;amp;j+blocks=17&amp;amp;left.layout=18&amp;amp;polarity=19&amp;amp;left+placement=20&amp;amp;right+placement=21&amp;amp;result+placement=22&amp;amp;left.left=23&amp;amp;left.left.left=24&amp;amp;left.left.left.left=25&amp;amp;folder=26&amp;amp;left.left.left.right=27&amp;amp;left.left.left.anim=28&amp;amp;left.left.left.block=29&amp;amp;left.left.left.layout=30&amp;amp;left.left.right=31&amp;amp;left.left.right.anim=32&amp;amp;left.left.right.block=33&amp;amp;left.left.right.layout=34&amp;amp;left.left.right.left=35&amp;amp;left.left.right.right=36&amp;amp;left.left.anim=37&amp;amp;left.left.block=38&amp;amp;left.left.layout=39&amp;amp;left.right=40&amp;amp;left.right.left=41&amp;amp;left.right.right=42&amp;amp;left.right.anim=43&amp;amp;left.right.block=44&amp;amp;left.right.layout=45&amp;amp;right=46&amp;amp;anim=47&amp;amp;fuse=48&amp;amp;speed=49&amp;amp;hide+inputs=50&amp;amp;spin=51&amp;amp;block=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row+guides=63&amp;amp;flow+guides=64&amp;amp;lens+size=65&amp;amp;magnification=66&amp;amp;interior+spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min+size=71&amp;amp;min+light=72&amp;amp;max+light=73&amp;amp;elem+scale=74&amp;amp;zero+hue=75&amp;amp;hue+gap=76&amp;amp;hue+spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;patterns seen in the input&lt;/a&gt; are themselves thought-provoking - in particular, the strong vertical threads are particular embedding positions whose values are uniformly high magnitude across long stretches of the sequence - sometimes almost the entire thing.&lt;/li&gt;
  &lt;li&gt;Interestingly, though, the &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO&amp;amp;1=out&amp;amp;2=none&amp;amp;24=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;23.24=open&amp;amp;25.1=wQ&amp;amp;25.4=false&amp;amp;25.5=768&amp;amp;25.6=64&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;25.0=&amp;amp;26.13=vmprod&amp;amp;27.15=1&amp;amp;28.17=positive&amp;amp;28.18=left&amp;amp;28.19=bottom&amp;amp;28.20=back&amp;amp;22.24=open&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.17=positive&amp;amp;32.18=right&amp;amp;32.19=top&amp;amp;32.20=back&amp;amp;33.1=wK_t&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=vmprod&amp;amp;36.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;21.24=open&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.17=negative&amp;amp;43.18=right&amp;amp;43.19=top&amp;amp;43.20=back&amp;amp;3.24=open&amp;amp;44.1=wO&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=none&amp;amp;45.49=0&amp;amp;45.24=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=5&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0&amp;amp;59.66=4.632&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.24=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.4&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.24=open&amp;amp;78.8=&amp;amp;78.24=open&amp;amp;79.80=-905.2149526505231&amp;amp;79.81=126.10717525695773&amp;amp;79.82=-90.1644865901155&amp;amp;83.80=-739.2766627330938&amp;amp;83.81=125.47333863007341&amp;amp;83.82=-229.39828071999955&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;folder=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;first vector in the input sequence is distinctive&lt;/a&gt;, not only breaking the pattern of these high-magnitude columns but carrying atypical values at almost every position (aside: not visualized here, but this pattern is repeated over multiple sample inputs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: apropos of the last two bullet points, it’s worth reiterating that we’re visualizing computation over a &lt;em&gt;single sample input&lt;/em&gt;. In practice I’ve found that each head has a characteristic pattern it will express consistently (though not identically) over a decent collection of samples (and the upcoming attention head browser will provide a collection of samples to play with), but when looking at any visualization that includes activations, it’s important to bear in mind that a full distribution of inputs may influence the ideas and intuitions it provokes it in subtle ways.&lt;/p&gt;

&lt;p&gt;Finally, one more pitch to &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_5_4&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_5&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_5&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_5_4&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_5_4&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=vmprod&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=8.38&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-382.8684269325278&amp;amp;79.81=293.7591554956184&amp;amp;79.82=395.95878922315694&amp;amp;83.80=-14.023727291338966&amp;amp;83.81=-38.22974037070054&amp;amp;83.82=-84.10726407282482&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;explore the animation directly&lt;/a&gt;!&lt;/p&gt;

&lt;h3 id=&quot;4c-heads-are-different-in-interesting-ways&quot;&gt;4c Heads are different in interesting ways&lt;/h3&gt;

&lt;p&gt;Before we move on, here’s one more demonstration of the usefulness of simply poking around a model to see how it works in detail.&lt;/p&gt;

&lt;p&gt;This is another attention head from GPT2. It behaves quite differently from layer 5, head 4 above - as one might expect, given that it’s in a very different part of the model. This head is in the very first layer: layer 0, head 2 (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=-217.09372134188362&amp;amp;79.81=412.82010718887307&amp;amp;79.82=523.3596617096426&amp;amp;83.80=127.59196458710655&amp;amp;83.81=35.32022663933653&amp;amp;83.82=87.43354119148215&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;, may take a few seconds to load model weights):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/gpt2_0_2c.jpg&quot; alt=&quot;This is another attention head from GPT2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Things to note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This head spreads attention very evenly. This has the effect of delivering a relatively &lt;em&gt;unweighted&lt;/em&gt; average of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; (or rather, the appropriate causal prefix of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;) to each row in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt;, as can be seen in &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=vmprod&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=closed&amp;amp;69.70=local&amp;amp;69.71=0.4&amp;amp;69.72=0.4&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=closed&amp;amp;79.80=11.34872888812131&amp;amp;79.81=324.07536950158396&amp;amp;79.82=239.8893041928473&amp;amp;83.80=11.804686909150822&amp;amp;83.81=25.33948904441141&amp;amp;83.82=46.896270190786204&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;this animation&lt;/a&gt;: as we move down the attention score triangle, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn[i] @ V&lt;/code&gt; vector-matrix product is small fluctuations away from being simply a downscaled, progressively revealed copy of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; has &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=-152.24249732960024&amp;amp;79.81=115.78244265148294&amp;amp;79.82=89.29496035154&amp;amp;83.80=20.231661185991296&amp;amp;83.81=61.75722293832386&amp;amp;83.82=52.45120329048098&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;striking vertical uniformity&lt;/a&gt; - in large columnar regions of the embedding, the same value patterns persist over &lt;em&gt;the entire sequence&lt;/em&gt;. One can think of these as properties shared by every token.&lt;/li&gt;
  &lt;li&gt;Aside: on the one hand one might expect &lt;em&gt;some&lt;/em&gt; uniformity in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; given the effect of very evenly spread attention. But each row has been constructed from only a causal subsequence of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; rather than the whole thing - why is that not causing more variation, like a progressive morphing as one moves down the sequence? &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=8.296178745251016&amp;amp;79.81=-533.8678069620822&amp;amp;79.82=35.64126972299759&amp;amp;83.80=8.29674894856322&amp;amp;83.81=36.25749961529174&amp;amp;83.82=35.64126624185369&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;By visual inspection V isn’t uniform along its length&lt;/a&gt;, so the answer must lie in some more subtle property of its distribution of values.&lt;/li&gt;
  &lt;li&gt;Finally, this head’s output is &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=41.37507219272118&amp;amp;79.81=4.367136718959145&amp;amp;79.82=430.5595129727994&amp;amp;83.80=607.5332301692057&amp;amp;83.81=-2.548000389888877&amp;amp;83.82=-122.74351758382484&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;even more vertically uniform after out-projection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;the strong impression being that the bulk of the information being delivered by this attention head consists of properties which are shared by every token in the sequence. The composition of its &lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_0%20%40%20wQ_0_2)%20%40%20(K_t%20%3D%20wK_t_0_2%20%40%20input_t_0_0))%20%40%20(V%20%3D%20input_0_0%20%40%20wV_0_2)%20%40%20wO_0_2&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_0&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_0_2&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq2_768_64.csv&amp;amp;24.0=&amp;amp;25.13=none&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_0_2&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t2_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_0&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=none&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_0&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_0_2&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv2_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_0_2&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo2_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=6&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.73&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=open&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=136.19712021298585&amp;amp;79.81=351.47497630691043&amp;amp;79.82=254.9066405965837&amp;amp;83.80=554.2569175811409&amp;amp;83.81=-39.63963114876448&amp;amp;83.82=-109.72659308933949&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;output projection weights&lt;/a&gt; reinforces this intuition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overall, it’s hard to resist the idea that the extremely regular, highly structured information this attention head produces might be obtained by computational means that are a bit… less lavish. Of course this isn’t an unexplored area, but the specificity and richness of signal of the visualized computation has been useful in generating new ideas, and reasoning about existing ones.&lt;/p&gt;

&lt;h3 id=&quot;4d-revisiting-the-pitch-invariants-for-free&quot;&gt;4d Revisiting the pitch: invariants for free&lt;/h3&gt;

&lt;p&gt;Stepping back, it’s worth reiterating that the reason we can visualize nontrivially compound operations like attention heads and have them remain intuitive is that important algebraic properties - like how argument shapes are constrained, or which parallelization axes intersect which operations - &lt;em&gt;don’t require additional thinking&lt;/em&gt;: they arise directly from the geometry of the visualized object, rather than being additional rules to keep in mind.&lt;/p&gt;

&lt;p&gt;For example, in these attention head visualizations it’s immediately obvious that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; are the same length, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; are the same length, and the lengths of these pairs are independent of each other&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; are the same width, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; are the same width, and the widths of these pairs are independent of each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These properties are true by construction, as a simple consequence of which parts of the compound structure the constituents inhabit and how they are oriented.&lt;/p&gt;

&lt;p&gt;This “properties for free” benefit can be especially useful when exploring variations on a canonical structure - an obvious example being the one-row-high attention matrix in autoregressive token-at-a-time decoding (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;24=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(x%2Fsqrt(k))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=1&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;23.24=open&amp;amp;25.1=wQ_5_4&amp;amp;25.4=false&amp;amp;25.5=768&amp;amp;25.6=64&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;25.0=&amp;amp;26.13=vmprod&amp;amp;27.15=1&amp;amp;28.17=positive&amp;amp;28.18=left&amp;amp;28.19=bottom&amp;amp;28.20=back&amp;amp;22.24=open&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.17=positive&amp;amp;32.18=right&amp;amp;32.19=top&amp;amp;32.20=back&amp;amp;33.1=wK_t_5_4&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t_0_5&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=vmprod&amp;amp;36.15=1&amp;amp;37.17=negative&amp;amp;37.18=left&amp;amp;37.19=top&amp;amp;37.20=front&amp;amp;21.24=open&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input_0_5&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV_5_4&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.17=negative&amp;amp;43.18=right&amp;amp;43.19=top&amp;amp;43.20=back&amp;amp;3.24=closed&amp;amp;44.1=wO_5_4&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=none&amp;amp;45.49=0&amp;amp;45.24=open&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;53.54=blocks&amp;amp;53.55=24&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.394&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.24=open&amp;amp;69.70=local&amp;amp;69.71=0.05&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;78.8=&amp;amp;79.80=-289.3020871171715&amp;amp;79.81=176.55051931108687&amp;amp;79.82=202.12550566094345&amp;amp;83.80=6.09420901744693&amp;amp;83.81=-60.94451681776672&amp;amp;83.82=-55.94371166611936&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;folder=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/gpt2_decode2.jpg&quot; alt=&quot;the one-row-high attention matrix in autoregressive token-at-a-time decoding&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-parallelizing-attention&quot;&gt;5 Parallelizing attention&lt;/h2&gt;

&lt;p&gt;In the animation of head 5, layer 4 above, we visualize 4 of the 6 matmuls in the attention head&lt;/p&gt;

&lt;p&gt;as a fused chain of vector-matrix products, confirming the geometric intuition that the entire left-associative chain from input to output is &lt;em&gt;laminar&lt;/em&gt; along the shared &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; axis, and can be parallelized.&lt;/p&gt;

&lt;h3 id=&quot;5a-example-partitioning-along-i&quot;&gt;5a Example: partitioning along &lt;code&gt;i&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;To parallelize the computation in practice, we would partition the input into blocks along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; axis. We can visualize this partition in the tool, by specifying that a given axis be partitioned into a particular number of blocks - in these examples we’ll use 8, but there’s nothing special about that number.&lt;/p&gt;

&lt;p&gt;Among other things, this visualization makes clear that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wQ&lt;/code&gt; (for in-projection), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K_t&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; (for attention) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wO&lt;/code&gt; (for out-projection) are needed in their entirety by each parallel computation, since they’re adjacent to the partitioned matrices along those matrices’ unpartitioned dimensions (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;49=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=1&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input_0_5&amp;amp;23.4=false&amp;amp;23.5=256&amp;amp;23.6=768&amp;amp;23.7=url&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;23.0=&amp;amp;24.1=wQ_5_4&amp;amp;24.4=false&amp;amp;24.5=768&amp;amp;24.6=64&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;24.0=&amp;amp;25.13=vmprod&amp;amp;26.15=1&amp;amp;27.17=positive&amp;amp;27.18=left&amp;amp;27.19=bottom&amp;amp;27.20=back&amp;amp;28.2=none&amp;amp;29.13=none&amp;amp;30.15=1&amp;amp;31.17=positive&amp;amp;31.18=right&amp;amp;31.19=top&amp;amp;31.20=back&amp;amp;32.1=wK_t_5_4&amp;amp;32.4=false&amp;amp;32.5=64&amp;amp;32.6=768&amp;amp;32.7=url&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.0=&amp;amp;33.1=input_t_0_5&amp;amp;33.4=false&amp;amp;33.5=768&amp;amp;33.6=256&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;28.1=K_t&amp;amp;28.4=true&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.17=negative&amp;amp;36.18=left&amp;amp;36.19=top&amp;amp;36.20=front&amp;amp;37.1=V&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=input_0_5&amp;amp;38.4=false&amp;amp;38.5=256&amp;amp;38.6=768&amp;amp;38.7=url&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;38.0=&amp;amp;39.1=wV_5_4&amp;amp;39.4=false&amp;amp;39.5=768&amp;amp;39.6=64&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;39.0=&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;42.17=negative&amp;amp;42.18=right&amp;amp;42.19=top&amp;amp;42.20=back&amp;amp;43.1=wO_5_4&amp;amp;43.4=false&amp;amp;43.5=64&amp;amp;43.6=768&amp;amp;43.7=url&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;44.45=sync&amp;amp;44.46=16&amp;amp;44.47=false&amp;amp;44.13=none&amp;amp;44.48=0&amp;amp;44.49=closed&amp;amp;50.51=8&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;50.49=open&amp;amp;53.54=blocks&amp;amp;53.55=10&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.17=negative&amp;amp;53.18=left&amp;amp;53.19=top&amp;amp;53.20=front&amp;amp;53.49=closed&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.507&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.49=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.3&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.49=closed&amp;amp;78.8=&amp;amp;78.49=open&amp;amp;79.80=-452.09425433307837&amp;amp;79.81=-10.01467989007457&amp;amp;79.82=392.9851223674549&amp;amp;83.80=-27.91725760321879&amp;amp;83.81=-18.858991089590095&amp;amp;83.82=-140.6826497984033&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.anim=25&amp;amp;left.left.left.block=26&amp;amp;left.left.left.layout=27&amp;amp;left.left.right=28&amp;amp;left.left.right.anim=29&amp;amp;left.left.right.block=30&amp;amp;left.left.right.layout=31&amp;amp;left.left.right.left=32&amp;amp;left.left.right.right=33&amp;amp;left.left.anim=34&amp;amp;left.left.block=35&amp;amp;left.left.layout=36&amp;amp;left.right=37&amp;amp;left.right.left=38&amp;amp;left.right.right=39&amp;amp;left.right.anim=40&amp;amp;left.right.block=41&amp;amp;left.right.layout=42&amp;amp;right=43&amp;amp;anim=44&amp;amp;fuse=45&amp;amp;speed=46&amp;amp;hide%20inputs=47&amp;amp;spin=48&amp;amp;folder=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/gpt2_parti.jpg&quot; alt=&quot;wQ (for in-projection), K_t and V (for attention) and wO (for out-projection) are needed in their entirety by each parallel computation&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5b-example-double-partitioning&quot;&gt;5b Example: double partitioning&lt;/h3&gt;

&lt;p&gt;As an example of partitioning along &lt;em&gt;multiple&lt;/em&gt; axes, we can visualize some recent work which innovates in this space (&lt;a href=&quot;https://arxiv.org/pdf/2305.19370.pdf&quot;&gt;Block Parallel Transformer&lt;/a&gt;, building on work done in e.g. &lt;a href=&quot;https://arxiv.org/pdf/2205.14135.pdf&quot;&gt;Flash Attention&lt;/a&gt; and its antecedents).&lt;/p&gt;

&lt;p&gt;First, BPT partitions along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; as described above - and actually extends this horizontal partitioning of the sequence into chunks all the way through the second (FFN) half of the attention layer as well. (We’ll visualize this in a later section.)&lt;/p&gt;

&lt;p&gt;To fully attack the context length problem, a second partitioning is then added to MHA - that of the attention calculation itself (i.e., a partition along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; axis of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q @ K_t&lt;/code&gt;). The two partitions together divide attention into a grid of blocks (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input_0_5%20%40%20wQ_5_4)%20%40%20(K_t%20%3D%20wK_t_5_4%20%40%20input_t_0_5))%20%40%20(V%20%3D%20input_0_5%20%40%20wV_5_4)%20%40%20wO_5_4&amp;amp;1=out&amp;amp;2=none&amp;amp;16=closed&amp;amp;84=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=vmprod&amp;amp;14.15=8&amp;amp;14.16=open&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.1=attn&amp;amp;22.4=true&amp;amp;22.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;23.1=Q&amp;amp;23.4=true&amp;amp;23.2=none&amp;amp;24.1=input_0_5&amp;amp;24.4=false&amp;amp;24.5=256&amp;amp;24.6=768&amp;amp;24.7=url&amp;amp;24.9=-1&amp;amp;24.10=1&amp;amp;24.11=0&amp;amp;24.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;24.0=&amp;amp;25.1=wQ_5_4&amp;amp;25.4=false&amp;amp;25.5=768&amp;amp;25.6=64&amp;amp;25.7=url&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;25.0=&amp;amp;26.13=vmprod&amp;amp;27.15=1&amp;amp;28.18=positive&amp;amp;28.19=left&amp;amp;28.20=bottom&amp;amp;28.21=back&amp;amp;29.2=none&amp;amp;30.13=none&amp;amp;31.15=1&amp;amp;32.18=positive&amp;amp;32.19=right&amp;amp;32.20=top&amp;amp;32.21=back&amp;amp;33.1=wK_t_5_4&amp;amp;33.4=false&amp;amp;33.5=64&amp;amp;33.6=768&amp;amp;33.7=url&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.0=&amp;amp;34.1=input_t_0_5&amp;amp;34.4=false&amp;amp;34.5=768&amp;amp;34.6=256&amp;amp;34.7=url&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.0=&amp;amp;29.1=K_t&amp;amp;29.4=true&amp;amp;35.13=vmprod&amp;amp;36.15=1&amp;amp;37.18=negative&amp;amp;37.19=left&amp;amp;37.20=top&amp;amp;37.21=front&amp;amp;38.1=V&amp;amp;38.4=true&amp;amp;38.2=none&amp;amp;39.1=input_0_5&amp;amp;39.4=false&amp;amp;39.5=256&amp;amp;39.6=768&amp;amp;39.7=url&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;39.0=&amp;amp;40.1=wV_5_4&amp;amp;40.4=false&amp;amp;40.5=768&amp;amp;40.6=64&amp;amp;40.7=url&amp;amp;40.9=-1&amp;amp;40.10=1&amp;amp;40.11=0&amp;amp;40.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;40.0=&amp;amp;41.13=none&amp;amp;42.15=1&amp;amp;43.18=negative&amp;amp;43.19=right&amp;amp;43.20=top&amp;amp;43.21=back&amp;amp;3.16=open&amp;amp;44.1=wO_5_4&amp;amp;44.4=false&amp;amp;44.5=64&amp;amp;44.6=768&amp;amp;44.7=url&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=none&amp;amp;45.49=0&amp;amp;45.16=closed&amp;amp;50.51=8&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;50.16=closed&amp;amp;53.54=blocks&amp;amp;53.55=10&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.18=negative&amp;amp;53.19=left&amp;amp;53.20=top&amp;amp;53.21=front&amp;amp;53.16=closed&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.507&amp;amp;59.64=0&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.16=open&amp;amp;69.70=local&amp;amp;69.71=0.2&amp;amp;69.72=0.3&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.16=closed&amp;amp;78.8=&amp;amp;78.16=open&amp;amp;79.80=-459.733038437248&amp;amp;79.81=-10.183892342609507&amp;amp;79.82=399.6251724834292&amp;amp;83.80=-27.91725760321879&amp;amp;83.81=-18.858991089590095&amp;amp;83.82=-140.6826497984033&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;folder=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.left=22&amp;amp;left.left.left=23&amp;amp;left.left.left.left=24&amp;amp;left.left.left.right=25&amp;amp;left.left.left.anim=26&amp;amp;left.left.left.block=27&amp;amp;left.left.left.layout=28&amp;amp;left.left.right=29&amp;amp;left.left.right.anim=30&amp;amp;left.left.right.block=31&amp;amp;left.left.right.layout=32&amp;amp;left.left.right.left=33&amp;amp;left.left.right.right=34&amp;amp;left.left.anim=35&amp;amp;left.left.block=36&amp;amp;left.left.layout=37&amp;amp;left.right=38&amp;amp;left.right.left=39&amp;amp;left.right.right=40&amp;amp;left.right.anim=41&amp;amp;left.right.block=42&amp;amp;left.right.layout=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/gpt2_ik.jpg&quot; alt=&quot;The two partitions together divide attention into a grid of blocks&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This visualization makes clear&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the effectiveness of this double partitioning as an attack on the context length problem, since we’ve now visibly partitioned every occurrence of sequence length in the attention calculation&lt;/li&gt;
  &lt;li&gt;the “reach” of this second partitioning: it’s clear from the geometry that the in-projection computations of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; can be partitioned along with the core double matmul&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note one subtlety: the visual implication here is that we can also parallelize the subsequent matmul &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; and sum the partial results &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#parallelized-reductions&quot;&gt;split-k style&lt;/a&gt;, thus parallelizing the entire double matmul. But the row-wise softmax in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sdpa()&lt;/code&gt; adds the requirement that each row have all its segments normalized before the corresponding row of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn @ V&lt;/code&gt; can be computed, adding an extra row-wise step between the attention calculation and the final matmul.&lt;/p&gt;

&lt;h2 id=&quot;6-sizes-in-an-attention-layer&quot;&gt;6 Sizes in an Attention Layer&lt;/h2&gt;

&lt;p&gt;The first (MHA) half of an attention layer is famously computationally demanding because of its quadratic complexity, but the second (FFN) half is demanding in its own right due to the width of its hidden dimension, typically 4 times that of the model’s embedding dimension. Visualizing the biomass of a full attention layer can be useful in building intuition about how the two halves of the layer compare to each other.&lt;/p&gt;

&lt;h3 id=&quot;6a-visualizing-the-full-layer&quot;&gt;6a Visualizing the full layer&lt;/h3&gt;

&lt;p&gt;Below is a full attention layer with the first half (MHA) in the background and the second (FFN) in the foreground. As usual, arrows point in the direction of computation.&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This visualization doesn’t depict individual attention heads, but instead shows the unsliced Q/K/V weights and projections surrounding a central double matmul. Of course this isn’t a faithful visualization of the full MHA operation - but the goal here is to give a clearer sense of the relative matrix &lt;em&gt;sizes&lt;/em&gt; in the two halves of the layer, rather than the relative amounts of computation each half performs. (Also, randomized values are used rather than real weights.)&lt;/li&gt;
  &lt;li&gt;The dimensions used here are downsized to keep the browser (relatively) happy, but the proportions are preserved (from &lt;a href=&quot;https://github.com/karpathy/nanoGPT/blob/master/model.py#L217&quot;&gt;NanoGPT’s small config&lt;/a&gt;): model embedding dimension = 192 (from 768), FFN embedding dimension = 768 (from 3072), sequence length = 256 (from 1024), although sequence length is not fundamental to the model. (Visually, changes in sequence length would appear as changes in the width of the input blades, and consequently in the size of the attention hub and the height of the downstream vertical planes.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out%20%3D%20(attn_out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO)%20%40%20FFN_1%20%40%20FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;94=true&amp;amp;3.1=attn_out%20%40%20FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=open&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.2=layernorm&amp;amp;23.13=inherit&amp;amp;24.15=1&amp;amp;24.16=open&amp;amp;25.18=negative&amp;amp;25.19=left&amp;amp;25.20=top&amp;amp;25.21=front&amp;amp;26.1=attn%20%40%20V&amp;amp;26.4=true&amp;amp;26.5=32&amp;amp;26.6=32&amp;amp;26.7=row%20major&amp;amp;26.8=&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.2=none&amp;amp;27.13=vmprod&amp;amp;28.15=1&amp;amp;28.16=open&amp;amp;29.18=positive&amp;amp;29.19=left&amp;amp;29.20=bottom&amp;amp;29.21=back&amp;amp;30.1=attn&amp;amp;30.4=true&amp;amp;30.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;31.1=Q&amp;amp;31.4=true&amp;amp;31.2=none&amp;amp;32.1=input&amp;amp;32.4=false&amp;amp;32.5=256&amp;amp;32.6=192&amp;amp;32.7=gaussian&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;32.0=&amp;amp;32.16=open&amp;amp;33.1=wQ&amp;amp;33.4=false&amp;amp;33.5=192&amp;amp;33.6=192&amp;amp;33.7=gaussian&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;33.0=&amp;amp;33.16=open&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.18=positive&amp;amp;36.19=left&amp;amp;36.20=bottom&amp;amp;36.21=back&amp;amp;31.16=closed&amp;amp;37.2=none&amp;amp;38.13=none&amp;amp;39.15=1&amp;amp;40.18=positive&amp;amp;40.19=right&amp;amp;40.20=top&amp;amp;40.21=back&amp;amp;41.1=wK_t&amp;amp;41.4=false&amp;amp;41.5=192&amp;amp;41.6=192&amp;amp;41.7=gaussian&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;41.0=&amp;amp;41.16=open&amp;amp;42.1=input_t&amp;amp;42.4=false&amp;amp;42.5=192&amp;amp;42.6=256&amp;amp;42.7=gaussian&amp;amp;42.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;42.9=-1&amp;amp;42.10=1&amp;amp;42.11=0&amp;amp;42.0=&amp;amp;42.16=open&amp;amp;37.1=K_t&amp;amp;37.4=true&amp;amp;37.16=closed&amp;amp;43.13=vmprod&amp;amp;44.15=1&amp;amp;44.16=open&amp;amp;45.18=negative&amp;amp;45.19=left&amp;amp;45.20=top&amp;amp;45.21=front&amp;amp;30.16=open&amp;amp;46.1=V&amp;amp;46.4=true&amp;amp;46.2=none&amp;amp;47.1=input&amp;amp;47.4=false&amp;amp;47.5=256&amp;amp;47.6=192&amp;amp;47.7=gaussian&amp;amp;47.9=-1&amp;amp;47.10=1&amp;amp;47.11=0&amp;amp;47.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;47.0=&amp;amp;47.16=open&amp;amp;48.1=wV&amp;amp;48.4=false&amp;amp;48.5=192&amp;amp;48.6=192&amp;amp;48.7=gaussian&amp;amp;48.9=-1&amp;amp;48.10=1&amp;amp;48.11=0&amp;amp;48.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;48.0=&amp;amp;48.16=open&amp;amp;49.13=none&amp;amp;50.15=1&amp;amp;51.18=negative&amp;amp;51.19=right&amp;amp;51.20=top&amp;amp;51.21=back&amp;amp;46.16=open&amp;amp;26.16=open&amp;amp;52.1=wO&amp;amp;52.4=false&amp;amp;52.5=192&amp;amp;52.6=192&amp;amp;52.7=gaussian&amp;amp;52.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;52.9=-1&amp;amp;52.10=0.996&amp;amp;52.11=0&amp;amp;52.0=&amp;amp;52.16=closed&amp;amp;22.1=attn_out&amp;amp;22.4=true&amp;amp;22.16=open&amp;amp;53.1=FFN_1&amp;amp;53.4=false&amp;amp;53.5=192&amp;amp;53.6=768&amp;amp;53.7=gaussian&amp;amp;53.8=&amp;amp;53.9=-1&amp;amp;53.10=1&amp;amp;53.11=0&amp;amp;53.0=&amp;amp;53.16=closed&amp;amp;3.16=open&amp;amp;54.1=FFN_2&amp;amp;54.4=false&amp;amp;54.5=768&amp;amp;54.6=192&amp;amp;54.7=gaussian&amp;amp;54.8=&amp;amp;54.9=-1&amp;amp;54.10=1&amp;amp;54.11=0&amp;amp;54.0=&amp;amp;54.16=closed&amp;amp;55.56=sync&amp;amp;55.57=16&amp;amp;55.58=false&amp;amp;55.13=none&amp;amp;55.59=0&amp;amp;55.16=closed&amp;amp;60.61=1&amp;amp;60.62=1&amp;amp;60.15=1&amp;amp;60.16=closed&amp;amp;63.64=blocks&amp;amp;63.65=8&amp;amp;63.66=0&amp;amp;63.67=1&amp;amp;63.68=0&amp;amp;63.18=negative&amp;amp;63.19=left&amp;amp;63.20=top&amp;amp;63.21=front&amp;amp;63.16=closed&amp;amp;69.70=10&amp;amp;69.71=true&amp;amp;69.72=4&amp;amp;69.73=0.507&amp;amp;69.74=0.524&amp;amp;69.75=0.5&amp;amp;69.76=12&amp;amp;69.77=false&amp;amp;69.78=false&amp;amp;69.16=closed&amp;amp;79.80=local&amp;amp;79.81=0.1&amp;amp;79.82=0.2&amp;amp;79.83=0.9&amp;amp;79.84=2&amp;amp;79.85=0.75&amp;amp;79.86=0.75&amp;amp;79.87=0.03&amp;amp;79.16=closed&amp;amp;88.8=&amp;amp;88.16=open&amp;amp;89.90=-738.1526976199144&amp;amp;89.91=919.9001193338946&amp;amp;89.92=957.7418906526483&amp;amp;93.90=0&amp;amp;93.91=0&amp;amp;93.92=0&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;folder=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.left=22&amp;amp;left.left.anim=23&amp;amp;left.left.block=24&amp;amp;left.left.layout=25&amp;amp;left.left.left=26&amp;amp;left.left.left.anim=27&amp;amp;left.left.left.block=28&amp;amp;left.left.left.layout=29&amp;amp;left.left.left.left=30&amp;amp;left.left.left.left.left=31&amp;amp;left.left.left.left.left.left=32&amp;amp;left.left.left.left.left.right=33&amp;amp;left.left.left.left.left.anim=34&amp;amp;left.left.left.left.left.block=35&amp;amp;left.left.left.left.left.layout=36&amp;amp;left.left.left.left.right=37&amp;amp;left.left.left.left.right.anim=38&amp;amp;left.left.left.left.right.block=39&amp;amp;left.left.left.left.right.layout=40&amp;amp;left.left.left.left.right.left=41&amp;amp;left.left.left.left.right.right=42&amp;amp;left.left.left.left.anim=43&amp;amp;left.left.left.left.block=44&amp;amp;left.left.left.left.layout=45&amp;amp;left.left.left.right=46&amp;amp;left.left.left.right.left=47&amp;amp;left.left.left.right.right=48&amp;amp;left.left.left.right.anim=49&amp;amp;left.left.left.right.block=50&amp;amp;left.left.left.right.layout=51&amp;amp;left.left.right=52&amp;amp;left.right=53&amp;amp;right=54&amp;amp;anim=55&amp;amp;fuse=56&amp;amp;speed=57&amp;amp;hide%20inputs=58&amp;amp;spin=59&amp;amp;block=60&amp;amp;i%20blocks=61&amp;amp;j%20blocks=62&amp;amp;layout=63&amp;amp;scheme=64&amp;amp;gap=65&amp;amp;scatter=66&amp;amp;molecule=67&amp;amp;blast=68&amp;amp;deco=69&amp;amp;legends=70&amp;amp;shape=71&amp;amp;spotlight=72&amp;amp;row%20guides=73&amp;amp;flow%20guides=74&amp;amp;lens%20size=75&amp;amp;magnification=76&amp;amp;interior%20spotlight=77&amp;amp;axes=78&amp;amp;viz=79&amp;amp;sensitivity=80&amp;amp;min%20size=81&amp;amp;min%20light=82&amp;amp;max%20light=83&amp;amp;elem%20scale=84&amp;amp;zero%20hue=85&amp;amp;hue%20gap=86&amp;amp;hue%20spread=87&amp;amp;diag=88&amp;amp;cam=89&amp;amp;x=90&amp;amp;y=91&amp;amp;z=92&amp;amp;cam.target=93&amp;amp;compress=94&quot;&gt;Open in mm&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/attnlayer2.jpg&quot; alt=&quot;a full attention layer with the first half (MHA) in the background and the second (FFN) in the foreground&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6b-visualizing-the-bpt-partitioned-layer&quot;&gt;6b Visualizing the BPT partitioned layer&lt;/h3&gt;

&lt;p&gt;Revisiting &lt;a href=&quot;https://arxiv.org/pdf/2305.19370.pdf&quot;&gt;Blockwise Parallel Transformer&lt;/a&gt; briefly, here we visualize BPT’s parallelization scheme in the context of an entire attention layer (with individual heads elided per above). In particular, note how the partitioning along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; (of sequence blocks) extends through both MHA and FFN halves (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out%20%3D%20(attn_out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20(K_t%20%3D%20wK_t%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20wV)%20%40%20wO)%20%40%20FFN_1%20%40%20FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;94=true&amp;amp;3.1=attn_out%20%40%20FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=closed&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.2=layernorm&amp;amp;23.13=inherit&amp;amp;24.15=1&amp;amp;24.16=closed&amp;amp;25.18=negative&amp;amp;25.19=left&amp;amp;25.20=top&amp;amp;25.21=front&amp;amp;26.1=attn%20%40%20V&amp;amp;26.4=true&amp;amp;26.5=32&amp;amp;26.6=32&amp;amp;26.7=row%20major&amp;amp;26.8=&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.2=none&amp;amp;27.13=vmprod&amp;amp;28.15=8&amp;amp;28.16=open&amp;amp;29.18=positive&amp;amp;29.19=left&amp;amp;29.20=bottom&amp;amp;29.21=back&amp;amp;30.1=attn&amp;amp;30.4=true&amp;amp;30.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;31.1=Q&amp;amp;31.4=true&amp;amp;31.2=none&amp;amp;32.1=input&amp;amp;32.4=false&amp;amp;32.5=256&amp;amp;32.6=192&amp;amp;32.7=gaussian&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;32.0=&amp;amp;32.16=open&amp;amp;33.1=wQ&amp;amp;33.4=false&amp;amp;33.5=192&amp;amp;33.6=192&amp;amp;33.7=gaussian&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;33.0=&amp;amp;33.16=open&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.18=positive&amp;amp;36.19=left&amp;amp;36.20=bottom&amp;amp;36.21=back&amp;amp;31.16=closed&amp;amp;37.2=none&amp;amp;38.13=none&amp;amp;39.15=1&amp;amp;40.18=positive&amp;amp;40.19=right&amp;amp;40.20=top&amp;amp;40.21=back&amp;amp;41.1=wK_t&amp;amp;41.4=false&amp;amp;41.5=192&amp;amp;41.6=192&amp;amp;41.7=gaussian&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;41.0=&amp;amp;41.16=open&amp;amp;42.1=input_t&amp;amp;42.4=false&amp;amp;42.5=192&amp;amp;42.6=256&amp;amp;42.7=gaussian&amp;amp;42.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;42.9=-1&amp;amp;42.10=1&amp;amp;42.11=0&amp;amp;42.0=&amp;amp;42.16=open&amp;amp;37.1=K_t&amp;amp;37.4=true&amp;amp;37.16=closed&amp;amp;43.13=vmprod&amp;amp;44.15=1&amp;amp;44.16=open&amp;amp;45.18=negative&amp;amp;45.19=left&amp;amp;45.20=top&amp;amp;45.21=front&amp;amp;30.16=closed&amp;amp;46.1=V&amp;amp;46.4=true&amp;amp;46.2=none&amp;amp;47.1=input&amp;amp;47.4=false&amp;amp;47.5=256&amp;amp;47.6=192&amp;amp;47.7=gaussian&amp;amp;47.9=-1&amp;amp;47.10=1&amp;amp;47.11=0&amp;amp;47.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;47.0=&amp;amp;47.16=open&amp;amp;48.1=wV&amp;amp;48.4=false&amp;amp;48.5=192&amp;amp;48.6=192&amp;amp;48.7=gaussian&amp;amp;48.9=-1&amp;amp;48.10=1&amp;amp;48.11=0&amp;amp;48.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;48.0=&amp;amp;48.16=open&amp;amp;49.13=none&amp;amp;50.15=1&amp;amp;51.18=negative&amp;amp;51.19=right&amp;amp;51.20=top&amp;amp;51.21=back&amp;amp;46.16=closed&amp;amp;26.16=open&amp;amp;52.1=wO&amp;amp;52.4=false&amp;amp;52.5=192&amp;amp;52.6=192&amp;amp;52.7=gaussian&amp;amp;52.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;52.9=-1&amp;amp;52.10=0.996&amp;amp;52.11=0&amp;amp;52.0=&amp;amp;52.16=closed&amp;amp;22.1=attn_out&amp;amp;22.4=true&amp;amp;22.16=open&amp;amp;53.1=FFN_1&amp;amp;53.4=false&amp;amp;53.5=192&amp;amp;53.6=768&amp;amp;53.7=gaussian&amp;amp;53.8=&amp;amp;53.9=-1&amp;amp;53.10=1&amp;amp;53.11=0&amp;amp;53.0=&amp;amp;53.16=closed&amp;amp;3.16=open&amp;amp;54.1=FFN_2&amp;amp;54.4=false&amp;amp;54.5=768&amp;amp;54.6=192&amp;amp;54.7=gaussian&amp;amp;54.8=&amp;amp;54.9=-1&amp;amp;54.10=1&amp;amp;54.11=0&amp;amp;54.0=&amp;amp;54.16=closed&amp;amp;55.56=sync&amp;amp;55.57=16&amp;amp;55.58=false&amp;amp;55.13=none&amp;amp;55.59=0&amp;amp;55.16=closed&amp;amp;60.61=8&amp;amp;60.62=1&amp;amp;60.15=1&amp;amp;60.16=open&amp;amp;63.64=blocks&amp;amp;63.65=8&amp;amp;63.66=0&amp;amp;63.67=1&amp;amp;63.68=0&amp;amp;63.18=negative&amp;amp;63.19=left&amp;amp;63.20=top&amp;amp;63.21=front&amp;amp;63.16=closed&amp;amp;69.70=10&amp;amp;69.71=true&amp;amp;69.72=4&amp;amp;69.73=0.507&amp;amp;69.74=0.524&amp;amp;69.75=0.5&amp;amp;69.76=12&amp;amp;69.77=false&amp;amp;69.78=false&amp;amp;69.16=closed&amp;amp;79.80=local&amp;amp;79.81=0.1&amp;amp;79.82=0.2&amp;amp;79.83=0.9&amp;amp;79.84=2&amp;amp;79.85=0.75&amp;amp;79.86=0.75&amp;amp;79.87=0.03&amp;amp;79.16=closed&amp;amp;88.8=&amp;amp;88.16=open&amp;amp;89.90=-766.4372214429399&amp;amp;89.91=955.1488380935747&amp;amp;89.92=994.4406298292719&amp;amp;93.90=0&amp;amp;93.91=0&amp;amp;93.92=0&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;folder=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.left=22&amp;amp;left.left.anim=23&amp;amp;left.left.block=24&amp;amp;left.left.layout=25&amp;amp;left.left.left=26&amp;amp;left.left.left.anim=27&amp;amp;left.left.left.block=28&amp;amp;left.left.left.layout=29&amp;amp;left.left.left.left=30&amp;amp;left.left.left.left.left=31&amp;amp;left.left.left.left.left.left=32&amp;amp;left.left.left.left.left.right=33&amp;amp;left.left.left.left.left.anim=34&amp;amp;left.left.left.left.left.block=35&amp;amp;left.left.left.left.left.layout=36&amp;amp;left.left.left.left.right=37&amp;amp;left.left.left.left.right.anim=38&amp;amp;left.left.left.left.right.block=39&amp;amp;left.left.left.left.right.layout=40&amp;amp;left.left.left.left.right.left=41&amp;amp;left.left.left.left.right.right=42&amp;amp;left.left.left.left.anim=43&amp;amp;left.left.left.left.block=44&amp;amp;left.left.left.left.layout=45&amp;amp;left.left.left.right=46&amp;amp;left.left.left.right.left=47&amp;amp;left.left.left.right.right=48&amp;amp;left.left.left.right.anim=49&amp;amp;left.left.left.right.block=50&amp;amp;left.left.left.right.layout=51&amp;amp;left.left.right=52&amp;amp;left.right=53&amp;amp;right=54&amp;amp;anim=55&amp;amp;fuse=56&amp;amp;speed=57&amp;amp;hide%20inputs=58&amp;amp;spin=59&amp;amp;block=60&amp;amp;i%20blocks=61&amp;amp;j%20blocks=62&amp;amp;layout=63&amp;amp;scheme=64&amp;amp;gap=65&amp;amp;scatter=66&amp;amp;molecule=67&amp;amp;blast=68&amp;amp;deco=69&amp;amp;legends=70&amp;amp;shape=71&amp;amp;spotlight=72&amp;amp;row%20guides=73&amp;amp;flow%20guides=74&amp;amp;lens%20size=75&amp;amp;magnification=76&amp;amp;interior%20spotlight=77&amp;amp;axes=78&amp;amp;viz=79&amp;amp;sensitivity=80&amp;amp;min%20size=81&amp;amp;min%20light=82&amp;amp;max%20light=83&amp;amp;elem%20scale=84&amp;amp;zero%20hue=85&amp;amp;hue%20gap=86&amp;amp;hue%20spread=87&amp;amp;diag=88&amp;amp;cam=89&amp;amp;x=90&amp;amp;y=91&amp;amp;z=92&amp;amp;cam.target=93&amp;amp;compress=94&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/bptlayer.jpg&quot; alt=&quot;visualize BPT's parallelization scheme in the context of an entire attention layer&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6c-partitioning-the-ffn&quot;&gt;6c Partitioning the FFN&lt;/h3&gt;

&lt;p&gt;The visualization suggests an additional partitioning, orthogonal to the ones described above - in the FFN half of the attention layer, splitting the double matmul &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(attn_out @ FFN_1) @ FFN_2&lt;/code&gt;, first along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn_out @ FFN_1&lt;/code&gt;, then along &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; in the subsequent matmul with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FFN_2&lt;/code&gt;. This partition slices both layers of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FFN&lt;/code&gt; weights, reducing the capacity requirements of each participant in the computation at the cost of a final summation of the partial results.&lt;/p&gt;

&lt;p&gt;Here’s what this partition looks like applied to an otherwise unpartitioned attention layer (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out+%3D+%28attn_out+%3D+%28attn+%3D+%28Q+%3D+input+%40+wQ%29+%40+%28K_t+%3D+wK_t+%40+input_t%29%29+%40+%28V+%3D+input+%40+wV%29+%40+wO%29+%40+FFN_1+%40+FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;94=true&amp;amp;3.1=attn_out+%40+FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row+major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=open&amp;amp;14.17=1&amp;amp;14.18=8&amp;amp;19.20=positive&amp;amp;19.21=left&amp;amp;19.22=bottom&amp;amp;19.23=back&amp;amp;24.2=layernorm&amp;amp;25.13=inherit&amp;amp;26.15=1&amp;amp;26.16=closed&amp;amp;26.17=1&amp;amp;26.18=1&amp;amp;27.20=negative&amp;amp;27.21=left&amp;amp;27.22=top&amp;amp;27.23=front&amp;amp;28.1=attn+%40+V&amp;amp;28.4=true&amp;amp;28.5=32&amp;amp;28.6=32&amp;amp;28.7=row+major&amp;amp;28.8=&amp;amp;28.9=-1&amp;amp;28.10=1&amp;amp;28.11=0&amp;amp;28.2=none&amp;amp;29.13=vmprod&amp;amp;30.15=1&amp;amp;30.16=open&amp;amp;30.17=1&amp;amp;30.18=1&amp;amp;31.20=positive&amp;amp;31.21=left&amp;amp;31.22=bottom&amp;amp;31.23=back&amp;amp;32.1=attn&amp;amp;32.4=true&amp;amp;32.2=softmax%28tril%28x%2Fsqrt%28k%29%29%29&amp;amp;33.1=Q&amp;amp;33.4=true&amp;amp;33.2=none&amp;amp;34.1=input&amp;amp;34.4=false&amp;amp;34.5=256&amp;amp;34.6=192&amp;amp;34.7=gaussian&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;34.0=&amp;amp;34.16=open&amp;amp;35.1=wQ&amp;amp;35.4=false&amp;amp;35.5=192&amp;amp;35.6=192&amp;amp;35.7=gaussian&amp;amp;35.9=-1&amp;amp;35.10=1&amp;amp;35.11=0&amp;amp;35.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;35.0=&amp;amp;35.16=open&amp;amp;36.13=vmprod&amp;amp;37.15=1&amp;amp;37.17=1&amp;amp;37.18=1&amp;amp;38.20=positive&amp;amp;38.21=left&amp;amp;38.22=bottom&amp;amp;38.23=back&amp;amp;33.16=closed&amp;amp;39.2=none&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;41.17=1&amp;amp;41.18=1&amp;amp;42.20=positive&amp;amp;42.21=right&amp;amp;42.22=top&amp;amp;42.23=back&amp;amp;43.1=wK_t&amp;amp;43.4=false&amp;amp;43.5=192&amp;amp;43.6=192&amp;amp;43.7=gaussian&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;43.16=open&amp;amp;44.1=input_t&amp;amp;44.4=false&amp;amp;44.5=192&amp;amp;44.6=256&amp;amp;44.7=gaussian&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;44.16=open&amp;amp;39.1=K_t&amp;amp;39.4=true&amp;amp;39.16=closed&amp;amp;45.13=vmprod&amp;amp;46.15=1&amp;amp;46.16=open&amp;amp;46.17=1&amp;amp;46.18=1&amp;amp;47.20=negative&amp;amp;47.21=left&amp;amp;47.22=top&amp;amp;47.23=front&amp;amp;32.16=closed&amp;amp;48.1=V&amp;amp;48.4=true&amp;amp;48.2=none&amp;amp;49.1=input&amp;amp;49.4=false&amp;amp;49.5=256&amp;amp;49.6=192&amp;amp;49.7=gaussian&amp;amp;49.9=-1&amp;amp;49.10=1&amp;amp;49.11=0&amp;amp;49.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;49.0=&amp;amp;49.16=open&amp;amp;50.1=wV&amp;amp;50.4=false&amp;amp;50.5=192&amp;amp;50.6=192&amp;amp;50.7=gaussian&amp;amp;50.9=-1&amp;amp;50.10=1&amp;amp;50.11=0&amp;amp;50.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;50.0=&amp;amp;50.16=open&amp;amp;51.13=none&amp;amp;52.15=1&amp;amp;52.17=1&amp;amp;52.18=1&amp;amp;53.20=negative&amp;amp;53.21=right&amp;amp;53.22=top&amp;amp;53.23=back&amp;amp;48.16=closed&amp;amp;28.16=open&amp;amp;54.1=wO&amp;amp;54.4=false&amp;amp;54.5=192&amp;amp;54.6=192&amp;amp;54.7=gaussian&amp;amp;54.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;54.9=-1&amp;amp;54.10=0.996&amp;amp;54.11=0&amp;amp;54.0=&amp;amp;54.16=closed&amp;amp;24.1=attn_out&amp;amp;24.4=true&amp;amp;24.16=closed&amp;amp;55.1=FFN_1&amp;amp;55.4=false&amp;amp;55.5=192&amp;amp;55.6=768&amp;amp;55.7=gaussian&amp;amp;55.8=&amp;amp;55.9=-1&amp;amp;55.10=1&amp;amp;55.11=0&amp;amp;55.0=&amp;amp;55.16=closed&amp;amp;3.16=open&amp;amp;56.1=FFN_2&amp;amp;56.4=false&amp;amp;56.5=768&amp;amp;56.6=192&amp;amp;56.7=gaussian&amp;amp;56.8=&amp;amp;56.9=-1&amp;amp;56.10=1&amp;amp;56.11=0&amp;amp;56.0=&amp;amp;56.16=closed&amp;amp;57.58=sync&amp;amp;57.59=16&amp;amp;57.60=false&amp;amp;57.13=none&amp;amp;57.61=0&amp;amp;57.16=closed&amp;amp;62.17=1&amp;amp;62.15=8&amp;amp;62.18=1&amp;amp;62.16=closed&amp;amp;63.64=blocks&amp;amp;63.65=8&amp;amp;63.66=0&amp;amp;63.67=1&amp;amp;63.68=0&amp;amp;63.20=negative&amp;amp;63.21=left&amp;amp;63.22=top&amp;amp;63.23=front&amp;amp;63.16=closed&amp;amp;69.70=10&amp;amp;69.71=true&amp;amp;69.72=4&amp;amp;69.73=0.507&amp;amp;69.74=0.524&amp;amp;69.75=0.5&amp;amp;69.76=12&amp;amp;69.77=false&amp;amp;69.78=false&amp;amp;69.16=closed&amp;amp;79.80=local&amp;amp;79.81=0.1&amp;amp;79.82=0.2&amp;amp;79.83=0.9&amp;amp;79.84=2&amp;amp;79.85=0.75&amp;amp;79.86=0.75&amp;amp;79.87=0.03&amp;amp;79.16=closed&amp;amp;88.8=&amp;amp;88.16=open&amp;amp;89.90=-725.0392607527422&amp;amp;89.91=909.2543497392985&amp;amp;89.92=1420.9035091451585&amp;amp;93.90=84.4062135237143&amp;amp;93.91=2.295441349889614&amp;amp;93.92=60.16668289640925&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k+blocks=15&amp;amp;folder=16&amp;amp;i+blocks=17&amp;amp;j+blocks=18&amp;amp;left.layout=19&amp;amp;polarity=20&amp;amp;left+placement=21&amp;amp;right+placement=22&amp;amp;result+placement=23&amp;amp;left.left=24&amp;amp;left.left.anim=25&amp;amp;left.left.block=26&amp;amp;left.left.layout=27&amp;amp;left.left.left=28&amp;amp;left.left.left.anim=29&amp;amp;left.left.left.block=30&amp;amp;left.left.left.layout=31&amp;amp;left.left.left.left=32&amp;amp;left.left.left.left.left=33&amp;amp;left.left.left.left.left.left=34&amp;amp;left.left.left.left.left.right=35&amp;amp;left.left.left.left.left.anim=36&amp;amp;left.left.left.left.left.block=37&amp;amp;left.left.left.left.left.layout=38&amp;amp;left.left.left.left.right=39&amp;amp;left.left.left.left.right.anim=40&amp;amp;left.left.left.left.right.block=41&amp;amp;left.left.left.left.right.layout=42&amp;amp;left.left.left.left.right.left=43&amp;amp;left.left.left.left.right.right=44&amp;amp;left.left.left.left.anim=45&amp;amp;left.left.left.left.block=46&amp;amp;left.left.left.left.layout=47&amp;amp;left.left.left.right=48&amp;amp;left.left.left.right.left=49&amp;amp;left.left.left.right.right=50&amp;amp;left.left.left.right.anim=51&amp;amp;left.left.left.right.block=52&amp;amp;left.left.left.right.layout=53&amp;amp;left.left.right=54&amp;amp;left.right=55&amp;amp;right=56&amp;amp;anim=57&amp;amp;fuse=58&amp;amp;speed=59&amp;amp;hide+inputs=60&amp;amp;spin=61&amp;amp;block=62&amp;amp;layout=63&amp;amp;scheme=64&amp;amp;gap=65&amp;amp;scatter=66&amp;amp;molecule=67&amp;amp;blast=68&amp;amp;deco=69&amp;amp;legends=70&amp;amp;shape=71&amp;amp;spotlight=72&amp;amp;row+guides=73&amp;amp;flow+guides=74&amp;amp;lens+size=75&amp;amp;magnification=76&amp;amp;interior+spotlight=77&amp;amp;axes=78&amp;amp;viz=79&amp;amp;sensitivity=80&amp;amp;min+size=81&amp;amp;min+light=82&amp;amp;max+light=83&amp;amp;elem+scale=84&amp;amp;zero+hue=85&amp;amp;hue+gap=86&amp;amp;hue+spread=87&amp;amp;diag=88&amp;amp;cam=89&amp;amp;x=90&amp;amp;y=91&amp;amp;z=92&amp;amp;cam.target=93&amp;amp;compress=94&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/attnlayer_ffnsplitk.jpg&quot; alt=&quot;what this partition looks like applied to an otherwise unpartitioned attention layer&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here it is applied to a layer partitioned a la BPT (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out+%3D+%28attn_out+%3D+%28attn+%3D+%28Q+%3D+input+%40+wQ%29+%40+%28K_t+%3D+wK_t+%40+input_t%29%29+%40+%28V+%3D+input+%40+wV%29+%40+wO%29+%40+FFN_1+%40+FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;94=true&amp;amp;3.1=attn_out+%40+FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row+major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=open&amp;amp;14.17=8&amp;amp;14.18=8&amp;amp;19.20=positive&amp;amp;19.21=left&amp;amp;19.22=bottom&amp;amp;19.23=back&amp;amp;24.2=layernorm&amp;amp;25.13=inherit&amp;amp;26.15=1&amp;amp;26.16=closed&amp;amp;26.17=8&amp;amp;26.18=1&amp;amp;27.20=negative&amp;amp;27.21=left&amp;amp;27.22=top&amp;amp;27.23=front&amp;amp;28.1=attn+%40+V&amp;amp;28.4=true&amp;amp;28.5=32&amp;amp;28.6=32&amp;amp;28.7=row+major&amp;amp;28.8=&amp;amp;28.9=-1&amp;amp;28.10=1&amp;amp;28.11=0&amp;amp;28.2=none&amp;amp;29.13=vmprod&amp;amp;30.15=8&amp;amp;30.16=open&amp;amp;30.17=8&amp;amp;30.18=1&amp;amp;31.20=positive&amp;amp;31.21=left&amp;amp;31.22=bottom&amp;amp;31.23=back&amp;amp;32.1=attn&amp;amp;32.4=true&amp;amp;32.2=softmax%28tril%28x%2Fsqrt%28k%29%29%29&amp;amp;33.1=Q&amp;amp;33.4=true&amp;amp;33.2=none&amp;amp;34.1=input&amp;amp;34.4=false&amp;amp;34.5=256&amp;amp;34.6=192&amp;amp;34.7=gaussian&amp;amp;34.9=-1&amp;amp;34.10=1&amp;amp;34.11=0&amp;amp;34.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;34.0=&amp;amp;34.16=open&amp;amp;35.1=wQ&amp;amp;35.4=false&amp;amp;35.5=192&amp;amp;35.6=192&amp;amp;35.7=gaussian&amp;amp;35.9=-1&amp;amp;35.10=1&amp;amp;35.11=0&amp;amp;35.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;35.0=&amp;amp;35.16=open&amp;amp;36.13=vmprod&amp;amp;37.15=1&amp;amp;37.17=8&amp;amp;37.18=1&amp;amp;38.20=positive&amp;amp;38.21=left&amp;amp;38.22=bottom&amp;amp;38.23=back&amp;amp;33.16=closed&amp;amp;39.2=none&amp;amp;40.13=none&amp;amp;41.15=1&amp;amp;41.17=1&amp;amp;41.18=1&amp;amp;42.20=positive&amp;amp;42.21=right&amp;amp;42.22=top&amp;amp;42.23=back&amp;amp;43.1=wK_t&amp;amp;43.4=false&amp;amp;43.5=192&amp;amp;43.6=192&amp;amp;43.7=gaussian&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wk_t4_64_768.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;43.16=open&amp;amp;44.1=input_t&amp;amp;44.4=false&amp;amp;44.5=192&amp;amp;44.6=256&amp;amp;44.7=gaussian&amp;amp;44.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;44.16=open&amp;amp;39.1=K_t&amp;amp;39.4=true&amp;amp;39.16=closed&amp;amp;45.13=vmprod&amp;amp;46.15=1&amp;amp;46.16=open&amp;amp;46.17=8&amp;amp;46.18=1&amp;amp;47.20=negative&amp;amp;47.21=left&amp;amp;47.22=top&amp;amp;47.23=front&amp;amp;32.16=closed&amp;amp;48.1=V&amp;amp;48.4=true&amp;amp;48.2=none&amp;amp;49.1=input&amp;amp;49.4=false&amp;amp;49.5=256&amp;amp;49.6=192&amp;amp;49.7=gaussian&amp;amp;49.9=-1&amp;amp;49.10=1&amp;amp;49.11=0&amp;amp;49.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;49.0=&amp;amp;49.16=open&amp;amp;50.1=wV&amp;amp;50.4=false&amp;amp;50.5=192&amp;amp;50.6=192&amp;amp;50.7=gaussian&amp;amp;50.9=-1&amp;amp;50.10=1&amp;amp;50.11=0&amp;amp;50.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;50.0=&amp;amp;50.16=open&amp;amp;51.13=none&amp;amp;52.15=1&amp;amp;52.17=1&amp;amp;52.18=1&amp;amp;53.20=negative&amp;amp;53.21=right&amp;amp;53.22=top&amp;amp;53.23=back&amp;amp;48.16=closed&amp;amp;28.16=open&amp;amp;54.1=wO&amp;amp;54.4=false&amp;amp;54.5=192&amp;amp;54.6=192&amp;amp;54.7=gaussian&amp;amp;54.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;54.9=-1&amp;amp;54.10=0.996&amp;amp;54.11=0&amp;amp;54.0=&amp;amp;54.16=closed&amp;amp;24.1=attn_out&amp;amp;24.4=true&amp;amp;24.16=closed&amp;amp;55.1=FFN_1&amp;amp;55.4=false&amp;amp;55.5=192&amp;amp;55.6=768&amp;amp;55.7=gaussian&amp;amp;55.8=&amp;amp;55.9=-1&amp;amp;55.10=1&amp;amp;55.11=0&amp;amp;55.0=&amp;amp;55.16=closed&amp;amp;3.16=open&amp;amp;56.1=FFN_2&amp;amp;56.4=false&amp;amp;56.5=768&amp;amp;56.6=192&amp;amp;56.7=gaussian&amp;amp;56.8=&amp;amp;56.9=-1&amp;amp;56.10=1&amp;amp;56.11=0&amp;amp;56.0=&amp;amp;56.16=closed&amp;amp;57.58=sync&amp;amp;57.59=16&amp;amp;57.60=false&amp;amp;57.13=none&amp;amp;57.61=0&amp;amp;57.16=closed&amp;amp;62.17=8&amp;amp;62.15=8&amp;amp;62.18=1&amp;amp;62.16=open&amp;amp;63.64=blocks&amp;amp;63.65=8&amp;amp;63.66=0&amp;amp;63.67=1&amp;amp;63.68=0&amp;amp;63.20=negative&amp;amp;63.21=left&amp;amp;63.22=top&amp;amp;63.23=front&amp;amp;63.16=closed&amp;amp;69.70=10&amp;amp;69.71=true&amp;amp;69.72=4&amp;amp;69.73=0.507&amp;amp;69.74=0.524&amp;amp;69.75=0.5&amp;amp;69.76=12&amp;amp;69.77=false&amp;amp;69.78=false&amp;amp;69.16=closed&amp;amp;79.80=local&amp;amp;79.81=0.1&amp;amp;79.82=0.2&amp;amp;79.83=0.9&amp;amp;79.84=2&amp;amp;79.85=0.75&amp;amp;79.86=0.75&amp;amp;79.87=0.03&amp;amp;79.16=closed&amp;amp;88.8=&amp;amp;88.16=open&amp;amp;89.90=-908.5990219796431&amp;amp;89.91=1012.984380609292&amp;amp;89.92=1378.7815259698948&amp;amp;93.90=57.978218494193676&amp;amp;93.91=-30.847130586978256&amp;amp;93.92=41.66771129059017&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k+blocks=15&amp;amp;folder=16&amp;amp;i+blocks=17&amp;amp;j+blocks=18&amp;amp;left.layout=19&amp;amp;polarity=20&amp;amp;left+placement=21&amp;amp;right+placement=22&amp;amp;result+placement=23&amp;amp;left.left=24&amp;amp;left.left.anim=25&amp;amp;left.left.block=26&amp;amp;left.left.layout=27&amp;amp;left.left.left=28&amp;amp;left.left.left.anim=29&amp;amp;left.left.left.block=30&amp;amp;left.left.left.layout=31&amp;amp;left.left.left.left=32&amp;amp;left.left.left.left.left=33&amp;amp;left.left.left.left.left.left=34&amp;amp;left.left.left.left.left.right=35&amp;amp;left.left.left.left.left.anim=36&amp;amp;left.left.left.left.left.block=37&amp;amp;left.left.left.left.left.layout=38&amp;amp;left.left.left.left.right=39&amp;amp;left.left.left.left.right.anim=40&amp;amp;left.left.left.left.right.block=41&amp;amp;left.left.left.left.right.layout=42&amp;amp;left.left.left.left.right.left=43&amp;amp;left.left.left.left.right.right=44&amp;amp;left.left.left.left.anim=45&amp;amp;left.left.left.left.block=46&amp;amp;left.left.left.left.layout=47&amp;amp;left.left.left.right=48&amp;amp;left.left.left.right.left=49&amp;amp;left.left.left.right.right=50&amp;amp;left.left.left.right.anim=51&amp;amp;left.left.left.right.block=52&amp;amp;left.left.left.right.layout=53&amp;amp;left.left.right=54&amp;amp;left.right=55&amp;amp;right=56&amp;amp;anim=57&amp;amp;fuse=58&amp;amp;speed=59&amp;amp;hide+inputs=60&amp;amp;spin=61&amp;amp;block=62&amp;amp;layout=63&amp;amp;scheme=64&amp;amp;gap=65&amp;amp;scatter=66&amp;amp;molecule=67&amp;amp;blast=68&amp;amp;deco=69&amp;amp;legends=70&amp;amp;shape=71&amp;amp;spotlight=72&amp;amp;row+guides=73&amp;amp;flow+guides=74&amp;amp;lens+size=75&amp;amp;magnification=76&amp;amp;interior+spotlight=77&amp;amp;axes=78&amp;amp;viz=79&amp;amp;sensitivity=80&amp;amp;min+size=81&amp;amp;min+light=82&amp;amp;max+light=83&amp;amp;elem+scale=84&amp;amp;zero+hue=85&amp;amp;hue+gap=86&amp;amp;hue+spread=87&amp;amp;diag=88&amp;amp;cam=89&amp;amp;x=90&amp;amp;y=91&amp;amp;z=92&amp;amp;cam.target=93&amp;amp;compress=94&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/bptlayer_ffnsplitk.jpg&quot; alt=&quot;applied to a layer partitioned a la BPT&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6d-visualizing-token-at-a-time-decoding&quot;&gt;6d Visualizing token-at-a-time decoding&lt;/h3&gt;

&lt;p&gt;During autoregressive token-at-a-time decoding, the query vector consists of a single token. It’s instructive to have a mental picture of what an attention layer looks like in that situation - a single embedding row working its way through an enormous tiled plane of weights.&lt;/p&gt;

&lt;p&gt;Aside from the emphasizing the sheer immensity of weights compared to activations, this view is also evocative of the notion that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K_t&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; function like dynamically generated layers in a 6-layer MLP, although the mux/demux computations of MHA itself (papered over here, per above) make the correspondence inexact (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=layer_out%20%3D%20(attn_out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20wQ)%20%40%20K_t)%20%40%20V%20%40%20wO)%20%40%20FFN_1%20%40%20FFN_2&amp;amp;1=layer_out&amp;amp;2=layernorm&amp;amp;16=closed&amp;amp;84=true&amp;amp;3.1=attn_out%20%40%20FFN_1&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=row%20major&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=gelu&amp;amp;12.13=inherit&amp;amp;14.15=1&amp;amp;14.16=open&amp;amp;17.18=positive&amp;amp;17.19=left&amp;amp;17.20=bottom&amp;amp;17.21=back&amp;amp;22.2=layernorm&amp;amp;23.13=inherit&amp;amp;24.15=1&amp;amp;24.16=open&amp;amp;25.18=negative&amp;amp;25.19=left&amp;amp;25.20=top&amp;amp;25.21=front&amp;amp;26.1=attn%20%40%20V&amp;amp;26.4=true&amp;amp;26.5=32&amp;amp;26.6=32&amp;amp;26.7=row%20major&amp;amp;26.8=&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.2=none&amp;amp;27.13=vmprod&amp;amp;28.15=1&amp;amp;28.16=open&amp;amp;29.18=positive&amp;amp;29.19=left&amp;amp;29.20=bottom&amp;amp;29.21=back&amp;amp;30.1=attn&amp;amp;30.4=true&amp;amp;30.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;31.1=Q&amp;amp;31.4=true&amp;amp;31.2=none&amp;amp;32.1=input&amp;amp;32.4=false&amp;amp;32.5=1&amp;amp;32.6=192&amp;amp;32.7=gaussian&amp;amp;32.9=-1&amp;amp;32.10=1&amp;amp;32.11=0&amp;amp;32.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input0_256_768.csv&amp;amp;32.0=&amp;amp;32.16=open&amp;amp;33.1=wQ&amp;amp;33.4=false&amp;amp;33.5=192&amp;amp;33.6=192&amp;amp;33.7=gaussian&amp;amp;33.9=-1&amp;amp;33.10=1&amp;amp;33.11=0&amp;amp;33.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wq4_768_64.csv&amp;amp;33.0=&amp;amp;33.16=open&amp;amp;34.13=vmprod&amp;amp;35.15=1&amp;amp;36.18=positive&amp;amp;36.19=left&amp;amp;36.20=bottom&amp;amp;36.21=back&amp;amp;31.16=open&amp;amp;37.1=K_t&amp;amp;37.4=false&amp;amp;37.5=192&amp;amp;37.6=256&amp;amp;37.7=gaussian&amp;amp;37.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_input_t0_768_256.csv&amp;amp;37.9=-1&amp;amp;37.10=1&amp;amp;37.11=0&amp;amp;37.0=&amp;amp;37.16=open&amp;amp;38.13=vmprod&amp;amp;39.15=1&amp;amp;39.16=open&amp;amp;40.18=negative&amp;amp;40.19=left&amp;amp;40.20=top&amp;amp;40.21=front&amp;amp;30.16=open&amp;amp;41.1=V&amp;amp;41.4=false&amp;amp;41.16=open&amp;amp;41.5=256&amp;amp;41.6=192&amp;amp;41.7=gaussian&amp;amp;41.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wv4_768_64.csv&amp;amp;41.0=&amp;amp;41.9=-1&amp;amp;41.10=1&amp;amp;41.11=0&amp;amp;26.16=open&amp;amp;42.1=wO&amp;amp;42.4=false&amp;amp;42.5=192&amp;amp;42.6=192&amp;amp;42.7=gaussian&amp;amp;42.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer5_wo4_64_768.csv&amp;amp;42.9=-1&amp;amp;42.10=0.996&amp;amp;42.11=0&amp;amp;42.0=&amp;amp;42.16=closed&amp;amp;22.1=attn_out&amp;amp;22.4=true&amp;amp;22.16=open&amp;amp;43.1=FFN_1&amp;amp;43.4=false&amp;amp;43.5=192&amp;amp;43.6=768&amp;amp;43.7=gaussian&amp;amp;43.8=&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;43.16=closed&amp;amp;3.16=open&amp;amp;44.1=FFN_2&amp;amp;44.4=false&amp;amp;44.5=768&amp;amp;44.6=192&amp;amp;44.7=gaussian&amp;amp;44.8=&amp;amp;44.9=-1&amp;amp;44.10=1&amp;amp;44.11=0&amp;amp;44.0=&amp;amp;44.16=closed&amp;amp;45.46=sync&amp;amp;45.47=16&amp;amp;45.48=false&amp;amp;45.13=none&amp;amp;45.49=0&amp;amp;45.16=closed&amp;amp;50.51=1&amp;amp;50.52=1&amp;amp;50.15=1&amp;amp;50.16=closed&amp;amp;53.54=blocks&amp;amp;53.55=8&amp;amp;53.56=0&amp;amp;53.57=1&amp;amp;53.58=0&amp;amp;53.18=negative&amp;amp;53.19=left&amp;amp;53.20=top&amp;amp;53.21=front&amp;amp;53.16=closed&amp;amp;59.60=10&amp;amp;59.61=true&amp;amp;59.62=4&amp;amp;59.63=0.507&amp;amp;59.64=0.524&amp;amp;59.65=0.5&amp;amp;59.66=12&amp;amp;59.67=false&amp;amp;59.68=false&amp;amp;59.16=closed&amp;amp;69.70=local&amp;amp;69.71=0.1&amp;amp;69.72=0.2&amp;amp;69.73=0.9&amp;amp;69.74=2&amp;amp;69.75=0.75&amp;amp;69.76=0.75&amp;amp;69.77=0.03&amp;amp;69.16=closed&amp;amp;78.8=&amp;amp;78.16=open&amp;amp;79.80=-621.3352100945762&amp;amp;79.81=774.3199147754915&amp;amp;79.82=806.172978523008&amp;amp;83.80=0&amp;amp;83.81=0&amp;amp;83.82=0&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;left.block=14&amp;amp;k%20blocks=15&amp;amp;folder=16&amp;amp;left.layout=17&amp;amp;polarity=18&amp;amp;left%20placement=19&amp;amp;right%20placement=20&amp;amp;result%20placement=21&amp;amp;left.left=22&amp;amp;left.left.anim=23&amp;amp;left.left.block=24&amp;amp;left.left.layout=25&amp;amp;left.left.left=26&amp;amp;left.left.left.anim=27&amp;amp;left.left.left.block=28&amp;amp;left.left.left.layout=29&amp;amp;left.left.left.left=30&amp;amp;left.left.left.left.left=31&amp;amp;left.left.left.left.left.left=32&amp;amp;left.left.left.left.left.right=33&amp;amp;left.left.left.left.left.anim=34&amp;amp;left.left.left.left.left.block=35&amp;amp;left.left.left.left.left.layout=36&amp;amp;left.left.left.left.right=37&amp;amp;left.left.left.left.anim=38&amp;amp;left.left.left.left.block=39&amp;amp;left.left.left.left.layout=40&amp;amp;left.left.left.right=41&amp;amp;left.left.right=42&amp;amp;left.right=43&amp;amp;right=44&amp;amp;anim=45&amp;amp;fuse=46&amp;amp;speed=47&amp;amp;hide%20inputs=48&amp;amp;spin=49&amp;amp;block=50&amp;amp;i%20blocks=51&amp;amp;j%20blocks=52&amp;amp;layout=53&amp;amp;scheme=54&amp;amp;gap=55&amp;amp;scatter=56&amp;amp;molecule=57&amp;amp;blast=58&amp;amp;deco=59&amp;amp;legends=60&amp;amp;shape=61&amp;amp;spotlight=62&amp;amp;row%20guides=63&amp;amp;flow%20guides=64&amp;amp;lens%20size=65&amp;amp;magnification=66&amp;amp;interior%20spotlight=67&amp;amp;axes=68&amp;amp;viz=69&amp;amp;sensitivity=70&amp;amp;min%20size=71&amp;amp;min%20light=72&amp;amp;max%20light=73&amp;amp;elem%20scale=74&amp;amp;zero%20hue=75&amp;amp;hue%20gap=76&amp;amp;hue%20spread=77&amp;amp;diag=78&amp;amp;cam=79&amp;amp;x=80&amp;amp;y=81&amp;amp;z=82&amp;amp;cam.target=83&amp;amp;compress=84&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/decoding.jpg&quot; alt=&quot;the mux/demux computations of MHA itself&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;7-lora&quot;&gt;7 LoRA&lt;/h2&gt;

&lt;p&gt;The recent LoRA paper (&lt;a href=&quot;https://arxiv.org/pdf/2106.09685.pdf&quot;&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/a&gt;) describes an efficient finetuning technique based on the idea that weight deltas introduced during finetuning are low-rank. Per the paper, this “allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers’ change during adaptation […], while keeping the pre-trained weights frozen.”&lt;/p&gt;

&lt;h3 id=&quot;7a-the-basic-idea&quot;&gt;7a The basic idea&lt;/h3&gt;

&lt;p&gt;In a nutshell, the key move is to train the &lt;em&gt;factors&lt;/em&gt; of a weight matrix rather than the matrix itself: replace an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I x J&lt;/code&gt; weights tensor with a matmul of an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I x K&lt;/code&gt; tensor and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K x J&lt;/code&gt; tensor, holding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; to some small number.&lt;/p&gt;

&lt;p&gt;If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; is small enough the size win can be huge, but the tradeoff is that lowering it lowers the rank of what the product can express. As a quick illustration of both the size savings and the structuring effect on the result, here’s a matmul of random &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;128 x 4&lt;/code&gt; left and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4 x 128&lt;/code&gt; right arguments - a.k.a. a rank-4 factorization of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;128 x 128&lt;/code&gt; matrix. Notice the vertical and horizontal patterning in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L @ R&lt;/code&gt; (&lt;a href=&quot;https://bhosmer.github.io/mm/?0=L%20%40%20R&amp;amp;1=L%20%40%20R&amp;amp;2=none&amp;amp;12=closed&amp;amp;59=true&amp;amp;3.1=L&amp;amp;3.4=false&amp;amp;3.5=128&amp;amp;3.6=4&amp;amp;3.7=gaussian&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.0=&amp;amp;3.12=open&amp;amp;13.1=R&amp;amp;13.4=false&amp;amp;13.5=4&amp;amp;13.6=128&amp;amp;13.7=gaussian&amp;amp;13.8=&amp;amp;13.9=-1&amp;amp;13.10=1&amp;amp;13.11=0&amp;amp;13.0=&amp;amp;14.15=none&amp;amp;14.16=1&amp;amp;14.17=false&amp;amp;14.18=none&amp;amp;14.19=0&amp;amp;14.20=1&amp;amp;14.21=1&amp;amp;14.22=1&amp;amp;23.20=1&amp;amp;23.22=1&amp;amp;23.21=1&amp;amp;24.25=blocks&amp;amp;24.26=11.214&amp;amp;24.27=0&amp;amp;24.28=1&amp;amp;24.29=0&amp;amp;24.30=negative&amp;amp;24.31=left&amp;amp;24.32=top&amp;amp;24.33=front&amp;amp;24.12=open&amp;amp;34.35=10&amp;amp;34.36=true&amp;amp;34.37=2&amp;amp;34.38=1&amp;amp;34.39=0&amp;amp;34.40=0.5&amp;amp;34.41=10&amp;amp;34.42=false&amp;amp;34.43=false&amp;amp;34.12=open&amp;amp;44.45=local&amp;amp;44.46=0.3&amp;amp;44.47=0.5&amp;amp;44.48=0.7&amp;amp;44.49=1.25&amp;amp;44.50=0.77&amp;amp;44.51=0.74&amp;amp;44.52=0.04&amp;amp;44.12=open&amp;amp;53.8=&amp;amp;54.55=-147.50937470998977&amp;amp;54.56=141.1312665550063&amp;amp;54.57=104.24779022699425&amp;amp;58.55=-7.97607936427614&amp;amp;58.56=3.391570600844088&amp;amp;58.57=-11.685048965074932&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;folder=12&amp;amp;right=13&amp;amp;anim=14&amp;amp;fuse=15&amp;amp;speed=16&amp;amp;hide%20inputs=17&amp;amp;alg=18&amp;amp;spin=19&amp;amp;i%20blocks=20&amp;amp;k%20blocks=21&amp;amp;j%20blocks=22&amp;amp;block=23&amp;amp;layout=24&amp;amp;scheme=25&amp;amp;gap=26&amp;amp;scatter=27&amp;amp;molecule=28&amp;amp;blast=29&amp;amp;polarity=30&amp;amp;left%20placement=31&amp;amp;right%20placement=32&amp;amp;result%20placement=33&amp;amp;deco=34&amp;amp;legends=35&amp;amp;shape=36&amp;amp;spotlight=37&amp;amp;row%20guides=38&amp;amp;flow%20guides=39&amp;amp;lens%20size=40&amp;amp;magnification=41&amp;amp;interior%20spotlight=42&amp;amp;axes=43&amp;amp;viz=44&amp;amp;sensitivity=45&amp;amp;min%20size=46&amp;amp;min%20light=47&amp;amp;max%20light=48&amp;amp;elem%20scale=49&amp;amp;zero%20hue=50&amp;amp;hue%20gap=51&amp;amp;hue%20spread=52&amp;amp;diag=53&amp;amp;cam=54&amp;amp;x=55&amp;amp;y=56&amp;amp;z=57&amp;amp;cam.target=58&amp;amp;compress=59&quot;&gt;open in mm&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inside-the-matrix/lora_single.jpg&quot; alt=&quot;a matmul of random 128 x 4 left and 4 x 128 right arguments&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;7b-applying-lora-to-an-attention-head&quot;&gt;7b Applying LoRA to an attention head&lt;/h3&gt;

&lt;p&gt;The way LoRA applies this factoring move to the fine tuning process is to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;create a low-rank factorization for each weight tensor to be fine-tuned and train the factors, keeping the original weights frozen&lt;/li&gt;
  &lt;li&gt;after fine tuning, multiply each pair of low-rank factors to get a matrix in the shape of the original weights tensor, and add it to the original pretrained weights tensor&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following visualization shows an attention head with the weight tensors &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wQ&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wK_t&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wV&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wO&lt;/code&gt; replaced by low rank factorizations &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wQ_A @ wQ_B&lt;/code&gt;, etc. Visually, the factor matrices show up as low fences along the edges of the windmill blades (&lt;a href=&quot;https://bhosmer.github.io/mm/index.html?0=out%20%3D%20(attn%20%3D%20(Q%20%3D%20input%20%40%20(wQ%20%3D%20wQ_A%20%40%20wQ_B))%20%40%20(K_t%20%3D%20(wK_t%20%3D%20wK_t_A%20%40%20wK_t_B)%20%40%20input_t))%20%40%20(V%20%3D%20input%20%40%20(wV%20%3D%20wV_A%20%40%20wV_B))%20%40%20(wO%20%3D%20wO_A%20%40%20wO_B)&amp;amp;1=out&amp;amp;2=none&amp;amp;15=closed&amp;amp;104=true&amp;amp;3.1=attn%20%40%20V&amp;amp;3.4=true&amp;amp;3.5=32&amp;amp;3.6=32&amp;amp;3.7=rows&amp;amp;3.8=&amp;amp;3.9=-1&amp;amp;3.10=1&amp;amp;3.11=0&amp;amp;3.2=none&amp;amp;12.13=inherit&amp;amp;12.14=1&amp;amp;12.15=open&amp;amp;16.17=positive&amp;amp;16.18=left&amp;amp;16.19=bottom&amp;amp;16.20=back&amp;amp;21.1=attn&amp;amp;21.4=true&amp;amp;21.2=softmax(tril(x%2Fsqrt(k)))&amp;amp;22.1=Q&amp;amp;22.4=true&amp;amp;22.2=none&amp;amp;23.1=input&amp;amp;23.4=false&amp;amp;23.5=64&amp;amp;23.6=96&amp;amp;23.7=gaussian&amp;amp;23.9=-1&amp;amp;23.10=1&amp;amp;23.11=0&amp;amp;23.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;23.0=&amp;amp;23.15=closed&amp;amp;24.1=wQ&amp;amp;24.4=true&amp;amp;24.2=none&amp;amp;25.1=wQ_A&amp;amp;25.4=false&amp;amp;25.5=96&amp;amp;25.6=8&amp;amp;25.7=gaussian&amp;amp;25.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq0_768_64.csv&amp;amp;25.9=-1&amp;amp;25.10=1&amp;amp;25.11=0&amp;amp;25.0=&amp;amp;25.15=closed&amp;amp;26.1=wQ_B&amp;amp;26.4=false&amp;amp;26.5=8&amp;amp;26.6=32&amp;amp;26.7=gaussian&amp;amp;26.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wq0_768_64.csv&amp;amp;26.9=-1&amp;amp;26.10=1&amp;amp;26.11=0&amp;amp;26.0=&amp;amp;26.15=open&amp;amp;27.13=inherit&amp;amp;27.14=1&amp;amp;27.15=closed&amp;amp;28.17=negative&amp;amp;28.18=right&amp;amp;28.19=top&amp;amp;28.20=back&amp;amp;29.30=1&amp;amp;24.15=closed&amp;amp;31.13=inherit&amp;amp;31.14=1&amp;amp;31.15=closed&amp;amp;32.17=positive&amp;amp;32.18=left&amp;amp;32.19=bottom&amp;amp;32.20=back&amp;amp;33.30=1&amp;amp;22.15=closed&amp;amp;34.2=none&amp;amp;35.13=inherit&amp;amp;35.14=1&amp;amp;35.15=closed&amp;amp;36.17=positive&amp;amp;36.18=right&amp;amp;36.19=top&amp;amp;36.20=back&amp;amp;37.1=wK_t&amp;amp;37.4=true&amp;amp;37.2=none&amp;amp;38.1=wK_t_A&amp;amp;38.4=false&amp;amp;38.5=32&amp;amp;38.6=8&amp;amp;38.7=gaussian&amp;amp;38.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t0_64_768.csv&amp;amp;38.9=-1&amp;amp;38.10=1&amp;amp;38.11=0&amp;amp;38.0=&amp;amp;38.15=open&amp;amp;39.1=wK_t_B&amp;amp;39.4=false&amp;amp;39.5=8&amp;amp;39.6=96&amp;amp;39.7=gaussian&amp;amp;39.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wk_t0_64_768.csv&amp;amp;39.9=-1&amp;amp;39.10=1&amp;amp;39.11=0&amp;amp;39.0=&amp;amp;39.15=closed&amp;amp;40.13=inherit&amp;amp;40.14=1&amp;amp;40.15=open&amp;amp;41.17=negative&amp;amp;41.18=left&amp;amp;41.19=bottom&amp;amp;41.20=back&amp;amp;42.30=1&amp;amp;37.15=closed&amp;amp;43.1=input_t&amp;amp;43.4=false&amp;amp;43.5=96&amp;amp;43.6=64&amp;amp;43.7=gaussian&amp;amp;43.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input_t0_768_256.csv&amp;amp;43.9=-1&amp;amp;43.10=1&amp;amp;43.11=0&amp;amp;43.0=&amp;amp;43.15=closed&amp;amp;34.1=K_t&amp;amp;34.4=true&amp;amp;44.30=1&amp;amp;34.15=closed&amp;amp;45.13=inherit&amp;amp;45.14=1&amp;amp;45.15=closed&amp;amp;46.17=negative&amp;amp;46.18=left&amp;amp;46.19=top&amp;amp;46.20=front&amp;amp;47.30=1&amp;amp;21.15=closed&amp;amp;48.1=V&amp;amp;48.4=true&amp;amp;48.2=none&amp;amp;49.1=input&amp;amp;49.4=false&amp;amp;49.5=64&amp;amp;49.6=96&amp;amp;49.7=gaussian&amp;amp;49.9=-1&amp;amp;49.10=1&amp;amp;49.11=0&amp;amp;49.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_input0_256_768.csv&amp;amp;49.0=&amp;amp;49.15=closed&amp;amp;50.1=wV&amp;amp;50.4=true&amp;amp;50.2=none&amp;amp;51.1=wV_A&amp;amp;51.4=false&amp;amp;51.5=96&amp;amp;51.6=8&amp;amp;51.7=gaussian&amp;amp;51.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv0_768_64.csv&amp;amp;51.9=-1&amp;amp;51.10=1&amp;amp;51.11=0&amp;amp;51.0=&amp;amp;51.15=open&amp;amp;52.1=wV_B&amp;amp;52.4=false&amp;amp;52.5=8&amp;amp;52.6=32&amp;amp;52.7=gaussian&amp;amp;52.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wv0_768_64.csv&amp;amp;52.9=-1&amp;amp;52.10=1&amp;amp;52.11=0&amp;amp;52.0=&amp;amp;52.15=open&amp;amp;53.13=inherit&amp;amp;53.14=1&amp;amp;53.15=open&amp;amp;54.17=positive&amp;amp;54.18=left&amp;amp;54.19=bottom&amp;amp;54.20=back&amp;amp;55.30=1&amp;amp;50.15=closed&amp;amp;56.13=inherit&amp;amp;56.14=1&amp;amp;56.15=closed&amp;amp;57.17=negative&amp;amp;57.18=right&amp;amp;57.19=top&amp;amp;57.20=back&amp;amp;58.30=1&amp;amp;48.15=closed&amp;amp;3.15=closed&amp;amp;59.30=1&amp;amp;60.1=wO&amp;amp;60.4=true&amp;amp;60.5=32&amp;amp;60.6=32&amp;amp;60.7=cols&amp;amp;60.8=&amp;amp;60.9=-1&amp;amp;60.10=1&amp;amp;60.11=0&amp;amp;60.2=none&amp;amp;61.1=wO_A&amp;amp;61.4=false&amp;amp;61.5=32&amp;amp;61.6=8&amp;amp;61.7=gaussian&amp;amp;61.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo0_64_768.csv&amp;amp;61.9=-1&amp;amp;61.10=1&amp;amp;61.11=0&amp;amp;61.0=&amp;amp;61.15=open&amp;amp;62.1=wO_B&amp;amp;62.4=false&amp;amp;62.5=8&amp;amp;62.6=96&amp;amp;62.7=gaussian&amp;amp;62.8=https%3A%2F%2Fraw.githubusercontent.com%2Fbhosmer%2Ftestdata%2Fmain%2Fweights%2Fgpt2%2Flayer0_wo0_64_768.csv&amp;amp;62.9=-1&amp;amp;62.10=1&amp;amp;62.11=0&amp;amp;62.0=&amp;amp;62.15=closed&amp;amp;63.13=inherit&amp;amp;63.14=1&amp;amp;63.15=closed&amp;amp;64.17=positive&amp;amp;64.18=right&amp;amp;64.19=top&amp;amp;64.20=back&amp;amp;60.15=closed&amp;amp;65.30=1&amp;amp;66.67=none&amp;amp;66.68=100&amp;amp;66.69=false&amp;amp;66.13=none&amp;amp;66.70=-3&amp;amp;66.71=1&amp;amp;66.30=1&amp;amp;66.14=1&amp;amp;66.15=open&amp;amp;72.71=1&amp;amp;72.14=1&amp;amp;72.30=1&amp;amp;73.74=blocks&amp;amp;73.75=2&amp;amp;73.76=8&amp;amp;73.77=2&amp;amp;73.78=0&amp;amp;73.17=negative&amp;amp;73.18=left&amp;amp;73.19=top&amp;amp;73.20=front&amp;amp;73.15=closed&amp;amp;79.80=6.5&amp;amp;79.81=false&amp;amp;79.82=1&amp;amp;79.83=1&amp;amp;79.84=0.757&amp;amp;79.85=0.5&amp;amp;79.86=10&amp;amp;79.87=false&amp;amp;79.88=false&amp;amp;79.15=open&amp;amp;89.90=local&amp;amp;89.91=0.2&amp;amp;89.92=0.3&amp;amp;89.93=1&amp;amp;89.94=1.25&amp;amp;89.95=0.75&amp;amp;89.96=0.75&amp;amp;89.97=0.03&amp;amp;89.15=closed&amp;amp;98.8=&amp;amp;98.15=closed&amp;amp;99.100=-172.19348886030096&amp;amp;99.101=179.87607098671913&amp;amp;99.102=291.20723943546824&amp;amp;103.100=-6.083689158200286&amp;amp;103.101=-2.2203698054118064&amp;amp;103.102=-20.406063431589725&amp;amp;expr=0&amp;amp;name=1&amp;amp;epilog=2&amp;amp;left=3&amp;amp;matmul=4&amp;amp;h=5&amp;amp;w=6&amp;amp;init=7&amp;amp;url=8&amp;amp;min=9&amp;amp;max=10&amp;amp;dropout=11&amp;amp;left.anim=12&amp;amp;alg=13&amp;amp;j%20blocks=14&amp;amp;folder=15&amp;amp;left.layout=16&amp;amp;polarity=17&amp;amp;left%20placement=18&amp;amp;right%20placement=19&amp;amp;result%20placement=20&amp;amp;left.left=21&amp;amp;left.left.left=22&amp;amp;left.left.left.left=23&amp;amp;left.left.left.right=24&amp;amp;left.left.left.right.left=25&amp;amp;left.left.left.right.right=26&amp;amp;left.left.left.right.anim=27&amp;amp;left.left.left.right.layout=28&amp;amp;left.left.left.right.block=29&amp;amp;k%20blocks=30&amp;amp;left.left.left.anim=31&amp;amp;left.left.left.layout=32&amp;amp;left.left.left.block=33&amp;amp;left.left.right=34&amp;amp;left.left.right.anim=35&amp;amp;left.left.right.layout=36&amp;amp;left.left.right.left=37&amp;amp;left.left.right.left.left=38&amp;amp;left.left.right.left.right=39&amp;amp;left.left.right.left.anim=40&amp;amp;left.left.right.left.layout=41&amp;amp;left.left.right.left.block=42&amp;amp;left.left.right.right=43&amp;amp;left.left.right.block=44&amp;amp;left.left.anim=45&amp;amp;left.left.layout=46&amp;amp;left.left.block=47&amp;amp;left.right=48&amp;amp;left.right.left=49&amp;amp;left.right.right=50&amp;amp;left.right.right.left=51&amp;amp;left.right.right.right=52&amp;amp;left.right.right.anim=53&amp;amp;left.right.right.layout=54&amp;amp;left.right.right.block=55&amp;amp;left.right.anim=56&amp;amp;left.right.layout=57&amp;amp;left.right.block=58&amp;amp;left.block=59&amp;amp;right=60&amp;amp;right.left=61&amp;amp;right.right=62&amp;amp;right.anim=63&amp;amp;right.layout=64&amp;amp;right.block=65&amp;amp;anim=66&amp;amp;fuse=67&amp;amp;speed=68&amp;amp;hide%20inputs=69&amp;amp;spin=70&amp;amp;i%20blocks=71&amp;amp;block=72&amp;amp;layout=73&amp;amp;scheme=74&amp;amp;gap=75&amp;amp;scatter=76&amp;amp;molecule=77&amp;amp;blast=78&amp;amp;deco=79&amp;amp;legends=80&amp;amp;shape=81&amp;amp;spotlight=82&amp;amp;row%20guides=83&amp;amp;flow%20guides=84&amp;amp;lens%20size=85&amp;amp;magnification=86&amp;amp;interior%20spotlight=87&amp;amp;axes=88&amp;amp;viz=89&amp;amp;sensitivity=90&amp;amp;min%20size=91&amp;amp;min%20light=92&amp;amp;max%20light=93&amp;amp;elem%20scale=94&amp;amp;zero%20hue=95&amp;amp;hue%20gap=96&amp;amp;hue%20spread=97&amp;amp;diag=98&amp;amp;cam=99&amp;amp;x=100&amp;amp;y=101&amp;amp;z=102&amp;amp;cam.target=103&amp;amp;compress=104&quot;&gt;open in mm&lt;/a&gt; - spacebar stops the spin):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/inside-the-matrix/lora_spin4b.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;h2 id=&quot;8-wrapup&quot;&gt;8 Wrapup&lt;/h2&gt;

&lt;h3 id=&quot;8a-call-for-feedback&quot;&gt;8a Call for feedback&lt;/h3&gt;

&lt;p&gt;I’ve found this way of visualizing matmul expressions extremely helpful for building intuition and reasoning about not just matrix multiplication itself, but also many aspects of ML models and their computation, from efficiency to interpretability.&lt;/p&gt;

&lt;p&gt;if you try it out and have suggestions or comments, I definitely want to hear, either in the comments here or &lt;a href=&quot;https://github.com/bhosmer/mm&quot;&gt;in the repo&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;8b-next-steps&quot;&gt;8b Next steps&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;There’s a &lt;a href=&quot;https://bhosmer.github.io/mm/examples/attngpt2/index.html&quot;&gt;GPT2 attention head explorer&lt;/a&gt; built on top of the tool which I’m currently using to inventory and classify the attention head traits found in that model. (This was the tool I used to find and explore the attention heads in this note.) Once complete I plan to post a note with the inventory.&lt;/li&gt;
  &lt;li&gt;As mentioned up top, embedding these visualizations in Python notebooks is &lt;a href=&quot;https://colab.research.google.com/drive/1wZIoU20eRWKtRNCW7e5Iugm3MhfaE1f7?usp=sharing&quot;&gt;dead simple&lt;/a&gt;. But session URLs can get… unwieldy, so it will be useful to have Python-side utilities for constructing them from configuration objects, similar to the simple JavaScript helpers used in the &lt;a href=&quot;https://bhosmer.github.io/mm/ref.html&quot;&gt;reference guide&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;If you’ve got a use case you think might benefit from visualizations like this but it’s not obvious how to use the tool to do it, get in touch! I’m not necessarily looking to expand its core visualization capabilities that much further (right tool for the job, etc.), but e.g. the API for driving it programmatically is pretty basic, there’s plenty that can be done there.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Use 3D to visualize matrix multiplication expressions, attention heads with real weights, and more.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated CPU Inference with PyTorch Inductor using torch.compile</title>
      <link href="https://pytorch.org/blog/accelerated-cpu-inference/" rel="alternate" type="text/html" title="Accelerated CPU Inference with PyTorch Inductor using torch.compile" />
      <published>2023-09-13T00:00:00-07:00</published>
      <updated>2023-09-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-cpu-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-cpu-inference/">&lt;h2 id=&quot;story-at-a-glance&quot;&gt;Story at a Glance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Although the PyTorch* Inductor C++/OpenMP* backend has enabled users to take advantage of modern CPU architectures and parallel processing, it has lacked optimizations, resulting in the backend performing worse than eager mode in terms of end-to-end performance.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Intel optimized the Inductor backend using a hybrid strategy that classified operations into two categories: Conv/GEMM and non-Conv/GEMM element-wise and reduction ops.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;For popular deep learning models, this hybrid strategy demonstrates promising performance improvements compared to eager mode and improves the C++/OpenMP backend’s efficiency and reliability for PyTorch models.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;inductor-backend-challenges&quot;&gt;Inductor Backend Challenges&lt;/h2&gt;

&lt;p&gt;The PyTorch Inductor C++/OpenMP backend enables users to take advantage of modern CPU architectures and parallel processing to accelerate computations.&lt;/p&gt;

&lt;p&gt;However, during the early stages of its development, the backend lacked some optimizations, which prevented it from fully utilizing the CPU computation capabilities. As a result, for most models the C++/OpenMP backend performed worse than eager mode in terms of end-to-end performance, with 45% of TorchBench, 100% of Hugging Face, and 75% of TIMM models performing worse than eager mode.&lt;/p&gt;

&lt;p&gt;In this post, we highlight Intel’s optimizations to the Inductor CPU backend, including the technologies and results.&lt;/p&gt;

&lt;p&gt;We optimized the backend by using a hybrid strategy that classified operations into two categories: Conv/GEMM and non-Conv/GEMM element-wise and reduction ops. Post-op fusion and weight prepacking using the oneDNN performance library were utilized to optimize the former, while explicit vectorization in C++ codegen was used to optimize the latter.&lt;/p&gt;

&lt;p&gt;This hybrid strategy demonstrated promising performance improvements compared to eager mode, particularly on popular deep learning models such as Inductor Hugging Face, Inductor TorchBench and Inductor TIMM. Overall, Intel’s optimizations improve the C++/OpenMP backend’s efficiency and reliability for PyTorch models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f1-pytorch-inference-speedup-ratio-trend-multi.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 1. Performance Speedup Ratio Trend&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Performance Speedup Ratio Trend&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;performance-status-of-intel-hybrid-optimizations&quot;&gt;Performance Status of Intel Hybrid Optimizations&lt;/h3&gt;

&lt;p&gt;Compared to eager mode with the hybrid optimizations, the C++/OpenMP backend shows promising performance improvements. We measured the performance of the three Inductor benchmark suites—TorchBench, Hugging Face, and TIMM—and the results are as follows. (&lt;em&gt;Note: we publish our performance data twice per week on &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/93531&quot;&gt;GitHub&lt;/a&gt;.&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Overall, these optimizations help to ensure that the C++/OpenMP backend provides efficient and reliable support for PyTorch models.&lt;/p&gt;

&lt;h3 id=&quot;passrate&quot;&gt;Passrate&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor | 93%, 56/60 | 96%, 44/46  | 100%, 61/61 |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;geometric-mean-speedup-single-socket-multi-threads&quot;&gt;Geometric mean speedup (Single-Socket Multi-threads)&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor |   1.39x    |    1.20x    |    1.73x    |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;individual-model-performance&quot;&gt;Individual Model Performance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f2-torchbench-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 2. TorchBench FP32 Performance (Single-Socket Multi-threads)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: TorchBench FP32 Performance (Single-Socket Multi-threads)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f3-huggingface-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 3. Hugging Face FP32 Performance (Single-Socket Multi-thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Hugging Face FP32 Performance (Single-Socket Multi-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f4-timm-fp32-performance-multithread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 4. TIMM FP32 Performance (Single-Socket Multi-threads)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: TIMM FP32 Performance (Single-Socket Multi-threads)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;geometric-mean-speedup-single-core-single-thread&quot;&gt;Geometric mean speedup (Single-core Single-thread)&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------+------------+-------------+-------------+
| Compiler | torchbench | huggingface | timm_models |
+----------+------------+-------------+-------------+
| inductor |    1.29x   |    1.15x    |    1.37x    |
+----------+------------+-------------+-------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f5-torchbench-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 5. TorchBench FP32 Performance (Single-Socket Single-thread)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: TorchBench FP32 Performance (Single-Socket Single-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f6-huggingface-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 6. Hugging Face FP32 Performance (Single-Socket Single Thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: Hugging Face FP32 Performance (Single-Socket Single Thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-cpu-inference/f7-timm-fp32-performance-single-thread.png.rendition.intel.web.1648.927.png&quot; alt=&quot;Figure 7. TIMM FP32 Performance (Single-Socket Single-thread)&quot; style=&quot;width:100%;margin-top: 3em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;: TIMM FP32 Performance (Single-Socket Single-thread)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;technical-deep-dive&quot;&gt;Technical Deep Dive&lt;/h2&gt;

&lt;p&gt;Now, let’s take a closer look at the two primary optimizations used in the Inductor C++/OpenMP backend:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;weight prepacking and post-operation fusion via oneDNN library&lt;/li&gt;
  &lt;li&gt;explicit vectorization in Inductor C++ codegen&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;weight-prepackaging--post-op-fusion-via-onednn&quot;&gt;Weight Prepackaging &amp;amp; Post-op Fusion via oneDNN&lt;/h3&gt;

&lt;p&gt;Shorthand for Intel® oneAPI Deep Neural Network Library, oneDNN library provides a range of post-op fusions (i.e., fuse convolution and matmal with its consecutive operation) that can benefit popular models. The &lt;a href=&quot;https://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; has implemented most of these fusions and has achieved significant performance improvements. As a result, we have upstreamed all of these fusions that have been applied in Intel’s PyTorch extension to Inductor, enabling a wider range of models to benefit from these optimizations. We have defined these fusions as operators under the mkldnn namespace. This allows the Python module to invoke these mkldnn operations directly.&lt;/p&gt;

&lt;p&gt;Currently, the defined fused operations are as follows. You can find these defined fused operations at &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/aten/src/ATen/native/mkldnn/RegisterMkldnnOpContextClass.cpp#L35-#L48&quot;&gt;RegisterMkldnnOpContextClass.cpp&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_linear_pointwise&lt;/code&gt;: Fuses Linear and its post-unary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_linear_pointwise.binary&lt;/code&gt;: Fuses Linear and its post-binary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_convolution_pointwise&lt;/code&gt;: Fuses Convolution and its post-unary element-wise operations&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_convolution_pointwise.binary&lt;/code&gt;: Fuses Convolution and its post-binary element-wise operations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The detailed fusion patterns are defined in the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/mkldnn.py#L774-#L818&quot;&gt;mkldnn.py&lt;/a&gt; file: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;convolution/linear + sigmoid/hardsigmoid/tanh/hardtanh/hardswish/leaky_relu/gelu/relu/relu6/siluconvolution/linear + add/add_/iadd/sub/sub_&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;On the Inductor side, we apply these fusions on the FX graph that has been lowered. We have defined &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/mkldnn.py#L491&quot;&gt;mkldnn_fuse_fx&lt;/a&gt; as the entry point to apply all the fusions. The code snippet for this is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def mkldnn_fuse_fx(gm: torch.fx.GraphModule, example_inputs):
    ...
    gm = fuse_unary(gm)
    gm = fuse_binary(gm)
    ...
    if config.cpp.weight_prepack:
        gm = pack_module(gm)
    return gm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkldnn_fuse_fx&lt;/code&gt; function, we apply fusion on the FX graph that hasn’t been lowered yet. To fuse convolution/linear and its consecutive elementwise operations, we invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fuse_unary&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fuse_binary&lt;/code&gt; as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   gm = fuse_unary(gm)
   gm = fuse_binary(gm)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to the post-op fusion, we apply weight prepacking to improve the Conv/GEMM performance further:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   gm = pack_module(gm)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Weight prepacking involves rearranging the weight tensor in a blocked layout, which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;can improve vectorization and cache reuse compared to plain formats like NCHW or NHWC and;&lt;/li&gt;
  &lt;li&gt;can help avoid weight reordering at runtime, which can reduce overhead and improve performance and;&lt;/li&gt;
  &lt;li&gt;increases memory usage as the tradeoff.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For these reasons, we provide &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config.cpp.weight_prepack&lt;/code&gt; flag in Inductor to provide users with more control over this optimization, allowing them to enable it based on their specific needs.&lt;/p&gt;

&lt;h3 id=&quot;explicit-vectorization-in-inductor-c-codegen&quot;&gt;Explicit Vectorization in Inductor C++ Codegen&lt;/h3&gt;

&lt;p&gt;Vectorization is a key optimization technique that can significantly improve the performance of numerical computations. By utilizing SIMD (Single Instruction, Multiple Data) instructions, vectorization enables multiple computations to be performed simultaneously on a single processor core, which can lead to significant performance improvements.&lt;/p&gt;

&lt;p&gt;In the Inductor C++/OpenMP backend, we use &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L372&quot;&gt;Intel® AVX2&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L359&quot;&gt;Intel® AVX-512&lt;/a&gt; ISA (Instruction Set Architecture) options for vectorization by leveraging the aten vectorization library to facilitate the implementation. Aten vectorization supports multiple platforms, including x86 and Arm, as well as multiple data types. It can be extended to support other ISAs easily by adding more &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codecache.py#L275&quot;&gt;VecISA&lt;/a&gt; sub-classes. This allows Inductor to easily support other platforms and data types in the future.&lt;/p&gt;

&lt;p&gt;Due to differences in platforms, the C++/OpenMP backend of Inductor starts by detecting the CPU features to determine the vectorization bit width at the beginning of code generation. By default, if the machine supports both AVX-512 and AVX2, the backend will choose 512-bit vectorization.&lt;/p&gt;

&lt;p&gt;If the hardware supports vectorization, the C++/OpenMP backend first detects if the loop body can be vectorized or not. There are primarily three scenarios that we are not able to generate kernel with vectorization:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Loop body lacks vector intrinsics support, e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rand&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;atomic_add&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Loop body lacks efficient vector intrinsics support, e.g., non-contiguous &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load/store&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Data types with vectorization not yet supported but work in progress, e.g., integer, double, half, and bfloat16.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To address this issue, the C++/OpenMP backend uses &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1396&quot;&gt;CppVecKernelChecker&lt;/a&gt; to detect whether all operations in a particular loop body can be vectorized or not. In general, we classified the operations into two categories by identifying if they depend on the context.&lt;/p&gt;

&lt;p&gt;For most elementwise operations such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relu&lt;/code&gt;, vectorization is straightforward, and their execution does not depend on context.&lt;/p&gt;

&lt;p&gt;However, for certain other operations, their semantics are more complex and their execution depends on context through static analysis.&lt;/p&gt;

&lt;p&gt;For example, let’s consider the where operation that takes in mask, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true_value&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false_value&lt;/code&gt; while the mask value is loaded from a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8&lt;/code&gt; tensor. The fx graph could be as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;graph():
    %ops : [#users=9] = placeholder[target=ops]
    %get_index : [#users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
    %load : [#users=1] = call_method[target=load](args = (%ops, arg1_1, %get_index), kwargs = {})
    %to_dtype : [#users=1] = call_method[target=to_dtype](args = (%ops, %load, torch.bool), kwargs = {})
    ...
    %where : [#users=1] = call_method[target=where](args = (%ops, %to_dtype, %to_dtype_2, %to_dtype_3), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Regarding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8&lt;/code&gt;, it is a general data type and could be used for computation but is not limited to being used as Boolean for mask. Hence, we need to analyze its context statically. In particular, the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1396&quot;&gt;CppVecKernelChecker&lt;/a&gt; will check whether a uint8 tensor is only used by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_dtype&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_dtype&lt;/code&gt; is only used by where. If yes, it could be vectorized. Otherwise, it will fall back to the scalar version. The generated code could be as follows:&lt;/p&gt;

&lt;p&gt;Scalar Version&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto tmp0 = in_ptr0[i1 + (17*i0)];
auto tmp3 = in_ptr1[i1 + (17*i0)];
auto tmp1 = static_cast&amp;lt;bool&amp;gt;(tmp0);
auto tmp2 = static_cast&amp;lt;float&amp;gt;(-33.0);
auto tmp4 = tmp1 ? tmp2 : tmp3;
tmp5 = std::max(tmp5, tmp4);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Vectorization Version&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;float g_tmp_buffer_in_ptr0[16] = {0};
// Convert the flag to float for vectorization. 
flag_to_float(in_ptr0 + (16*i1) + (17*i0), g_tmp_buffer_in_ptr0, 16);
auto tmp0 = at::vec::Vectorized&amp;lt;float&amp;gt;::loadu(g_tmp_buffer_in_ptr0);
auto tmp3 = at::vec::Vectorized&amp;lt;float&amp;gt;::loadu(in_ptr1 + (16*i1) + (17*i0));
auto tmp1 = (tmp0);
auto tmp2 = at::vec::Vectorized&amp;lt;float&amp;gt;(static_cast&amp;lt;float&amp;gt;(-33.0));
auto tmp4 = decltype(tmp2)::blendv(tmp3, tmp2, tmp1);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to context analysis, the C++/OpenMP backend also incorporates several other vectorization-related optimizations. These include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tiled kernel implementation for supporting transpose load - &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1211&quot;&gt;cpp.py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Data type demotion based on value range - &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_inductor/codegen/cpp.py#L1647-#L1672&quot;&gt;cpp.py&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Replacement of &lt;a href=&quot;http://github.com/shibatch/sleef/tree/e0a003ee838b75d11763aa9c3ef17bf71a725bff&quot;&gt;sleef&lt;/a&gt; implementation with oneDNN/oneMKL implementation for optimizing aten vectorization - &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/94577&quot;&gt;#94577&lt;/a&gt;, &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/92289&quot;&gt;#92289&lt;/a&gt;, &lt;a href=&quot;http://github.com/pytorch/pytorch/pull/91613&quot;&gt;#91613&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, we examined vectorization optimization in Inductor C++ backend for FP32 training and inference of 150 benchmark models with 90% of inference kernels and 71% of training kernels being vectorized.&lt;/p&gt;

&lt;p&gt;In terms of inference, a total of 28,185 CPP kernels were generated, with 25,579 (90%) of them being vectorized, while the remaining 10% were scalar. As for training, 103,084 kernels were generated, with 73,909 (71%) being vectorized and 29% not vectorized.&lt;/p&gt;

&lt;p&gt;The results indicate that &lt;strong&gt;the vectorization of inference kernels is quite impressive&lt;/strong&gt; (there is still some work to be done in training kernels since we just started to work on the training). The remaining non-vectorized kernels are analyzed in different categories, highlighting the next steps to improve vectorization coverage: index-related operations, int64 support, vertical reduction, vectorization with fallback, and more.&lt;/p&gt;

&lt;p&gt;In addition, we also optimized the C++/OpenMP backend with other optimizations like buffer-reuse and CppWrapper.&lt;/p&gt;

&lt;h4 id=&quot;future-work&quot;&gt;Future Work&lt;/h4&gt;

&lt;p&gt;The next step, we will continue optimizing the C++/OpenMP backend and extend it to support more data types as the next step. This includes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Improve vectorization coverage&lt;/li&gt;
  &lt;li&gt;Support and optimize low precision kernel including BF16, FP16, Quantization&lt;/li&gt;
  &lt;li&gt;Training optimization&lt;/li&gt;
  &lt;li&gt;Loop tiling&lt;/li&gt;
  &lt;li&gt;Autotune&lt;/li&gt;
  &lt;li&gt;Further fusion optimization of Conv/GEMM kernels.&lt;/li&gt;
  &lt;li&gt;Explore alternative codegen paths: clang/llvm/triton&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Inductor C++/OpenMP backend is a flexible and efficient backend for the CPU. This blog describes the optimizations used in the C++/OpenMP backend of Inductor for inference and training of three benchmark suites – TorchBench, Hugging&lt;/p&gt;

&lt;p&gt;Face and TIMM. The primary optimizations include weight prepacking and post-operation fusion via the oneDNN library, as well as explicit vectorization in Inductor C++ codegen using AVX2 and AVX-512 instructions.&lt;/p&gt;

&lt;p&gt;The results show that 90% of inference kernels and 71% of training kernels are vectorized, indicating impressive vectorization for inference and room for improvement in training. In addition, we also applied other optimizations like buffer-reuse and CppWrapper. And we will continuously focus on the future work mentioned above to further improve the performance.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;The results presented in this blog post are the culmination of a collaborative effort between the Intel PyTorch team and Meta. We would like to express our sincere gratitude to &lt;a href=&quot;http://dev-discuss.pytorch.org/u/jansel&quot;&gt;@jansel&lt;/a&gt;, &lt;a href=&quot;http://dev-discuss.pytorch.org/u/desertfire&quot;&gt;@desertfire&lt;/a&gt;, and &lt;a href=&quot;http://dev-discuss.pytorch.org/u/chillee&quot;&gt;@Chillee&lt;/a&gt; for their invaluable contributions and unwavering support throughout the development process. Their expertise and dedication have been instrumental in achieving the optimizations and performance improvements discussed here.&lt;/p&gt;

&lt;h3 id=&quot;configuration-details&quot;&gt;Configuration Details&lt;/h3&gt;

&lt;h4 id=&quot;hardware-details&quot;&gt;Hardware Details&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Item &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Value &lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Manufacturer 
   &lt;/td&gt;
   &lt;td&gt;
Amazon EC2 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Product Name 
   &lt;/td&gt;
   &lt;td&gt;
c6i.16xlarge 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
CPU Model 
   &lt;/td&gt;
   &lt;td&gt;
Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Installed Memory 
   &lt;/td&gt;
   &lt;td&gt;
128GB (1x128GB DDR4 3200 MT/s [Unknown]) 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
OS 
   &lt;/td&gt;
   &lt;td&gt;
Ubuntu 22.04.2 LTS 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Kernel 
   &lt;/td&gt;
   &lt;td&gt;
5.19.0-1022-aws 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Microcode 
   &lt;/td&gt;
   &lt;td&gt;
0xd000389 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
GCC 
   &lt;/td&gt;
   &lt;td&gt;
gcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
GLIBC 
   &lt;/td&gt;
   &lt;td&gt;
ldd (Ubuntu GLIBC 2.35-0ubuntu3.1) 2.35 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Binutils 
   &lt;/td&gt;
   &lt;td&gt;
GNU ld (GNU Binutils for Ubuntu) 2.38 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Python 
   &lt;/td&gt;
   &lt;td&gt;
Python 3.10.6 
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
OpenSSL 
   &lt;/td&gt;
   &lt;td&gt;
OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022) 
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;software-details&quot;&gt;Software Details&lt;/h4&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;SW&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Nightly commit&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Main commit&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Pytorch
   &lt;/td&gt;
   &lt;td&gt;
a977a12
   &lt;/td&gt;
   &lt;td&gt;
0b1b063
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Torchbench
   &lt;/td&gt;
   &lt;td&gt;
/
   &lt;/td&gt;
   &lt;td&gt;
a0848e19
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchaudio
   &lt;/td&gt;
   &lt;td&gt;
0a652f5
   &lt;/td&gt;
   &lt;td&gt;
d5b2996
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchtext
   &lt;/td&gt;
   &lt;td&gt;
c4ad5dd
   &lt;/td&gt;
   &lt;td&gt;
79100a6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchvision
   &lt;/td&gt;
   &lt;td&gt;
f2009ab
   &lt;/td&gt;
   &lt;td&gt;
b78d98b
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
torchdata
   &lt;/td&gt;
   &lt;td&gt;
5cb3e6d
   &lt;/td&gt;
   &lt;td&gt;
f2bfd3d
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
dynamo_benchmarks
   &lt;/td&gt;
   &lt;td&gt;
fea73cb
   &lt;/td&gt;
   &lt;td&gt;
/
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h4 id=&quot;configuration&quot;&gt;Configuration&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Intel OpenMP&lt;/li&gt;
  &lt;li&gt;Jemalloc - oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:-1,muzzy_decay_ms:-1&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-Socket Multi-threads:&lt;/strong&gt; #of Instances: 1; Cores/Instance: 32&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Single-Core Single-thread:&lt;/strong&gt; #of Instances: 1; Cores/Instance: 1&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Story at a Glance</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">One Year of PyTorch Foundation</title>
      <link href="https://pytorch.org/blog/one-year-pytorch/" rel="alternate" type="text/html" title="One Year of PyTorch Foundation" />
      <published>2023-09-12T00:00:00-07:00</published>
      <updated>2023-09-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/one-year-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/one-year-pytorch/">&lt;p&gt;It’s been one year since we announced the formation of the PyTorch Foundation! 🎉&lt;/p&gt;

&lt;p&gt;In its inaugural year, the PyTorch Foundation made a significant impact by launching PyTorch 2.0, growing contributors and adding new member companies. We’re grateful to our founding members for their support to move the foundation forward.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A few milestones in the past year include:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;💻 Over 600,000 repositories on GitHub&lt;br /&gt;
✅ 60% of AI implementations choosing PyTorch&lt;br /&gt;
📈 More than 20% year over year growth in new repositories&lt;br /&gt;
🤝 Over 12,000 commits since last year&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;And a look at what the foundation has been up to this past year:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-timeline.svg&quot; alt=&quot;PyTorch project timeline&quot; style=&quot;width:100%; max-width: 662px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We look forward to growing our community for the years to come through supporting our contributors, democratizing the AI field, and creating new innovations.&lt;/p&gt;

&lt;p&gt;We invite you to join us at this year’s &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference&lt;/a&gt; on October 16-17 in San Francisco. Conference registration is filling up quickly, so take advantage of your chance to be part of this exciting event.&lt;/p&gt;

&lt;p&gt;Join us to stay informed about the latest announcements and have the opportunity to connect with both the founding members and new additions to the PyTorch community.&lt;/p&gt;

&lt;p&gt;With thanks and gratitude,&lt;br /&gt;
The PyTorch Foundation Team&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">It’s been one year since we announced the formation of the PyTorch Foundation! 🎉</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Graphcore Joins the PyTorch Foundation as a General Member</title>
      <link href="https://pytorch.org/blog/graphcore-joins-pytorch/" rel="alternate" type="text/html" title="Graphcore Joins the PyTorch Foundation as a General Member" />
      <published>2023-09-06T00:00:00-07:00</published>
      <updated>2023-09-06T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/graphcore-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/graphcore-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/graphcore-logo.jpg&quot; alt=&quot;Graphcore logo&quot; style=&quot;max-width:350px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Graphcore has joined as a general member.&lt;/p&gt;

&lt;p&gt;Graphcore is a UK-based company that specializes in designing and manufacturing AI accelerators, hardware and software specifically tailored for artificial intelligence and machine learning workloads.&lt;/p&gt;

&lt;p&gt;“We’re thrilled that PyTorch is the leading framework for development on the Graphcore  platform,” said Executive Director of the PyTorch Foundation Ibrahim Haddad. “Graphcore has played  an important role in the hardware and open source space, and we look forward to their continued contributions to PyTorch.”&lt;/p&gt;

&lt;p&gt;Graphcore has contributed to the PyTorch ecosystem by developing integrations to run on their IPU hardware. These integrations enable researchers and practitioners to use their preferred frameworks while taking advantage of Graphcore’s specialized hardware.&lt;/p&gt;

&lt;p&gt;“At Graphcore we’re truly aligned with PyTorch’s objective of reducing the barrier of entry to AI practitioners. By supporting a native PyTorch software environment for IPUs we are giving developers access to new underlying hardware, designed from the ground up for AI, to help unlock new AI techniques to improve efficiency or performance and to drive breakthroughs in AI research and applications, with the same user-friendly PyTorch framework they know and expect. We look forward to contributing to and growing the global AI community as an active member of the PyTorch Foundation and are proud to be the first general member.” Anthony Barbier, Software Frameworks Lead at Graphcore.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-graphcore&quot;&gt;About Graphcore&lt;/h2&gt;

&lt;p&gt;Graphcore compute systems are accelerating the AI revolution.  Powered by the groundbreaking Intelligence Processing Unit (IPU), Graphcore delivers leading-edge AI performance with unprecedented efficiency. IPUs are used around the world by organisations building their intelligent compute capabilities, including AI-centric startups, large multinational corporations and both public and private research institutions.  Graphcore is backed by some of the world’s leading investors and has attracted more than $700m of funding. The company is based in Bristol, UK, with offices across Europe, Asia and North America.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Automated trace collection and analysis</title>
      <link href="https://pytorch.org/blog/automated-trace-collection/" rel="alternate" type="text/html" title="Automated trace collection and analysis" />
      <published>2023-09-05T00:00:00-07:00</published>
      <updated>2023-09-05T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/automated-trace-collection</id>
      <content type="html" xml:base="https://pytorch.org/blog/automated-trace-collection/">&lt;p&gt;In this blog, we share how we enabled the collection and analysis of PyTorch Profiler traces for training workloads &lt;strong&gt;without any user side code instrumentation&lt;/strong&gt;. We leveraged Dynolog - an open source daemon for CPU and GPU telemetry to collect PyTorch Profiler traces, and analyzed the collected traces using Holistic Trace Analysis - an open source library for analyzing PyTorch Profiler traces. This toolchain has allowed engineers at Meta to accelerate their performance optimization workflows. The keystone to our solution was implementing pre and post hooks for the base Optimizer class in PyTorch. We demo PyTorch trace collection using Dynolog in a short video.&lt;/p&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;Software developers at Meta run a large number of distributed training runs daily. In order to ensure that GPUs are being used effectively it is necessary to measure and analyze GPU performance for all jobs. Moreover, developers need the capability to introspect models and understand how CPUs and GPUs interact to debug performance issues. Developers build initial prototypes using a handful of GPUs and the production versions scale out to hundreds or thousands of GPUs, serving numerous business use cases such as generative AI, recommendation systems, ad ranking etc.&lt;/p&gt;

&lt;p&gt;Given the scale at Meta, it is necessary to have toolchains for performance measurement and monitoring which have low overhead and operate seamlessly with each other, to maintain high developer efficiency.&lt;/p&gt;

&lt;p&gt;In this blog, we describe how we use the PyTorch Profiler, Dynolog (a telemetry daemon) and Holistic Trace Analysis (a performance debugging library) to collect traces without any user side code instrumentation and analyze them to identify jobs with low GPU utilization.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;p&gt;The diagram below shares an overview of how the toolchain works together.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;User launches a PyTorch application.&lt;/li&gt;
  &lt;li&gt;A training service or user triggers a profiling session using the Dynolog CLI which sends a request over the network to the Dynolog daemon.&lt;/li&gt;
  &lt;li&gt;Dynolog daemon relays the profiling configuration to the PyTorch application, setting it temporarily in a profiling mode.&lt;/li&gt;
  &lt;li&gt;PyTorch Profiler collects a trace and stores it to the database (e.g., network file system or S3 bucket).&lt;/li&gt;
  &lt;li&gt;The collected traces are then analyzed using Holistic Trace Analysis (HTA).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dyno_hta.png&quot; alt=&quot;Figure 1: Dynolog, PyTorch Profiler and HTA toolchain workflow&quot; style=&quot;width:100%; max-width: 662px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;mb-3&quot; style=&quot;text-align: center&quot;&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Dynolog, PyTorch Profiler and HTA toolchain workflow&lt;/em&gt;&lt;/small&gt;
&lt;/div&gt;

&lt;p&gt;Let’s dig a bit deeper in each of the components.&lt;/p&gt;

&lt;h3 id=&quot;dynolog&quot;&gt;Dynolog&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://developers.facebook.com/blog/post/2022/11/16/dynolog-open-source-system-observability/&quot;&gt;Dynolog&lt;/a&gt; is a lightweight monitoring daemon for heterogeneous CPU-GPU systems. It supports continuous monitoring of &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/docs/Metrics.md&quot;&gt;performance metrics&lt;/a&gt; from the CPU (utilization, network bandwidth, instructions/second) and GPU (SM Occupancy, DRAM bandwidth, GPU power draw). Additionally, dynolog exports APIs to collect deep-dive profiling data that can be accessed via the dyno CLI.&lt;/p&gt;

&lt;p&gt;One of the chief integrations Dynolog offers is interfacing with the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html&quot;&gt;PyTorch Profiler&lt;/a&gt;. This enables &lt;a href=&quot;https://pytorch.org/blog/performance-debugging-of-production-pytorch-models-at-meta/&quot;&gt;on-demand remote tracing&lt;/a&gt; using a single command to trace thousands of servers. This can be accomplished by using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; command.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-profiler&quot;&gt;PyTorch Profiler&lt;/h3&gt;

&lt;p&gt;GPU kernels execute asynchronously, and GPU-side support is needed to create the trace. NVIDIA provides this visibility via the CUPTI library. Kineto is the subsystem within Profiler that interfaces with CUPTI. The &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/&quot;&gt;PyTorch Profiler&lt;/a&gt; leverages the &lt;a href=&quot;https://github.com/pytorch/kineto&quot;&gt;Kineto library&lt;/a&gt; to collect GPU traces. To enable automated profiling of training workloads at scale &lt;strong&gt;without any user side code instrumentation&lt;/strong&gt; we made a few fundamental changes to PyTorch. These changes enable trace collection without any user intervention.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Registration:** **First, we modified PyTorch to register with the Dynolog daemon on start up. This feature is switched on by setting the environment variable KINETO_USE_DAEMON=True. With this environment variable set to True, the PyTorch Profiler periodically polls Dynolog to check for on-demand tracing requests.&lt;/li&gt;
  &lt;li&gt;Iteration hooks: Then, we &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/89176&quot;&gt;implemented pre and post hooks for the base Optimizer class&lt;/a&gt;. This allowed us to annotate start/end of training iterations. The profiler is then aware of the iteration count and can safely capture a fixed number of iterations in the trace.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;holistic-trace-analysis-hta&quot;&gt;Holistic Trace Analysis (HTA)&lt;/h3&gt;

&lt;p&gt;ML researchers and engineers often struggle to computationally scale up their models as they are unaware of the performance bottlenecks in their workloads. Large distributed training jobs could generate thousands of traces, containing way too much data for a human to inspect. This is where &lt;a href=&quot;https://pytorch.org/blog/trace-analysis-for-masses/&quot;&gt;Holistic Trace Analysis&lt;/a&gt; comes in. HTA is an open source library for performance analysis - it takes as input PyTorch Profiler traces and up-levels the performance information contained in them. Its goal is to help researchers and engineers achieve the best performance from the hardware stack. To aid performance debugging HTA provides the following features (partial list):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/temporal_breakdown.html&quot;&gt;Temporal Breakdown&lt;/a&gt;: Breakdown of GPU time in terms of time spent in computation, communication, memory events, and idle time on a single node and across all ranks.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/idle_time_breakdown.html&quot;&gt;Idle Time Breakdown&lt;/a&gt;: Breakdown of GPU idle time into waiting for the host, waiting for another kernel or attributed to an unknown cause.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/kernel_breakdown.html&quot;&gt;Kernel Breakdown&lt;/a&gt;: Find kernels with the longest duration on each rank.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/kernel_breakdown.html#kernel-duration-distribution&quot;&gt;Kernel Duration Distribution&lt;/a&gt;: Distribution of average time taken by longest kernels across different ranks.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hta.readthedocs.io/en/latest/source/features/comm_comp_overlap.html&quot;&gt;Communication Computation Overlap&lt;/a&gt;: Calculate the percentage of time when communication overlaps computation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We invite you to check out these &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/tree/main/examples&quot;&gt;Jupyter notebooks&lt;/a&gt; to see what HTA can do for you. If you are a first time user we recommend starting with the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/trace_analysis_demo.ipynb&quot;&gt;trace_analysis_demo&lt;/a&gt; notebook.&lt;/p&gt;

&lt;p&gt;To summarize, Dynolog allows us to collect PyTorch Profiler traces on-the-fly in a scalable manner. Furthermore, by leveraging HTA we can automate performance analysis and identify bottlenecks. At Meta, we use the Dynolog, PyTorch Profiler and HTA toolchain to accelerate our performance optimization workflows.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;We share a screencast showcasing trace collection without any user side code instrumentation for a toy PyTorch program. The demo runs in a docker container and the trace collection is triggered using Dynolog. HTA can be used to subsequently analyze the collected trace.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FjmHYMJLIdw?si=xahelamoBIja94Ox&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;faqs&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Q. What else can &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; do for me?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace&lt;/code&gt; command supports several custom PyTorch Profiler options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;capturing python stacks&lt;/li&gt;
  &lt;li&gt;memory profiling&lt;/li&gt;
  &lt;li&gt;record input shapes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dyno gputrace --help&lt;/code&gt; for all the options.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Q. Does Dynolog collect hardware performance metrics?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Dynolog can also be used for always-on monitoring:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It incorporates out-of-box &lt;a href=&quot;https://github.com/facebookincubator/dynolog/tree/main#gpu-monitoring&quot;&gt;GPU performance monitoring&lt;/a&gt; for NVIDIA GPUs using &lt;a href=&quot;https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/index.html#&quot;&gt;DCGM&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Dynolog provides basic Linux kernel &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/docs/Metrics.md&quot;&gt;performance metrics&lt;/a&gt; including CPU, network and IO resource usage.&lt;/li&gt;
  &lt;li&gt;Dynolog manages hardware performance counters for micro-architecture specific events related to CPU Cache, TLBs etc on Intel and AMD CPUs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Q: How can I build the Docker image used in the demo?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The dockerfile is available &lt;a href=&quot;https://github.com/facebookincubator/dynolog/blob/main/dynolog_hta.dockerfile&quot;&gt;here&lt;/a&gt;. Use the command below to build the Docker image.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -f /path/to/dynolog_repo/dynolog_hta.dockerfile -t &amp;lt;image_name:tag&amp;gt; .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Q. How can I run the docker image?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You can refer to this &lt;a href=&quot;https://gist.github.com/anupambhatnagar/07ebff374bc45e4b63eb42893cca7e87&quot;&gt;cheat sheet&lt;/a&gt; to run the Docker image.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Anupam Bhatnagar, Brian Coutinho</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we share how we enabled the collection and analysis of PyTorch Profiler traces for training workloads without any user side code instrumentation. We leveraged Dynolog - an open source daemon for CPU and GPU telemetry to collect PyTorch Profiler traces, and analyzed the collected traces using Holistic Trace Analysis - an open source library for analyzing PyTorch Profiler traces. This toolchain has allowed engineers at Meta to accelerate their performance optimization workflows. The keystone to our solution was implementing pre and post hooks for the base Optimizer class in PyTorch. We demo PyTorch trace collection using Dynolog in a short video.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization</title>
      <link href="https://pytorch.org/blog/pytorch-xla-spmd/" rel="alternate" type="text/html" title="PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization" />
      <published>2023-08-31T00:00:00-07:00</published>
      <updated>2023-08-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-xla-spmd</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-xla-spmd/">&lt;p&gt;Today, we are delighted to announce PyTorch/XLA SPMD: the integration of &lt;a href=&quot;https://arxiv.org/pdf/2105.04663.pdf&quot;&gt;GSPMD&lt;/a&gt; into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; is an automatic parallelization system for ML workloads. The XLA compiler transforms the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. This allows developers to write PyTorch programs as if they are on a single large device without any custom sharded computation and/or collective communication ops to scale models.&lt;/p&gt;

&lt;p&gt;PyTorch/XLA SPMD allows PyTorch users to parallelize their ML workloads with GSPMD with less effort and with better performance. Some of the key highlights are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Better developer experience. Everything happens with a few &lt;a href=&quot;#simple-example-with-sharding-annotation&quot;&gt;sharding annotations&lt;/a&gt; from the user, and PyTorch/XLA SPMD achieves comparable performance to the most efficient PyTorch sharding implementation (see the Examples and Results section below). PyTorch/XLA SPMD separates the task of programming an ML model from the challenge of parallelization. Its automated approach to model sharding frees up the user from implementing the sharded version of ops with proper collectives in place.&lt;/li&gt;
  &lt;li&gt;A single API that enables a large variety of parallelism algorithms (including data parallelism, fully sharded data parallelism, spatial partitioning tensor and pipeline parallelism, as well as combinations of these algorithms) for different ML workloads and model architectures.&lt;/li&gt;
  &lt;li&gt;Industry-leading performance in large model training. PyTorch/XLA SPMD brings the powerful XLA GSPMD to PyTorch, enabling users to harness the full power of Google Cloud TPUs.&lt;/li&gt;
  &lt;li&gt;Enabling PyTorch and JAX developers take advantage of the same underlying XLA API to scale models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-concepts&quot;&gt;Key Concepts&lt;/h2&gt;

&lt;p&gt;The key concepts behind the sharding annotation API are: 1) Mesh, 2) Partition Spec, and 3) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API to express sharding intent using Mesh and Partition Spec. A more detailed design overview is available as a user guide &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/spmd.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mesh&quot;&gt;Mesh&lt;/h3&gt;

&lt;p&gt;For a given cluster of devices, a physical mesh is a representation of the interconnect topology.&lt;/p&gt;

&lt;p&gt;We derive a logical mesh based on this topology to create sub-groups of devices which can be used for partitioning different axes of tensors in a model. We apply sharding annotations to map the program across the logical mesh; this automatically inserts communication collectives in the program graph to support functional correctness (see the figure below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig1.png&quot; alt=&quot;SPMD on PyTorch/XLA&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We abstract logical mesh with &lt;a href=&quot;https://github.com/pytorch/xla/blob/028df4da388468fa9a41b1f98ea08bfce13b4c63/torch_xla/experimental/xla_sharding.py#L16&quot;&gt;Mesh API&lt;/a&gt;. The axes of the logical Mesh can be named. Here is an example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch_xla.runtime as xr
import torch_xla.experimental.xla_sharding as xs
from torch_xla.experimental.xla_sharding import Mesh

# Enable XLA SPMD execution mode.
xr.use_spmd()

# Assuming you are running on a TPU host that has 8 devices attached
num_devices = xr.global_runtime_device_count()
# mesh shape will be (4,2) in this example
mesh_shape = (num_devices // 2, 2)
device_ids = np.array(range(num_devices))
# axis_names 'x' nad 'y' are optional
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))

mesh.get_logical_mesh()
&amp;gt;&amp;gt; array([[0, 1],
          [2, 3],
          [4, 5],
          [6, 7]])
mesh.shape()
&amp;gt;&amp;gt; OrderedDict([('x', 4), ('y', 2)])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;partition-spec&quot;&gt;Partition Spec&lt;/h3&gt;

&lt;p&gt;partition_spec has the same rank as the input tensor. Each dimension describes how the corresponding input tensor dimension is sharded across the device mesh (logically defined by mesh_shape). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition_spec&lt;/code&gt; is a tuple of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device_mesh&lt;/code&gt; dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt;, None, or a tuple of mesh dimension indices. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt; can be an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;str&lt;/code&gt; if the corresponding mesh dimension is named. This specifies how each input rank is sharded (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mesh_shape&lt;/code&gt;) or replicated (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Provide optional mesh axis names and use them in the partition spec
mesh = Mesh(device_ids, (4, 2), ('data', 'model'))
partition_spec = ('model', 'data')
xs.mark_sharding(input_tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We support all three types of sharding described in the original &lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; paper. For instance, one can specify partial replication like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Provide optional mesh axis names and use them in the partition spec
mesh = Mesh(device_ids, (2, 2, 2), ('x', 'y', 'z'))

# evenly shard across x and z and replicate among y
partition_spec = ('x', 'z')  # equivalent to ('x', None, 'z')
xs.mark_sharding(input_tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;simple-example-with-sharding-annotation&quot;&gt;Simple Example With Sharding Annotation&lt;/h3&gt;

&lt;p&gt;Users can annotate native PyTorch tensors using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API (&lt;a href=&quot;https://github.com/pytorch/xla/blob/9a5fdf3920c18275cf7dba785193636f1b39ced9/torch_xla/experimental/xla_sharding.py#L388&quot;&gt;src&lt;/a&gt;). This takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt; as input and returns a &lt;a href=&quot;https://github.com/pytorch/xla/blob/03991d44a0a0297ced3ba9fc10ba451a4b6c94ab/torch_xla/experimental/xla_sharded_tensor.py#L55-L62&quot;&gt;XLAShardedTensor&lt;/a&gt; as output.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def mark_sharding(t: Union[torch.Tensor, XLAShardedTensor], mesh: Mesh, partition_spec: Tuple[Union[int, None]]) -&amp;gt; XLAShardedTensor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Invoking &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API takes a user defined logical &lt;a href=&quot;#mesh&quot;&gt;mesh&lt;/a&gt; and &lt;a href=&quot;#partition-spec&quot;&gt;partition_spec&lt;/a&gt; and generates a sharding annotation for the XLA compiler. The sharding specification is attached to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XLATensor&lt;/code&gt;, as well as the original input tensor. Here is a simple usage example from the [&lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt;], to illustrate how the sharding annotation API works:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch
import torch_xla.core.xla_model as xm
import torch_xla.runtime as xr
import torch_xla.experimental.xla_sharding as xs
from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
from torch_xla.experimental.xla_sharding import Mesh

# Enable XLA SPMD execution mode.
xr.use_spmd()

# Device mesh, this and partition spec as well as the input tensor shape define the individual shard shape.
num_devices = xr.global_runtime_device_count()
mesh_shape = (2, num_devicese // 2)  # 2x4 on v3-8, 2x2 on v4-8  
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))

t = torch.randn(8, 4).to(xm.xla_device())

# Mesh partitioning, each device holds 1/8-th of the input
partition_spec = (0, 1)
m1_sharded = xs.mark_sharding(t, mesh, partition_spec)
assert isinstance(m1_sharded, XLAShardedTensor) == True
# Note that the sharding annotation is also in-placed updated to t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can annotate different tensors in the PyTorch program to enable different parallelism techniques, as described in the comment below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Sharding annotate the linear layer weights. SimpleLinear() is a nn.Module.
model = SimpleLinear().to(xm.xla_device())
xs.mark_sharding(model.fc1.weight, mesh, partition_spec)

# Training loop
model.train()
for step, (data, target) in enumerate(loader):
  # Assumes `loader` returns data, target on XLA device
  optimizer.zero_grad()
  # Sharding annotate input data, we can shard any input
  # dimensions. Sharding the batch dimension enables 
  # data parallelism, sharding the feature dimension enables
  # spatial partitioning.
  xs.mark_sharding(data, mesh, partition_spec)
  ouput = model(data)
  loss = loss_fn(output, target)
  optimizer.step()
  xm.mark_step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More complete unit test cases and integration test examples are available in the PyTorch/XLA &lt;a href=&quot;https://github.com/pytorch/xla/tree/r2.0/test/spmd&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;We measured the performance of PyTorch/XLA SPMD using a GPT-2 model (&lt;a href=&quot;https://github.com/pytorch-tpu/transformers/tree/yeounoh_gpt2_spmd&quot;&gt;src&lt;/a&gt;) and compared it with &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;user-mode FSDP&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here, SPMD applies the same sharding scheme as the FSDP plot (i.e. 1D sharding). Users are expected to achieve better MFU results by exploring more advanced SPMD sharding schemes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig2.png&quot; alt=&quot;SPMD vs. FSDP&quot; style=&quot;width:100%; max-width: 600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We use Model FLOPS Utilization (MFU) as a metric for comparison. MFU is “the ratio of the observed throughput relative to the theoretical maximum throughput of a system operating at peak FLOPs” (&lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM paper&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;flops_per_step = 6 * global_batch_size * seq_len * num_params
model_flops_utilization = flops_per_step / step_time(s) / chip_count / flops_per_chip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This estimation assumes that the input dimensionality is much larger than the input sequence length (d_model » seq_len). If this assumption is violated the self-attention FLOPs start to be significant enough and this expression will underestimate the true MFU.&lt;/p&gt;

&lt;h3 id=&quot;scalability&quot;&gt;Scalability&lt;/h3&gt;

&lt;p&gt;One of the core benefits of SPMD is the flexible partitioning which can be used to save accelerator memory (HBM) usage and improve scalability. For scalability analysis, we present two studies: 1) we examine the peak HBM across 4 model sizes using Hugging Face transformers (GPT-2) as the base implementation; 2) we examine the peak HBM usage with &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus&quot;&gt;spatial partitioning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig3.png&quot; alt=&quot;Peak HBM Utilization&quot; style=&quot;width:100%; max-width: 600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above figure illustrates the unsharded 2B parameters model peak memory footprint stands at 26GB (red dashed line). harding model weights (model parallelism) reduces the peak memory footprint, and thus, enables larger model training with a given TPU pod slice. In  these experiments, we achieved up to 39.75% MFU on a 4B parameters model on Google Cloud TPU v4-16.&lt;/p&gt;

&lt;p&gt;We also ran an input batch scalability test using &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus&quot;&gt;spatial partitioning&lt;/a&gt; and a simple ResNet50 example (&lt;a href=&quot;https://github.com/pytorch/xla/blob/master/test/spmd/test_train_spmd_imagenet.py&quot;&gt;src&lt;/a&gt;) on Cloud TPU v4-8. Input batch is commonly sharded across the batch dimension for data parallelism (DDP, FSDP), but PyTorch/XLA SPMD enables input sharding across input feature dimensions for spatial sharding. As shown in the below figure, one can push the per-device batch size to 512 with spatial partitioning which is not possible with other data parallelism techniques.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig4.png&quot; alt=&quot;Batch size scaling with spatial partitioning&quot; style=&quot;width:100%; max-width: 741px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-road-forward-for-pytorchxla-spmd&quot;&gt;The Road Forward for PyTorch/XLA SPMD&lt;/h2&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. SPMD is still experimental, and we continuously add new features to it. In future releases, we plan to address async dataloading, partially replicated sharding, and other improvements. We’d love to &lt;a href=&quot;https://github.com/pytorch/xla#providing-feedback&quot;&gt;hear from you&lt;/a&gt;, answer your questions about PyTorch/XLA SPMD, and learn how you use SPMD.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yeounoh Chung, Jon Bolin, Milad Mohammadi, Jiewen Tan, Jack Cao, Joe Spisak, Alex Spiridonov, Shauheen Zahirazami, Steven Krawczyk, Wonjoo Lee Mohit Khatwani, Wanchao Liang, Vaibhav Singh</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we are delighted to announce PyTorch/XLA SPMD: the integration of GSPMD into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP</title>
      <link href="https://pytorch.org/blog/large-scale-training-hugging-face/" rel="alternate" type="text/html" title="Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP" />
      <published>2023-08-24T00:00:00-07:00</published>
      <updated>2023-08-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/large-scale-training-hugging-face</id>
      <content type="html" xml:base="https://pytorch.org/blog/large-scale-training-hugging-face/">&lt;p&gt;AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.&lt;/p&gt;

&lt;p&gt;To meet the growing demands of AI training at scale, last year we introduced &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;Fully Sharded Data Parallel (FSDP)&lt;/a&gt; in PyTorch/XLA. FSDP is a model parallelism architecture that unlocks the ability to easily and efficiently scale AI models into hundreds of billions of parameters. With &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/fsdp.md&quot;&gt;PyTorch/XLA FSDP&lt;/a&gt;, during distributed training, each device can store a specific model shard, and all-gather the full model weights when it is time to perform the forward pass. Nested FSDP further optimizes performance by only using a given layer’s full parameters during its forward pass.&lt;/p&gt;

&lt;p&gt;We are excited to announce that PyTorch/XLA FSDP has &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v4.27.0&quot;&gt;landed&lt;/a&gt; in &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Hugging Face Transformers&lt;/a&gt;. Now, Hugging Face users can train PyTorch models with up to 20 times more parameters using the same amount of computing power as before.&lt;/p&gt;

&lt;p&gt;We built PyTorch/XLA FSDP support directly into the Hugging Face Trainer class, so that any model using Trainer can leverage FSDP. And with the &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla/#fsdp-beta&quot;&gt;addition of automatic wrapping to PyTorch/XLA FSDP&lt;/a&gt;, nested FSDP wrapping is both flexible and simple to apply. These new features make it easy to train a wide range of Hugging Face models at large scales. In this guide, we demonstrate training GPT-2 models with up to 128B parameters on Google Cloud TPUs. PyTorch/XLA FSDP training on TPUs is highly efficient, achieving up to 45.1% model FLOPS utilization (MFU) for GPT-2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hugging_face_transformers.svg&quot; alt=&quot;Figure 1: Model FLOPS utilization for Hugging Face GPT-2 on Google Cloud TPU v4&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Model FLOPS utilization for Hugging Face GPT-2 on Google Cloud TPU v4&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;configuring-pytorchxla-fsdp-in-the-hugging-face-trainer&quot;&gt;Configuring PyTorch/XLA FSDP in the Hugging Face Trainer&lt;/h2&gt;

&lt;p&gt;First, follow your preferred method to create your TPU(s) and install PyTorch and PyTorch/XLA. You need versions &amp;gt;= 2.0 for PyTorch and PyTorch/XLA.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    pip3 install https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torc h-2.0-cp38-cp38-linux_x86_64.whl --user

    pip3 install https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torc h_xla-2.0-cp38-cp38-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, clone and install the Hugging Face Transformers repo. Install all necessary dependencies (e.g., datasets, evaluate, scikit-learn, accelerate).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    cd $HOME
    git clone https://github.com/huggingface/transformers.git cd transformers
    git checkout v4.31-release
    pip3 install -e .
    pip3 install datasets evaluate scikit-learn
    pip3 install accelerate==0.21.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/transformers&lt;/code&gt;, create any model-specific configuration files you might need. Here is an example of a configuration file for a GPT-2 model with 2B parameters, which we later refer to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt2_config.json&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;activation_function&quot;: &quot;gelu_new&quot;, 
    &quot;architectures&quot;: [
        &quot;GPT2LMHeadModel&quot;
    ],
    &quot;attn_pdrop&quot;: 0.1,
    &quot;bos_token_id&quot;: 50256, &quot;embd_pdrop&quot;: 0.1, &quot;eos_token_id&quot;: 50256, &quot;initializer_range&quot;: 0.02, &quot;layer_norm_epsilon&quot;: 1e-05, &quot;model_type&quot;: &quot;gpt2&quot;,
    &quot;n_embd&quot;: 3072,
    &quot;n_head&quot;: 24,
    &quot;n_layer&quot;: 18,
    &quot;n_positions&quot;: 1024,
    &quot;resid_pdrop&quot;: 0.1,
    &quot;summary_activation&quot;: null,
    &quot;summary_first_dropout&quot;: 0.1,
    &quot;summary_proj_to_labels&quot;: true,
    &quot;summary_type&quot;: &quot;cls_index&quot;,
    &quot;summary_use_proj&quot;: true,
    &quot;task_specific_params&quot;: {
        &quot;text-generation&quot;: {
            &quot;do_sample&quot;: true,
            &quot;max_length&quot;: 50
        }
    },
    &quot;vocab_size&quot;: 50257
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With PyTorch/XLA FSDP, it is possible to train model sizes much bigger than this on large accelerator slices. We have trained GPT-2 models as large as 128B parameters with these techniques; for expert tips on how to replicate this scale, see the appendix.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/transformers&lt;/code&gt;, create your FSDP configuration file, a JSON file containing all of the configurable aspects of your XLA FSDP wrapping stored as a dictionary. Following the &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;official Hugging Face Transformers XLA FSDP documentation&lt;/a&gt;, the following arguments are available to set:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla (bool, \*optional\*, defaults to False)&lt;/code&gt;: This is a boolean which determines whether or not you use XLA FSDP. Make sure to set this to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_fsdp_settings (dict, \*optional\*)&lt;/code&gt;: This is a dictionary which stores all of the XLA FSDP wrapping parameters you want to set; note that you do not have to specify settings for parameters where you are using the default value. For a complete list of settings, see &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_dtype&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buffer_dtype&lt;/code&gt;, enter these as strings which contain the corresponding torch data type, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bfloat16&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params (int, \*optional\*, defaults to 0)&lt;/code&gt;: An integer which sets the minimum number of parameters for size-based auto wrapping. Every module with at least as many parameters as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; will be XLA FSDP wrapped.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap (List[str], \*optional\*)&lt;/code&gt;: A list of (case-sensitive) transformer layer class names to wrap. Note that this is mutually exclusive with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt;. Example: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[&quot;GPT2Block&quot;, &quot;GPT2MLP&quot;]&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_fsdp_grad_ckpt (bool, \*optional\*, defaults to False)&lt;/code&gt;: This is a boolean which determines whether to use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be used when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla&lt;/code&gt; flag is set to true, and an auto wrapping policy is specified through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For transformer-based models, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; when performing automatic nested FSDP wrapping. Layers which share weights should not belong to separate FSDP wrapped units, and the input and output embedding layers in transformer-based models share weights.&lt;/p&gt;

&lt;p&gt;For this GPT-2 example, here is what the corresponding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_config.json&lt;/code&gt; file looks like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    {
        &quot;fsdp_transformer_layer_cls_to_wrap&quot;: [
            &quot;GPT2Block&quot;
        ],
        &quot;xla&quot;: true,
        &quot;xla_fsdp_settings&quot;: {
            &quot;compute_dtype&quot;: &quot;bfloat16&quot;,
            &quot;shard_param_on_dim_0&quot;: true,
            &quot;pin_layout_in_collective_ops&quot;: true
        },
       &quot;xla_fsdp_grad_ckpt&quot;: true
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Now, it’s time to train your model! First, ensure that you have your PyTorch/XLA runtime set up appropriately by setting&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    export PJRT_DEVICE=TPU
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When running training, the key flags to pass are:&lt;/p&gt;

&lt;p&gt;a) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fsdp &quot;full_shard&quot;&lt;/code&gt;
b) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fsdp_config fsdp_config.json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where you should replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_config.json&lt;/code&gt; with whatever you named your FSDP configuration file. Here is a sample command to train our example 2B GPT-2 model, where training is started by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_spawn.py&lt;/code&gt;, a &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/examples/pytorch/xla_spawn.py&quot;&gt;launcher script for&lt;/a&gt; distributed TPU training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    python3 -u examples/pytorch/xla_spawn.py --num_cores 4 examples/pytorch/language-modeling/run_clm.py \
    --num_train_epochs 1 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \ --per_device_train_batch_size 32 \ --per_device_eval_batch_size 32 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm \
    --overwrite_output_dir \
    --config_name gpt2_config.json \
    --cache_dir /tmp \
    --tokenizer_name gpt2 \
    --block_size 1024 \
    --optim adafactor \
    --adafactor true \
    --save_strategy no \
    --logging_strategy no \
    --fsdp &quot;full_shard&quot; \
    --fsdp_config fsdp_config.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;measuring-model-flops-utilization-mfu-for-gpt-2&quot;&gt;Measuring Model FLOPS Utilization (MFU) for GPT-2&lt;/h2&gt;

&lt;p&gt;Model FLOPS are the floating point operations required to perform a single forward and backward pass. Model FLOPS are hardware- and implementation- independent, and only depend on the underlying model. In each step, the number of FLOPS is computed via the following formulas:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tokens_per_batch = global_batch_size \* seq_len

FLOPS_per_step = 6 \* tokens_per_batch \* num_params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq_len&lt;/code&gt; is the sequence length and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_params&lt;/code&gt; is the number of parameters in the model. We note that this estimation assumes that the input dimensionality is much larger than the input sequence length (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d_model &amp;gt;&amp;gt; seq_len&lt;/code&gt;). If this assumption is violated the self-attention FLOPs start to be significant enough and this expression will underestimate the true MFU.&lt;/p&gt;

&lt;p&gt;Based on the step time and the hardware details (numbers of chips and the peak FLOPS per chip), we can compute Model FLOPS Utilization (MFU), which measures how effectively our implementation is using the underlying hardware. Achieving 100% MFU means that the hardware is being used perfectly by that model. We calculate MFU using the following formula:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model_FLOPS_utilization = FLOPS_per_step / step_time(s) / chip_count / FLOPS_per_chip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When training a GPT-2 model with 2B parameters with the XLA FSDP configuration file above on a Cloud TPU v4-8, we measure a step time of 4.191s. Using the above formula, we calculate 35.7% MFU on a v4-8. For further details on calculating MFU, refer to the &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The table below presents MFU for GPT-2 models with sizes between 2B and 128B, with a sequence length of 1024.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;TPU NumCores&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-8&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-64&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-128&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-128&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-256&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-512&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;# of Tokens / Batch&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;131,072&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;1,048,576&lt;/td&gt;
      &lt;td&gt;1,048,576&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;# of Parameters&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;2B&lt;/td&gt;
      &lt;td&gt;16B&lt;/td&gt;
      &lt;td&gt;20B&lt;/td&gt;
      &lt;td&gt;32B&lt;/td&gt;
      &lt;td&gt;64B&lt;/td&gt;
      &lt;td&gt;128B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Step Time (ms)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;4,191&lt;/td&gt;
      &lt;td&gt;14,592&lt;/td&gt;
      &lt;td&gt;7,824&lt;/td&gt;
      &lt;td&gt;12,970&lt;/td&gt;
      &lt;td&gt;25,653&lt;/td&gt;
      &lt;td&gt;30,460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PFLOPS / Step&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;404&lt;/td&gt;
      &lt;td&gt;809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;MFU&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;35.7%&lt;/td&gt;
      &lt;td&gt;38.8%&lt;/td&gt;
      &lt;td&gt;45.1%&lt;/td&gt;
      &lt;td&gt;44.4%&lt;/td&gt;
      &lt;td&gt;44.7%&lt;/td&gt;
      &lt;td&gt;37.7%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1: GPT-2 model FLOPS utilization calculation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Among these configurations, MFU peaks at 45.1% for the 20B parameter model on v4-128. This result compares favorably to, for example, 41.5% MFU for &lt;a href=&quot;https://arxiv.org/pdf/2205.05198.pdf&quot;&gt;a 22B Megatron-like model&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are two actionable insights from these experiments:&lt;/p&gt;

&lt;p&gt;First, simply increasing the number of chips without increasing the batch size generally means lower FLOPS utilization, because more time is spent on sharing the model shards. FSDP uses all-reduce communication collectives which are not asynchronous, which means that chip-to-chip communication cannot be overlapped with computation. As the number of chips increases, the number of model shards that must be communicated increases, and so we should expect the portion of the step time spent on communication to increase with the number of chips.&lt;/p&gt;

&lt;p&gt;Second, increasing the batch size generally means better FLOPS utilization. As the number of chips increases, the memory footprint of the model decreases, which often frees up high bandwidth memory (HBM) to scale up the global batch size. With a larger global batch size, the number of tokens processed in each step increases, and thus, so does the FLOPS per step. As long as the step time does not increase proportionally, we expect a larger global batch size to improve MFU.&lt;/p&gt;

&lt;p&gt;Therefore, to maximize the MFU, we recommend training with the largest global batch size possible that can fit in the HBM of the TPU slice, using FSDP to reduce memory required for the model parameters.&lt;/p&gt;

&lt;h2 id=&quot;training-very-large-models-tested-to-128b-parameters&quot;&gt;Training Very Large Models (tested to 128B parameters)&lt;/h2&gt;

&lt;p&gt;When using PyTorch/XLA, tensors must be initialized on the CPU before being moved to the XLA device. This means one may encounter host-side out-of-memory errors if the model is sufficiently large, even though the model can fit in the device HBM after sharding. To avoid this, we must defer each submodule’s initialization until it is FSDP wrapped, which ensures that submodules are sharded as soon as their values are populated, avoiding host-side limitations.&lt;/p&gt;

&lt;p&gt;Below, we explain how to modify a local copy of the Hugging Face transformers repository to train a GPT-2 model with up to 128B parameters using this technique.&lt;/p&gt;

&lt;p&gt;First, using the commands below, install torchdistX, which is a library containing experimental PyTorch Distributed features. This is the engine behind deferred initialization, and allows you to create tensors that don’t require immediate storage and can be materialized later. You also need to install a specific PyTorch/XLA 2.0 version that takes advantage of this package; note that you must uninstall PyTorch and PyTorch/XLA first, if you installed them earlier.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install torch==2.0 --index-url [https://download.pytorch.org/whl/test/cpu](https://download.pytorch.org/whl/test/cpu) --user
pip3 install torch_xla[torchdistx] -f https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/experimen tal/torch_xla-2.0-cp38-cp38-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, apply the following changes to your local copy of Hugging Face Transformers:&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src/transformers/trainer.py&lt;/code&gt;, add the following function in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_wrap_model&lt;/code&gt; on the line immediately prior to PyTorch/XLA FSDP wrapping:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchdistx import deferred_init

def _init_with_torchdistX(module):
    def check_fn(k):
        return not isinstance(k, FSDP)
    deferred_init.materialize_module(module, check_fn=check_fn)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;materialize_module&lt;/code&gt; will initialize the model tensors if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_fn&lt;/code&gt; returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt;. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_fn&lt;/code&gt; checks whether the module has been FSDP wrapped.&lt;/p&gt;

&lt;p&gt;Within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_wrap_model&lt;/code&gt;, modify your FSDP wrapping to accept the additional argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;param_init_fn=_init_with_torchdistX&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model = model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        auto_wrapper_callable=auto_wrapper_callable,
        param_init_fn=_init_with_torchdistX,
        \*\*fsdp_kwargs,
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;examples/pytorch/language-modeling/run_clm.py&lt;/code&gt;, add the following import statement at the beginning of the file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchdistx import deferred_init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Edit the model initialization so that the model is wrapped with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deferred_init.deferred_init&lt;/code&gt; by replacing the line&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = AutoModelForCausalLM.from_config(config)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = deferred_init.deferred_init(AutoModelForCausalLM.from_config, config)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that this assumes you are supplying your own model configuration file. Otherwise, you should modify your model initialization statement accordingly.&lt;/p&gt;

&lt;p&gt;You should also comment out these two lines which immediately follow the line above:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values()) logger.info(f&quot;Training new model from scratch - Total size={n_params/2\*\*20:.2f}M params&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;They will cause an error if left unmodified, since the model tensors do not actually have storage when these lines are executed.&lt;/p&gt;

&lt;p&gt;With these changes, you can now run GPT-2 models with as many as 128B parameters, provided the accelerator size is suitably large.&lt;/p&gt;

&lt;h2 id=&quot;next-steps--acknowledgements&quot;&gt;Next Steps &amp;amp; Acknowledgements&lt;/h2&gt;

&lt;p&gt;To learn more, the docs can be found &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;here&lt;/a&gt;. We’d love to &lt;a href=&quot;https://github.com/pytorch/xla#providing-feedback&quot;&gt;hear from you&lt;/a&gt; if you run into any issues with FSDP in PyTorch/XLA, or just want to tell us about how you are using it.&lt;/p&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; so that we can openly collaborate.&lt;/p&gt;

&lt;p&gt;We’d like to thank Ronghang Hu and Ross Girshick at Meta AI and Lysandre Debut, Sourab Mangrulkar, Sylvain Gugger and Arthur Zucker for all the support and collaboration. We’d also like to thank Jiewen Tan, Liyang Lu, Will Cromar, Vaibhav Singh, and Chandra Devarakonda for their assistance in preparing this post.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Alex Wertheim, Milad Mohammadi, Jack Cao, Alex Spiridonov, Joe Spisak, Lysandre Debut, Sylvain Gugger, Sourab Mangrulkar</name>
        
        
      </author>

      

      

      
        <summary type="html">AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.</summary>
      

      
      
    </entry>
  
</feed>


