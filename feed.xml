<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-09-03T18:01:20-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization</title>
      <link href="https://pytorch.org/blog/pytorch-xla-spmd/" rel="alternate" type="text/html" title="PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization" />
      <published>2023-08-31T00:00:00-07:00</published>
      <updated>2023-08-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-xla-spmd</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-xla-spmd/">&lt;p&gt;Today, we are delighted to announce PyTorch/XLA SPMD: the integration of &lt;a href=&quot;https://arxiv.org/pdf/2105.04663.pdf&quot;&gt;GSPMD&lt;/a&gt; into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; is an automatic parallelization system for ML workloads. The XLA compiler transforms the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. This allows developers to write PyTorch programs as if they are on a single large device without any custom sharded computation and/or collective communication ops to scale models.&lt;/p&gt;

&lt;p&gt;PyTorch/XLA SPMD allows PyTorch users to parallelize their ML workloads with GSPMD with less effort and with better performance. Some of the key highlights are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Better developer experience. Everything happens with a few &lt;a href=&quot;#simple-example-with-sharding-annotation&quot;&gt;sharding annotations&lt;/a&gt; from the user, and PyTorch/XLA SPMD achieves comparable performance to the most efficient PyTorch sharding implementation (see the Examples and Results section below). PyTorch/XLA SPMD separates the task of programming an ML model from the challenge of parallelization. Its automated approach to model sharding frees up the user from implementing the sharded version of ops with proper collectives in place.&lt;/li&gt;
  &lt;li&gt;A single API that enables a large variety of parallelism algorithms (including data parallelism, fully sharded data parallelism, spatial partitioning tensor and pipeline parallelism, as well as combinations of these algorithms) for different ML workloads and model architectures.&lt;/li&gt;
  &lt;li&gt;Industry-leading performance in large model training. PyTorch/XLA SPMD brings the powerful XLA GSPMD to PyTorch, enabling users to harness the full power of Google Cloud TPUs.&lt;/li&gt;
  &lt;li&gt;Enabling PyTorch and JAX developers take advantage of the same underlying XLA API to scale models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-concepts&quot;&gt;Key Concepts&lt;/h2&gt;

&lt;p&gt;The key concepts behind the sharding annotation API are: 1) Mesh, 2) Partition Spec, and 3) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API to express sharding intent using Mesh and Partition Spec. A more detailed design overview is available as a user guide &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/spmd.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mesh&quot;&gt;Mesh&lt;/h3&gt;

&lt;p&gt;For a given cluster of devices, a physical mesh is a representation of the interconnect topology.&lt;/p&gt;

&lt;p&gt;We derive a logical mesh based on this topology to create sub-groups of devices which can be used for partitioning different axes of tensors in a model. We apply sharding annotations to map the program across the logical mesh; this automatically inserts communication collectives in the program graph to support functional correctness (see the figure below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig1.png&quot; alt=&quot;SPMD on PyTorch/XLA&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We abstract logical mesh with &lt;a href=&quot;https://github.com/pytorch/xla/blob/028df4da388468fa9a41b1f98ea08bfce13b4c63/torch_xla/experimental/xla_sharding.py#L16&quot;&gt;Mesh API&lt;/a&gt;. The axes of the logical Mesh can be named. Here is an example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch_xla.runtime as xr
import torch_xla.experimental.xla_sharding as xs
from torch_xla.experimental.xla_sharding import Mesh

# Enable XLA SPMD execution mode.
xr.use_spmd()

# Assuming you are running on a TPU host that has 8 devices attached
num_devices = xr.global_runtime_device_count()
# mesh shape will be (4,2) in this example
mesh_shape = (num_devices // 2, 2)
device_ids = np.array(range(num_devices))
# axis_names 'x' nad 'y' are optional
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))

mesh.get_logical_mesh()
&amp;gt;&amp;gt; array([[0, 1],
          [2, 3],
          [4, 5],
          [6, 7]])
mesh.shape()
&amp;gt;&amp;gt; OrderedDict([('x', 4), ('y', 2)])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;partition-spec&quot;&gt;Partition Spec&lt;/h3&gt;

&lt;p&gt;partition_spec has the same rank as the input tensor. Each dimension describes how the corresponding input tensor dimension is sharded across the device mesh (logically defined by mesh_shape). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partition_spec&lt;/code&gt; is a tuple of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device_mesh&lt;/code&gt; dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt;, None, or a tuple of mesh dimension indices. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt; can be an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;str&lt;/code&gt; if the corresponding mesh dimension is named. This specifies how each input rank is sharded (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mesh_shape&lt;/code&gt;) or replicated (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Provide optional mesh axis names and use them in the partition spec
mesh = Mesh(device_ids, (4, 2), ('data', 'model'))
partition_spec = ('model', 'data')
xs.mark_sharding(input_tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We support all three types of sharding described in the original &lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; paper. For instance, one can specify partial replication like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Provide optional mesh axis names and use them in the partition spec
mesh = Mesh(device_ids, (2, 2, 2), ('x', 'y', 'z'))

# evenly shard across x and z and replicate among y
partition_spec = ('x', 'z')  # equivalent to ('x', None, 'z')
xs.mark_sharding(input_tensor, mesh, partition_spec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;simple-example-with-sharding-annotation&quot;&gt;Simple Example With Sharding Annotation&lt;/h3&gt;

&lt;p&gt;Users can annotate native PyTorch tensors using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API (&lt;a href=&quot;https://github.com/pytorch/xla/blob/9a5fdf3920c18275cf7dba785193636f1b39ced9/torch_xla/experimental/xla_sharding.py#L388&quot;&gt;src&lt;/a&gt;). This takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.Tensor&lt;/code&gt; as input and returns a &lt;a href=&quot;https://github.com/pytorch/xla/blob/03991d44a0a0297ced3ba9fc10ba451a4b6c94ab/torch_xla/experimental/xla_sharded_tensor.py#L55-L62&quot;&gt;XLAShardedTensor&lt;/a&gt; as output.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def mark_sharding(t: Union[torch.Tensor, XLAShardedTensor], mesh: Mesh, partition_spec: Tuple[Union[int, None]]) -&amp;gt; XLAShardedTensor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Invoking &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark_sharding&lt;/code&gt; API takes a user defined logical &lt;a href=&quot;#mesh&quot;&gt;mesh&lt;/a&gt; and &lt;a href=&quot;#partition-spec&quot;&gt;partition_spec&lt;/a&gt; and generates a sharding annotation for the XLA compiler. The sharding specification is attached to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XLATensor&lt;/code&gt;, as well as the original input tensor. Here is a simple usage example from the [&lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt;], to illustrate how the sharding annotation API works:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import torch
import torch_xla.core.xla_model as xm
import torch_xla.runtime as xr
import torch_xla.experimental.xla_sharding as xs
from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
from torch_xla.experimental.xla_sharding import Mesh

# Enable XLA SPMD execution mode.
xr.use_spmd()

# Device mesh, this and partition spec as well as the input tensor shape define the individual shard shape.
num_devices = xr.global_runtime_device_count()
mesh_shape = (2, num_devicese // 2)  # 2x4 on v3-8, 2x2 on v4-8  
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))

t = torch.randn(8, 4).to(xm.xla_device())

# Mesh partitioning, each device holds 1/8-th of the input
partition_spec = (0, 1)
m1_sharded = xs.mark_sharding(t, mesh, partition_spec)
assert isinstance(m1_sharded, XLAShardedTensor) == True
# Note that the sharding annotation is also in-placed updated to t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can annotate different tensors in the PyTorch program to enable different parallelism techniques, as described in the comment below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Sharding annotate the linear layer weights. SimpleLinear() is a nn.Module.
model = SimpleLinear().to(xm.xla_device())
xs.mark_sharding(model.fc1.weight, mesh, partition_spec)

# Training loop
model.train()
for step, (data, target) in enumerate(loader):
  # Assumes `loader` returns data, target on XLA device
  optimizer.zero_grad()
  # Sharding annotate input data, we can shard any input
  # dimensions. Sharding the batch dimension enables 
  # data parallelism, sharding the feature dimension enables
  # spatial partitioning.
  xs.mark_sharding(data, mesh, partition_spec)
  ouput = model(data)
  loss = loss_fn(output, target)
  optimizer.step()
  xm.mark_step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More complete unit test cases and integration test examples are available in the PyTorch/XLA &lt;a href=&quot;https://github.com/pytorch/xla/tree/r2.0/test/spmd&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;We measured the performance of PyTorch/XLA SPMD using a GPT-2 model (&lt;a href=&quot;https://github.com/pytorch-tpu/transformers/tree/yeounoh_gpt2_spmd&quot;&gt;src&lt;/a&gt;) and compared it with &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;user-mode FSDP&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here, SPMD applies the same sharding scheme as the FSDP plot (i.e. 1D sharding). Users are expected to achieve better MFU results by exploring more advanced SPMD sharding schemes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig2.png&quot; alt=&quot;SPMD vs. FSDP&quot; style=&quot;width:100%; max-width: 600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We use Model FLOPS Utilization (MFU) as a metric for comparison. MFU is “the ratio of the observed throughput relative to the theoretical maximum throughput of a system operating at peak FLOPs” (&lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM paper&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;flops_per_step = 6 * global_batch_size * seq_len * num_params
model_flops_utilization = flops_per_step / step_time(s) / chip_count / flops_per_chip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This estimation assumes that the input dimensionality is much larger than the input sequence length (d_model » seq_len). If this assumption is violated the self-attention FLOPs start to be significant enough and this expression will underestimate the true MFU.&lt;/p&gt;

&lt;h3 id=&quot;scalability&quot;&gt;Scalability&lt;/h3&gt;

&lt;p&gt;One of the core benefits of SPMD is the flexible partitioning which can be used to save accelerator memory (HBM) usage and improve scalability. For scalability analysis, we present two studies: 1) we examine the peak HBM across 4 model sizes using Hugging Face transformers (GPT-2) as the base implementation; 2) we examine the peak HBM usage with &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus&quot;&gt;spatial partitioning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig3.png&quot; alt=&quot;Peak HBM Utilization&quot; style=&quot;width:100%; max-width: 600px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above figure illustrates the unsharded 2B parameters model peak memory footprint stands at 26GB (red dashed line). harding model weights (model parallelism) reduces the peak memory footprint, and thus, enables larger model training with a given TPU pod slice. In  these experiments, we achieved up to 39.75% MFU on a 4B parameters model on Google Cloud TPU v4-16.&lt;/p&gt;

&lt;p&gt;We also ran an input batch scalability test using &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus&quot;&gt;spatial partitioning&lt;/a&gt; and a simple ResNet50 example (&lt;a href=&quot;https://github.com/pytorch/xla/blob/master/test/spmd/test_train_spmd_imagenet.py&quot;&gt;src&lt;/a&gt;) on Cloud TPU v4-8. Input batch is commonly sharded across the batch dimension for data parallelism (DDP, FSDP), but PyTorch/XLA SPMD enables input sharding across input feature dimensions for spatial sharding. As shown in the below figure, one can push the per-device batch size to 512 with spatial partitioning which is not possible with other data parallelism techniques.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-xla-spmd/fig4.png&quot; alt=&quot;Batch size scaling with spatial partitioning&quot; style=&quot;width:100%; max-width: 741px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-road-forward-for-pytorchxla-spmd&quot;&gt;The Road Forward for PyTorch/XLA SPMD&lt;/h2&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. SPMD is still experimental, and we continuously add new features to it. In future releases, we plan to address async dataloading, partially replicated sharding, and other improvements. We’d love to &lt;a href=&quot;https://github.com/pytorch/xla#providing-feedback&quot;&gt;hear from you&lt;/a&gt;, answer your questions about PyTorch/XLA SPMD, and learn how you use SPMD.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yeounoh Chung, Jon Bolin, Milad Mohammadi, Jiewen Tan, Jack Cao, Joe Spisak, Alex Spiridonov, Shauheen Zahirazami, Steven Krawczyk, Wonjoo Lee Mohit Khatwani, Wanchao Liang, Vaibhav Singh</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we are delighted to announce PyTorch/XLA SPMD: the integration of GSPMD into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP</title>
      <link href="https://pytorch.org/blog/large-scale-training-hugging-face/" rel="alternate" type="text/html" title="Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP" />
      <published>2023-08-24T00:00:00-07:00</published>
      <updated>2023-08-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/large-scale-training-hugging-face</id>
      <content type="html" xml:base="https://pytorch.org/blog/large-scale-training-hugging-face/">&lt;p&gt;AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.&lt;/p&gt;

&lt;p&gt;To meet the growing demands of AI training at scale, last year we introduced &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;Fully Sharded Data Parallel (FSDP)&lt;/a&gt; in PyTorch/XLA. FSDP is a model parallelism architecture that unlocks the ability to easily and efficiently scale AI models into hundreds of billions of parameters. With &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/fsdp.md&quot;&gt;PyTorch/XLA FSDP&lt;/a&gt;, during distributed training, each device can store a specific model shard, and all-gather the full model weights when it is time to perform the forward pass. Nested FSDP further optimizes performance by only using a given layer’s full parameters during its forward pass.&lt;/p&gt;

&lt;p&gt;We are excited to announce that PyTorch/XLA FSDP has &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v4.27.0&quot;&gt;landed&lt;/a&gt; in &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Hugging Face Transformers&lt;/a&gt;. Now, Hugging Face users can train PyTorch models with up to 20 times more parameters using the same amount of computing power as before.&lt;/p&gt;

&lt;p&gt;We built PyTorch/XLA FSDP support directly into the Hugging Face Trainer class, so that any model using Trainer can leverage FSDP. And with the &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla/#fsdp-beta&quot;&gt;addition of automatic wrapping to PyTorch/XLA FSDP&lt;/a&gt;, nested FSDP wrapping is both flexible and simple to apply. These new features make it easy to train a wide range of Hugging Face models at large scales. In this guide, we demonstrate training GPT-2 models with up to 128B parameters on Google Cloud TPUs. PyTorch/XLA FSDP training on TPUs is highly efficient, achieving up to 45.1% model FLOPS utilization (MFU) for GPT-2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hugging_face_transformers.svg&quot; alt=&quot;Figure 1: Model FLOPS utilization for Hugging Face GPT-2 on Google Cloud TPU v4&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Model FLOPS utilization for Hugging Face GPT-2 on Google Cloud TPU v4&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;configuring-pytorchxla-fsdp-in-the-hugging-face-trainer&quot;&gt;Configuring PyTorch/XLA FSDP in the Hugging Face Trainer&lt;/h2&gt;

&lt;p&gt;First, follow your preferred method to create your TPU(s) and install PyTorch and PyTorch/XLA. You need versions &amp;gt;= 2.0 for PyTorch and PyTorch/XLA.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    pip3 install https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torc h-2.0-cp38-cp38-linux_x86_64.whl --user

    pip3 install https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torc h_xla-2.0-cp38-cp38-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, clone and install the Hugging Face Transformers repo. Install all necessary dependencies (e.g., datasets, evaluate, scikit-learn, accelerate).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    cd $HOME
    git clone https://github.com/huggingface/transformers.git cd transformers
    git checkout v4.31-release
    pip3 install -e .
    pip3 install datasets evaluate scikit-learn
    pip3 install accelerate==0.21.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/transformers&lt;/code&gt;, create any model-specific configuration files you might need. Here is an example of a configuration file for a GPT-2 model with 2B parameters, which we later refer to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt2_config.json&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;activation_function&quot;: &quot;gelu_new&quot;, 
    &quot;architectures&quot;: [
        &quot;GPT2LMHeadModel&quot;
    ],
    &quot;attn_pdrop&quot;: 0.1,
    &quot;bos_token_id&quot;: 50256, &quot;embd_pdrop&quot;: 0.1, &quot;eos_token_id&quot;: 50256, &quot;initializer_range&quot;: 0.02, &quot;layer_norm_epsilon&quot;: 1e-05, &quot;model_type&quot;: &quot;gpt2&quot;,
    &quot;n_embd&quot;: 3072,
    &quot;n_head&quot;: 24,
    &quot;n_layer&quot;: 18,
    &quot;n_positions&quot;: 1024,
    &quot;resid_pdrop&quot;: 0.1,
    &quot;summary_activation&quot;: null,
    &quot;summary_first_dropout&quot;: 0.1,
    &quot;summary_proj_to_labels&quot;: true,
    &quot;summary_type&quot;: &quot;cls_index&quot;,
    &quot;summary_use_proj&quot;: true,
    &quot;task_specific_params&quot;: {
        &quot;text-generation&quot;: {
            &quot;do_sample&quot;: true,
            &quot;max_length&quot;: 50
        }
    },
    &quot;vocab_size&quot;: 50257
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With PyTorch/XLA FSDP, it is possible to train model sizes much bigger than this on large accelerator slices. We have trained GPT-2 models as large as 128B parameters with these techniques; for expert tips on how to replicate this scale, see the appendix.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/transformers&lt;/code&gt;, create your FSDP configuration file, a JSON file containing all of the configurable aspects of your XLA FSDP wrapping stored as a dictionary. Following the &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;official Hugging Face Transformers XLA FSDP documentation&lt;/a&gt;, the following arguments are available to set:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla (bool, \*optional\*, defaults to False)&lt;/code&gt;: This is a boolean which determines whether or not you use XLA FSDP. Make sure to set this to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_fsdp_settings (dict, \*optional\*)&lt;/code&gt;: This is a dictionary which stores all of the XLA FSDP wrapping parameters you want to set; note that you do not have to specify settings for parameters where you are using the default value. For a complete list of settings, see &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_dtype&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buffer_dtype&lt;/code&gt;, enter these as strings which contain the corresponding torch data type, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bfloat16&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params (int, \*optional\*, defaults to 0)&lt;/code&gt;: An integer which sets the minimum number of parameters for size-based auto wrapping. Every module with at least as many parameters as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; will be XLA FSDP wrapped.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap (List[str], \*optional\*)&lt;/code&gt;: A list of (case-sensitive) transformer layer class names to wrap. Note that this is mutually exclusive with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt;. Example: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[&quot;GPT2Block&quot;, &quot;GPT2MLP&quot;]&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_fsdp_grad_ckpt (bool, \*optional\*, defaults to False)&lt;/code&gt;: This is a boolean which determines whether to use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be used when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla&lt;/code&gt; flag is set to true, and an auto wrapping policy is specified through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For transformer-based models, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_transformer_layer_cls_to_wrap&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_min_num_params&lt;/code&gt; when performing automatic nested FSDP wrapping. Layers which share weights should not belong to separate FSDP wrapped units, and the input and output embedding layers in transformer-based models share weights.&lt;/p&gt;

&lt;p&gt;For this GPT-2 example, here is what the corresponding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_config.json&lt;/code&gt; file looks like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    {
        &quot;fsdp_transformer_layer_cls_to_wrap&quot;: [
            &quot;GPT2Block&quot;
        ],
        &quot;xla&quot;: true,
        &quot;xla_fsdp_settings&quot;: {
            &quot;compute_dtype&quot;: &quot;bfloat16&quot;,
            &quot;shard_param_on_dim_0&quot;: true,
            &quot;pin_layout_in_collective_ops&quot;: true
        },
       &quot;xla_fsdp_grad_ckpt&quot;: true
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Now, it’s time to train your model! First, ensure that you have your PyTorch/XLA runtime set up appropriately by setting&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    export PJRT_DEVICE=TPU
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When running training, the key flags to pass are:&lt;/p&gt;

&lt;p&gt;a) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fsdp &quot;full_shard&quot;&lt;/code&gt;
b) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fsdp_config fsdp_config.json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where you should replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsdp_config.json&lt;/code&gt; with whatever you named your FSDP configuration file. Here is a sample command to train our example 2B GPT-2 model, where training is started by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xla_spawn.py&lt;/code&gt;, a &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/examples/pytorch/xla_spawn.py&quot;&gt;launcher script for&lt;/a&gt; distributed TPU training.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    python3 -u examples/pytorch/xla_spawn.py --num_cores 4 examples/pytorch/language-modeling/run_clm.py \
    --num_train_epochs 1 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \ --per_device_train_batch_size 32 \ --per_device_eval_batch_size 32 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm \
    --overwrite_output_dir \
    --config_name gpt2_config.json \
    --cache_dir /tmp \
    --tokenizer_name gpt2 \
    --block_size 1024 \
    --optim adafactor \
    --adafactor true \
    --save_strategy no \
    --logging_strategy no \
    --fsdp &quot;full_shard&quot; \
    --fsdp_config fsdp_config.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;measuring-model-flops-utilization-mfu-for-gpt-2&quot;&gt;Measuring Model FLOPS Utilization (MFU) for GPT-2&lt;/h2&gt;

&lt;p&gt;Model FLOPS are the floating point operations required to perform a single forward and backward pass. Model FLOPS are hardware- and implementation- independent, and only depend on the underlying model. In each step, the number of FLOPS is computed via the following formulas:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tokens_per_batch = global_batch_size \* seq_len

FLOPS_per_step = 6 \* tokens_per_batch \* num_params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq_len&lt;/code&gt; is the sequence length and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_params&lt;/code&gt; is the number of parameters in the model. We note that this estimation assumes that the input dimensionality is much larger than the input sequence length (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d_model &amp;gt;&amp;gt; seq_len&lt;/code&gt;). If this assumption is violated the self-attention FLOPs start to be significant enough and this expression will underestimate the true MFU.&lt;/p&gt;

&lt;p&gt;Based on the step time and the hardware details (numbers of chips and the peak FLOPS per chip), we can compute Model FLOPS Utilization (MFU), which measures how effectively our implementation is using the underlying hardware. Achieving 100% MFU means that the hardware is being used perfectly by that model. We calculate MFU using the following formula:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model_FLOPS_utilization = FLOPS_per_step / step_time(s) / chip_count / FLOPS_per_chip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When training a GPT-2 model with 2B parameters with the XLA FSDP configuration file above on a Cloud TPU v4-8, we measure a step time of 4.191s. Using the above formula, we calculate 35.7% MFU on a v4-8. For further details on calculating MFU, refer to the &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The table below presents MFU for GPT-2 models with sizes between 2B and 128B, with a sequence length of 1024.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;TPU NumCores&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-8&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-64&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-128&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-128&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-256&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;v4-512&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;# of Tokens / Batch&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;131,072&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;524,288&lt;/td&gt;
      &lt;td&gt;1,048,576&lt;/td&gt;
      &lt;td&gt;1,048,576&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;# of Parameters&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;2B&lt;/td&gt;
      &lt;td&gt;16B&lt;/td&gt;
      &lt;td&gt;20B&lt;/td&gt;
      &lt;td&gt;32B&lt;/td&gt;
      &lt;td&gt;64B&lt;/td&gt;
      &lt;td&gt;128B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Step Time (ms)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;4,191&lt;/td&gt;
      &lt;td&gt;14,592&lt;/td&gt;
      &lt;td&gt;7,824&lt;/td&gt;
      &lt;td&gt;12,970&lt;/td&gt;
      &lt;td&gt;25,653&lt;/td&gt;
      &lt;td&gt;30,460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PFLOPS / Step&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1.65&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;62&lt;/td&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;404&lt;/td&gt;
      &lt;td&gt;809&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;MFU&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;35.7%&lt;/td&gt;
      &lt;td&gt;38.8%&lt;/td&gt;
      &lt;td&gt;45.1%&lt;/td&gt;
      &lt;td&gt;44.4%&lt;/td&gt;
      &lt;td&gt;44.7%&lt;/td&gt;
      &lt;td&gt;37.7%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1: GPT-2 model FLOPS utilization calculation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Among these configurations, MFU peaks at 45.1% for the 20B parameter model on v4-128. This result compares favorably to, for example, 41.5% MFU for &lt;a href=&quot;https://arxiv.org/pdf/2205.05198.pdf&quot;&gt;a 22B Megatron-like model&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are two actionable insights from these experiments:&lt;/p&gt;

&lt;p&gt;First, simply increasing the number of chips without increasing the batch size generally means lower FLOPS utilization, because more time is spent on sharing the model shards. FSDP uses all-reduce communication collectives which are not asynchronous, which means that chip-to-chip communication cannot be overlapped with computation. As the number of chips increases, the number of model shards that must be communicated increases, and so we should expect the portion of the step time spent on communication to increase with the number of chips.&lt;/p&gt;

&lt;p&gt;Second, increasing the batch size generally means better FLOPS utilization. As the number of chips increases, the memory footprint of the model decreases, which often frees up high bandwidth memory (HBM) to scale up the global batch size. With a larger global batch size, the number of tokens processed in each step increases, and thus, so does the FLOPS per step. As long as the step time does not increase proportionally, we expect a larger global batch size to improve MFU.&lt;/p&gt;

&lt;p&gt;Therefore, to maximize the MFU, we recommend training with the largest global batch size possible that can fit in the HBM of the TPU slice, using FSDP to reduce memory required for the model parameters.&lt;/p&gt;

&lt;h2 id=&quot;training-very-large-models-tested-to-128b-parameters&quot;&gt;Training Very Large Models (tested to 128B parameters)&lt;/h2&gt;

&lt;p&gt;When using PyTorch/XLA, tensors must be initialized on the CPU before being moved to the XLA device. This means one may encounter host-side out-of-memory errors if the model is sufficiently large, even though the model can fit in the device HBM after sharding. To avoid this, we must defer each submodule’s initialization until it is FSDP wrapped, which ensures that submodules are sharded as soon as their values are populated, avoiding host-side limitations.&lt;/p&gt;

&lt;p&gt;Below, we explain how to modify a local copy of the Hugging Face transformers repository to train a GPT-2 model with up to 128B parameters using this technique.&lt;/p&gt;

&lt;p&gt;First, using the commands below, install torchdistX, which is a library containing experimental PyTorch Distributed features. This is the engine behind deferred initialization, and allows you to create tensors that don’t require immediate storage and can be materialized later. You also need to install a specific PyTorch/XLA 2.0 version that takes advantage of this package; note that you must uninstall PyTorch and PyTorch/XLA first, if you installed them earlier.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install torch==2.0 --index-url [https://download.pytorch.org/whl/test/cpu](https://download.pytorch.org/whl/test/cpu) --user
pip3 install torch_xla[torchdistx] -f https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/experimen tal/torch_xla-2.0-cp38-cp38-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, apply the following changes to your local copy of Hugging Face Transformers:&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src/transformers/trainer.py&lt;/code&gt;, add the following function in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_wrap_model&lt;/code&gt; on the line immediately prior to PyTorch/XLA FSDP wrapping:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchdistx import deferred_init

def _init_with_torchdistX(module):
    def check_fn(k):
        return not isinstance(k, FSDP)
    deferred_init.materialize_module(module, check_fn=check_fn)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;materialize_module&lt;/code&gt; will initialize the model tensors if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_fn&lt;/code&gt; returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt;. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_fn&lt;/code&gt; checks whether the module has been FSDP wrapped.&lt;/p&gt;

&lt;p&gt;Within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_wrap_model&lt;/code&gt;, modify your FSDP wrapping to accept the additional argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;param_init_fn=_init_with_torchdistX&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model = model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        auto_wrapper_callable=auto_wrapper_callable,
        param_init_fn=_init_with_torchdistX,
        \*\*fsdp_kwargs,
    )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;examples/pytorch/language-modeling/run_clm.py&lt;/code&gt;, add the following import statement at the beginning of the file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchdistx import deferred_init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Edit the model initialization so that the model is wrapped with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deferred_init.deferred_init&lt;/code&gt; by replacing the line&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = AutoModelForCausalLM.from_config(config)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = deferred_init.deferred_init(AutoModelForCausalLM.from_config, config)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that this assumes you are supplying your own model configuration file. Otherwise, you should modify your model initialization statement accordingly.&lt;/p&gt;

&lt;p&gt;You should also comment out these two lines which immediately follow the line above:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values()) logger.info(f&quot;Training new model from scratch - Total size={n_params/2\*\*20:.2f}M params&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;They will cause an error if left unmodified, since the model tensors do not actually have storage when these lines are executed.&lt;/p&gt;

&lt;p&gt;With these changes, you can now run GPT-2 models with as many as 128B parameters, provided the accelerator size is suitably large.&lt;/p&gt;

&lt;h2 id=&quot;next-steps--acknowledgements&quot;&gt;Next Steps &amp;amp; Acknowledgements&lt;/h2&gt;

&lt;p&gt;To learn more, the docs can be found &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;here&lt;/a&gt;. We’d love to &lt;a href=&quot;https://github.com/pytorch/xla#providing-feedback&quot;&gt;hear from you&lt;/a&gt; if you run into any issues with FSDP in PyTorch/XLA, or just want to tell us about how you are using it.&lt;/p&gt;

&lt;p&gt;We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; so that we can openly collaborate.&lt;/p&gt;

&lt;p&gt;We’d like to thank Ronghang Hu and Ross Girshick at Meta AI and Lysandre Debut, Sourab Mangrulkar, Sylvain Gugger and Arthur Zucker for all the support and collaboration. We’d also like to thank Jiewen Tan, Liyang Lu, Will Cromar, Vaibhav Singh, and Chandra Devarakonda for their assistance in preparing this post.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Alex Wertheim, Milad Mohammadi, Jack Cao, Alex Spiridonov, Joe Spisak, Lysandre Debut, Sylvain Gugger, Sourab Mangrulkar</name>
        
        
      </author>

      

      

      
        <summary type="html">AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Intel Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/intel-joins-pytorch/" rel="alternate" type="text/html" title="Intel Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-08-10T00:00:00-07:00</published>
      <updated>2023-08-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/intel-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/intel-joins-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/intel-new-logo.svg&quot; alt=&quot;Intel logo&quot; style=&quot;max-width:250px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Intel has joined as a premier member.&lt;/p&gt;

&lt;p&gt;“The PyTorch Foundation is thrilled to welcome Intel as a premier member, marking a significant milestone in our mission to empower the global AI community. Intel’s extensive expertise and commitment to advancing cutting-edge technologies align perfectly with our vision of fostering open-source innovation,” said PyTorch Foundation Executive Director Ibrahim Haddad. “Together, we will accelerate the development and democratization of PyTorch, and use the collaboration to shape a vibrant future of AI for all.”&lt;/p&gt;

&lt;p&gt;Intel has developed and released several PyTorch-based tools and libraries to enable developers to accelerate their AI workflows, and is actively working on optimizing PyTorch to leverage Intel hardware capabilities.&lt;/p&gt;

&lt;p&gt;“At Intel, we believe in the power of collaboration and open-source innovation to propel the ecosystem towards an AI Everywhere future. Joining the Governing Board of the PyTorch Foundation is a testament to Intel’s commitment to advancing and democratizing AI,” said Wei Li, Vice President and General Manager of Artificial Intelligence and Analytics (AIA) at Intel. “By harnessing the collective expertise and resources within the deep learning community, we aim to accelerate the development of PyTorch and continue to drive breakthroughs in AI research and applications.”&lt;/p&gt;

&lt;p&gt;Intel fosters industry collaboration, co-engineering, and open source contributions to accelerate software innovation and develop new technologies that bring benefits to the open source community. By working together with other member companies and under the guidance of the PyTorch Foundation, Intel remains committed to actively contributing to and advocating for the community.&lt;/p&gt;

&lt;p&gt;As a premier member, Intel is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/wei-li.jpg&quot; alt=&quot;Wei Li&quot; style=&quot;max-width:250px;float:right;margin: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Wei Li, Vice President and General Manager of Artificial Intelligence and Analytics (AIA)  at Intel, to our board.  Dr. Wei Li is Vice President and General Manager of Artificial Intelligence and Analytics (AIA) at Intel, where he leads a world-wide team of engineering “magicians” who make AI Everywhere a reality by supercharging machine performance and developer productivity.  Wei and his team have been instrumental in Intel’s recent multi-billion-dollar AI revenue growth by delivering 10-100X software acceleration, across deep learning, statistical machine learning and big data analytics, to complement Intel’s AI-optimized hardware portfolio.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read more about Intel’s commitment to the PyTorch Community &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/ai-everywhere-intel-joins-pytorch-foundation.html#gs.4984sj&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-intel&quot;&gt;About Intel&lt;/h2&gt;

&lt;p&gt;Intel (Nasdaq: INTC) is an industry leader, creating world-changing technology that enables global progress and enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and manufacturing of semiconductors to help address our customers’ greatest challenges. By embedding intelligence in the cloud, network, edge and every kind of computing device, we unleash the potential of data to transform business and society for the better. To learn more about Intel’s innovations, go to&lt;a href=&quot;https://newsroom.intel.com/&quot;&gt; newsroom.intel.com&lt;/a&gt; and &lt;a href=&quot;https://intel.com/&quot;&gt;intel.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;© Intel Corporation. Intel, the Intel logo and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its &lt;a href=&quot;https://www.linuxfoundation.org/legal/trademark-usage&quot;&gt;trademark usage page&lt;/a&gt;. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">INT8 Quantization for x86 CPU in PyTorch</title>
      <link href="https://pytorch.org/blog/int8-quantization/" rel="alternate" type="text/html" title="INT8 Quantization for x86 CPU in PyTorch" />
      <published>2023-08-07T00:00:00-07:00</published>
      <updated>2023-08-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/int8-quantization</id>
      <content type="html" xml:base="https://pytorch.org/blog/int8-quantization/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;INT8 quantization is a powerful technique for speeding up deep learning inference on x86 CPU platforms. By reducing the precision of the model’s weights and activations from 32-bit floating-point (FP32) to 8-bit integer (INT8), INT8 quantization can significantly improve the inference speed and reduce memory requirements without sacrificing accuracy.&lt;/p&gt;

&lt;p&gt;In this blog, we will discuss the recent progress on INT8 quantization for x86 CPU in PyTorch, focusing on the new x86 quantization backend. We will also briefly look at the new quantization path with PyTorch 2.0 Export (PT2E) and TorchInductor.&lt;/p&gt;

&lt;h2 id=&quot;x86-quantization-backend&quot;&gt;X86 Quantization Backend&lt;/h2&gt;

&lt;p&gt;The current recommended way of quantization in PyTorch is &lt;a href=&quot;http://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html?highlight=fx&quot;&gt;FX&lt;/a&gt;. Before PyTorch 2.0, the default quantization backend (a.k.a. QEngine) on x86 CPUs was FBGEMM, which leveraged the FBGEMM performance library to achieve the performance speedup. In the PyTorch 2.0 release, a new quantization backend called X86 was introduced to replace FBGEMM. The x86 quantization backend offers improved INT8 inference performance when compared to the original FBGEMM backend by leveraging the strengths of both FBGEMM and the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/onednn.html&quot;&gt;Intel® oneAPI Deep Neural Network Library (oneDNN)&lt;/a&gt; kernel libraries.&lt;/p&gt;

&lt;h2 id=&quot;performance-benefit-from-x86-backend&quot;&gt;Performance Benefit from X86 Backend&lt;/h2&gt;

&lt;p&gt;To measure the performance benefits of the new X86 backend, we ran INT8 inference on 69 popular deep learning models (shown in &lt;strong&gt;Figures 1-3&lt;/strong&gt; below) using &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;4th Gen Intel® Xeon® Scalable processors&lt;/a&gt;. The results showed a 2.97X geomean performance speedup compared to FP32 inference performance, while the speedup was 1.43X with the FBGEMM backend. The charts below show the per-model performance speedup comparing the x86 backend and the FBGEMM backend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_1.jpg&quot; alt=&quot;Figure 1: Models with less than 2x performance boost with x86 backend1&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Models with less than 2x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_2.jpg&quot; alt=&quot;Figure 2: Models with 2x-4x performance boost with x86 backend1&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Models with 2x-4x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_3.jpg&quot; alt=&quot;Figure 3: Models with larger than 4x performance boost with x86 backend1&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Models with larger than 4x performance boost with x86 backend1&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;usage-of-x86-backend&quot;&gt;Usage of x86 Backend&lt;/h2&gt;

&lt;p&gt;By default in 2.0, users on x86 platforms will use the x86 quantization backend and their PyTorch programs will remain unchanged when using the default backend. Alternatively, users can specify x86 as the quantization backend explicitly. &lt;br /&gt;
Below is an example code snippet of PyTorch static post-training quantization with x86 quantization backend.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torch.ao.quantization import get_default_qconfig_mapping
from torch.quantization.quantize_fx import prepare_fx, convert_fx

qconfig_mapping = get_default_qconfig_mapping()
# Or explicity specify the qengine
# qengine = 'x86'
# torch.backends.quantized.engine = qengine
# qconfig_mapping = get_default_qconfig_mapping(qengine)

model_fp32 = MyModel().eval()
x = torch.randn((1, 3, 224, 224), dtype=torch.float)
x = x.to(memory_format=torch.channels_last)

# Insert observers according to qconfig and backend config
prepared_model = prepare_fx(model_fp32, qconfig_mapping, example_inputs=x)

# Calibration code not shown

# Convert to quantized model
quantized_model = convert_fx(prepared_model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;technical-details-of-x86-backend&quot;&gt;Technical Details of x86 Backend&lt;/h2&gt;

&lt;p&gt;We devised heuristic dispatching rules according to the performance numbers from the models we benchmarked to decide whether to invoke oneDNN or FBGEMM performance library to execute the convolution or matrix multiplication operations. The rules are a combination of operation kinds, shapes, CPU architecture information, etc. Detailed logic is available &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/93ff71ec37e3c946603600a46edef70b42f81213/aten/src/ATen/native/quantized/cpu/OnednnUtils.h#L396&quot;&gt;here&lt;/a&gt;. For more design and technical discussion, please refer to the &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;Request for Comments&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps-with-a-new-quantization-path-pytorch-20-export&quot;&gt;Next Steps With a New Quantization Path PyTorch 2.0 Export&lt;/h2&gt;

&lt;p&gt;Although still far from finalized, a new quantization path, PyTorch 2.0 Export (PT2E), is in early design and PoC stage. The new approach is slated to replace the FX quantization path in the future. It is built upon the capabilities of TorchDynamo Export, a feature introduced in the PyTorch 2.0 release for FX graph capturing. This graph is then quantized and lowered to different backends. TorchInductor, the new DL compiler of PyTorch, has shown promising results in terms of FP32 inference speedup on x86 CPU. We are working actively to enable it as one of the quantization backends of PT2E. We believe the new path will lead to further improvements in INT8 inference performance due to more flexibility of fusion at different levels.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The x86 backend introduced in PyTorch 2.0 release has demonstrated a remarkable improvement in INT8 inference speed on x86 CPU platforms. It offers a 1.43X speedup compared to the original FBGEMM backend while maintaining backward compatibility. This enhancement can benefit end users with minimal or no modifications to their programs. Furthermore, a new quantization path, PT2E, is currently in development and is expected to provide even more possibilities in the future.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;Special thanks to Nikita Shulga, Vasiliy Kuznetsov, Supriya Rao, and Jongsoo Park. Together, we made one more step forward on the path of improving the PyTorch CPU ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; AWS EC2 r7iz.metal-16xl instance (Intel(R) Xeon(R) Gold 6455B, 32-core/64-thread, Turbo Boost On, Hyper-Threading On, Memory: 8x64GB, Storage: 192GB); OS: Ubuntu 22.04.1 LTS; Kernel: 5.15.0-1028-aws; Batch Size: 1; Core per Instance: 4; PyTorch 2.0 RC3; TorchVision 0.15.0+cpu, test by Intel on 3/77/2023. May not reflect all publicly available security updates.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Hugging Face Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/hugging-face-joins/" rel="alternate" type="text/html" title="Hugging Face Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-08-03T00:00:00-07:00</published>
      <updated>2023-08-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hugging-face-joins</id>
      <content type="html" xml:base="https://pytorch.org/blog/hugging-face-joins/">&lt;p&gt;&lt;img src=&quot;/assets/images/huggingface-joins-1.jpg&quot; alt=&quot;Smiling hugging face&quot; style=&quot;max-width:250px;float:right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Hugging Face has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Hugging Face has been a long time supporter and contributor to the PyTorch Ecosystem by providing powerful models and resources that accelerate research, development, and adoption of AI technologies, particularly in the field of natural language processing.&lt;/p&gt;

&lt;p&gt;“Our mission has always been to democratize AI and make it accessible to everyone. We’re truly aligned with PyTorch’s objective of reducing the barrier of entry to practitioners. By joining the PyTorch Foundation, we can further amplify that impact and support this very important framework of the ecosystem that is PyTorch,” said Lysandre Debut, Head of Open Source at Hugging Face. “We believe the two ecosystems have significant overlap, and collaborating with the foundation will allow us to bridge the gap to provide the best software, the best tools to the machine learning community at large.”&lt;/p&gt;

&lt;p&gt;Hugging Face’s Model Hub and open source libraries promote collaboration and knowledge sharing within the AI open source community, making Hugging Face a great match to the growing PyTorch Foundation. They continue to drive industry adoption and collaboration by creating user-friendly tools and resources and providing accessible and well-documented libraries.&lt;/p&gt;

&lt;p&gt;“Hugging Face’s commitment to open source development and their exceptional contributions to the PyTorch ecosystem have truly impressed us. With their help, we will drive innovation, foster collaboration, and empower the global AI community to create transformative solutions for the AI community,” said PyTorch Foundation Executive Director Ibrahim Haddad. “We welcome Hugging Face to the PyTorch Foundation and look forward to the achievements that lie ahead.”&lt;/p&gt;

&lt;p&gt;As a premier member, Hugging Face is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/huggingface-joins-2.jpg&quot; alt=&quot;Lysandre Debut&quot; style=&quot;max-width:250px;float:right;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Lysandre Debut, Head of Open Source at Hugging Face to our board.  Lysandre has been at Hugging Face since the company’s pivot to open-source, and was the first engineer to focus entirely on the open-source mission. Now leading the open-source part of the organization, Lysandre remains technically involved by being a core maintainer of the Transformers library.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-hugging-face&quot;&gt;About Hugging Face&lt;/h2&gt;

&lt;p&gt;Hugging Face is a community and company dedicated to lowering the barrier of entry to Machine Learning and Deep Learning. Strong advocates for open-source and open-science, their model Hub hosts more than 250,000 public models and 50,000 public datasets that are very simple to use. Transformers, Diffusers, PEFT, Accelerate, and Datasets are some of the open-source tools made available by Hugging Face.&lt;/p&gt;

&lt;h2 id=&quot;about--pytorch-foundation&quot;&gt;About  PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its trademark usage page: www.linuxfoundation.org/trademark-usage. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AMD’s Journey to Openness and Performance</title>
      <link href="https://pytorch.org/blog/amd-journey/" rel="alternate" type="text/html" title="AMD's Journey to Openness and Performance" />
      <published>2023-08-01T00:00:00-07:00</published>
      <updated>2023-08-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/amd-journey</id>
      <content type="html" xml:base="https://pytorch.org/blog/amd-journey/">&lt;p&gt;AMD has gained progress in building a robust software stack that supports an open ecosystem of models, libraries, frameworks, and tools. With proven platforms gaining momentum, there is significance of a leadership software stack and an optimized ecosystem for achieving application performance. PyTorch is a key part of AMD’s AI  journey, and AMD’s Victor Peng, AMD President and Soumith Chintala, founder of PyTorch discussed the latest progress at the DC &amp;amp; AI Keynote on June 12.&lt;/p&gt;

&lt;h2 id=&quot;building-a-powerful-sw-stack-with-rocm&quot;&gt;Building a Powerful SW Stack with ROCm&lt;/h2&gt;

&lt;p&gt;Victor introduced ROCm, AMD’s SW stack for Instinct Data Center GPUs. It offers a comprehensive set of open-source libraries, runtime, compilers, and tools for developing, running, and fine-tuning AI models. The fifth generation ROCm incorporates optimizations for AI and high-performance computing workloads, including tailored kernels for low-latency memory systems, support for new data types, and integration with OpenAI Triton. With tools for porting AI software to AMD Instinct platforms, ROCm ensures quality and robustness, tested extensively and compliant with PyTorch and TensorFlow frameworks.&lt;/p&gt;

&lt;h2 id=&quot;collaboration-with-pytorch&quot;&gt;Collaboration with PyTorch&lt;/h2&gt;

&lt;p&gt;To shed light on the partnership between AMD and PyTorch, Victor invited &lt;a href=&quot;https://www.linkedin.com/in/soumith/&quot;&gt;Soumith Chintala&lt;/a&gt;, the founder of PyTorch, to discuss the advancements and integration between the two. PyTorch, the industry’s most famous AI framework, boasts a vibrant developer community and is known for its continuous innovation and incorporation of cutting-edge research.&lt;/p&gt;

&lt;p&gt;To highlight the AMD and PyTorch partnership, Victor hosted a discussion with Soumith Chintala, the founder of PyTorch. PyTorch, renowned for its innovation and community, is the industry’s leading AI framework. The latest version, PyTorch 2.0, integrates with hardware-agnostic software compilers like OpenAI Triton, enabling efficient training and deployment of AI models. With optimized techniques, PyTorch 2.0 enhances productivity and offers remarkable speed improvements. The collaboration between AMD and the PyTorch Foundation ensures seamless utilization of AMD GPUs, expanding AI accelerator accessibility worldwide and paving the way for future optimizations and broader hardware support.&lt;/p&gt;

&lt;h2 id=&quot;empowering-the-developer-community&quot;&gt;Empowering the Developer Community&lt;/h2&gt;

&lt;p&gt;The partnership between AMD and PyTorch benefits the developer community by democratizing access to AI accelerators. Support for AMD GPUs in PyTorch allows developers to train and deploy models across various platforms, including CPUs like EPYC and Ryzen, GPUs like Instinct and Radeon, and embedded devices like Versal SoCs. By ensuring immediate compatibility of new models on AMD platforms, the collaboration streamlines the development process and empowers developers to leverage the full potential of AMD’s hardware. This increased accessibility and flexibility enable developers worldwide to push the boundaries of AI innovation.&lt;/p&gt;

&lt;h2 id=&quot;hugging-face-and-ai-model-innovation&quot;&gt;Hugging Face and AI Model Innovation&lt;/h2&gt;

&lt;p&gt;Victor praised Hugging Face as the leading force behind open-source AI model innovation, empowering generative AI with transformative transformers. AMD’s optimized software enables a high-performing development stack, supporting groundbreaking AI advancements for customers and developers through scalable real-world deployments.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At the DC &amp;amp; AI Keynote, AMD demonstrated its dedication to openness, performance, and collaboration. The ROCm SW stack, PyTorch integration, and support for Hugging Face exemplify AMD’s commitment to empowering developers and researchers to achieve AI breakthroughs. By offering accessible, high-performing solutions, AMD fuels the future of AI as a leading GPU platform integrated with PyTorch.&lt;/p&gt;

&lt;p&gt;To listen to the full keynote visit the &lt;a href=&quot;https://www.youtube.com/watch?v=l3pe_qx95E0&quot;&gt;AMD Youtube&lt;/a&gt; channel&lt;/p&gt;

&lt;p&gt;To listen to Soumith Chintala’s section of the &lt;a href=&quot;https://www.youtube.com/watch?v=RgQEG2G1iaY&quot;&gt;keynote&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">AMD has gained progress in building a robust software stack that supports an open ecosystem of models, libraries, frameworks, and tools. With proven platforms gaining momentum, there is significance of a leadership software stack and an optimized ecosystem for achieving application performance. PyTorch is a key part of AMD’s AI journey, and AMD’s Victor Peng, AMD President and Soumith Chintala, founder of PyTorch discussed the latest progress at the DC &amp;amp; AI Keynote on June 12.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Performant Distributed checkpointing in Production with IBM</title>
      <link href="https://pytorch.org/blog/performant-distributed-checkpointing/" rel="alternate" type="text/html" title="Performant Distributed checkpointing in Production with IBM" />
      <published>2023-07-31T00:00:00-07:00</published>
      <updated>2023-07-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/performant-distributed-checkpointing</id>
      <content type="html" xml:base="https://pytorch.org/blog/performant-distributed-checkpointing/">&lt;p&gt;&lt;img src=&quot;/assets/images/2023-07-31-performant-distributed-checkpointing-1.png&quot; alt=&quot;Params saved per minute&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Last year, IBM Research began collaborating with us to onboard Fully Sharded Data Parallelism (FSDP) for their large foundation models. They became interested as FSDP is a PyTorch native offering for scaling their distributed training efforts on IBM Cloud.&lt;/p&gt;

&lt;p&gt;We are pleased to share that, in collaboration with IBM, we have achieved substantial checkpointing speedups for large models (72x vs the original PyTorch 1.13 save speed), proven model and optimizer checkpoint scaling to 30B parameters, and enabled cloud first training using FSDP + Distributed Checkpoint on S3 backends.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-distributed-checkpoint&quot;&gt;What is a Distributed Checkpoint?&lt;/h2&gt;

&lt;p&gt;Distributed checkpointing is the PyTorch native solution for saving and loading PyTorch models and optimizer states from multiple ranks, as well as supporting dynamically changing world sizes between reloads.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-07-31-performant-distributed-checkpointing-2.png&quot; alt=&quot;Checkpoint time vs model params&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Distributed Checkpoint (DCP) APIs were introduced in PyTorch 1.13, and are included as an official prototype feature in PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;Distributed checkpoint is different from torch.save() and torch.load() in a few significant ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;DCP produces multiples files per checkpoint, with at least one file per rank,&lt;/li&gt;
  &lt;li&gt;DCP operates in place, meaning that the model should allocate its data first and the Distributed Checkpoint will then use the storage.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A major improvement from 1.13 to 2.0 includes adding sharded_state_dict support for checkpointing FSDP models. This allows checkpointing for larger sized models, as well as adding support for load-time resharding. Load time resharding enables saving in one cluster topology, and loading into another.  This feature was highly requested as it allows training jobs to be run on one cluster, saved, and then continued on a different cluster with different world size.&lt;/p&gt;

&lt;p&gt;Another major change is that we decouple the storage layer from the checkpoint planning layer and separate implementation from the interface for both layers. With this change, users can now specify how their state_dict should be chunked or transformed during the checkpoint planning phase. Additionally, the customizable storage layer can easily accommodate different backends.&lt;/p&gt;

&lt;p&gt;More information on the Distributed Checkpoint package can be found &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.checkpoint.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performant-distributed-checkpointing-in-production-with-ibm&quot;&gt;Performant Distributed checkpointing in Production with IBM&lt;/h2&gt;

&lt;p&gt;IBM at Think 2023 announced its &lt;a href=&quot;https://www.ibm.com/products/watsonx-ai&quot;&gt;watsonx.ai&lt;/a&gt; platform for development and deployment of foundation models for the enterprise. Built on Hybrid Cloud, the platform enables use cases across multiple modalities such as NLP, timeseries, weather, chemistry, tabular data, and cybersecurity, with model sizes from 100s of millions to 10s of billions of parameters. Model architectures range from vision transformers, to multi-modal RoBERTa-style feature extractors, to large-scale generative language models similar to T5, GPT and Llama.&lt;/p&gt;

&lt;p&gt;As of today, IBM has now enabled checkpointing for T5-style architectures up to 11B parameters, and decoder architectures (GPT style) up to 30B.&lt;/p&gt;

&lt;p&gt;IBM helped us identify that this limits the scaling power of DCP from both memory and performance standpoints. With their suggestion, we enhanced our FileSystemWriter to produce single checkpoint per rank to reduce read write overhead.&lt;/p&gt;

&lt;p&gt;With this option as the new default, DCP now creates a single file per rank during checkpoint saving, which would then be sliced when reading parameters at load time.&lt;/p&gt;

&lt;p&gt;By combining sharded_state_dict support with single filer per rank writer, distributed checkpoint was able to accelerate checkpoint saving time over 72x vs the original PyTorch 1.13 save speed, and enable rapid checkpointing for models sizes over 15B which would previously simply time out.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Looking back, it’s really astounding the speedups we’ve seen, handling training for many of these models. We went from taking almost half an hour to write a single 11B checkpoint in PyTorch 1.13, to being able to handle a 30B parameter model, with optimizer and dataloader state - so that’s over eight times the raw data - in just over 3 minutes. That’s done wonders for both the stability and efficiency of our jobs, as we scale up training to hundreds of gpus.”  – &lt;strong&gt;Davis Wertheimer, IBM Research&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;IBM’s adoption has also helped us validate and improve our solutions in a real world, large-scale training environment. As an example, IBM discovered that DCP was working well for them on a single node with multiple GPUs, but erred out when used on multiple nodes.&lt;/p&gt;

&lt;p&gt;Upon investigating the issue, we realized that we were assuming writing to a NFS-like shared file system, which assumes strong read-after-write consistencies. Object stores with file system APIs such as S3FS provide eventual consistency semantics, thus causing the distributed checkpoint in such a setting to fail. Working together with IBM, we identified this issue and fixed it by making &lt;a href=&quot;https://research.ibm.com/blog/ibm-pytorch-ai-training&quot;&gt;one line code change&lt;/a&gt; and enabled object storage backend for DCP! Such storage approaches are typically an order of magnitude cheaper than shared file systems thus enabling finer grained checkpointing.&lt;/p&gt;

&lt;h2 id=&quot;looking-for-collaboration&quot;&gt;Looking for Collaboration&lt;/h2&gt;

&lt;p&gt;If you are interested in trying Distributed Checkpoint, feel free to reach out to us!&lt;/p&gt;

&lt;p&gt;If you run into any issue when trying it, you can open an &lt;a href=&quot;https://github.com/pytorch/pytorch/labels/module%3A%20distributed_checkpoint&quot;&gt;issue&lt;/a&gt; at our Github repo.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;This project would not have been possible without the assistance from many collaborators. We would like to thank Yanli Zhao, Andrew Gu, Rohan Varma for their support of FSDP. Thanks to Pritam Damania, Junjie Zhao, and Wanchao Liang for their support of ShardedTensor.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta:  Iris Zhang, Less Wright, Rodrigo Kumpera, Chien-Chin Huang, IBM: Davis Wertheimer, Supriyo Chakraboty, Sophia Wen, Raghu Ganti, Mudhakar Srivatsa, Seethrami Seelam</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">IBM Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/ibm-joins-pytorch/" rel="alternate" type="text/html" title="IBM Joins the PyTorch Foundation as a Premier Member" />
      <published>2023-07-27T00:00:00-07:00</published>
      <updated>2023-07-27T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/ibm-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/ibm-joins-pytorch/">&lt;p&gt;The PyTorch Foundation, part of The Linux Foundation, is pleased to announce that IBM has joined as a premier member.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-ibm-logo.png&quot; alt=&quot;IBM Logo&quot; style=&quot;max-width:250px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The foundation serves as a neutral space for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. With its extensive industry expertise and leadership in open source and AI, IBM is committed to actively contributing to the PyTorch community.&lt;/p&gt;

&lt;p&gt;IBM offers a comprehensive portfolio of enterprise AI solutions and recently released watsonx, its next-generation data and AI platform. IBM’s watsonx platform leverages PyTorch to offer an enterprise-grade software stack for end-to-end training and fine-tuning of AI foundation models.&lt;/p&gt;

&lt;p&gt;“By joining the PyTorch Foundation, we aim to contribute our expertise and resources to further advance PyTorch’s capabilities and make AI more accessible in hybrid cloud environments with flexible hardware options,” said Priya Nagpurkar, Vice President, Hybrid Cloud Platform and Developer Productivity, IBM Research. “We intend for our collaboration with PyTorch to bring the power of foundation models and generative AI to enterprises using the watsonx platform to drive business transformation.”&lt;/p&gt;

&lt;p&gt;IBM and PyTorch have already collaborated on two projects. The first enables foundation models with billions of parameters to train efficiently on standard cloud networking infrastructure, such as Ethernet networking. Together, IBM and PyTorch have also worked on ways to make checkpointing for AI training considerably more cost-effective, by fixing the distributed checkpointing within PyTorch to support certain types of object storage.&lt;/p&gt;

&lt;p&gt;“We’re happy to welcome IBM as a premier member. IBM’s expertise and dedication to advancing the field of artificial intelligence align perfectly with the mission of the PyTorch community,” said PyTorch Foundation Executive Director Ibrahim Haddad. “Their commitment to open collaboration and innovation will strengthen our collective efforts to empower developers and researchers worldwide.”&lt;/p&gt;

&lt;p&gt;As a premier member, IBM  is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-ibm-headshot.png&quot; alt=&quot;Raghu Ganti Headshot&quot; style=&quot;max-width:250px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Raghu Ganti, Principal Research Scientist at IBM Research, to our board.  Raghu co-leads IBM Research’s foundation model training and validation platform, built on Red Hat OpenShift. His team primarily contributes to the PyTorch training components, with the mission of democratizing training and validation of foundation models.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/join&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, part of The Linux Foundation, is pleased to announce that IBM has joined as a premier member.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing CPP-based S3 IO DataPipes</title>
      <link href="https://pytorch.org/blog/announcing-cpp/" rel="alternate" type="text/html" title="Announcing CPP-based S3 IO DataPipes" />
      <published>2023-07-25T00:00:00-07:00</published>
      <updated>2023-07-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/announcing-cpp</id>
      <content type="html" xml:base="https://pytorch.org/blog/announcing-cpp/">&lt;p&gt;Training large deep learning models requires large datasets. &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) is a scalable cloud object store service used for storing large training datasets. Machine learning (ML) practitioners need an efficient data pipe that can download data from Amazon S3, transform the data, and feed the data to GPUs for training models with high throughput and low latency.&lt;/p&gt;

&lt;p&gt;In this post, we introduce the new S3 IO DataPipes for PyTorch, &lt;a href=&quot;hhttps://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/load/s3io.py#L19&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLister&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/load/s3io.py#L106&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt;&lt;/a&gt;. For memory efficiency and fast runs, the new DataPipes use the C++ extension to access Amazon S3. Benchmarking shows that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; is 59.8% faster than &lt;a href=&quot;https://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/load/fsspec.py#L125&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FSSpecFileOpener&lt;/code&gt;&lt;/a&gt; for downloading a natural language processing (NLP) dataset from Amazon S3. You can build &lt;a href=&quot;https://pytorch.org/data/beta/torchdata.datapipes.iter.html&quot;&gt;IterDataPipe&lt;/a&gt; training pipelines with the new DataPipes. We also demonstrate that the new DataPipe can reduce overall Bert and ResNet50 training time by 7%. The new DataPipes have been upstreamed to the open-source &lt;a href=&quot;https://github.com/pytorch/data/releases/tag/v0.4.0&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TorchData 0.4.0&lt;/code&gt;&lt;/a&gt; with &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v1.12.0&quot;&gt;PyTorch 1.12.0&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Amazon S3 is a scalable cloud storage service with no limit on data volume. Loading data from Amazon S3 and feeding the data to high-performance GPUs such as NVIDIA A100 can be challenging. It requires an efficient data pipeline that can meet the data processing speed of GPUs. To help with this, we released a new high performance tool for PyTorch: S3 IO DataPipes. DataPipes are subclassed from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchdata.datapipes.iter.IterDataPipe&lt;/code&gt;, so they can interact with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IterableDataPipe&lt;/code&gt; interface. Developers can quickly build their DataPipe DAGs to access, transform, and manipulate data with shuffle, sharding, and batch features.&lt;/p&gt;

&lt;p&gt;The new DataPipes are designed to be file format agnostic and Amazon S3 data is downloaded as binary large objects (BLOBs). It can be used as a composable building block to assemble a DataPipe graph that can load tabular, NLP, and computer vision (CV) data into your training pipelines.&lt;/p&gt;

&lt;p&gt;Under the hood, the new S3 IO DataPipes employ a C++ S3 handler with the AWS C++ SDK. In general, a C++ implementation is more memory efficient and has better CPU core usage (no Global Interpreter Lock) in threading compared to Python. The new C++ S3 IO DataPipes are recommended for high throughput, low latency data loading in training large deep learning models.&lt;/p&gt;

&lt;p&gt;The new S3 IO DataPipes provide two first-class citizen APIs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;S3FileLister&lt;/strong&gt; – Iterable that lists S3 file URLs within the given S3 prefixes. The functional name for this API is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;list_files_by_s3&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;S3FileLoader&lt;/strong&gt; – Iterable that loads S3 files from the given S3 prefixes. The functional name for this API is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_files_by_s3&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;usage&quot;&gt;Usage&lt;/h2&gt;

&lt;p&gt;In this section, we provide instructions for using the new S3 IO DataPipes. We also provide a code snippet for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_files_by_s3()&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;build-from-source&quot;&gt;Build from source&lt;/h3&gt;
&lt;p&gt;The new S3 IO DataPipes use the C++ extension. It is built into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchdata&lt;/code&gt; package by default. However, if the new DataPipes are not available within the environment, for example Windows on Conda, you need to build from the source. For more information, refer to &lt;a href=&quot;https://github.com/pytorch/data/tree/main/torchdata/datapipes/iter/load#s3-io-datapipe-documentation&quot;&gt;Iterable Datapipes&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;configuration&quot;&gt;Configuration&lt;/h3&gt;
&lt;p&gt;Amazon S3 supports global buckets. However, a bucket is created within a Region. You can pass a Region to the DataPipes by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__init__()&lt;/code&gt;. Alternatively, you can either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;export AWS_REGION=us-west-2&lt;/code&gt; into your shell or set an environment variable with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;os.environ['AWS_REGION'] = 'us-east-1'&lt;/code&gt; in your code.&lt;/p&gt;

&lt;p&gt;To read objects in a bucket that aren’t publicly accessible, you must provide AWS credentials through one of the following methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html&quot;&gt;Install and configure&lt;/a&gt; the &lt;a href=&quot;aws.amazon.com/cli&quot;&gt;AWS Command Line Interface&lt;/a&gt; (AWS CLI) with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS configure&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Set credentials in the AWS credentials profile file on the local system, located at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.aws/credentials&lt;/code&gt; on Linux, macOS, or Unix&lt;/li&gt;
  &lt;li&gt;Set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; environment variables&lt;/li&gt;
  &lt;li&gt;If you’re using this library on an &lt;a href=&quot;aws.amazon.com/ec2&quot;&gt;Amazon Elastic Compute Cloud&lt;/a&gt; (Amazon EC2) instance, specify an &lt;a href=&quot;aws.amazon.com/iam&quot;&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) role and then give the EC2 instance access to that role&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;example-code&quot;&gt;Example code&lt;/h3&gt;
&lt;p&gt;The following code snippet provides a typical usage of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_files_by_s3()&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torch.utils.data import DataLoader 
from torchdata.datapipes.iter import IterableWrapper  

s3_shard_urls = IterableWrapper([&quot;s3://bucket/prefix/&quot;,]) .list_files_by_s3()
s3_shards = s3_shard_urls.load_files_by_s3() 
# text data 
training_data = s3_shards.readlines(return_path=False) 
data_loader = DataLoader(
      training_data,
      batch_size=batch_size,
      num_workers=num_workers, 
) # training loop 
for epoch in range(epochs):     
      # training step     
      for bach_data in data_loader:         
         # forward pass, backward pass, model update  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;benchmark&quot;&gt;Benchmark&lt;/h2&gt;

&lt;p&gt;In this section, we demonstrate how the new DataPipe can reduce overall Bert and ResNet50 training time.&lt;/p&gt;

&lt;h3 id=&quot;isolated-dataloader-performance-evaluation-against-fsspec&quot;&gt;Isolated DataLoader performance evaluation against FSSpec&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FSSpecFileOpener&lt;/code&gt; is another PyTorch S3 DataPipe. It uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;botocore&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aiohttp/asyncio&lt;/code&gt; to access S3 data. The following is the performance test setup and result (quoted from &lt;a href=&quot;https://github.com/pytorch/data/issues/500&quot;&gt;Performance Comparison between native AWSSDK and FSSpec (boto3) based DataPipes&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The S3 data in the test is a sharded text dataset. Each shard has about 100,000 lines and each line is around 1.6 KB, making each shard about 156 MB. The measurements in this benchmark are averaged over 1,000 batches. No shuffling, sampling, or transforms were performed.&lt;/p&gt;

&lt;p&gt;The following chart reports the throughput comparison for various batch sizes for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_workers=0&lt;/code&gt;, the data loader runs in the main process. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; has higher queries per second (QPS). It is 90% higher than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsspec&lt;/code&gt; at batch size 512.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-1.png&quot; alt=&quot;Batch Sizes 1&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following chart reports the results for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_workers=4&lt;/code&gt;, the data loaders runs in the main process. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; is 59.8% higher than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsspec&lt;/code&gt; at batch size 512.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-5.png&quot; alt=&quot;Batch Sizes 2&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-resnet50-model-against-boto3&quot;&gt;Training ResNet50 Model against Boto3&lt;/h3&gt;
&lt;p&gt;For the following chart, we trained a ResNet50 model on a cluster of 4 p3.16xlarge instances with a total 32 GPUs. The training dataset is ImageNet with 1.2 million images organized into 1,000-image shards. The training batch size is 64. The training time is measured in seconds. For eight epochs, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; is 7.5% faster than Boto3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-2.png&quot; alt=&quot;Boto3&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-a-bert-model-against-boto3&quot;&gt;Training a Bert model against Boto3&lt;/h3&gt;
&lt;p&gt;For the following cart, we trained a Bert model on a cluster of 4 p3.16xlarge instances with a total 32 GPUs. The training corpus has 1474 files. Each file has around 150,000 samples. To run a shorter epoch, we use 0.05% (approximately 75 samples) per file. The batch size is 2,048. The training time is measured in seconds. For one epoch, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; is 7% faster than Boto3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-3.png&quot; alt=&quot;Boto3 2&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;comparison-against-the-original-pytorch-s3-plugin&quot;&gt;Comparison against the original PyTorch S3 plugin&lt;/h3&gt;
&lt;p&gt;The new PyTorch S3 DataPipes perform substantially better than the original &lt;a href=&quot;https://github.com/aws/amazon-s3-plugin-for-pytorch&quot;&gt;PyTorch S3 plugin&lt;/a&gt;. We have tuned the internal buffer size for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt;. The loading time is measured in seconds.&lt;/p&gt;

&lt;p&gt;For the 10 sharded charades files (approximately 1.5 GiB each), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3FileLoader&lt;/code&gt; was 3.5 times faster in our experiments.&lt;/p&gt;

&lt;h3 id=&quot;best-practices&quot;&gt;Best practices&lt;/h3&gt;
&lt;p&gt;Training large deep learning models may require a massive compute cluster with tens or even hundreds of nodes. Each node in the cluster may generate a large number of data loading requests that hit a specific S3 shard. To avoid throttle, we recommend sharding training data across S3 buckets and S3 folders.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-7-25-announcing-ccp-based-s3-io-datapipes-4.png&quot; alt=&quot;Best Practices&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To achieve good performance, it helps to have file sizes that are big enough to parallelize across a given file, but not so big that we hit the limits of throughput on that object on Amazon S3 depending on the training job. The optimal size can be between 50–200 MB.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-next-steps&quot;&gt;Conclusion and next steps&lt;/h2&gt;

&lt;p&gt;In this post, we introduced you to the new PyTorch IO DataPipes. The new DataPipes use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws-sdk-cpp&lt;/code&gt; and show better performance against Boto3-based data loaders.&lt;/p&gt;

&lt;p&gt;For next steps, we plan to improve on usability, performance, and functionality by focusing on the following features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;S3 authorization with IAM roles&lt;/strong&gt; – Currently, the S3 DataPipes support explicit access credentials, instance profiles, and S3 bucket policies. However, there are use cases where IAM roles are preferred.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Double buffering&lt;/strong&gt; – We plan to offer double buffering to support multi-worker downloading.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Local caching&lt;/strong&gt; – We plan on making model training able to traverse the training dataset for multiple passes. Local caching after the first epoch can cut out time of flight delays from Amazon S3, which can substantially accelerate data retrieval time for subsequent epochs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Customizable configuration&lt;/strong&gt; – We plan to expose more parameters such as internal buffer size, multi-part chunk size, and executor count and allow users to further tune data loading efficiency.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Amazon S3 upload&lt;/strong&gt; – We plan to expand the S3 DataPipes to support upload for checkpointing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Merge with fsspec&lt;/strong&gt; – &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsspec&lt;/code&gt; is used in other systems such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.save()&lt;/code&gt;. We can integrate the new S3 DataPipes with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fsspec&lt;/code&gt; so they can have more use cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h3&gt;

&lt;p&gt;We would like to thank Vijay Rajakumar and Kiuk Chung from Amazon for providing their guidance for S3 Common RunTime and PyTorch DataLoader. We also want to thank Erjia Guan, Kevin Tse, Vitaly Fedyunin , Mark Saroufim, Hamid Shojanazeri, Matthias Reso, and Geeta Chauhan from Meta AI/ML, and Joe Evans from AWS for reviewing the blog and the GitHub PRs.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/announcing-the-amazon-s3-plugin-for-pytorch/&quot;&gt;Announcing the Amazon S3 plugin for PyTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/data/issues/500&quot;&gt;Performance Comparison between native AWSSDK and FSSpec (boto3) based DataPipes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>John He, Khaled ElGalaind, Roshani Nagmote, Daiming Yang</name>
        
        
      </author>

      

      

      
        <summary type="html">Training large deep learning models requires large datasets. Amazon Simple Storage Service (Amazon S3) is a scalable cloud object store service used for storing large training datasets. Machine learning (ML) practitioners need an efficient data pipe that can download data from Amazon S3, transform the data, and feed the data to GPUs for training models with high throughput and low latency. In this post, we introduce the new S3 IO DataPipes for PyTorch, S3FileLister and S3FileLoader. For memory efficiency and fast runs, the new DataPipes use the C++ extension to access Amazon S3. Benchmarking shows that S3FileLoader is 59.8% faster than FSSpecFileOpener for downloading a natural language processing (NLP) dataset from Amazon S3. You can build IterDataPipe training pipelines with the new DataPipes. We also demonstrate that the new DataPipe can reduce overall Bert and ResNet50 training time by 7%. The new DataPipes have been upstreamed to the open-source TorchData 0.4.0 with PyTorch 1.12.0.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">How to Accelerate PyTorch Geometric on Intel® CPUs</title>
      <link href="https://pytorch.org/blog/how-to-accelerate/" rel="alternate" type="text/html" title="How to Accelerate PyTorch Geometric on Intel® CPUs" />
      <published>2023-07-10T00:00:00-07:00</published>
      <updated>2023-07-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/how-to-accelerate</id>
      <content type="html" xml:base="https://pytorch.org/blog/how-to-accelerate/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The Intel PyTorch team has been collaborating with the PyTorch Geometric (PyG) community to provide CPU performance optimizations for Graph Neural Network (GNN) and PyG workloads. In the PyTorch 2.0 release, several critical optimizations were introduced to improve GNN training and inference performance on CPU. Developers and researchers can now take advantage of &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Intel’s AI/ML Framework optimizations&lt;/a&gt; for significantly faster model training and inference, which unlocks the ability for GNN workflows directly using PyG.&lt;/p&gt;

&lt;p&gt;In this blog, we will perform a deep dive on how to optimize PyG performance for both training and inference while using the PyTorch 2.0 flagship torch.compile feature to speed up PyG models.&lt;/p&gt;

&lt;h2 id=&quot;message-passing-paradigm&quot;&gt;Message Passing Paradigm&lt;/h2&gt;

&lt;p&gt;Message passing refers to the process of nodes exchanging information with their respective neighbors by sending messages to one another. In PyG, the process of message passing can be generalized into three steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Gather&lt;/strong&gt;: Collect edge-level information of adjacent nodes and edges.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Apply&lt;/strong&gt;: Update the collected information with user-defined functions (UDFs).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scatter&lt;/strong&gt;: Aggregate to node-level information, e.g., via a particular reduce function such as sum, mean, or max.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-to-accelerate/f1-pyg-message-passing-paradigm.png&quot; alt=&quot;Figure 1: The message passing paradigm&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: The message passing paradigm (Source: &lt;a href=&quot;http://github.com/rusty1s&quot;&gt;Matthias Fey&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Message passing performance is highly related to the storage format of the adjacency matrix of the graph, which records how pairs of nodes are connected. Two methods for the storage format are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Adjacency matrix in COO (Coordinate Format):&lt;/strong&gt; The graph data is physically stored in a two-dimensional tensor shape of &lt;strong&gt;[2, num_edges]&lt;/strong&gt;, which maps each connection of source and destination nodes. The performance hotspot is scatter-reduce.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Adjacency matrix in CSR (Compressed Sparse Row):&lt;/strong&gt; Similar format to COO, but compressed on the row indices. This format allows for more efficient row access and faster sparse matrix-matrix multiplication (SpMM). The performance hotspot is sparse matrix related reduction ops.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;scatter-reduce&quot;&gt;Scatter-Reduce&lt;/h2&gt;

&lt;p&gt;The pattern of scatter-reduce is parallel in nature, which updates values of a &lt;strong&gt;self&lt;/strong&gt; tensor using values from a &lt;strong&gt;src&lt;/strong&gt; tensor at the entries specified by &lt;strong&gt;index&lt;/strong&gt;. Ideally, parallelizing on the outer dimension would be most performant. However, direct parallelization leads to write conflicts, as different threads might try to update the same entry simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-to-accelerate/f2-scatter-reduce-scheme.png&quot; alt=&quot;Figure 2: Scatter-reduce and its optimization scheme&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Scatter-reduce and its optimization scheme (Source: Mingfei Ma)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;To optimize this kernel, we use sorting followed by a reduction:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sorting:&lt;/strong&gt; Sort the &lt;strong&gt;index&lt;/strong&gt; tensor in ascending order with parallel radix sort, such that indices pointing to the same entry in the &lt;strong&gt;self&lt;/strong&gt; tensor are managed in the same thread.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reduction:&lt;/strong&gt; Paralleled on the outer dimension of &lt;strong&gt;self&lt;/strong&gt;, and do vectorized reduction for each indexed &lt;strong&gt;src&lt;/strong&gt; entry.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For its backward path during the training process (i.e., gather), sorting is not needed because its memory access pattern will not lead to any write conflicts.&lt;/p&gt;

&lt;h2 id=&quot;spmm-reduce&quot;&gt;SpMM-Reduce&lt;/h2&gt;

&lt;p&gt;Sparse matrix-matrix reduction is a fundamental operator in GNNs, where &lt;strong&gt;A&lt;/strong&gt; is sparse adjacency matrix in CSR format and &lt;strong&gt;B&lt;/strong&gt; is a dense feature matrix where the reduction type could be &lt;em&gt;sum&lt;/em&gt;, &lt;em&gt;mean&lt;/em&gt; or &lt;em&gt;max&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-to-accelerate/f3-spmm-optimization-scheme.png&quot; alt=&quot;Figure 3: SpMM optimization scheme&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: SpMM optimization scheme (Source: Mingfei Ma)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The biggest challenge when optimizing this kernel is how to balance thread payload when parallelizing along rows of the sparse matrix &lt;strong&gt;A&lt;/strong&gt;. Each row in &lt;strong&gt;A&lt;/strong&gt; corresponds to a node, and its number of connections may vary vastly from one to another; this results in thread payload imbalance. One technique to address such issues is to do payload scanning before thread partition. Aside from that, other techniques are also introduced to further exploit CPU performance such as vectorization and unrolling and blocking.&lt;/p&gt;

&lt;p&gt;These optimizations are done via &lt;strong&gt;torch.sparse.mm&lt;/strong&gt; using the reduce flags of &lt;em&gt;amax&lt;/em&gt;, &lt;em&gt;amin&lt;/em&gt;, &lt;em&gt;mean&lt;/em&gt;, &lt;em&gt;sum&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-gains-up-to-41x-speedup&quot;&gt;Performance Gains: Up to 4.1x Speedup&lt;/h2&gt;

&lt;p&gt;We collected benchmark performance for both inference and training in &lt;a href=&quot;http://github.com/pyg-team/pytorch_geometric/tree/master/benchmark&quot;&gt;pytorch_geometric/benchmark&lt;/a&gt; and in the &lt;a href=&quot;http://github.com/snap-stanford/ogb&quot;&gt;Open Graph Benchmark (OGB)&lt;/a&gt; to demonstrate the performance improvement from the above-mentioned methods on Intel® Xeon® Platinum 8380 Processor.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;

  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model – Dataset&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Option&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Speedup ratio&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;4&quot;&gt; 
        GCN-Reddit (inference)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.22x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.25x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.31x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.68x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;4&quot;&gt; 
        GraphSage-ogbn-products (inference)
   &lt;/td&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.15x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.33x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;full-batch-sparse
   &lt;/td&gt;
   &lt;td&gt;4.07x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-PROTEINS (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-REDDIT-BINARY (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;GCN-Reddit (training)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.12x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: Performance Speedup on PyG Benchmark&lt;sup&gt;1&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;From the benchmark results, we can see that our optimizations in PyTorch and PyG achieved &lt;strong&gt;1.1x-4.1x speed-up&lt;/strong&gt; for inference and training.&lt;/p&gt;

&lt;h2 id=&quot;torchcompile-for-pyg&quot;&gt;torch.compile for PyG&lt;/h2&gt;

&lt;p&gt;The PyTorch2.0 flagship feature torch.compile is fully compatible with PyG 2.3 release, bringing additional speed-up in PyG model inference/training over imperative mode, thanks to TorchInductor C++/OpenMP backend for CPUs. In particular, &lt;strong&gt;a 3.0x – 5.4x performance speed-up&lt;/strong&gt; is measured on &lt;a href=&quot;http://github.com/pyg-team/pytorch_geometric/blob/master/test/nn/models/test_basic_gnn.py&quot;&gt;basic GNN models&lt;/a&gt; with Intel Xeon Platinum 8380 Processor on model training&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-to-accelerate/f4-torch-compile-performance-speedup.png&quot; alt=&quot;Figure 4: Performance Speedup with Torch Compile&quot; style=&quot;max-width:620px; width:100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Performance Speedup with Torch Compile&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Torch.compile can fuse the multiple stages of message passing into a single kernel, which provides significant speedup due to the saved memory bandwidth. Refer to this &lt;a href=&quot;http://pytorch-geometric.readthedocs.io/en/latest/tutorial/compile.html&quot;&gt;pytorch geometric tutorial&lt;/a&gt; for additional support.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt; that torch.compile within PyG is in beta mode and under active development. Currently, some features do not yet work together seamlessly such as torch.compile(model, dynamic=True), but fixes are on the way from Intel.&lt;/p&gt;

&lt;h2 id=&quot;conclusion--future-work&quot;&gt;Conclusion &amp;amp; Future Work&lt;/h2&gt;

&lt;p&gt;In this blog, we introduced the GNN performance optimizations included in PyTorch 2.0 on CPU. We are closely collaborating with the PyG community for future optimization work, which will focus on in-depth optimizations from torch.compile, sparse optimization, and distributed training.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h3&gt;

&lt;p&gt;The results presented in this blog is a joint effort of Intel PyTorch team and Kumo. Special thanks to &lt;a href=&quot;http://github.com/rusty1s&quot;&gt;Matthias Fey&lt;/a&gt; (Kumo), &lt;a href=&quot;http://github.com/pearu&quot;&gt;Pearu Peterson&lt;/a&gt; (Quansight) and &lt;a href=&quot;http://www.linkedin.com/in/christianpuhrsch/&quot;&gt;Christian Puhrsch&lt;/a&gt; (Meta) who spent precious time and gave substantial assistance! Together, we made one more step forward on the path of improving the PyTorch CPU ecosystem.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;Accelerating PyG on Intel CPUs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/pyg-team/pytorch_geometric/releases/tag/2.3.0&quot;&gt;PyG 2.3.0&lt;/a&gt;: PyTorch 2.0 support, native sparse tensor support, explainability and accelerations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;h4 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h4&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Platinum 8380: 1-node, 2x Intel Xeon Platinum 8380 processor with 256GB (16 slots/ 16GB/3200) total DDR4 memory, uCode 0xd000389, HT on, Turbo on, Ubuntu 20.04.5 LTS,  5.4.0-146-generic, INTEL SSDPE2KE016T8 1.5T; GCN + Reddit FP32 inference, GCN+Reddit FP32 training, GraphSAGE + ogbn-products FP32 inference, GCN-PROTAIN, GCN-REDDIT-BINARY FP32 training; Software: PyTorch 2.1.0.dev20230302+cpu, pytorch_geometric 2.3.0, torch-scatter 2.1.0, torch-sparse 0.6.16, test by Intel on 3/02/2023.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt;Platinum 8380: 1-node, 2x Intel Xeon Platinum 8380 processor with 256GB (16 slots/ 16GB/3200) total DDR4 memory, uCode 0xd000389, HT on, Turbo on, Ubuntu 20.04.5 LTS,  5.4.0-146-generic, INTEL SSDPE2KE016T8 1.5T; GCN, GraphSAGE, GIN and EdgeCNN, FP32; Software: PyTorch 2.1.0.dev20230411+cpu, pytorch_geometric 2.4.0, torch-scatter 2.1.1+pt20cpu, torch-sparse 0.6.17+pt20cpu, test by Intel on 4/11/2023.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt;Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
</feed>


