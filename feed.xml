<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-05-01T17:48:03-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving</title>
      <link href="https://pytorch.org/blog/introducing-hidet/" rel="alternate" type="text/html" title="Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving" />
      <published>2023-04-27T00:00:00-07:00</published>
      <updated>2023-04-27T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/introducing-hidet</id>
      <content type="html" xml:base="https://pytorch.org/blog/introducing-hidet/">&lt;p&gt;&lt;a href=&quot;https://github.com/hidet-org/hidet&quot;&gt;Hidet&lt;/a&gt; is a powerful deep learning compiler that simplifies the process of implementing high-performing deep learning operators on modern accelerators (e.g., NVIDIA GPUs). With the new feature of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; in PyTorch 2.0, integrating a novel compiler into PyTorch is easier than ever - Hidet now can be used as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; backend to accelerate PyTorch models, making it an attractive option for PyTorch users who want to improve the inference performance of their models, especially for those who also need to implement extremely optimized custom operators.&lt;/p&gt;

&lt;h2 id=&quot;using-hidet-to-compile-a-pytorch-model&quot;&gt;Using Hidet to Compile A PyTorch Model&lt;/h2&gt;

&lt;p&gt;To use Hidet in PyTorch, you need to first install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet&lt;/code&gt; package via pip:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install hidet
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hidet is integrated with PyTorch as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; backend following the &lt;a href=&quot;https://pytorch.org/docs/stable/dynamo/custom-backends.html&quot;&gt;Custom Backends tutorial&lt;/a&gt;. You can specify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet&lt;/code&gt; as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backend&lt;/code&gt; when you compile a model. (Note: requires PyTorch version 2.0+):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.compile(..., backend='hidet')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hidet converts the given PyTorch model in the torch.fx.Graph format into its internal graph representation, and conducts a series of optimizations. Hidet provides a few options to configure the optimizations. For example, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet.torch.dynamo_config.use_tensor_core(True)&lt;/code&gt; to allow Hidet to generate CUDA kernels that leverage the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tensor-cores/&quot;&gt;Tensor Cores on NVIDIA GPUs&lt;/a&gt;, and use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet.torch.dynamo_config.search_space(2)&lt;/code&gt; to allow Hidet to search for the best operator schedule specific for your hardware and input sizes. More configurations can be found in &lt;a href=&quot;https://docs.hidet.org/stable/gallery/tutorials/optimize-pytorch-model.html&quot;&gt;Hidet’s documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s a complete example of how to use Hidet to compile and optimize a pre-trained ResNet50 model from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import hidet
import torch

# Load a pre-trained ResNet50 model
x = torch.randn(1, 3, 224, 224, device='cuda').half()
model = torch.hub.load(
    'pytorch/vision:v0.6.0', 'resnet50', pretrained=True
).cuda().half().eval()

# Configure hidet to use tensor core and enable tuning
hidet.torch.dynamo_config.use_tensor_core(True)
hidet.torch.dynamo_config.search_space(2) 

# Compile the model using Hidet
model_opt = torch.compile(model, backend='hidet')

# Check correctness
torch.testing.assert_close(actual=model_opt(x), expected=model(x), rtol=1e-2, atol=1e-2)

# Benchmark
from hidet.utils import benchmark_func
print('eager: {:2f}'.format(benchmark_func(lambda: model(x))))
print('hidet: {:2f}'.format(benchmark_func(lambda: model_opt(x))))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We encourage you to try out the above script on your own NVIDIA GPU(s)! If you run this script on an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws.g5.2xlarge&lt;/code&gt; instance, you would get the result shown in the following figure. Hidet achieves the speedup because it could automatically fuse multiple operators, tune operator schedules, and use CUDA Graph to reduce framework-level overhead. More results can be found in the &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3575693.3575702&quot;&gt;ASPLOS’23 publication of Hidet&lt;/a&gt; and our &lt;a href=&quot;https://github.com/hidet-org/hidet/issues/154&quot;&gt;performance tracking&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-4-27-hidet.png&quot; alt=&quot;Eager vs Hidet latency&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;using-hidet-script-to-write-custom-operators&quot;&gt;Using Hidet Script to Write Custom Operators&lt;/h2&gt;

&lt;p&gt;Hidet Script is one approach to implement tensor operators in Python. The following example shows how to implement a naive matrix multiplication using Hidet Script and integrate it as a PyTorch operator.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import hidet


def matmul(m_size, n_size, k_size):
    from hidet.lang import f32, attr
    from hidet.lang.cuda import threadIdx, blockIdx, blockDim

    with hidet.script_module() as script_module:
        @hidet.script
        def matmul(
            a: f32[m_size, k_size],
            b: f32[k_size, n_size],
            c: f32[m_size, n_size]
        ):
            attr.cuda_grid_dim = ((m_size + 31) // 32, (n_size + 31) // 32)
            attr.cuda_block_dim = (32, 32)
            i = threadIdx.x + blockIdx.x * blockDim.x
            j = threadIdx.y + blockIdx.y * blockDim.y
            if i &amp;lt; m_size and j &amp;lt; n_size:
                c[i, j] = 0.0
                for k in range(k_size):
                    c[i, j] += a[i, k] * b[k, j]

    ir_module = script_module.ir_module()
    func = hidet.driver.build_ir_module(ir_module)
    return func


class NaiveMatmul(torch.autograd.Function):
    @staticmethod
    def forward(ctx, a, b):
        m, k = a.shape
        k, n = b.shape
        c = torch.empty([m, n], dtype=a.dtype, device=a.device)
        func = matmul(m, n, k)
        func(a, b, c)
        return c


a = torch.randn([3, 4], device='cuda')
b = torch.randn([4, 5], device='cuda')
c = NaiveMatmul.apply(a, b)
cc = torch.matmul(a, b)
torch.testing.assert_close(c, cc)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More optimizations can be applied, see the &lt;a href=&quot;https://docs.hidet.org/stable/gallery/developer-guides/hidet-script-dynamic-kernel.html&quot;&gt;example&lt;/a&gt; in our documentation to learn more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hidet Script vs. Triton&lt;/strong&gt;: Triton greatly simplifies the CUDA programming by introducing the tile-based programming model where the parallel execution unit is thread blocks instead of threads. However, this simplification also prevents the tensor program developers from manipulating the fine-grained computation and memory resources (e.g., warps, shared memory) in their preferred ways. It would be challenging to implement an optimization that requires fine-grained control of these resources using Triton if it has not been implemented by the Triton compiler itself. Hidet Script, on the other hand, simplifies tensor programming while still enabling users to implement their own optimizations with extensive flexibility. It’s worth noting that the more granular control of Hidet Script also brings added complexity compared to Triton.&lt;/p&gt;

&lt;h2 id=&quot;more-about-hidet&quot;&gt;More about Hidet&lt;/h2&gt;

&lt;p&gt;Hidet originates from a research project led by the &lt;a href=&quot;https://www.cs.toronto.edu/ecosystem/&quot;&gt;EcoSystem lab&lt;/a&gt; at the University of Toronto (UofT) and AWS. The authors propose a new way, named the task-mapping programming paradigm, to construct tensor programs. It aims to simplify the tensor programming without sacrificing any optimization opportunity. Now, Hidet is an open-source project, jointly supported by &lt;a href=&quot;https://centml.ai/&quot;&gt;CentML&lt;/a&gt; and the EcoSystem lab, that aims to provide an efficient solution to end-to-end inference on modern accelerators (e.g., NVIDIA GPUs).&lt;/p&gt;

&lt;h3 id=&quot;additional-resources&quot;&gt;Additional Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;GitHub Repository: &lt;a href=&quot;https://github.com/hidet-org/hidet&quot;&gt;https://github.com/hidet-org/hidet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hidet’s Documentation: &lt;a href=&quot;https://docs.hidet.org&quot;&gt;https://docs.hidet.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ASPLOS ’23 Publication: &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3575693.3575702&quot;&gt;https://dl.acm.org/doi/10.1145/3575693.3575702&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ASPLOS ’23 Tutorial: &lt;a href=&quot;https://centml.github.io/asplos23-tutorial/&quot;&gt;https://centml.github.io/asplos23-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;We would like to thank Jerry Park, Mark Saroufim, Jason Liang and Helen Suk for their valuable help on preparing the blog post and feedback on the text. We also would like to thank Nikita Shulga, Jason Ansel, and Dmytro Dzhulgakov for reviewing and improving our PR https://github.com/pytorch/pytorch/pull/93873 on the 3rd-party dynamo backend registration.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team Hidet</name>
        
        
      </author>

      

      

      
        <summary type="html">Hidet is a powerful deep learning compiler that simplifies the process of implementing high-performing deep learning operators on modern accelerators (e.g., NVIDIA GPUs). With the new feature of torch.compile(...) in PyTorch 2.0, integrating a novel compiler into PyTorch is easier than ever - Hidet now can be used as a torch.compile(...) backend to accelerate PyTorch models, making it an attractive option for PyTorch users who want to improve the inference performance of their models, especially for those who also need to implement extremely optimized custom operators.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Large Language Models with Accelerated Transformers</title>
      <link href="https://pytorch.org/blog/accelerating-large-language-models/" rel="alternate" type="text/html" title="Accelerating Large Language Models with Accelerated Transformers" />
      <published>2023-04-19T00:00:00-07:00</published>
      <updated>2023-04-19T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-large-language-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-large-language-models/">&lt;p&gt;&lt;strong&gt;TL;DR.&lt;/strong&gt; We show how to use Accelerated PyTorch 2.0 Transformers and the newly introduced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; method to accelerate Large Language Models on the example of &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt;, a compact open-source implementation of the GPT model from Andrej Karpathy. Using the new &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;scaled dot product attention operator&lt;/a&gt; introduced with Accelerated PT2 Transformers, we select the flash_attention custom kernel and achieve faster training time per batch (measured with Nvidia A100 GPUs), going from a ~143ms/batch baseline to ~113 ms/batch. In addition, the enhanced implementation using the SDPA operator offers better numerical stability. Finally, further optimizations are achieved using padded inputs, which when combined with flash attention lead to ~87ms/batch.&lt;/p&gt;

&lt;p&gt;Recent times have seen exponential adoption of large language models (LLMs) and Generative AI in everyday life. Tightly coupled with these ever-growing models is the ever-growing training cost - in terms of both time and hardware utilization. The PyTorch team has tackled these challenges head on with &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-2/&quot;&gt;Accelerated PyTorch 2 Transformers&lt;/a&gt; (previously known as “Better Transformer”) and JIT Compilation in &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-release/&quot;&gt;PyTorch 2.0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post, we explore training optimizations gained by utilizing custom kernel implementations of SDPA - also known as scaled dot product attention - a critical layer in transformer models. The custom kernel for SDPA replaces several discrete sequential operations with one globally optimized kernel which avoids allocating a large amount of intermediate CUDA memory. This approach offers a number of advantages, including but not limited to:  higher performance computation of SDPA by reducing memory bandwidth bottleneck, reduced memory footprint to support larger batch sizes, and finally added numerical stability by prescaling input tensors. These optimizations are demonstrated on nanoGPT, an open-source implementation of GPT from Andrej Karpathy.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Scaled dot product attention is the fundamental building block of multihead attention, as introduced in &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;“Attention is All You Need”&lt;/a&gt;, and has a wide range of applications in LLM and Generative AI models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/PyTorch_Better-Transformer_Figure-1.png&quot; alt=&quot;The Transformer model architecture&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; The Transformer model architecture based on &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;“Attention is All You Need”&lt;/a&gt;. With the new PyTorch SDPA operator, Multi-Head Attention is efficiently implemented by a linear layer for the in-projection, the SDPA operator, and a linear layer for the out-projection.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;With the new scaled_dot_product_attention operator, multihead attention can be implemented in just 3 steps: in projection with a linear layer, SDPA, and out projection with a linear layer.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# In Projection
# variable descriptions:
# q,k,v = Query, Key, Value tensors
# bsz = batch size
# num_heads = Numner of heads for Multihead Attention
# tgt_len = Target length
# src_len = Source Length
# head_dim: Head Dimension
    q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)
    q = q.view(bsz, num_heads, tgt_len, head_dim)
    k = k.view(bsz, num_heads, src_len, head_dim)
    v = v.view(bsz, num_heads, src_len, head_dim)

    # Scaled Dot Product Attention
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)

    # Out Projection
    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)
    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
    attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch 2. supports multiple different kernels optimized for specific use cases, with specific requirements. A kernel picker picks the best kernel for a particular combination of input parameters. If no optimized “custom kernel” for a particular combination of input parameters can be identified, the kernel picker selects a general kernel that can handle all input combinations.&lt;/p&gt;

&lt;p&gt;While future releases may extend this set of operators, PyTorch 2.0 launches with 3 implementations for the SDPA operator:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A generic kernel which implements the mathematical equation of SDPA in the function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sdpa_math()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;An optimized kernel based on the paper “&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;Flash Attention&lt;/a&gt;”, which supports evaluation of SDPA with 16 bit floating point data types on compute architecture SM80 (A100).&lt;/li&gt;
  &lt;li&gt;An optimized kernel based on the paper “&lt;a href=&quot;https://arxiv.org/abs/2112.0568&quot;&gt;Self-Attention Does Not Need O(n^2) Memory&lt;/a&gt;” and implemented in &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormer&lt;/a&gt;, which supports both 32 and 16 bit floating data types on a wider range of architectures (SM40 and later). This blog post refers to this kernel as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mem_efficient&lt;/code&gt; kernel.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that both optimized kernels (two and three listed above), support a key padding mask and limit the supported attention mask to causal attention. Accelerated PyTorch 2.0 Transformers today only support the causal mask when it is specified using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_causal&lt;/code&gt; boolean. When a mask is specified, the general-purpose kernel will be selected because it is too expensive to analyze the contents of a provided mask to determine if it is the causal mask. Additional explanations on the constraints for each kernel can be found in the &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-2/&quot;&gt;Accelerated PT2 Transformer blog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;enabling-accelerated-transformers-with-nanogpt&quot;&gt;Enabling Accelerated Transformers with nanoGPT&lt;/h2&gt;

&lt;p&gt;The SDPA operator being a critical component of the GPT model,  we identified the open source nanoGPT model as an excellent candidate for both demonstrating the ease of implementation and benefits of PyTorch 2.0’s Accelerated Transformers. The following demonstrates the exact process by which Accelerated Transformers was enabled on nanoGPT.&lt;/p&gt;

&lt;p&gt;This process largely revolves around replacing the existing SDPA implementation with the newly added F.scaled_dot_product_attention operator from &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/df14650f0b14b80db132b0c1797dc595fbee1054/torch/nn/functional.py#L4834&quot;&gt;functional.py&lt;/a&gt;. This process can be easily adapted to enable the operator in many other LLMs. Alternatively, users can instead choose to call F.multi_head_attention_forward() or utilize the nn.MultiHeadAttention module directly where applicable. The following code snippets are adapted from Karpathy’s nanoGPT repository.&lt;/p&gt;

&lt;h3 id=&quot;step-1-identify-the-existing-sdpa-implementation&quot;&gt;Step 1: Identify the existing SDPA implementation&lt;/h3&gt;

&lt;p&gt;In the case of nanoGPT, SDPA is implemented in the model’s &lt;a href=&quot;https://github.com/karpathy/nanoGPT/blob/master/model.py#L37&quot;&gt;CausalSelfAttention&lt;/a&gt; class. The original implementation at time of writing is adapted below for this post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/causal_attention_step_1.png&quot; alt=&quot;The original implementation at time of writing&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-2-replace-with-torchs-scaled_dot_product_attention&quot;&gt;Step 2: Replace with Torch’s &lt;em&gt;scaled_dot_product_attention&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;At this point we can note the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lines 36 - 42 define the mathematical implementation of SDPA which we are replacing&lt;/li&gt;
  &lt;li&gt;The mask applied on line 39 is no longer relevant since we are using scaled_dot_product_attention’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_causal&lt;/code&gt; flag.&lt;/li&gt;
  &lt;li&gt;The dropout layer used in line 41 is also now unnecessary.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Swapping out the SDPA implementation for torch’s scaled_dot_product_attention and removing the now redundant code yields the following implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/causal_attention_step_2.png&quot; alt=&quot;Swapping out the SDPA implementation for torch’s scaled_dot_product_attention and removing the now redundant code yields the following implementation.&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, the original mask can be passed into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn_mask&lt;/code&gt; field however due to the mentioned kernel constraints that would limit the implementation to only support the generic &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sdpa_math&lt;/code&gt; kernel.&lt;/p&gt;

&lt;h3 id=&quot;step-3-bonus-faster-matmuls-with-padding&quot;&gt;Step 3 (Bonus): Faster matmuls with padding&lt;/h3&gt;

&lt;p&gt;On top of the performance improvements from SDPA, our analysis yielded a nice ancillary win.  In Andrej’s words “The most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase the vocab size from 50257 to 50304 (nearest multiple of 64).”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/tweet.png&quot; alt=&quot;Tweet by Andrej Karpathy&quot; style=&quot;max-height:800px; width:100%; max-width:600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The vocab size determines the dimensions of matmuls in the output layer of GPT, and these are so large that they were taking a &lt;em&gt;majority&lt;/em&gt; of the time for the entire training loop!  We discovered that they were achieving performance significantly below the peak throughput achievable on the A100 GPU, and guessed from &lt;a href=&quot;https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html&quot;&gt;NVIDIA’s matmul documentation&lt;/a&gt; that 64-element alignment would yield better results.  Indeed, padding these matmuls achieves nearly a 3x speedup!  The underlying cause is that unaligned memory accesses significantly reduce efficiency.  A deeper analysis can be found in &lt;a href=&quot;https://twitter.com/cHHillee/status/1630274804795445248&quot;&gt;this Twitter thread&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this optimization we were able to further reduce training time from ~113 ms (using flash attention) to ~87 ms per batch.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The figure below demonstrates the performance gained using Pytorch custom kernels. Here are the exact figures:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;baseline (nanoGPT implementation):  ~143ms&lt;/li&gt;
  &lt;li&gt;sdpa_math (generic): ~134ms (6.71% faster)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mem_efficient&lt;/code&gt; kernel: ~119ms (20.16% faster)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flash_attention&lt;/code&gt; kernel: ~113ms (26.54% faster)&lt;/li&gt;
  &lt;li&gt;flash_attention + padded vocab:  ~87ms (64.37% faster)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All code was run on an 8 x NVIDIA Corporation A100 server with 80 GB HBM [A100 SXM4 80GB], and for the purpose of this experiment dropout was set to 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/PyTorch_Better-Transformer_Chart-2.png&quot; alt=&quot;Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;enhancing-numerical-model-stability&quot;&gt;Enhancing Numerical Model Stability&lt;/h2&gt;

&lt;p&gt;In addition to being faster, PyTorch’s implementation offers increased numerical stability by avoiding loss of precision in many execution scenarios. There is a great explanation &lt;a href=&quot;https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/118&quot;&gt;here&lt;/a&gt;, but essentially the PyTorch implementation scales the Query and Key matrices &lt;em&gt;before&lt;/em&gt; multiplication, which is said to be more stable and avoid loss of precision. Because of the merged custom kernel architecture of SDPA, this scaling does not introduce additional overhead in the computation of the attention result.  In comparison, an implementation from the individual computational components would require separate pre-scaling at additional cost. For an additional explanation, see Appendix A.&lt;/p&gt;

&lt;h3 id=&quot;improved-memory-consumption&quot;&gt;Improved Memory Consumption&lt;/h3&gt;

&lt;p&gt;Yet another large advantage of using the torch SDPA kernels is the reduced memory footprint, which allows for the utilization of larger batch sizes. The following chart compares the best validation loss after one hour of training for both flash attention and the baseline implementations of causal attention. As can be seen, the maximum batch size achieved with the baseline causal attention implementation (on 8 x NVIDIA Corporation A100 server with 80 GB HBM) was 24, significantly less then the maximum achieved with flash attention, which was 39.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/chart.png&quot; alt=&quot;Using Flash Attention enables the usage of larger batch sizes&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Using Flash Attention enables the usage of larger batch sizes, allowing users to achieve lower validation loss after one hour of training (smaller is better).&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Accelerated PyTorch 2 Transformers were designed to make the training and production deployment of state-of-the-art transformer models affordable and integrated with PyTorch 2.0 model JIT compilation.  The newly introduced PyTorch SDPA operator provides improved performance for training Transformer models and is particularly valuable for the expensive Large Language Model training. In this post we demonstrate a number of optimizations on the exemplary nanoGPT model  including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Over 26% training speedup, when compared against the baseline with constant batch size&lt;/li&gt;
  &lt;li&gt;An additional speedup achieved with padded vocabulary, bringing the total optimization to approximately 64% compared to the baseline&lt;/li&gt;
  &lt;li&gt;Additional numerical stability&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;appendix-a-analyzing-attention-numeric-stability&quot;&gt;Appendix A: Analyzing Attention Numeric Stability&lt;/h2&gt;

&lt;p&gt;In this section we provide a more in depth explanation of the previously mentioned enhanced numerical stability which is gained by prescaling SDPA’s input vectors. The following is a simplified version of nanoGPT’s mathematical implementation of SDPA. The important thing to note here is that the query undergoes matrix multiplication without being scaled.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# nanoGPT implementation of SDPA
# notice q (our query vector) is not scaled !
att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
att = F.softmax(att, dim=-1)

# Dropout is set to 0, so we can safely ignore this line in the implementation# att = self.attn_dropout(att) 

y_nanogpt = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&amp;gt; (B, nh, T, hs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following is the equivalent mathematical implementation in torch’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# PyTorch implementation of SDPA
embed_size = q.size(-1)
scaling_factor = math.sqrt(math.sqrt(embed_size))
q = q / scaling_factor 	# notice q _is_ scaled here !

# same as above, but with scaling factor
att = q @ (k.transpose(-2, -1) / scaling_factor)
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
att = F.softmax(att0, dim=-1)

# Dropout is set to 0, so we can safely ignore this line in the implementation# att = self.attn_dropout(att) 

y_scale_before = att @ v
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Mathematically both approaches should be equivalent, however our experimentation shows that in practice we receive different results from each approach.&lt;/p&gt;

&lt;p&gt;Using the approach above, we verified &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_scale_before&lt;/code&gt; matches the expected output from using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention &lt;/code&gt;method while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_nanogpt&lt;/code&gt; does not.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.allclose&lt;/code&gt; method was used to test equivalence. Specifically, we showed that:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y_sdpa = torch.nn.functional._scaled_dot_product_attention(
	q,
	k,
	v,
	attn_mask=self.bias[:,:,:T,:T] != 0,
	dropout_p=0.0,
	need_attn_weights=False,
	is_causal=False,
)

torch.allclose(y_sdpa, y_nanogpt) # False, indicating fp issues
torch.allclose(y_sdpa, y_scale_before) # True, as expected
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;appendix-b-reproducing-experiment-results&quot;&gt;Appendix B: Reproducing Experiment Results&lt;/h2&gt;

&lt;p&gt;Researchers seeking to reproduce these results should start with the following commit from Andrej’s nanoGPT repository - &lt;strong&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;b3c17c6c6a363357623f223aaa4a8b1e89d0a465&lt;/span&gt;&lt;/strong&gt;. This commit was used as the baseline when measuring the per batch speed improvements. For results which include padded vocabulary optimizations (which yielded the most significant improvements to batch speed), use the following commit - &lt;strong&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;77e7e04c2657846ddf30c1ca2dd9f7cbb93ddeab&lt;/span&gt;&lt;/strong&gt;. From either checkout, selecting kernels for experimentation is made trivial with the use of the &lt;a href=&quot;https://pytorch.org/docs/stable/backends.html&quot;&gt;torch.backends&lt;/a&gt; API.&lt;/p&gt;

&lt;p&gt;The desired kernel can be selected via a context manager:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.backends.cuda.sdp_kernel (
    enable_math = False,
    enable_flash = False,
    enable_mem_efficient = True
):
    train(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Lucas Pasqualin, Driss Guessous, Christian Puhrsch, Bertrand Maher, Michael Gschwind</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR. We show how to use Accelerated PyTorch 2.0 Transformers and the newly introduced torch.compile() method to accelerate Large Language Models on the example of nanoGPT, a compact open-source implementation of the GPT model from Andrej Karpathy. Using the new scaled dot product attention operator introduced with Accelerated PT2 Transformers, we select the flash_attention custom kernel and achieve faster training time per batch (measured with Nvidia A100 GPUs), going from a ~143ms/batch baseline to ~113 ms/batch. In addition, the enhanced implementation using the SDPA operator offers better numerical stability. Finally, further optimizations are achieved using padded inputs, which when combined with flash attention lead to ~87ms/batch.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Experience the power of PyTorch 2.0 on AMD Solutions</title>
      <link href="https://pytorch.org/blog/experience-power-pytorch-2.0/" rel="alternate" type="text/html" title="Experience the power of PyTorch 2.0 on AMD Solutions" />
      <published>2023-04-15T00:00:00-07:00</published>
      <updated>2023-04-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/experience-power-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/experience-power-pytorch-2.0/">&lt;p&gt;PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework.  The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that the PyTorch 2.0 stable release includes support for AMD Instinct™ and Radeon™ GPUs that are supported by the ROCm™ software platform.&lt;/p&gt;

&lt;p&gt;With the stable PyTorch 2.0 release, PyTorch 2.0 introduces torch.compile as a beta feature underpinned by TorchInductor with support for AMD Instinct and Radeon GPUs through OpenAI Triton deep learning compiler. Through TorchInductor, developers can now generate low level kernels using Triton that are portable and performant to hand-written kernels on native hardware centric kernel programming models.&lt;/p&gt;

&lt;p&gt;OpenAI Triton is a language and compiler for blocked algorithms, which aims to provide an abstraction layer between CUDA/HIP and Torch at which developers can write efficient kernels more productively.  We have written a new backend which interfaces Triton’s custom MLIR dialects with our ROCm compiler stack.&lt;/p&gt;

&lt;p&gt;Triton can automatically optimize kernels generated by machine learning compilers such as TorchInductor for multiple AI accelerators including AMD Instinct GPU accelerator by leveraging hardware-specific features of the AMD CDNA™ GPU architecture. This makes it easy for developers and users to switch seamlessly from any HW to AMD Instinct GPU accelerators and get great out of the box performance.&lt;/p&gt;

&lt;p&gt;In addition, compilers like Triton can also enable developers to use high-level programming languages, such as Python, to write machine learning code that can be efficiently compiled and executed on specialized hardware. This can help greatly improve the productivity of machine learning developers, as they can focus on the algorithmic aspects of their models and rely on the compiler to generate efficient code.&lt;/p&gt;

&lt;p&gt;By design, PyTorch 2.0 is backward compatible to earlier PyTorch releases. This holds true for the ROCm build of PyTorch 2.0 as well. Developers using PyTorch with AMD GPUs can migrate to PyTorch 2.0 with the confidence that their existing code will continue to work without any required changes, so there is no penalty to access the improvements that come with this release. On the other hand, using PyTorch 2.0 and TorchInductor can result in significant performance improvement over the default eager-mode as shown below.&lt;/p&gt;

&lt;p&gt;The initial results using AMD Instinct MI250 GPUs already shows strong performance improvement with minimal optimization on TorchInductor compared to the default eager-mode. We see an average performance increase of up to 1.54X on 44 out of the 45 models on HuggingFace benchmarks suite with CamemBert, DistillGPT2 and T5Small being a few of the standout models with up to 1.5X or more performance improvement over eager-mode. We are looking forward to continued engagement with members of the PyTorch team at Meta to enable further optimization on ROCm software stack and the additional performance improvement for future PyTorch releases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/t-vs-eager-mode.svg&quot; alt=&quot;Image 1: AMD MI250 GPU performance improvement for TorchInductor vs eager-mode using HuggingFace&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Image 1: AMD MI250 GPU performance improvement for TorchInductor vs eager-mode using HuggingFace &lt;sup&gt;MI200-89.&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.0 follows the same set of install options as before to build and install for supporting AMD GPUs. These include an installable Python package hosted at &lt;a href=&quot;https://pytorch.org/&quot;&gt;pytorch.org&lt;/a&gt;, AMD’s public PyTorch docker image, and of course the option to build from source using the upstream PyTorch repository. As with PyTorch builds for other platforms, the specific command line to be run for pip-based install is provided by the configurator at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The GPUs supported by the ROCm software platform which forms the basis for PyTorch support on AMD GPUs are documented at &lt;a href=&quot;https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html&quot;&gt;https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 represents a major step in continuing to broaden support for ML developers by increasing performance while maintaining a simple, Pythonic interface. This performance uplift is made possible in large part by the new TorchInductor infrastructure, which in turn harnesses the Triton ML programming language and just-in-time compiler. AMD’s support for these technologies allows users to realize the full promise of the new PyTorch architecture.  Our GPU support in PyTorch 2.0 is just one manifestation of a larger vision around AI and machine learning. AI/ML plays an important role in multiple AMD product lines, including Instinct and Radeon GPUs, Alveo™ data center accelerators, and both Ryzen™ and EPYC processors. These hardware and software initiatives are all part of AMD’s Pervasive AI vision, and we look forward to addressing the many new challenges and opportunities of this dynamic space.&lt;/p&gt;

&lt;p&gt;MI200-89 – PyTorch Inductor mode HuggingFace Transformers training speedup, running the standard PyTorch 2.0 test suite, over PyTorch eager-mode comparison based on AMD internal testing on a single GCD as of 3/10/2023 using a 2P AMD EPYC™ 7763 production server with 4x AMD Instinct™ MI250 (128GB HBM2e) 560W GPUs with Infinity Fabric™ technology; host ROCm™ 5.3, guest ROCm™ 5.4.4, PyTorch 2.0.0, Triton 2.0. Server manufacturers may vary configurations, yielding different results. Performance may vary based on factors including use of latest drivers and optimizations.&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;© 2023 Advanced Micro Devices, Inc. All rights reserved. AMD, the AMD Arrow logo, AMD CDNA, AMD Instinct, EPYC, Radeon, ROCm, Ryzen, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective owners.&lt;/small&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>AMD</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework. The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that the PyTorch 2.0 stable release includes support for AMD Instinct™ and Radeon™ GPUs that are supported by the ROCm™ software platform.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Generative Diffusion Models with PyTorch 2</title>
      <link href="https://pytorch.org/blog/accelerated-generative-diffusion-models/" rel="alternate" type="text/html" title="Accelerated Generative Diffusion Models with PyTorch 2" />
      <published>2023-04-14T00:00:00-07:00</published>
      <updated>2023-04-14T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-generative-diffusion-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-generative-diffusion-models/">&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;A large part of the recent progress in Generative AI came from denoising diffusion models, which allow producing high quality images and videos from text prompts. This family includes Imagen, DALLE, Latent Diffusion, and others. However, all models in this family share a common drawback: generation is rather slow, due to the iterative nature of the sampling process by which the images are produced. This makes it important to optimize the code running inside the sampling loop.&lt;/p&gt;

&lt;p&gt;We took an open source implementation of a popular text-to-image diffusion model as a starting point and accelerated its generation using two optimizations available in PyTorch 2: compilation and fast attention implementation. Together with a few minor memory processing improvements in the code these optimizations give up to 49% inference speedup relative to the original implementation without &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt;, and 39% inference speedup relative to using the original code with xFormers (excluding the compilation time), depending on the GPU architecture and batch size. Importantly, the speedup comes without a need to install xFormers or any other extra dependencies.&lt;/p&gt;

&lt;p&gt;The table below shows the improvement in runtime between the original implementation with xFormers installed and our optimized version with PyTorch-integrated memory efficient attention (originally developed for and released in the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; library)  and PyTorch compilation. The compilation time is excluded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Runtime improvement in % compared to original+xFormers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;See the absolute runtime numbers in section “Benchmarking setup and results summary”&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;GPU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 1&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 2&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 4&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;P100 (no compilation)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-3.8
   &lt;/td&gt;
   &lt;td&gt;0.44
   &lt;/td&gt;
   &lt;td&gt;5.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;2.12
   &lt;/td&gt;
   &lt;td&gt;10.51
   &lt;/td&gt;
   &lt;td&gt;14.2
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-2.34
   &lt;/td&gt;
   &lt;td&gt;8.99
   &lt;/td&gt;
   &lt;td&gt;10.57
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;18.63
   &lt;/td&gt;
   &lt;td&gt;6.39
   &lt;/td&gt;
   &lt;td&gt;10.43
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;38.5
   &lt;/td&gt;
   &lt;td&gt;20.33
   &lt;/td&gt;
   &lt;td&gt;12.17
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;One can notice the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The improvements are significant for powerful GPUs like A100 and V100. For those GPUs the improvement is most pronounced for batch size 1&lt;/li&gt;
  &lt;li&gt;For less powerful GPUs we observe smaller speedups (or in two cases slight regressions). The batch size trend is reversed here: improvement is larger for larger batches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following sections we describe the applied optimizations and provide detailed benchmarking data, comparing the generation time with various optimization features on/off.&lt;/p&gt;

&lt;p&gt;Specifically, we benchmark 5 configurations and the plots below compare their absolute performance for different GPUs and batch sizes. For definitions of these configurations see section “Benchmarking setup and results”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models1.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 1&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models2.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 2&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models3.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 1&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizations&quot;&gt;Optimizations&lt;/h2&gt;

&lt;p&gt;Here we’ll go into more detail about the optimizations introduced into the model code. These optimizations rely on features of PyTorch 2.0 which has been released recently.&lt;/p&gt;

&lt;h3 id=&quot;optimized-attention&quot;&gt;Optimized Attention&lt;/h3&gt;

&lt;p&gt;One part of the code which we optimized is the scaled dot-product attention. Attention is known to be a heavy operation: naive implementation materializes the attention matrix, leading to time and memory complexity quadratic in sequence length. It is common for diffusion models to use attention (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CrossAttention&lt;/code&gt;) as part of Transformer blocks in multiple parts of the U-Net. Since the U-Net runs at every sampling step, this becomes a critical point to optimize. Instead of custom attention implementation one can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.MultiheadAttention,&lt;/code&gt; which in PyTorch 2 has optimized attention implementation is integrated into it. This optimization schematically boils down to the following pseudocode:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class CrossAttention(nn.Module):
    def __init__(self, ...):
        # Create matrices: Q, K, V, out_proj
        ...
    def forward(self, x, context=None, mask=None):
       # Compute out = SoftMax(Q*K/sqrt(d))V
       # Return out_proj(out)
       …
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gets replaced with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class CrossAttention(nn.Module):
    def __init__(self, ...):
        self.mha = nn.MultiheadAttention(...)
    def forward(self, x, context):
	return self.mha(x, context, context)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The optimized implementation of attention was available already in PyTorch 1.13 (see &lt;a href=&quot;https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/&quot;&gt;here&lt;/a&gt;) and widely adopted (see e.g. &lt;a href=&quot;https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2&quot;&gt;HuggingFace transformers library example&lt;/a&gt;). In particular, it integrates memory-efficient attention from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; library and flash attention from &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;. PyTorch 2.0 expands this to additional attention functions such as cross attention and custom kernels for further acceleration, making it applicable to diffusion models.&lt;/p&gt;

&lt;p&gt;Flash attention is available on GPUs with compute capability SM 7.5 or SM 8.x - for example, on T4, A10, and A100, which are included in our benchmark (you can check compute capability of each NVIDIA GPU &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot;&gt;here&lt;/a&gt;). However, in our tests on A100 the memory efficient attention performed better than flash attention for the particular case of diffusion models, due to the small number of attention heads and small batch size.  PyTorch understands this and in this case chooses memory efficient attention over flash attention when both are available (see the logic &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/d8e795ecd53670682bd3b2e5ff1f378402b147d5/aten/src/ATen/native/transformers/cuda/sdp_utils.h#L33-L71&quot;&gt;here&lt;/a&gt;). For full control over the attention backends (memory-efficient attention, flash attention, “vanilla math”, or any future ones), power users can enable and disable them manually with the help of the context manager &lt;a href=&quot;https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel&quot;&gt;torch.backends.cuda.sdp_kernel&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;compilation&quot;&gt;Compilation&lt;/h3&gt;

&lt;p&gt;Compilation is a &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/#user-experience&quot;&gt;new feature of PyTorch 2.0&lt;/a&gt;, enabling significant speedups with a very simple user experience. To invoke the default behavior, simply wrap a PyTorch module or a function into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.compile(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch compiler then turns Python code into a set of instructions which can be executed efficiently without Python overhead. The compilation happens dynamically the first time the code is executed. With the default behavior, under the hood PyTorch utilized &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/index.html&quot;&gt;TorchDynamo&lt;/a&gt; to compile the code and &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747&quot;&gt;TorchInductor&lt;/a&gt; to further optimize it. See &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamo_tutorial.html&quot;&gt;this tutorial&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Although the one-liner above is enough for compilation, certain modifications in the code can squeeze a larger speedup. In particular, one should avoid so-called graph breaks - places in the code which PyTorch can’t compile. As opposed to previous PyTorch compilation approaches (like TorchScript), PyTorch 2 compiler doesn’t break in this case. Instead it falls back on eager execution - so the code runs, but with reduced performance. We introduced a few minor changes to the model code to get rid of graph breaks. This included eliminating functions from libraries not supported by the compiler, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inspect.isfunction&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;einops.rearrange&lt;/code&gt;. See this &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&quot;&gt;doc&lt;/a&gt; to learn more about graph breaks and how to eliminate them.&lt;/p&gt;

&lt;p&gt;Theoretically, one can apply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile &lt;/code&gt;on the whole diffusion sampling loop. However, in practice it is enough to just compile the U-Net. The reason is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; doesn’t yet have a loop analyzer and would recompile the code for each iteration of the sampling loop. Moreover, compiled sampler code is likely to generate graph breaks - so one would need to adjust it if one wants to get a good performance from the compiled version.&lt;/p&gt;

&lt;p&gt;Note that compilation &lt;a href=&quot;https://github.com/openai/triton/blob/b5d32896b1f89fc44a82f8df3bb010934c53f4f5/README.md?plain=1#L66-L68&quot;&gt;requires GPU compute capability &amp;gt;= SM 7.0&lt;/a&gt; to run in non-eager mode. This covers all GPUs in our benchmarks -  T4, V100, A10, A100 - except for P100 (see the &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot;&gt;full list&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;other-optimizations&quot;&gt;Other optimizations&lt;/h3&gt;

&lt;p&gt;In addition, we have improved efficiency of GPU memory operations by eliminating some common pitfalls, e.g. creating a tensor on GPU directly rather than creating it on CPU and later moving to GPU. The places where such optimizations were necessary were determined by line-profiling and looking at CPU/GPU traces and &lt;a href=&quot;https://github.com/brendangregg/FlameGraph&quot;&gt;Flame Graphs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-setup-and-results-summary&quot;&gt;Benchmarking setup and results summary&lt;/h2&gt;

&lt;p&gt;We have two versions of code to compare: &lt;em&gt;original&lt;/em&gt; and &lt;em&gt;optimized&lt;/em&gt;. On top of this, several optimization features (xFormers, PyTorch memory efficient attention, compilation) can be turned on/off. Overall, as mentioned in the introduction, we will be benchmarking 5 configurations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Original code without xFormers&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Original code with xFormers&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with vanilla math attention backend and no compilation&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with memory-efficient attention backend and no compilation&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with memory-efficient attention backend and compilation&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As the &lt;em&gt;original version&lt;/em&gt; we took the version of the code which uses PyTorch 1.12 and a custom implementation of attention. The &lt;em&gt;optimized version&lt;/em&gt; uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.MultiheadAttention&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CrossAttention&lt;/code&gt; and PyTorch 2.0.0.dev20230111+cu117. It also has a few other minor optimizations in PyTorch-related code.&lt;/p&gt;

&lt;p&gt;The table below shows runtime of each version of the code in seconds, and the percentage improvement compared to the _original with xFormers. _The compilation time is excluded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 1. In parenthesis - relative improvement with respect to the “Original with xFormers” row&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;30.4s (-19.3%)
   &lt;/td&gt;
   &lt;td&gt;29.8s (-77.3%)
   &lt;/td&gt;
   &lt;td&gt;13.0s (-83.9%)
   &lt;/td&gt;
   &lt;td&gt;10.9s (-33.1%)
   &lt;/td&gt;
   &lt;td&gt;8.0s (-19.3%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;25.5s&lt;/strong&gt; (0.0%)
   &lt;/td&gt;
   &lt;td&gt;16.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;7.1s&lt;/strong&gt; (0.0%)
   &lt;/td&gt;
   &lt;td&gt;8.2s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;6.7s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;27.3s (-7.0%)
   &lt;/td&gt;
   &lt;td&gt;19.9s (-18.7%)
   &lt;/td&gt;
   &lt;td&gt;13.2s (-87.2%)
   &lt;/td&gt;
   &lt;td&gt;7.5s (8.7%)
   &lt;/td&gt;
   &lt;td&gt;5.7s (15.1%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;26.5s (-3.8%)
   &lt;/td&gt;
   &lt;td&gt;16.8s (0.2%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;7.1s&lt;/strong&gt; (-0.8%)
   &lt;/td&gt;
   &lt;td&gt;6.9s (16.0%)
   &lt;/td&gt;
   &lt;td&gt;5.3s (20.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;16.4s &lt;/strong&gt;(2.1%)
   &lt;/td&gt;
   &lt;td&gt;7.2s (-2.3%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;6.6s&lt;/strong&gt; (18.6%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;4.1s&lt;/strong&gt; (38.5%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 2&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;58.0s (-21.6%)
   &lt;/td&gt;
   &lt;td&gt;57.6s (-84.0%)
   &lt;/td&gt;
   &lt;td&gt;24.4s (-95.2%)
   &lt;/td&gt;
   &lt;td&gt;18.6s (-63.0%)
   &lt;/td&gt;
   &lt;td&gt;12.0s (-50.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;47.7s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;31.3s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;12.5s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;11.4s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;8.0s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;49.3s (-3.5%)
   &lt;/td&gt;
   &lt;td&gt;37.9s (-21.0%)
   &lt;/td&gt;
   &lt;td&gt;17.8s (-42.2%)
   &lt;/td&gt;
   &lt;td&gt;12.7s (-10.7%)
   &lt;/td&gt;
   &lt;td&gt;7.8s (1.8%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;47.5s &lt;/strong&gt;(0.4%)
   &lt;/td&gt;
   &lt;td&gt;31.2s (0.5%)
   &lt;/td&gt;
   &lt;td&gt;12.2s (2.6%)
   &lt;/td&gt;
   &lt;td&gt;11.5s (-0.7%)
   &lt;/td&gt;
   &lt;td&gt;7.0s (12.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;28.0s&lt;/strong&gt; (10.5%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;11.4s&lt;/strong&gt; (9.0%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;10.7s &lt;/strong&gt;(6.4%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;6.4s&lt;/strong&gt; (20.3%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 4&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;117.9s (-20.0%)
   &lt;/td&gt;
   &lt;td&gt;112.4s (-81.8%)
   &lt;/td&gt;
   &lt;td&gt;47.2s (-101.7%)
   &lt;/td&gt;
   &lt;td&gt;35.8s (-71.9%)
   &lt;/td&gt;
   &lt;td&gt;22.8s (-78.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;98.3s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;61.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;23.4s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;20.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;12.7s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;101.1s (-2.9%)
   &lt;/td&gt;
   &lt;td&gt;73.0s (-18.0%)
   &lt;/td&gt;
   &lt;td&gt;28.3s (-21.0%)
   &lt;/td&gt;
   &lt;td&gt;23.3s (-11.9%)
   &lt;/td&gt;
   &lt;td&gt;14.5s (-13.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;92.9s &lt;/strong&gt;(5.5%)
   &lt;/td&gt;
   &lt;td&gt;61.1s (1.2%)
   &lt;/td&gt;
   &lt;td&gt;23.9s (-1.9%)
   &lt;/td&gt;
   &lt;td&gt;20.8s (-0.1%)
   &lt;/td&gt;
   &lt;td&gt;12.8s (-0.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;53.1s &lt;/strong&gt;(14.2%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;20.9s&lt;/strong&gt; (10.6%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;18.6s&lt;/strong&gt; (10.4%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;11.2s&lt;/strong&gt; (12.2%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To minimize fluctuations and external influence on the performance of the benchmarked code, we ran each version of the code one after another, and then repeated this sequence 10 times: A, B, C, D, E,  A, B, … So the results of a typical run would look like the one in the picture below.. Note that one shouldn’t rely on comparison of absolute run times between different graphs, but comparison of run times_ inside_ one graph is pretty reliable, thanks to our benchmarking setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models4.png&quot; alt=&quot;Denoising diffusion model generation benchmarks&quot; style=&quot;max-height:700px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each run of text-to-image generation script produces several batches, the number of which is regulated by the CLI parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--n_iter&lt;/code&gt;. In the benchmarks we used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_iter = 2&lt;/code&gt;, but introduced an additional “warm-up” iteration, which doesn’t contribute to the run time. This was necessary for the runs with compilation, because compilation happens the first time the code runs, and so the first iteration is much longer than all subsequent. To make comparison fair, we also introduced this additional “warm-up” iteration to all other runs.&lt;/p&gt;

&lt;p&gt;The numbers in the table above are for number of iterations 2 (plus a “warm-up one”), prompt ”A photo”, seed 1, PLMS sampler, and autocast turned on.&lt;/p&gt;

&lt;p&gt;Benchmarks were done using P100, V100, A100, A10 and T4 GPUs. The T4 benchmarks were done in Google Colab Pro. The A10 benchmarks were done on g5.4xlarge AWS instances with 1 GPU.&lt;/p&gt;

&lt;h2 id=&quot;conclusions-and-next-steps&quot;&gt;Conclusions and next steps&lt;/h2&gt;

&lt;p&gt;We have shown that new features of PyTorch 2 - compiler and optimized attention implementation - give performance improvements exceeding or comparable with what previously required installation of an external dependency (xFormers). PyTorch achieved this, in particular, by integrating memory efficient attention from xFormers into its codebase. This is a significant improvement for user experience, given that xFormers, being a state-of-the-art library, in many scenarios requires custom installation process and long builds.&lt;/p&gt;

&lt;p&gt;There are a few natural directions in which this work can be continued:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The optimizations we implemented and described here are only benchmarked for text-to-image inference so far. It would be interesting to see how they affect training performance. PyTorch compilation can be directly applied to training; enabling training with PyTorch optimized attention is on the roadmap&lt;/li&gt;
  &lt;li&gt;We intentionally minimized changes to the original model code. Further profiling and optimization can probably bring more improvements&lt;/li&gt;
  &lt;li&gt;At the moment compilation is applied only to the U-Net model inside the sampler. Since there is a lot happening outside of U-Net (e.g. operations directly in the sampling loop), it would be beneficial to compile the whole sampler. However, this would require analysis of the compilation process to avoid recompilation at every sampling step&lt;/li&gt;
  &lt;li&gt;Current code only applies compilation within the PLMS sampler, but it should be trivial to extend it to other samplers&lt;/li&gt;
  &lt;li&gt;Besides text-to-image generation, diffusion models are also applied to other tasks - image-to-image and inpainting. It would be interesting to measure how their performance improves from PyTorch 2 optimizations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See if you can increase performance of open source diffusion models using the methods we described, and share the results!&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch 2.0 overview, which has a lot of information on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile:&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;https://pytorch.org/get-started/pytorch-2.0/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tutorial on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;General compilation troubleshooting: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/troubleshooting.html&quot;&gt;https://pytorch.org/docs/master/dynamo/troubleshooting.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Details on graph breaks: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&quot;&gt;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Details on guards: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/guards-overview.html&quot;&gt;https://pytorch.org/docs/master/dynamo/guards-overview.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Video deep dive on TorchDynamo &lt;a href=&quot;https://www.youtube.com/watch?v=egZB5Uxki0I&quot;&gt;https://www.youtube.com/watch?v=egZB5Uxki0I&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tutorial on optimized attention in PyTorch 1.12: &lt;a href=&quot;https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html&quot;&gt;https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank Geeta Chauhan, Natalia Gimelshein, Patrick Labatut, Bert Maher, Mark Saroufim, Michael Voznesensky and Francisco Massa for their valuable advice and early feedback on the text.&lt;/p&gt;

&lt;p&gt;Special thanks to Yudong Tao initiating the work on using PyTorch native attention in diffusion models.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Grigory Sizov, Michael Gschwind, Hamid Shojanazeri, Driss Guessous, Daniel Haziza, Christian Puhrsch</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new torch.compile() compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Celebrate PyTorch 2.0 with New Performance Features for AI Developers</title>
      <link href="https://pytorch.org/blog/celebrate-pytorch-2.0/" rel="alternate" type="text/html" title="Celebrate PyTorch 2.0 with New Performance Features for AI Developers" />
      <published>2023-04-07T00:00:00-07:00</published>
      <updated>2023-04-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/celebrate-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/celebrate-pytorch-2.0/">&lt;p&gt;Congratulations to the PyTorch Foundation for its release of &lt;strong&gt;PyTorch 2.0&lt;/strong&gt;! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;TorchInductor&lt;/li&gt;
  &lt;li&gt;GNN&lt;/li&gt;
  &lt;li&gt;INT8 Inference Optimization&lt;/li&gt;
  &lt;li&gt;oneDNN Graph API&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We at Intel are delighted to be part of the PyTorch community and appreciate the collaboration with and feedback from our colleagues at &lt;a href=&quot;http://www.meta.com/&quot;&gt;Meta&lt;/a&gt; as we co-developed these features.&lt;/p&gt;

&lt;p&gt;Let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;1-torchinductor-cpu-fp32-inference-optimized&quot;&gt;1. TorchInductor CPU FP32 Inference Optimized&lt;/h2&gt;

&lt;p&gt;As part of the PyTorch 2.0 compilation stack, TorchInductor CPU backend optimization brings notable performance improvements via graph compilation over the PyTorch eager mode.&lt;/p&gt;

&lt;p&gt;The TorchInductor CPU backend is sped up by leveraging the technologies from the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; for Conv/GEMM ops with post-op fusion and weight prepacking, and PyTorch ATen CPU kernels for memory-bound ops with explicit vectorization on top of OpenMP*-based thread parallelization.&lt;/p&gt;

&lt;p&gt;With these optimizations on top of the powerful loop fusions in TorchInductor codegen, we achieved up to a &lt;strong&gt;1.7x&lt;/strong&gt; FP32 inference performance boost over three representative deep learning benchmarks: TorchBench, HuggingFace, and timm1. Training and low-precision support are under development.&lt;/p&gt;

&lt;h3 id=&quot;see-the-improvements&quot;&gt;See the Improvements&lt;/h3&gt;

&lt;p&gt;The performance improvements on various backends are tracked on this &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/93531#issuecomment-1457373890&quot;&gt;TouchInductor CPU Performance Dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;improve-graph-neural-network-gnn-in-pyg-for-inference-and-training-performance-on-cpu&quot;&gt;Improve Graph Neural Network (GNN) in PyG for Inference and Training Performance on CPU&lt;/h2&gt;

&lt;p&gt;GNN is a powerful tool to analyze graph structure data. This feature is designed to improve GNN inference and training performance on Intel® CPUs, including the new 4th Gen Intel® Xeon® Scalable processors.&lt;/p&gt;

&lt;p&gt;PyTorch Geometric (PyG) is a very popular library built upon PyTorch to perform GNN workflows. Currently on CPU, GNN models of PyG run slowly due to the lack of GNN-related sparse matrix multiplication operations (i.e., SpMM_reduce) and the lack of several critical kernel-level optimizations (scatter/gather, etc.) tuned for GNN compute.&lt;/p&gt;

&lt;p&gt;To address this, optimizations are provided for message passing between adjacent neural network nodes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;scatter_reduce:&lt;/strong&gt; performance hotspot in message-passing when the edge index is stored in coordinate format (COO).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;gather:&lt;/strong&gt; backward computation of scatter_reduce, specially tuned for the GNN compute when the index is an expanded tensor.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;torch.sparse.mm with reduce flag:&lt;/strong&gt; performance hotspot in message-passing when the edge index is stored in compressed sparse row (CSR). Supported reduce flag for: sum, mean, amax, amin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;End-to-end performance benchmark results for both inference and training on 3rd Gen Intel® Xeon® Scalable processors 8380 platform and on 4th Gen 8480+ platform are discussed in &lt;a href=&quot;http://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;Accelerating PyG on Intel CPUs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;optimize-int8-inference-with-unified-quantization-backend-for-x86-cpu-platforms&quot;&gt;Optimize int8 Inference with Unified Quantization Backend for x86 CPU Platforms&lt;/h2&gt;

&lt;p&gt;The new X86 quantization backend is a combination of &lt;a href=&quot;http://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt; (Facebook General Matrix-Matrix Multiplication) and &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneAPI Deep Neural Network Library (oneDNN&lt;/a&gt;) backends and replaces FBGEMM as the default quantization backend for x86 platforms. The result: better end-to-end int8 inference performance than FBGEMM.&lt;/p&gt;

&lt;p&gt;Users access the x86 quantization backend by default for x86 platforms, and the selection between different kernels is automatically done behind the scenes. The rules of selection are based on prior performance testing data done by Intel during feature development. Thus, the x86 backend replaces FBGEMM and may offer better performance, depending on the use case.&lt;/p&gt;

&lt;p&gt;The selection rules are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On platforms without VNNI (e.g., Intel® Core™ i7 processors), FBGEMM is always used.&lt;/li&gt;
  &lt;li&gt;On platforms with VNNI (e.g., 2nd-4th Gen Intel® Xeon® Scalable processors and future platforms):
    &lt;ul&gt;
      &lt;li&gt;For linear, FBGEMM is always used.&lt;/li&gt;
      &lt;li&gt;For convolution layers, FBGEMM is used for depth-wise convolution whose layers &amp;gt; 100; otherwise, oneDNN is used.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that as the kernels continue to evolve.&lt;/p&gt;

&lt;p&gt;The selection rules above are subject to change to achieve better performance. Performance metrics for through-put speed-up ratios of unified x86 backend vs. pure FBGEMM are discussed in &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;[RFC] Unified quantization backend for x86 CPU platforms #83888&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;leverage-onednn-graph-api-to-accelerate-inference-on-cpu&quot;&gt;Leverage oneDNN Graph API to Accelerate Inference on CPU&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on Intel® AI hardware. It automatically identifies the graph partitions to be accelerated via fusion. The &lt;a href=&quot;http://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul, and their neighbor operations for both inference and training use cases.&lt;/p&gt;

&lt;p&gt;Currently, BFloat16 and Float32 datatypes are supported and only inference workloads can be optimized.  BF16 is only optimized on machines with Intel® Advanced Vector Extensions 512 (Intel® AVX-512) BF16 support.&lt;/p&gt;

&lt;p&gt;Few or no modifications are needed in PyTorch to support newer oneDNN Graph fusions/optimized kernels. To use oneDNN Graph, users can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Either use the API &lt;em&gt;torch.jit.enable_onednn_fusion(True)&lt;/em&gt; before JIT tracing a model, OR …&lt;/li&gt;
  &lt;li&gt;Use its context manager, viz. &lt;em&gt;with torch.jit.fuser(“fuser3”).&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;For accelerating &lt;a href=&quot;http://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16 inference&lt;/a&gt;, we rely on eager-mode AMP (Automatic Mixed Precision) support in PyTorch and disable JIT mode’s AMP.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;PyTorch performance tuning guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;h3 id=&quot;get-the-software&quot;&gt;Get the Software&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://pytorch.org/get-started/locally/&quot;&gt;Try out PyTorch 2.0&lt;/a&gt; and realize the performance benefits for yourself from these Intel-contributed features.&lt;/p&gt;

&lt;p&gt;We encourage you to check out Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Framework&lt;/a&gt; optimizations and learn about the open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel’s AI software portfolio.&lt;/p&gt;

&lt;p&gt;For more details about 4th Gen Intel Xeon Scalable processor, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI Platform&lt;/a&gt; where you can learn about how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-resources&quot;&gt;PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Congratulations to the PyTorch Foundation for its release of PyTorch 2.0! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Straggler Mitigation On PyTorch DDP By Hierarchical SGD</title>
      <link href="https://pytorch.org/blog/straggler-mitigation/" rel="alternate" type="text/html" title="Straggler Mitigation On PyTorch DDP By Hierarchical SGD" />
      <published>2023-04-07T00:00:00-07:00</published>
      <updated>2023-04-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/straggler-mitigation</id>
      <content type="html" xml:base="https://pytorch.org/blog/straggler-mitigation/">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/ddp.html&quot;&gt;PyTorch DDP&lt;/a&gt; has been widely adopted across the industry for distributed training, which by default runs synchronous SGD to synchronize gradients across model replicas at every step. The performance of this technique is critical for fast iteration during model exploration as well as resource and cost saving. The performance is critical for fast iteration and cost saving of model development and exploration. To resolve a ubiquitous performance bottleneck introduced by slow nodes in large-scale training, Cruise and Meta co-developed a solution based on the &lt;a href=&quot;https://arxiv.org/abs/2007.13819&quot;&gt;Hierarchical SGD&lt;/a&gt; algorithm to significantly accelerate training in the presence of these stragglers.&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-straggler-mitigation&quot;&gt;The Need For Straggler Mitigation&lt;/h2&gt;

&lt;p&gt;In DDP setup, a straggler problem can occur when one or more processes run much slower (“stragglers”) than other processes. When this happens, all the processes have to wait for the stragglers before synchronizing gradients and completing the communication, which essentially bottlenecks distributed performance to the slowest worker.As a result, even for the cases of training relatively small models, the communication cost can still be a major performance bottleneck.&lt;/p&gt;

&lt;h3 id=&quot;potential-causes-of-stragglers&quot;&gt;Potential Causes of Stragglers&lt;/h3&gt;

&lt;p&gt;Severe straggler issues are usually caused by workload imbalance before synchronization, and many factors can contribute to this imbalance. For instance, some data loader workers in the distributed environment can become stragglers, because some input examples can be outliers in terms of the data size, or the data transfer of some examples can be drastically slowed down due to unstable network I/O, or the on-the-fly data transformation costs can have a high variance.&lt;/p&gt;

&lt;p&gt;Besides data loading, other phases before gradient synchronization can also cause stragglers, such as unbalanced workloads of embedding table lookup during the forward pass in recommendation systems.&lt;/p&gt;

&lt;h3 id=&quot;the-appearance-of-stragglers&quot;&gt;The Appearance of Stragglers&lt;/h3&gt;

&lt;p&gt;If we profile DDP training jobs that have stragglers, we can find that some processes may have much higher gradient synchronization costs (a.k.a., allreducing gradients) than other processes at a certain step. As a result, the distributed performance can be dominated by the communication cost even if the model size is very small. In this case, some processes run faster than the straggler(s) at a step, and hence they have to wait for the stragglers and spend a much longer time on allreduce.&lt;/p&gt;

&lt;p&gt;The below shows screenshots of two trace files output by PyTorch profiler in a use case. Each screenshot profiles 3 steps.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first screenshot shows that a process has a very high allreduce cost in both the first and the third steps, because this process reaches the synchronization phase earlier than the straggler(s), and it spends more time on waiting. On the other hand, the allreduce cost is relatively small in the second step, this suggests that 1) there is no straggler at this step; or 2) this process is the straggler among all the processes, so it does not need to wait for any other process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-1.png&quot; alt=&quot;chart showing allreduce cost&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Both the 1st and the 3rd Steps Are Slowed Down by Stragglers&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The second screenshot shows a normal case without stragglers. In this case, all the gradient synchronizations are relatively short.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-2.png&quot; alt=&quot;chart showing normal case without stragglers&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Normal Case Without Stragglers&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;hierarchical-sgd-in-pytorch&quot;&gt;Hierarchical SGD in PyTorch&lt;/h2&gt;

&lt;p&gt;Recently hierarchical SGD has been proposed to optimize the communication costs by mainly reducing the total amount of data transfer in large-scale distributed training, and multiple convergence analyses have been provided (&lt;a href=&quot;https://arxiv.org/pdf/2010.12998.pdf&quot;&gt;example&lt;/a&gt;). As a main novelty of this post, at Cruise we could leverage hierarchical SGD to mitigate stragglers, which may also occur on training relatively small models. Our implementation has been upstreamed by Cruise to PyTorch in early 2022.&lt;/p&gt;

&lt;h3 id=&quot;how-does-hierarchical-sgd-work&quot;&gt;How Does Hierarchical SGD Work?&lt;/h3&gt;

&lt;p&gt;As the name implies, hierarchical SGD organizes all the processes into groups at different levels as a hierarchy, and runs synchronization by following the rules below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the groups at the same level have the same number of processes, and the processes in these groups synchronize at the same frequency concurrently, where the synchronization period is pre-defined by the user.&lt;/li&gt;
  &lt;li&gt;The higher level a group is, the larger synchronization period is used, as the synchronization becomes more expensive.&lt;/li&gt;
  &lt;li&gt;When multiple overlapping groups are supposed to synchronize according to their periods, to reduce redundant synchronization and avoid data race across groups, only the highest-level group runs synchronization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following figure illustrates an example of 4-level hierarchy SGD among 16 processes on 8 machines, each of which has 2 GPUs:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Level 1:&lt;/strong&gt; Each process runs mini-batch SGD locally;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 2:&lt;/strong&gt; Each 4-process group across 2 machines runs synchronization every 2 steps;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 3:&lt;/strong&gt; Each 8-process group across 4 machines runs synchronization every 4 steps;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 4:&lt;/strong&gt; The global process group of all 16 processes over 8 machines runs synchronization every 8 steps.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Particularly, when the step number can be divided by 8, only the synchronization at 3) is executed, and when the step number can be divided by 4 but not 8, only the synchronization at 2) is executed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-3.png&quot; alt=&quot;An example of 4-level hierarchy SGD among 16 processes on 8 machines, each of which has 2 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Intuitively, hierarchical SGD can be viewed as an extension of &lt;a href=&quot;https://core.ac.uk/download/pdf/211998087.pdf&quot;&gt;local SGD&lt;/a&gt;, which only has a two-level hierarchy – every process runs mini-batch SGD locally and then synchronizes globally at a certain frequency. This can also help explain that, just like local SGD, hierarchical SGD synchronizes model parameters instead of gradients. Otherwise the gradient descent will be mathematically incorrect when the frequency is greater than 1.&lt;/p&gt;

&lt;h3 id=&quot;why-can-hierarchical-sgd-mitigate-stragglers&quot;&gt;Why Can Hierarchical SGD Mitigate Stragglers?&lt;/h3&gt;

&lt;p&gt;The key insight here is that, when there is a random straggler, it only directly slows down a relatively small group of processes instead of all the processes. Next time another random straggler is very likely to slow down a different small group, and hence a hierarchy can help smooth out the straggler effect.&lt;/p&gt;

&lt;p&gt;The example below assumes that there is a random straggler among totally 8 processes at every step. After 4 steps, vanilla DDP that runs synchronous SGD will be slowed down by straggler 4 times, because it runs global synchronization at every step. In contrast, hierarchical SGD runs synchronization with the groups of 4 processes after the first two steps, and then a global synchronization after another two steps. We can see that both the first two and the last two stragglers have a large overlap, and hence the performance loss can be mitigated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-4.png&quot; alt=&quot;flow diagram&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Essentially, the mitigation effect of this hierarchical SGD example actually is between local SGD at a frequency of every 2 steps and every 4 steps. The main advantage of hierarchical SGD over local SGD is a better convergence efficiency of the same global synchronization frequency, because hierarchical SGD allows more low-level synchronization. Moreover, it is possible for hierarchical SGD to provide a global synchronization frequency lower than local SGD with model parity, leading to a higher training performance, especially in a large-scale distributed training.&lt;/p&gt;

&lt;h3 id=&quot;ease-of-use&quot;&gt;Ease of Use&lt;/h3&gt;

&lt;p&gt;Straggler mitigation is not a novel study in distributed training. Multiple approaches have been proposed, such as &lt;a href=&quot;https://arxiv.org/pdf/1705.09056.pdf&quot;&gt;gossip SGD&lt;/a&gt;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf&quot;&gt;data encoding&lt;/a&gt;, &lt;a href=&quot;http://proceedings.mlr.press/v70/tandon17a/tandon17a.pdf&quot;&gt;gradient coding&lt;/a&gt;, as well as some particularly designed for parameter-server architecture, including &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45187.pdf&quot;&gt;backup workers&lt;/a&gt; and &lt;a href=&quot;http://www.cs.cmu.edu/~seunghak/SSPTable_NIPS2013.pdf&quot;&gt;stale synchronous parallel&lt;/a&gt;. However, to the best of our knowledge, before this effort we have not found a good open-source PyTorch implementation of straggler mitigation that can work like a plugin to our training system at Cruise. In contrast, our implementation only requires the minimal changes – no need to modify the existing code or tune any existing hyperparameters. This is a very appealing advantage for industry users.&lt;/p&gt;

&lt;p&gt;As the code example below shows, only a few lines need to be added to the setup of DDP model, and the training loop code can keep untouched. As explained previously, hierarchical SGD is an extended form of local SGD, so the enablement can be quite similar to local SGD (see PyTorch docs of &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer&quot;&gt;PostLocalSGDOptimizer&lt;/a&gt;):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Register a post-local SGD communication hook to run a warmup stage of fully synchronous SGD and defer hierarchical SGD.&lt;/li&gt;
  &lt;li&gt;Create a post-local SGD optimizer that wraps an existing local optimizer and a hierarchical SGD configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch.distributed.algorithms.model_averaging.hierarchical_model_averager as hierarchicalSGD
from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (
    PostLocalSGDState,
    post_localSGD_hook,
)
from torch.distributed.optim import PostLocalSGDOptimizer

ddp_model = nn.parallel.DistributedDataParallel(
    module=model,
    device_ids=[rank],
)

# Register a post-local SGD communication hook for the warmup.
subgroup, _ = torch.distributed.new_subgroups()
state = PostLocalSGDState(subgroup=subgroup, start_localSGD_iter=1_000)
ddp_model.register_comm_hook(state, post_localSGD_hook)

# Wraps the existing (local) optimizer to run hierarchical model averaging.
optim = PostLocalSGDOptimizer(
  optim=optim,
  averager=hierarchicalSGD.HierarchicalModelAverager(
    # The config runs a 4-level hierarchy SGD among 128 processes:
    # 1) Each process runs mini-batch SGD locally;
    # 2) Each 8-process group synchronize every 2 steps;
    # 3) Each 32-process group synchronize every 4 steps;
    # 4) All 128 processes synchronize every 8 steps.
    period_group_size_dict=OrderedDict([(2, 8), (4, 32), (8, 128)]),
    # Do not run hierarchical SGD until 1K steps for model parity.
    warmup_steps=1_000)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;algorithm-hyperparameters&quot;&gt;Algorithm Hyperparameters&lt;/h3&gt;

&lt;p&gt;Hierarchical SGD has two major hyperparameters: &lt;em&gt;period_group_size_dict&lt;/em&gt; and &lt;em&gt;warmup_steps&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;period_group_size_dict&lt;/strong&gt; is an ordered dictionary mapping from synchronization period to process group size, used for initializing process groups of different sizes in a hierarchy to synchronize parameters concurrently. A larger group is expected to use a larger synchronization period.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;warmup_steps&lt;/strong&gt; specifies a number of steps as the warmup stage to run synchronous SGD before hierarchical SGD. Similar to &lt;a href=&quot;https://arxiv.org/pdf/1808.07217.pdf&quot;&gt;post-local SGD&lt;/a&gt; algorithm, a warmup stage is usually recommended to achieve a higher accuracy. The value should be the same as &lt;em&gt;start_localSGD_iter&lt;/em&gt; arg used in &lt;em&gt;PostLocalSGDState&lt;/em&gt; when post_localSGD_hook is registered. Typically the warmup stage should at least cover the beginning of training when the loss is decreased drastically.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A subtle difference between the PyTorch implementation and the initial design proposed by relevant papers is that, after the warmup stage, by default the processes within each host still run intra-host gradient synchronization at every step. This is because that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The intra-host communication is relatively cheap, and it can usually significantly accelerate the convergence;&lt;/li&gt;
  &lt;li&gt;The intra-host group (of size 4 or 8 for most industry users) can usually be a good choice of the smallest group of processes that synchronize most frequently in hierarchical SGD. If the synchronization period is 1, then gradient synchronization is faster than model parameter synchronization (a.k.a., model averaging), because DDP automatically overlaps gradient synchronization and the backward pass.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Such intra-host gradient synchronization can be disabled by unsetting &lt;em&gt;post_local_gradient_allreduce&lt;/em&gt; arg in &lt;em&gt;PostLocalSGDState&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;demonstration&quot;&gt;Demonstration&lt;/h2&gt;

&lt;p&gt;Now we demonstrate that hierarchical SGD can accelerate distributed training by mitigating stragglers.&lt;/p&gt;

&lt;h3 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h3&gt;

&lt;p&gt;We compared the performance of hierarchical SGD against local SGD and synchronous SGD on &lt;a href=&quot;https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html&quot;&gt;ResNet18&lt;/a&gt; (model size: 45MB). Since the model is so small, the training is not bottlenecked by data transfer cost during synchronization. To avoid the noises incurred by data loading from remote storage, the input data was randomly simulated from memory. We varied the number of GPUs used by training from 64 to 256. The batch size per worker is 32, and the number of iterations of training is 1,000. Since we don’t evaluate convergence efficiency in this set of experiments, warmup is not enabled.&lt;/p&gt;

&lt;p&gt;We also emulated stragglers at a rate of 1% on 128 and 256 GPUs, and 2% on 64 GPUs, to make sure at least one stragglers at every step on average. These stragglers randomly appear on different CUDA devices. Each straggler stalls for 1 second besides the normal per-step training time (~55ms in our setup). This can be perceived as a practical scenario where 1% or 2% of input data are outliers in terms of the data pre-processing cost (I/O and/or data transformation on the fly) during training, and such cost is 20X+ larger than the average.&lt;/p&gt;

&lt;p&gt;The code snippet below shows how a straggler can be emulated in the training loop. We applied it to a ResNet model, and it can be easily applied to the other models as well.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     loss = loss_fn(y_pred, y)
     # Emulate a straggler that lags for 1 second at a rate of 1%.
     if random.randint(1, 100) == 1:
         time.sleep(1)
     loss.backward()
     optimizer.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The experiments are conducted on us-central1 GCP cluster. Each machine has 4 NVIDIA Tesla T4 GPUs with 16 GB memory per GPU, connected through a 32 Gbit/s ethernet network. Each instance also features 96 vCPUs, 360 GB RAM.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot; style=&quot;max-width: 450px;&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Architecture
   &lt;/td&gt;
   &lt;td&gt;ResNet18 (45MB)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Workers
   &lt;/td&gt;
   &lt;td&gt;64, 128, 256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Backend
   &lt;/td&gt;
   &lt;td&gt;NCCL
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;Tesla T4, 16 GB memory
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Batch size
   &lt;/td&gt;
   &lt;td&gt;32 x ## of workers
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Straggler Duration
   &lt;/td&gt;
   &lt;td&gt;1 sec
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Straggler Rate
   &lt;/td&gt;
   &lt;td&gt;1% on 128 and 256 GPUs, 2% on 64 GPUs
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We used multiple configurations for both local SGD and hierarchical SGD. Local SGD runs global synchronization every 2, 4, and 8 steps, respectively.&lt;/p&gt;

&lt;p&gt;We ran hierarchical SGD with the following configurations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;On 64 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 8-process group, 32-process, and the global 64-process group synchronizes every 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-32,8-64&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 32-process group and the global 64-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-32,8-64&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;On 128 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 8-process group, 32-process group, and the global 128-process group synchronizes every 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-32,8-128&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 32-process group and the global 128-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-32,8-128&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;On 256 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 4-process group, 16-process group, 64-process group, and the global 256-process group synchronizes every 1, 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 1-4,2-16,4-64,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 8-process group, 64-process group, and the global 256-process group synchronizes every 2, 4, and 8 steps. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-64,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 16-process group and the global 256-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-16,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h3&gt;

&lt;p&gt;The figures below show the speedups of different communication schemes against the baseline of synchronous SGD, with the emulated stragglers. We can make the following observations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As expected, we can see that both hierarchical SGD and local SGD can achieve a higher speedup with a lower synchronization frequency.&lt;/li&gt;
  &lt;li&gt;The speedups of the hierarchical SGD schemes are &lt;strong&gt;2.08X-2.45X&lt;/strong&gt; on 64 GPUs, &lt;strong&gt;2.57X-2.68X&lt;/strong&gt; on 128 GPUs, and &lt;strong&gt;2.63X-3.25X&lt;/strong&gt; on 256 GPUs, respectively. This shows that hierarchical SGD can significantly mitigate stragglers, and such mitigation can be more effective at a larger scale.&lt;/li&gt;
  &lt;li&gt;The performance of local SGD with the synchronization period of 2 steps and 8 steps can be perceived as the lower bound and upper bound of the experimented hierarchical SGD schemes, respectively. This is because the hierarchical SGD schemes synchronize less frequently than every 2 steps globally, but their low-level synchronization at small groups are the extra overheads in comparison with the global synchronization every 8 steps.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Overall, hierarchical SGD can provide a finer-grained trade-off between communication cost and model quality than local SGD. Therefore, when local SGD at a relatively large synchronization period like 8 or 4 cannot give a satisfactory convergence efficiency, hierarchical SGD can have a much better chance to achieve both a good speedup and a model parity.&lt;/p&gt;

&lt;p&gt;Since only simulated data is used in the experiments, we did not demonstrate the model parity here, which in practice can be achieved in two ways:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Tuning the hyperparameters including both hierarchy and warmup steps;&lt;/li&gt;
  &lt;li&gt;For some cases, hierarchical SGD could lead to a slightly lower quality than the original model for the same number of training steps (i.e., lower convergence rate), but with a speedup like 2X+ per training step, it is still possible to achieve model parity with more steps but still less total training time.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-5.png&quot; alt=&quot;Speedups on 64 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-6.png&quot; alt=&quot;Speedups on 128 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-7.png&quot; alt=&quot;Speedups on 256 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;Before applying hierarchical SGD to straggler mitigation, the user should be aware of a few limitations of this approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;This approach can only mitigate non-persistent stragglers, which occur to different workers at different times. However, for the case of persistent stragglers, which can be caused by hardware degradation or a network issue on a specific host, these stragglers will slow down the same low-level subgroup at every time, leading to nearly no straggler mitigation.&lt;/li&gt;
  &lt;li&gt;This approach can only mitigate low-frequency stragglers. E.g., if 30% workers can randomly become stragglers at every step, then most low-level synchronizations will still be slowed down by stragglers. As a result, hierarchical SGD may not show an obvious performance advantage over synchronous SGD.&lt;/li&gt;
  &lt;li&gt;Since hierarchical SGD applies model averaging that does not overlap with backward like gradient averaging used by vanilla DDP, its performance gain of straggler mitigation must outweigh the performance loss of no overlap between communication and backward pass. Therefore, if stragglers only slow down training by less than 10%, hierarchical SGD may not be able to bring much speedup. This limitation can be addressed by &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/release/1.13/torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py&quot;&gt;overlapping optimizer step and backward pass&lt;/a&gt; in the future.&lt;/li&gt;
  &lt;li&gt;Since hierarchical SGD is less well-studied than local SGD, there is no guarantee that hierarchical SGD with a finer-grained synchronization granularity can converge faster than certain advanced forms of local SGD, such as &lt;a href=&quot;https://openreview.net/pdf?id=SkxJ8REYPH&quot;&gt;SlowMo&lt;/a&gt;, which can improve convergence efficiency with slow momentum. However, to the best of our knowledge, these advanced algorithms cannot be natively supported as a PyTorch DDP plugin like hierarchical SGD yet.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank Cruise teammates &lt;strong&gt;Bo Tian&lt;/strong&gt;, &lt;strong&gt;Sergei Vorobev&lt;/strong&gt;, &lt;strong&gt;Eugene Selivonchyk, Tsugn-Hsien Lee&lt;/strong&gt;, &lt;strong&gt;Dan Ring&lt;/strong&gt;, &lt;strong&gt;Ian Ackerman&lt;/strong&gt;, &lt;strong&gt;Lei Chen&lt;/strong&gt;, &lt;strong&gt;Maegan Chew&lt;/strong&gt;, &lt;strong&gt;Viet Anh To&lt;/strong&gt;, &lt;strong&gt;Xiaohui Long&lt;/strong&gt;, &lt;strong&gt;Zeyu Chen&lt;/strong&gt;, &lt;strong&gt;Alexander Sidorov&lt;/strong&gt;, &lt;strong&gt;Igor Tsvetkov&lt;/strong&gt;, &lt;strong&gt;Xin Hu&lt;/strong&gt;, &lt;strong&gt;Manav Kataria&lt;/strong&gt;, &lt;strong&gt;Marina Rubtsova&lt;/strong&gt;, and &lt;strong&gt;Mohamed Fawzy&lt;/strong&gt;, as well as Meta teammates &lt;strong&gt;Shen Li, Yanli Zhao, Suraj Subramanian, Hamid Shojanzeri, Anjali Sridhar&lt;/strong&gt; and &lt;strong&gt;Bernard Nguyen&lt;/strong&gt; for the support.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yi Wang (Cruise AI), Rohan Varma (Meta AI)</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch DDP has been widely adopted across the industry for distributed training, which by default runs synchronous SGD to synchronize gradients across model replicas at every step. The performance of this technique is critical for fast iteration during model exploration as well as resource and cost saving. The performance is critical for fast iteration and cost saving of model development and exploration. To resolve a ubiquitous performance bottleneck introduced by slow nodes in large-scale training, Cruise and Meta co-developed a solution based on the Hierarchical SGD algorithm to significantly accelerate training in the presence of these stragglers.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch &amp;amp; OpenXLA: The Path Forward</title>
      <link href="https://pytorch.org/blog/pytorch-2.0-xla-path-forward/" rel="alternate" type="text/html" title="PyTorch &amp; OpenXLA: The Path Forward" />
      <published>2023-04-03T00:00:00-07:00</published>
      <updated>2023-04-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2.0-xla-path-forward</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2.0-xla-path-forward/">&lt;p&gt;As we celebrate the release of &lt;a href=&quot;https://opensource.googleblog.com/2023/03/openxla-is-ready-to-accelerate-and-simplify-ml-development.html&quot;&gt;OpenXLA&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-release/&quot;&gt;PyTorch 2.0&lt;/a&gt;, and &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla/&quot;&gt;PyTorch/XLA 2.0&lt;/a&gt;, it’s worth taking a step back and sharing where we see it all going in the short to medium term. With PyTorch adoption leading in the AI space and XLA supporting best-in-class compiler features, PyTorch/XLA is well positioned to provide a cutting edge development stack for both model training and inference. To achieve this, we see investments in three main areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Training Large Models&lt;/strong&gt; - Large language models (LLM) and diffusion models have quickly risen in popularity and many cutting edge applications today are built on them. Further to this, training these models requires scale and more specifically the ability to train across thousands of accelerators. To achieve this we are investing in features such as AMP for mixed precision training, PjRt for increased runtime performance, SPMD / FSDP for efficient model sharding, Dynamic Shapes to enable new research approaches, faster data loading through Ray and tf.data, and a toolchain that packages all of these features together into a seamless workflow. Some of these features are already available in experimental or beta stages, and others are coming up this year with many heavily leveraging the underlying OpenXLA compiler stack.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Inference&lt;/strong&gt; - With large models continuing to grow in size and computational cost, deployment becomes the next challenge as these models continue to find their way into applications. With the introduction of Dynamo in the PyTorch 2.0 release, PyTorch/XLA delivers performance competitive inference. We are, however, incorporating additional inference-oriented including model serving support, Dynamo for sharded large models, quantization via Torch.Export and StableHLO.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ecosystem integration&lt;/strong&gt; - We are expanding integration with &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; and &lt;a href=&quot;https://lightning.ai/docs/pytorch/stable/&quot;&gt;PyTorch Lightning&lt;/a&gt; so users can take advantage of upcoming PyTorch/XLA cutting edge features (e.g. FSDP support in Hugging Face) and the downstream OpenXLA features (e.g. Quantization) through familiar APIs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, PyTorch/XLA is set to migrate to the open source &lt;a href=&quot;https://github.com/openxla&quot;&gt;OpenXLA&lt;/a&gt; as its default downstream compiler; allowing the PyTorch community to gain access to a leading, framework-agnostic compiler stack that enjoys industry-wide contribution and innovation. To achieve this, we will begin supporting StableHLO. As a result, OpenXLA will replace the existing TF:XLA dependency, overall streamlining the dependencies and creating leverage from the broader compiler ecosystem. PyTorch/XLA will also sunset the XRT runtime after migration. You can see the resulting high level stack below with the TensorFlow dependency stricken out:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/PyTorch_XLA Future Stack.svg&quot; alt=&quot;the upcoming PyTorch/XLA features and integrations&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure:&lt;/strong&gt; the upcoming PyTorch/XLA features and integrations are illustrated here&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;We cannot be more excited about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source so please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; such that we can openly collaborate. You can also &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;try out&lt;/a&gt; PyTorch/XLA for yourself on various XLA devices including TPUs and GPUs.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br /&gt;
The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Milad Mohammadi, Jack Cao, Shauheen Zahirazami, Joe Spisak, and Jiewen Tan</name>
        
        
      </author>

      

      

      
        <summary type="html">As we celebrate the release of OpenXLA, PyTorch 2.0, and PyTorch/XLA 2.0, it’s worth taking a step back and sharing where we see it all going in the short to medium term. With PyTorch adoption leading in the AI space and XLA supporting best-in-class compiler features, PyTorch/XLA is well positioned to provide a cutting edge development stack for both model training and inference. To achieve this, we see investments in three main areas:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated PyTorch 2 Transformers</title>
      <link href="https://pytorch.org/blog/accelerated-pytorch-2/" rel="alternate" type="text/html" title="Accelerated PyTorch 2 Transformers" />
      <published>2023-03-28T00:00:00-07:00</published>
      <updated>2023-03-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-pytorch-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-pytorch-2/">&lt;p&gt;The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API with the goal of making training and deployment of state-of-the-art Transformer models affordable.  Following the successful release of “fastpath” inference execution (“Better Transformer”), this release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA).&lt;/p&gt;

&lt;p&gt;You can take advantage of the new fused SDPA kernels either by calling the new SDPA operator directly (as described in the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#beta-implementing-high-performance-transformers-with-scaled-dot-product-attention-sdpa&quot;&gt;SDPA tutorial&lt;/a&gt;), or transparently via integration into the pre-existing PyTorch Transformer API.  All features of the PyTorch Transformer API will continue to work compatibly, with many features mapped to high-performance SDPA kernels, while other features are impossible to support with higher performance (e.g., need_weights, as per below) while expanded high-performance support for other features may still be under active development.  &lt;br /&gt;
 &lt;br /&gt;
Similar to the “fastpath” architecture, custom kernels are fully integrated into the PyTorch Transformer API – thus, using the native Transformer and MultiHeadAttention API will enable users to transparently see significant speed improvements.  Unlike the “fastpath” architecture, the newly introduced “custom kernels” support many more use cases including models using Cross-Attention, Transformer Decoders, and for training models, in addition to the existing fastpath inference for fixed and variable sequence length Transformer Encoder and Self Attention use cases.&lt;/p&gt;

&lt;p&gt;To take full advantage of different hardware models and Transformer use cases, multiple SDPA custom kernels are supported, with custom kernel selection logic that will pick the highest-performance kernel for a given model and hardware type.  In particular, the first custom kernels included with the PyTorch 2.0 release are the &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;Flash Attention&lt;/a&gt; kernel (sdpa_flash, for 16-bit floating point training and inference on Nvidia GPUs with SM80+ architecture level) and the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers memory-efficient attention&lt;/a&gt; kernel (sdpa_mem_eff, for 16-bit and 32-bit floating point training and inference on a broad range of Nvidia GPUs).  A general-purpose kernel sdpa_math provides an implementation when the custom kernels are not applicable.&lt;/p&gt;

&lt;p&gt;As mentioned, custom kernels provide a wider range of support for execution scenarios To ensure efficient execution (e,g., to use GPU tensor cores), model configurations need to meet a small number of requirements.  This list of requirements will evolve over time, prospectively relaxing constraints limiting the usage of currently supported custom kernels, or providing additional kernels in the future.&lt;/p&gt;

&lt;p&gt;For the most up to date list of custom kernels and dispatch constraints, you can refer to &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/transformers/cuda/sdp_utils.h&quot;&gt;sdp_utils.h&lt;/a&gt;.  As of PyTorch 2.0, the existing fused SDPA kernels have the following constraints:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flash Attention only supports 16 bit floating point data types (float16 and bfloat16).&lt;/li&gt;
  &lt;li&gt;The head dimension must be a multiple of 8 for 16-bit floating point numbers and a multiple of 4 for 32-bit floating point numbers. At present, the maximum head_dim support for the Flash Attention custom kernel is 128.&lt;/li&gt;
  &lt;li&gt;The CUDA architecture level must be sm5x or better for the mem_efficient kernel, and sm80 for Flash Attention.&lt;/li&gt;
  &lt;li&gt;Flash Attention supports arbitrary dropout, in PyTorch 2.0 the mem_efficient kernel does not support dropout (i.e., dropout must be set to zero for this kernel to be selected in PyTorch 2.0).&lt;/li&gt;
  &lt;li&gt;To support variable-sequence length batches, all SDPA kernels support Nested Tensor inputs that combine input data and padding information using variable sequence length tensors for forward. (You can find more information about Nested Tensors in the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/nestedtensor.html&quot;&gt;Nested Tensor tutorial&lt;/a&gt;.)&lt;/li&gt;
  &lt;li&gt;You can specify both a &lt;em&gt;key_padding_mask&lt;/em&gt; and an &lt;em&gt;attn_mask&lt;/em&gt; by combining them before passing them to the SDPA operator. In particular, you can use the per-batch-element key padding mask of the nn.Transformer API to implement training for variable-sequence length inputs in a batch.&lt;/li&gt;
  &lt;li&gt;At present, the only attention mask supported by fused kernel implementation is the causal mask commonly used for training. To specify the causal mask in custom kernels, it must be specified with the &lt;em&gt;is_causal&lt;/em&gt; boolean and &lt;em&gt;attn_mask&lt;/em&gt; must be None.&lt;/li&gt;
  &lt;li&gt;Support for Nested Tensors is still under development.  Specifically, in PyTorch 2.0, only the sdpa_math kernel supports training with Nested Tensors. Also, PyTorch 2.0 does not support Nested Tensors as part of code being compiled with torch.compile().&lt;/li&gt;
  &lt;li&gt;The SDPA operator does not support returning averaged attention weights because computing them defeats the optimizations that enabled fused kernels to execute more efficiently.  The argument &lt;em&gt;need_weights&lt;/em&gt; for torch.nn.MultiheadAttention’s forward function defaults to True. In order to use the fused kernels, &lt;em&gt;need_weights&lt;/em&gt; needs to be set to &lt;em&gt;need_weights=False&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We find that an attention mask is rarely used in real-world applications, except for the causal mask during training.  Consequently, we reduce kernel complexity and compute cost by building in the option to use a causal mask as attention mask, and select this new capability with the &lt;em&gt;is_causal&lt;/em&gt; parameter introduced in conjunction with the new SDPA operator.&lt;/p&gt;

&lt;p&gt;Providing the &lt;em&gt;is_causal&lt;/em&gt; Boolean flag for the frequently used causal mask also obviates the expensive and memory-intensive allocation of a causal mask, increasing training memory efficiency by allowing more memory to be used for large batch sizes, and reduce memory bandwidth and cache contention – which are both at a premium in GPU accelerators – by not needing to load an attention mask tensor.&lt;/p&gt;

&lt;p&gt;If the constraints of none of the available custom kernels are met, then training falls back to using the default sdpa_math kernel, implementing the mathematical equations for scaled dot product attention using a sequence of PyTorch operator to implement SDPA.  This is the most general “catch-all” fallback kernel to ensure successful training for all models.&lt;/p&gt;

&lt;p&gt;In addition to the existing Transformer API, model developers may also use the scaled dot product attention kernels directly by calling the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention()&lt;/code&gt; operator.  This operator may be used to efficiently implement multi-head attention by combining it with in-projection and outprojection, as described in the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;SDPA tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition to adding custom kernels, Accelerated PyTorch 2 Transformers are integrated with PyTorch 2.0 compilation.  To use your model while benefiting from the additional acceleration of PT2-compilation (for inference or training), pre-process the model with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.compile(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have achieved major speedups for training transformer models and in particular large language models with Accelerated PyTorch 2 Transformers using a combination of custom kernels and torch.compile().&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch_better_transformer_chart1.png&quot; alt=&quot;Better Transformer chart&quot; width=&quot;100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Figure: Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Finally, because the custom kernels are much more memory efficient, try to increase the size of training batches to achieve faster training with increased batch size.&lt;/p&gt;

&lt;p&gt;In addition to automatic kernel selection, a context manager enables developers to override the kernel selection algorithm – this is not required for day to day operation, but enables developers to debug their code as well as enable performance engineers to override kernel selection. The SDPA tutorial provides additional information on using the SDPA context manager.&lt;/p&gt;

&lt;p&gt;In addition to availability as part of the nn.Transformer API, Accelerated PyTorch 2 Transformer custom kernels are also available in conjunction with the torchtext, torchvision, and fairseq domain libraries with the launch of PyTorch 2.0.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Michael Gschwind, Driss Guessous, Christian Puhrsch</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API with the goal of making training and deployment of state-of-the-art Transformer models affordable. Following the successful release of “fastpath” inference execution (“Better Transformer”), this release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.0 &amp;amp; XLA—The Latest Cutting Edge Features</title>
      <link href="https://pytorch.org/blog/pytorch-2.0-xla/" rel="alternate" type="text/html" title="PyTorch 2.0 &amp; XLA—The Latest Cutting Edge Features" />
      <published>2023-03-22T00:00:00-07:00</published>
      <updated>2023-03-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2.0-xla</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2.0-xla/">&lt;p&gt;Today, we are excited to share our latest work for &lt;a href=&quot;https://github.com/pytorch/xla/releases/tag/v2.0.0&quot;&gt;PyTorch/XLA 2.0&lt;/a&gt;. The release of &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch 2.0&lt;/a&gt; is yet another major milestone for this storied community and we are excited to continue to be part of it. When the &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;PyTorch/XLA&lt;/a&gt; project started in 2018 between Google and Meta, the focus was on bringing cutting edge Cloud TPUs to help support the PyTorch community. Along the way, others in the community such as Amazon joined the project and very quickly the community expanded. We are excited about XLA’s &lt;a href=&quot;https://opensource.googleblog.com/2023/03/openxla-is-ready-to-accelerate-and-simplify-ml-development.html&quot;&gt;direction&lt;/a&gt; and the benefits this project continues to bring to the PyTorch community. In this blog we’d like to showcase some key features that have been in development, show code snippets, and illustrate the benefit through some benchmarks.&lt;/p&gt;

&lt;h2 id=&quot;torchdynamo--torchcompile-experimental&quot;&gt;TorchDynamo / torch.compile (Experimental)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/torchdynamo&quot;&gt;TorchDynamo&lt;/a&gt; (Dynamo) is a Python-level JIT compiler designed to make unmodified PyTorch programs faster. It provides a clean API for compiler backends to hook in; its biggest feature is to dynamically modify Python bytecode just before execution. In the PyTorch/XLA 2.0 release, an experimental backend for Dynamo is provided for both inference and training.&lt;/p&gt;

&lt;p&gt;Dynamo provides a &lt;a href=&quot;https://pytorch.org/docs/stable/fx.html&quot;&gt;Torch FX&lt;/a&gt; (FX) graph when it recognizes a model pattern and PyTorch/XLA uses a Lazy Tensor approach to compile the FX graph and return the compiled function. To get more insight regarding the technical details about PyTorch/XLA’s dynamo implementation, check out &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchdynamo-update-10-integrating-with-pytorch-xla-for-inference-and-training/935&quot;&gt;this&lt;/a&gt; dev-discuss post and &lt;a href=&quot;https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md&quot;&gt;dynamo doc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is a small code example of running ResNet18 with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torchvision
import torch_xla.core.xla_model as xm

def eval_model(loader):
  device = xm.xla_device()
  xla_resnet18 = torchvision.models.resnet18().to(device)
  xla_resnet18.eval()
  dynamo_resnet18 = torch.compile(
      xla_resnet18, backend='torchxla_trace_once')
  for data, _ in loader:
    output = dynamo_resnet18(data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; PyTorch/XLA only traces the ResNet18 model once during the init time and executes the compiled binary everytime &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamo_resnet18&lt;/code&gt; is invoked, instead of tracing the model every step. To illustrate the benefits of Dynamo+XLA, below is an inference speedup analysis to compare Dynamo and LazyTensor (without Dynamo) using TorchBench on a Cloud TPU v4-8 where the y-axis is the speedup multiplier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-inferencespeedup.svg&quot; alt=&quot;Inference Speedup - PyTorch/XLA Dynamo on TPU&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dynamo for training is in the development stage with its implementation being at an earlier stage than inference. Developers are welcome to test this early feature, however, in the 2.0 release, PyTorch/XLA supports the forward and backward pass graphs and not the optimizer graph; the optimizer graph is available in the nightly builds and will land in the PyTorch/XLA 2.1 release. Below is an example of what training looks like using the ResNet18 example with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torchvision
import torch_xla.core.xla_model as xm

def train_model(model, data, target):
  loss_fn = torch.nn.CrossEntropyLoss()
  pred = model(data)
  loss = loss_fn(pred, target)
  loss.backward()
  return pred

def train_model_main(loader):
  device = xm.xla_device()
  xla_resnet18 = torchvision.models.resnet18().to(device)
  xla_resnet18.train()
  dynamo_train_model = torch.compile(
        train_model, backend='aot_torchxla_trace_once')
  for data, target in loader:
    output = dynamo_train_model(xla_resnet18, data, target)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the backend for training is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aot_torchxla_trace_once&lt;/code&gt; (API will be updated for stable release) whereas the inference backend is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchxla_trace_once&lt;/code&gt; (name subject to change). We expect to extract and execute 3 graphs per training step instead of 1 training step if you use the Lazy tensor. Below is a training speedup analysis to compare Dynamo and Lazy using the TorchBench on Cloud TPU v4-8.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-trainingspeedup.svg&quot; alt=&quot;Training Speedup - PyTorch/XLA Dynamo on TPU&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pjrt-runtime-beta&quot;&gt;PJRT Runtime (Beta)&lt;/h2&gt;

&lt;p&gt;PyTorch/XLA is migrating from XRT to the new PJRT runtime. PJRT is a better-maintained stack, with demonstrated performance advantages, including, on average, a 35% performance for training on TorchBench 2.0 models. It also supports a richer set of features enabling technologies like SPMD. In the PyTorch/XLA 2.0 release, PJRT is the default runtime for TPU and CPU; GPU support is in experimental state. The PJRT features included in the PyTorch/XLA 2.0 release are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TPU runtime implementation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libtpu&lt;/code&gt; using the &lt;a href=&quot;https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.md#rfc-openxla-pjrt-plugin&quot;&gt;PJRT Plugin API&lt;/a&gt; improves performance by up to 30%&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.distributed&lt;/code&gt; support for TPU v2 and v3, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pjrt://&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;init_method&lt;/code&gt; (Experimental)&lt;/li&gt;
  &lt;li&gt;Single-host GPU support. Multi-host support coming soon. (Experimental)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Switching to PJRT requires no change (or minimal change for GPUs) to user code (see &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/pjrt.md&quot;&gt;pjrt.md&lt;/a&gt; for more details). Runtime configuration is as simple as setting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PJRT_DEVICE&lt;/code&gt; environment variable to the local device type (i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TPU&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPU&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CPU&lt;/code&gt;). Below are examples of using PJRT runtimes on different devices.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TPU Device
PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TPU Pod Device
gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;git clone --depth=1 --branch r2.0 https://github.com/pytorch/xla.git&quot;

gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# GPU Device (Experimental)
PJRT_DEVICE=GPU GPU_NUM_DEVICES=4 python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=128 --num_epochs=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is a performance comparison between XRT and PJRT by task on TorchBench 2.0 on v4-8 TPU. To learn more about PJRT vs. XRT please review the &lt;a href=&quot;https://github.com/pytorch/xla/blob/r2.0/docs/pjrt.md#tpu&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-torchbenchtraining.svg&quot; alt=&quot;TorchBench Training Time&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;parallelization&quot;&gt;Parallelization&lt;/h2&gt;

&lt;h3 id=&quot;gspmd-experimental&quot;&gt;GSPMD (Experimental)&lt;/h3&gt;

&lt;p&gt;We are delighted to introduce General and Scalable Parallelization for ML Computation Graphs (&lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt;) in PyTorch as a new experimental data &amp;amp; model sharding solution. &lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; provides automatic parallelization for common ML workloads, allowing developers to write PyTorch programs as if on a single large device and without custom sharded computation ops and/or collective communication ops. The XLA compiler transforms the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. The API (&lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt;) will be available in the PyTorch/XLA 2.0 release as an experimental feature on a single TPU VM host.&lt;/p&gt;

&lt;h4 id=&quot;next-steps-for-gspmd&quot;&gt;Next Steps for GSPMD&lt;/h4&gt;

&lt;p&gt;GSPMD is experimental in 2.0 release. To bring it to Stable status, we plan to address a number of feature gaps and known issues in the following releases, including multi-host support, DTensor integration, partial replication sharding, asynchronous data loading, and checkpointing.&lt;/p&gt;

&lt;h3 id=&quot;fsdp-beta&quot;&gt;FSDP (Beta)&lt;/h3&gt;

&lt;p&gt;PyTorch/XLA &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;introduced&lt;/a&gt; fully sharded data parallel (FSDP) experimental support in version 1.12. This feature is a parallel representation of PyTorch FSDP and there are subtle differences in how XLA and upstream CUDA kernels are set up. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt; is a new argument that enables developers to automatically specify conditions for propagating partitioning specifications to neural network submodules. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt;s may be simply passed in as an argument when wrapping a model with FSDP. Two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt; callables worth noting are: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_auto_wrap_policy&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transformer_auto_wrap_policy&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_auto_wrap_policy&lt;/code&gt; enables users to wrap submodules with a minimum number of parameters. The example below wraps model submodules having at least 10M parameters.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto_wrap_policy = partial(size_based_auto_wrap_policy, min_num_params=1e7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transformer_auto_wrap_policy&lt;/code&gt; enables users to wrap all submodules that match a specific layer type. The example below wraps model submodules named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Conv2d&lt;/code&gt;. To learn more, review &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_fsdp.py#L237-L255&quot;&gt;this ResNet example&lt;/a&gt; by Ronghang Hu.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto_wrap_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={torch.nn.Conv2d})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch/XLA FSDP is now integrated in HuggingFace trainer class (&lt;a href=&quot;https://github.com/huggingface/transformers/pull/21406&quot;&gt;PR&lt;/a&gt;) enabling users to train much larger models on PyTorch/XLA (&lt;a href=&quot;https://huggingface.co/docs/transformers/main/en/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;official Hugging Face documentation&lt;/a&gt;). A 16B parameters GPT2 model trained on Cloud TPU v4-64 with this FSDP configuration achieved 39% hardware utilization.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot; style=&quot;max-width: 450px;&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;TPU Accelerator - Num Devices&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;v4-64
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GPT2 Parameter Count&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;16B
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Layers Wrapped with FSDP&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;GPT2Block
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;TFLOPs / Chip&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;275
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;PFLOPs / Step&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;50
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Hardware Utilization&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;39%
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;differences-between-fsdp--gspmd&quot;&gt;Differences Between FSDP &amp;amp; GSPMD&lt;/h3&gt;

&lt;p&gt;FSDP is a data parallelism technique that reduces device memory footprint by storing model parameters, optimizer states, and gradients all sharded. Note that the actual computation is still local to the device and requires all-gathering the sharded model parameters for both forward and backward passes, hence the name “data parallel”. FSDP is one of the newest additions to PyTorch/XLA to scale large model training.&lt;/p&gt;

&lt;p&gt;GSPMD on the other hand, is a general parallelization system that enables various types of parallelisms, including both data and model parallelisms. PyTorch/XLA provides a sharding annotation API and XLAShardedTensor abstraction, so a user can annotate any tensor with sharding specs in the PyTorch program. Developers don’t need to manually implement sharded computations or inject collective communications ops to get it right. The XLA compiler does the work so that each computation can run in a distributed manner on multiple devices.&lt;/p&gt;

&lt;h3 id=&quot;examples--preliminary-results&quot;&gt;Examples &amp;amp; Preliminary Results&lt;/h3&gt;

&lt;p&gt;To learn about PyTorch/XLA parallelism sharding API, visit our &lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt; and see the &lt;a href=&quot;https://github.com/pytorch/xla/tree/r2.0/test/spmd&quot;&gt;Sample Code&lt;/a&gt; references. Below is a simple example to enable data and model parallelism.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = SimpleLinear().to(xm.xla_device())
# Sharding annotate the linear layer weights.
xs.mark_sharding(model.fc1.weight, mesh, partition_spec)
# Training loop
model.train()
for step, (data, target) in enumerate(loader):
  optimizer.zero_grad()
  data = data.to(xm.xla_device())
  target = target.to(xm.xla_device())
  # Sharding annotate input data, we can shard any input
  # dimensions. Sharidng the batch dimension enables 
  # data parallelism, sharding the feature dimension enables
  # spatial partitioning.
  xs.mark_sharding(data, mesh, partition_spec)
  ouput = model(data)
  loss = loss_fn(output, target)
  optimizer.step()
  xm.mark_step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following graph highlights the memory efficiency benefits of PyTorch/XLA FSDP and SPMD on Cloud TPU v4-8 running ResNet50.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-batchsizescaling.svg&quot; alt=&quot;Batch Size Scaling with Spatial Partitioning&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing Thoughts…&lt;/h2&gt;

&lt;p&gt;We are excited to bring these features to the PyTorch community, and this is really just the beginning. Areas like dynamic shapes, deeper support for OpenXLA and many others are in development and we plan to put out more blogs to dive into the details. PyTorch/XLA is developed fully open source and we invite you to join the community of developers by filing issues, submitting pull requests, and sending RFCs on &lt;a href=&quot;github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt;. You can try PyTorch/XLA on a variety of XLA devices including TPUs and GPUs. &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;Here&lt;/a&gt; is how to get started.&lt;/p&gt;

&lt;p&gt;Congratulations again to the PyTorch community on this milestone!&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;The PyTorch Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Jack Cao, Milad Mohammadi, Alex Wertheim, Yeounoh Chung, Joe Spisak, Will Cromar, Shauheen Zahirazami</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we are excited to share our latest work for PyTorch/XLA 2.0. The release of PyTorch 2.0 is yet another major milestone for this storied community and we are excited to continue to be part of it. When the PyTorch/XLA project started in 2018 between Google and Meta, the focus was on bringing cutting edge Cloud TPUs to help support the PyTorch community. Along the way, others in the community such as Amazon joined the project and very quickly the community expanded. We are excited about XLA’s direction and the benefits this project continues to bring to the PyTorch community. In this blog we’d like to showcase some key features that have been in development, show code snippets, and illustrate the benefit through some benchmarks.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Diffusers with PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/accelerated-diffusers-pt-20/" rel="alternate" type="text/html" title="Accelerated Diffusers with PyTorch 2.0" />
      <published>2023-03-16T00:00:00-07:00</published>
      <updated>2023-03-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-diffusers-pt-20</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-diffusers-pt-20/">&lt;p&gt;PyTorch 2.0 has just been released. Its flagship new feature is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;, a one-line code change that promises to automatically improve performance across codebases. We have previously &lt;a href=&quot;https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models/&quot;&gt;checked on that promise in Hugging Face Transformers and TIMM models&lt;/a&gt;, and delved deep into its &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;motivation, architecture and the road ahead&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As important as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; is, there’s much more to PyTorch 2.0. Notably, PyTorch 2.0 incorporates several strategies to accelerate transformer blocks, and these improvements are very relevant for diffusion models too. Techniques such as &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt;, for example, have become very popular in the diffusion community thanks to their ability to significantly speed up Stable Diffusion and achieve larger batch sizes, and they are now part of PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;In this post we discuss how attention layers are optimized in PyTorch 2.0 and how these optimization are applied to the popular &lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;🧨 Diffusers library&lt;/a&gt;. We finish with a benchmark that shows how the use of PyTorch 2.0 and Diffusers immediately translates to significant performance improvements across different hardware.&lt;/p&gt;

&lt;h2 id=&quot;accelerating-transformer-blocks&quot;&gt;Accelerating transformer blocks&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 includes a &lt;em&gt;scaled dot-product attention&lt;/em&gt; function as part of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional&lt;/code&gt;. This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. Before PyTorch 2.0, you had to search for third-party implementations and install separate packages in order to take advantage of memory optimized algorithms, such as FlashAttention. The available implementations are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;FlashAttention, from the official &lt;a href=&quot;https://github.com/HazyResearch/flash-attention&quot;&gt;FlashAttention project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Memory-Efficient Attention, from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A native C++ implementation suitable for non-CUDA devices or when high-precision is required.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these methods are available by default, and PyTorch will try to select the optimal one automatically through the use of the new scaled dot-product attention (SDPA) API. You can also individually toggle them for finer-grained control, see &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention&quot;&gt;the documentation&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;using-scaled-dot-product-attention-in-diffusers&quot;&gt;Using scaled dot-product attention in diffusers&lt;/h2&gt;

&lt;p&gt;The incorporation of Accelerated PyTorch 2.0 Transformer attention to the Diffusers library was achieved through the use of the &lt;a href=&quot;https://huggingface.co/docs/diffusers/v0.13.0/en/api/models#diffusers.UNet2DConditionModel.set_attn_processor&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set_attn_processor&lt;/code&gt; method&lt;/a&gt;, which allows for pluggable attention modules to be configured. In this case, a &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/856dad57/src/diffusers/models/cross_attention.py#L469&quot;&gt;new attention processor was created&lt;/a&gt;, which is &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/856dad57bb7a9ee13af4a08492e524b0a145a2c5/src/diffusers/models/cross_attention.py#L105&quot;&gt;enabled by default when PyTorch 2.0 is available&lt;/a&gt;. For clarity, this is how you could enable it manually (but it’s usually not necessary since diffusers will automatically take care of it):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from diffusers import StableDiffusionPipeline
from diffusers.models.cross_attention import AttnProcessor2_0

pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;)
pipe.to(&quot;cuda&quot;)
pipe.unet.set_attn_processor(AttnProcessor2_0())

prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
image = pipe(prompt).images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;stable-diffusion-benchmark&quot;&gt;Stable Diffusion Benchmark&lt;/h2&gt;

&lt;p&gt;We ran a number of tests using accelerated dot-product attention from PyTorch 2.0 in Diffusers. We installed diffusers from pip and used nightly versions of PyTorch 2.0, since our tests were performed before the official release. We also used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.set_float32_matmul_precision('high')&lt;/code&gt; to enable additional fast matrix multiplication algorithms.&lt;/p&gt;

&lt;p&gt;We compared results with the traditional attention implementation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diffusers&lt;/code&gt; (referred to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vanilla&lt;/code&gt; below) as well as with the best-performing solution in pre-2.0 PyTorch: PyTorch 1.13.1 with the xFormers package (v0.0.16) installed.&lt;/p&gt;

&lt;p&gt;Results were measured without compilation (i.e., no code changes at all), and also with a single call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to wrap the UNet module. We did not compile the image decoder because most of the time is spent in the 50 denoising iterations that run UNet evaluations.&lt;/p&gt;

&lt;h3 id=&quot;results-in-float32&quot;&gt;Results in float32&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig1-latest.png&quot; alt=&quot;Diffusers Speedup vs xFormers float32&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following figures explore performance improvement vs batch size for various representative GPUs belonging to different generations. We collected data for each combination until we reached maximum memory utilization. Vanilla attention runs out of memory earlier than xFormers or PyTorch 2.0, which explains the missing bars for larger batch sizes. Similarly, A100 (we used the 40 GB version) is capable of running batch sizes of 64, but the other GPUs could only reach 32 in our tests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig2-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (A100, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig3-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (3090, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig4-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (4090, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig5-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (V100, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We found very significant performance improvements over vanilla attention across the board, without even using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;. An out of the box installation of PyTorch 2.0 and diffusers yields about 50% speedup on A100 and between 35% and 50% on 4090 GPUs, depending on batch size. Performance improvements are more pronounced for modern CUDA architectures such as Ada (4090) or Ampere (A100), but they are still very significant for older architectures still heavily in use in cloud services.&lt;/p&gt;

&lt;p&gt;In addition to faster speeds, the accelerated transformers implementation in PyTorch 2.0 allows much larger batch sizes to be used. A single 40GB A100 GPU runs out of memory with a batch size of 10, and 24 GB high-end consumer cards such as 3090 and 4090 cannot generate 8 images at once. Using PyTorch 2.0 and diffusers we could achieve batch sizes of &lt;strong&gt;48&lt;/strong&gt; for 3090 and 4090, and &lt;strong&gt;64&lt;/strong&gt; for A100. This is of great significance for cloud services and applications, as they can efficiently process more images at a time.&lt;/p&gt;

&lt;p&gt;When compared with PyTorch 1.13.1 + xFormers, the new accelerated transformers implementation is still faster and requires no additional packages or dependencies. In this case we found moderate speedups of up to 2% on datacenter cards such as A100 or T4, but performance was great on the two last generations of consumer cards: up to 20% speed improvement on 3090 and between 10% and 45% on 4090, depending on batch size.&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; is used, we get an additional performance boost of (typically) 2% and 3% over the previous improvements. As compilation takes some time, this is better geared towards user-facing inference services or training.&lt;/p&gt;

&lt;h3 id=&quot;results-in-float16&quot;&gt;Results in float16&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig6-latest.png&quot; alt=&quot;Diffusers Speedup vs xFormers float16&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig7-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (A100, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig8-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (4090, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig9-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (3090, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float16&lt;/code&gt; inference, the performance improvements of the accelerated transformers implementation in PyTorch 2.0 are between 20% and 28% over standard attention, across all the GPUs we tested, except for the 4090, which belongs to the more modern Ada architecture. This GPU benefits from a dramatic performance improvement when using PyTorch 2.0 nightlies. With respect to optimized SDPA vs xFormers, results are usually on par for most GPUs, except again for the 4090. Adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to the mix boosts performance a few more percentage points across the board.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 comes with multiple features to optimize the crucial components of the foundational transformer block, and they can be further improved with the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. These optimizations lead to significant memory and time improvements for diffusion models, and remove the need for third-party library installations.&lt;/p&gt;

&lt;p&gt;To take advantage of these speed and memory improvements all you have to do is upgrade to PyTorch 2.0 and use diffusers &amp;gt;= 0.13.0.&lt;/p&gt;

&lt;p&gt;For more examples and in-detail benchmark numbers, please also have a look at the &lt;a href=&quot;https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/torch2.0&quot;&gt;Diffusers with PyTorch 2.0&lt;/a&gt; docs.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The authors are grateful to the PyTorch team for creating such excellent software.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Pedro Cuenca, Patrick von Platen, Suraj Patil</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.0 has just been released. Its flagship new feature is torch.compile(), a one-line code change that promises to automatically improve performance across codebases. We have previously checked on that promise in Hugging Face Transformers and TIMM models, and delved deep into its motivation, architecture and the road ahead.</summary>
      

      
      
    </entry>
  
</feed>


