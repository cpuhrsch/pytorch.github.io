<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-05-24T17:53:12-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Out of the box acceleration and memory savings of ü§ó decoder models with PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/out-of-the-box-acceleration/" rel="alternate" type="text/html" title="Out of the box acceleration and memory savings of ü§ó decoder models with PyTorch 2.0" />
      <published>2023-05-22T00:00:00-07:00</published>
      <updated>2023-05-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/out-of-the-box-acceleration</id>
      <content type="html" xml:base="https://pytorch.org/blog/out-of-the-box-acceleration/">&lt;p&gt;As part of PyTorch 2.0 release, an accelerated implementation of the attention mechanism as part of the ‚ÄúBetter Transformer‚Äù project (and known in PyTorch as Accelerated Transformers) has been added natively into PyTorch as &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product_attention#torch.nn.functional.scaled_dot_product_attention&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;&lt;/a&gt;. This implementation leverages fused kernels from &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2112.05682&quot;&gt;Memory-efficient attention&lt;/a&gt;, and supports both training and inference.&lt;/p&gt;

&lt;p&gt;We also release a notebook showcasing an example of this integration &lt;a href=&quot;https://colab.research.google.com/drive/1_zuAiiBFoFWpexxeWsTS694tCSlMYydo?usp=sharing&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After seeing &lt;a href=&quot;https://pytorch.org/blog/accelerated-diffusers-pt-20/&quot;&gt;20-30% speedups at inference for diffusion models&lt;/a&gt;, we went ahead and implemented an integration with ü§ó Transformers models through the &lt;a href=&quot;https://huggingface.co/docs/optimum/main/en/bettertransformer/overview&quot;&gt;ü§ó Optimum library&lt;/a&gt;. Similar to &lt;a href=&quot;https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2&quot;&gt;the previous integration for encoder models&lt;/a&gt;, the integration replaces modules from Transformers with efficient implementations that use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;. The usage is as follow:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from optimum.bettertransformer import BetterTransformer
from transformers import AutoModelForCausalLM

with torch.device(‚Äúcuda‚Äù):
model = AutoModelForCausalLM.from_pretrained(‚Äúgpt2-large‚Äù, torch_dtype=torch.float16)

model = BetterTransformer.transform(model)

# do your inference or training here

# if training and want to save the model
model = BetterTransformer.reverse(model)
model.save_pretrained(‚Äúfine_tuned_model‚Äù)
model.push_to_hub(‚Äúfine_tuned_model‚Äù) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Summarizing our findings below about &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is most useful to fit larger models, sequence length, or batch size to train on a given hardware.&lt;/li&gt;
  &lt;li&gt;Memory footprint savings on GPU during training range from 20% to 110%+.&lt;/li&gt;
  &lt;li&gt;Speedups during training range from 10% to 70%.&lt;/li&gt;
  &lt;li&gt;Speedups during inference range from 5% to 20%.&lt;/li&gt;
  &lt;li&gt;Standalone, for small head dimensions, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; speedups go up to 3x, memory savings go as high as 40x (depending on the sequence length).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may be surprised by the wide range of memory savings and speedups. In this blog post, we discuss our benchmarks, where this feature shines and upcoming improvements in future PyTorch releases.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the next release of transformers you will just need to install the proper version of optimum and run:&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = model.to_bettertransformer()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;To convert your model using the BetterTransformer API. You can already try this feature out by installing transformers from source.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmark-and-usage-with--transformers&quot;&gt;Benchmark and usage with ü§ó Transformers&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt; is usable with any architecture that uses standard attention, and namely replaces the boiler-plate code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# native scaled_dot_product_attention is equivalent to the following:
def eager_sdpa(query, key, value, attn_mask, dropout_p, is_causal, scale):
	scale_factor = 1 / math.sqrt(Q.size(-1)) if scale is None else scale
	attn_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0) if is_causal else attn_mask
	attn_mask = attn_mask.masked_fill(not attn_mask, -float('inf')) if attn_mask.dtype==torch.bool else attn_mask
	attn_weight = torch.softmax((Q @ K.transpose(-2, -1) * scale_factor) + attn_mask, dim=-1)
	attn_weight = torch.dropout(attn_weight, dropout_p)
	return attn_weight @ V
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the ü§ó Optimum integration with Transformers models, the following architectures are supported for now: gpt2, gpt-neo, gpt-neox, gptj, t5, bart, codegen, pegasus, opt, LLaMA, blenderbot, m2m100. You can expect this list to be extended in the near future!&lt;/p&gt;

&lt;p&gt;To validate the benefits from the native scaled dot-product attention, we ran inference and training benchmarks, whose results are presented below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig1.jpg&quot; alt=&quot;Inference benchmark on a single A10G GPU, AWS g5.4xlarge instance&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Inference benchmark on a single A10G GPU, AWS g5.4xlarge instance&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig2.jpg&quot; alt=&quot;Training benchmark on a single A10G GPU, AWS g5.4xlarge instance&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Training benchmark on a single A10G GPU, AWS g5.4xlarge instance&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig3.jpg&quot; alt=&quot;Training benchmark on a single A100-SXM4-80GB, Nvidia DGX&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Training benchmark on a single A100-SXM4-80GB, Nvidia DGX&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;Out of this benchmark, the most interesting finding is that native SDPA allows for the usage of longer sequence lengths and batch sizes without running into out of memory issues. Moreover, up to 20% speedups can be seen during inference, and even larger during training.&lt;/p&gt;

&lt;p&gt;As seen on the training benchmarks, it appears that smaller head dimension brings higher speedups and memory savings, which we will discuss in the following section.&lt;/p&gt;

&lt;p&gt;The implementation supports multi-GPU settings as well, thanks to ü§ó Accelerate library by passing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device_map=‚Äùauto‚Äù&lt;/code&gt; to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from_pretrained&lt;/code&gt; method. Here are some results for training on two A100-SXM4-80GB.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig4.jpg&quot; alt=&quot;Training benchmark on two A100-SXM4-80GB, Nvidia DGX, using ü§ó Accelerate library for distributed training&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Training benchmark on two A100-SXM4-80GB, Nvidia DGX, using ü§ó Accelerate library for distributed training&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;Note that some kernels support only the sm_80 compute capability (which is the one from A100 GPUs), which limits usability on a wide range of hardware, notably if the head dimension is not a power of two. For example, as of PyTorch 2.0.0 during training, opt-2.7b (headim=80) and gpt-neox-20b (headdim=96) can not dispatch to a kernel using flash attention, unless run on an A100 GPU. Better kernels may be developed in the future: https://github.com/pytorch/pytorch/issues/98140#issuecomment-1518101895&lt;/p&gt;

&lt;h2 id=&quot;flash-attention-memory-efficient-attention--math-differences&quot;&gt;Flash Attention, Memory-efficient attention &amp;amp; math differences&lt;/h2&gt;

&lt;p&gt;The native &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; relies on three possible backend implementations: flash attention, memory-efficient attention, and the so-called math implementation which provides a hardware-neutral fallback for all PyTorch platforms.&lt;/p&gt;

&lt;p&gt;When fused kernels are available for a given problem size, flash-attention or memory-efficient attention will be used, effectively allowing for a lower memory footprint, as in the memory-efficient attention case O(N) memory allocations are done on the GPU global memory instead of the classic O(N^2) for the traditional eager attention implementation. With flash attention, a reduced number of memory accesses (read and writes) is expected, hence both giving speedups and memory savings.&lt;/p&gt;

&lt;p&gt;The ‚Äúmath‚Äù implementation is simply an &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/aten/src/ATen/native/transformers/attention.cpp#L812-L868&quot;&gt;implementation using the PyTorch‚Äôs C++ API&lt;/a&gt;. Interesting to note in this implementation is that the query and key tensors are scaled individually for numerical stability, thus launching two aten::div operations instead of possibly only one in an eager implementation that does not contain this optimization for numerical stability.&lt;/p&gt;

&lt;h3 id=&quot;head-dimension-influence-on-speedups-memory-savings&quot;&gt;Head dimension influence on speedups, memory savings&lt;/h3&gt;

&lt;p&gt;Benchmarking &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;, we notice a decrease in the speedup / memory gains as the head dimension increases. This is an issue for some architectures like EleutherAI/gpt-neo-2.7B, that has a relatively large head dimension of 128, or EleutherAI/gpt-j-6B (and derived models as PygmalionAI/pygmalion-6b) that has a head dimension of 256 (that actually currently do not dispatch on fused kernels as the head dimension is too large).&lt;/p&gt;

&lt;p&gt;This trend can be seen in the figures below, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.scaled_dot_production&lt;/code&gt; is benchmarked standalone versus the above eager implementation. Moreover, we use the &lt;a href=&quot;https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.backends.cuda.sdp_kernel&lt;/code&gt;&lt;/a&gt; context manager to force the usage of respectively math, flash attention, and memory-efficient attention implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig5.jpg&quot; alt=&quot;Using memory-efficient attention SDP kernel (forward-only), A100&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Using memory-efficient attention SDP kernel (forward-only), A100&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig6.jpg&quot; alt=&quot;Using math (without dropout), A100&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Using math (without dropout), A100&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig7.jpg&quot; alt=&quot;Using flash attention SDP kernel (without dropout), A100&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Using flash attention SDP kernel (without dropout), A100&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig8.jpg&quot; alt=&quot;Using memory-efficient attention SDP kernel (without dropout), A100&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Using memory-efficient attention SDP kernel (without dropout), A100&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p class=&quot;pb-3&quot;&gt;&lt;/p&gt;

&lt;p&gt;We see that for the same problem size, be it for inference-only or training, the speedup decreases with higher head dimension, e.g. from 3.4x for headdim=8 to 1.01x for headdim=128 using flash attention kernel.&lt;/p&gt;

&lt;p&gt;The reduced memory saving is expected with larger head dimensions. Recall the standard attention computation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig9.jpg&quot; alt=&quot;Math equation&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Due to the intermediate computations, the global memory footprint is 2 * N * N + N * d in this standard step by step computation. Memory-efficient attention proposes to iteratively update the softmax renormalization constant and moving its computation at the very end, allowing for only a constant output memory allocation N * d.&lt;/p&gt;

&lt;p&gt;Thus, the memory saving ratio is 2 * N / d + 1, which decreases with larger head dimension.&lt;/p&gt;

&lt;p&gt;In flash attention, the tradeoff is between the head dimension d and the shared memory size M of a GPU streaming multiprocessor, with a total number of memory accesses of O(N¬≤ * d¬≤/M). Thus, the memory accesses scale quadratically in the head dimension, contrary to the standard attention that scales linearly. The reason is that in flash attention, for larger head dimension d, the key and value K, V need to be split into more blocks to fit into shared memory, and in turn each block needs to load the full query Q and output O.&lt;/p&gt;

&lt;p&gt;Thus, the highest speedups for flash attention are in a regime where the ratio d¬≤ / M is small enough.&lt;/p&gt;

&lt;h2 id=&quot;current-limitations-as-of-pytorch-200&quot;&gt;Current limitations as of PyTorch 2.0.0&lt;/h2&gt;

&lt;h3 id=&quot;absence-of-a-scale-argument&quot;&gt;Absence of a scale argument&lt;/h3&gt;

&lt;p&gt;As of PyTorch 2.0.0, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt; has no scale argument and uses the default square root of the hidden size sqrt(d_k).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/out-of-the-box/Fig10.jpg&quot; alt=&quot;Math equation&quot; style=&quot;max-height:800px; width:100%; max-width: 400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, some architectures as OPT or T5 do not use a scaling in the attention, which as of Pytorch 2.0.0 forces it to artificially rescale before the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; call. This introduces an unnecessary overhead, as an additional multiplication is necessary, on top of unneeded divisions in the attention.&lt;/p&gt;

&lt;p&gt;A fix for this issue has been merged &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/95259&quot;&gt;in PyTorch repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;support-of-flash-attention--memory-efficient-attention-with-custom-mask&quot;&gt;Support of flash attention / memory-efficient attention with custom mask&lt;/h3&gt;

&lt;p&gt;As of PyTorch 2.0.0, when passing a custom attention mask, flash attention and memory-efficient attention can not be used. In this case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; automatically dispatches to the C++ implementation.&lt;/p&gt;

&lt;p&gt;However, as we have seen, some architectures require a custom attention mask, as T5 that uses positional bias. Moreover, in the case of a batch size larger than one where some inputs may be padded, a custom attention mask also needs to be passed. For this latter case, an alternative would be to use &lt;a href=&quot;https://pytorch.org/docs/stable/nested.html&quot;&gt;NestedTensor&lt;/a&gt;, which SDPA supports.&lt;/p&gt;

&lt;p&gt;This limited support for custom masks thus limits the benefits from SDPA in these specific cases, although we can hope for an extended support &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/96099#issuecomment-1458609375&quot;&gt;in the future&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that xformers, from which PyTorch‚Äôs SDPA partially takes inspiration, currently supports arbitrary attention masks: https://github.com/facebookresearch/xformers/blob/658ebab39545f180a6075385b3897921623d6c3b/xformers/ops/fmha/cutlass.py#L147-L156 . HazyResearch implementation of flash attention also supports an equivalent implementation of padding, as a cumulative sequence length array is used along with packed query/key/values - similar in essence to NestedTensor.&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In conclusion&lt;/h2&gt;

&lt;p&gt;Using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt; is a free-lunch optimization, both making your code more readable, uses less memory, and is in most common cases faster.&lt;/p&gt;

&lt;p&gt;Although the implementation in PyTorch 2.0.0 has still minor limitations, inference and training already massively benefit from SDPA in most cases. We encourage you to use this native implementation be it to train or deploy your PyTorch models, and for ü§ó Transformers models as a one-line transformation!&lt;/p&gt;

&lt;p&gt;In the future, we would like to adapt the API to enable users to use SDPA in encoder-based models as well.&lt;/p&gt;

&lt;p&gt;We thank Benjamin Lefaudeux, Daniel Haziza and Francisco Massa for their advice on the head dimension influence, as well as Michael Gschwind, Christian Puhrsch and Driss Guessous for their feedback on the blog post!&lt;/p&gt;

&lt;h2 id=&quot;benchmark-reproduction&quot;&gt;Benchmark reproduction&lt;/h2&gt;

&lt;p&gt;The benchmark presented in this post was done using torch==2.0.0, transformers==4.27.4, accelerate==0.18.0 and optimum==1.8.0.&lt;/p&gt;

&lt;p&gt;The benchmarks can be easily reproduced using the scripts for &lt;a href=&quot;https://github.com/huggingface/optimum/blob/main/tests/benchmark/benchmark_bettertransformer.py&quot;&gt;inference&lt;/a&gt;, &lt;a href=&quot;https://github.com/huggingface/optimum/blob/main/tests/benchmark/benchmark_bettertransformer_training_minimal.py&quot;&gt;training&lt;/a&gt; for ü§ó Transformers models, and &lt;a href=&quot;https://github.com/fxmarty/efficient-attention-benchmark&quot;&gt;standalone SDPA&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Felix Marty, Younes Belkada, Hamid Shojanazeri</name>
        
        
      </author>

      

      

      
        <summary type="html">As part of PyTorch 2.0 release, an accelerated implementation of the attention mechanism as part of the ‚ÄúBetter Transformer‚Äù project (and known in PyTorch as Accelerated Transformers) has been added natively into PyTorch as torch.nn.functional.scaled_dot_product_attention. This implementation leverages fused kernels from FlashAttention and Memory-efficient attention, and supports both training and inference.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Conference 2023: Join us in San Francisco October 16-17</title>
      <link href="https://pytorch.org/blog/pytorch-conference-2023/" rel="alternate" type="text/html" title="PyTorch Conference 2023: Join us in San Francisco October 16-17" />
      <published>2023-05-16T00:00:00-07:00</published>
      <updated>2023-05-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-conference-2023</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-conference-2023/">&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conf-2023.png&quot; alt=&quot;PyTorch Conference 2023&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We‚Äôre thrilled to announce the upcoming &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference 2023&lt;/a&gt;! On October 16-17, the conference will showcase PyTorch 2.0, the next-generation release of the popular machine learning framework. As part of the Linux Foundation, the PyTorch Foundation Conference continues the tradition of bringing together leading researchers, developers, and academic communities to advance the education and development of end-to-end machine learning.&lt;/p&gt;

&lt;p&gt;The conference agenda features an engaging lineup of events, including an opening reception, engaging community and partner discussions, informative panels, poster sessions, enlightening use cases and community stories, as well as discussions on the latest trends in machine learning and deep learning development and deployment.&lt;/p&gt;

&lt;h2 id=&quot;call-for-proposals&quot;&gt;Call for Proposals&lt;/h2&gt;

&lt;p&gt;We are now accepting speaker proposals for the conference until &lt;strong&gt;July 21&lt;/strong&gt;. The program committee will carefully review all submissions, and selected speakers will be notified by &lt;strong&gt;August 8&lt;/strong&gt;. We strongly encourage both experienced and first-time speakers to submit their proposals. This conference provides an excellent opportunity to connect with the PyTorch community, share your ideas, and showcase your work.&lt;/p&gt;

&lt;p&gt;When preparing your proposal, please consider the following guidelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are you hoping to get from your presentation?&lt;/li&gt;
  &lt;li&gt;What do you expect the audience to gain from your presentation?&lt;/li&gt;
  &lt;li&gt;How will your presentation help better the open source ecosystem?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To help you shape your proposal, here are some suggested topics for the conference:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deployments on AWS, Azure&lt;/li&gt;
  &lt;li&gt;Use cases and real-world applications&lt;/li&gt;
  &lt;li&gt;Foundational models&lt;/li&gt;
  &lt;li&gt;AI practices&lt;/li&gt;
  &lt;li&gt;Production considerations&lt;/li&gt;
  &lt;li&gt;PyTorch 2.X features and updates&lt;/li&gt;
  &lt;li&gt;Training techniques and best practices&lt;/li&gt;
  &lt;li&gt;Inference methodologies&lt;/li&gt;
  &lt;li&gt;Hardware advancements and optimizations&lt;/li&gt;
  &lt;li&gt;Edge computing applications&lt;/li&gt;
  &lt;li&gt;Scalability solutions&lt;/li&gt;
  &lt;li&gt;Latest research breakthroughs&lt;/li&gt;
  &lt;li&gt;Optimization strategies&lt;/li&gt;
  &lt;li&gt;Extending PyTorch through customizations and plugins&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We kindly request that you refrain from submitting sales or marketing pitches and avoid discussing unlicensed or closed-source technologies. Such talks tend to detract from the integrity of our events and are not well-received by conference attendees.&lt;/p&gt;

&lt;h2 id=&quot;register-today&quot;&gt;Register Today&lt;/h2&gt;

&lt;p&gt;Registration is now open! Get your ticket today and secure your spot: &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/register/&quot;&gt;https://events.linuxfoundation.org/pytorch-conference/register/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you for your interest, and we look forward to a successful PyTorch Conference 2023!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Language Identification: Building an End-to-End AI Solution using PyTorch</title>
      <link href="https://pytorch.org/blog/language-identification/" rel="alternate" type="text/html" title="Language Identification: Building an End-to-End AI Solution using PyTorch" />
      <published>2023-05-12T00:00:00-07:00</published>
      <updated>2023-05-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/language-identification</id>
      <content type="html" xml:base="https://pytorch.org/blog/language-identification/">&lt;p&gt;Language Identification is the process of identifying the primary language from multiple audio input samples. In natural language processing (NLP), language identification is an important problem and a challenging issue. There are many language-related tasks such as entering text on your phone, finding news articles you enjoy, or discovering answers to questions that you may have. All these tasks are powered by NLP models. To decide which model to invoke at a particular point in time, we must perform language identification.&lt;/p&gt;

&lt;p&gt;This article presents an in-depth solution and code sample for language identification using &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/&quot;&gt;Intel¬Æ Extension for PyTorch&lt;/a&gt;, which is a version of the popular PyTorch AI framework optimized for use on Intel¬Æ processors, and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&quot;&gt;Intel¬Æ Neural Compressor&lt;/a&gt;, which is a tool to accelerate AI inference without sacrificing accuracy.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification&quot;&gt;code sample&lt;/a&gt; demonstrates how to train a model to perform language identification using the Hugging Face SpeechBrain* toolkit and optimize it using the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit-download.html&quot;&gt;Intel¬Æ AI Analytics Toolkit (AI Kit)&lt;/a&gt;. The user can modify the code sample and identify up to 93 languages using the Common Voice dataset.&lt;/p&gt;

&lt;h2 id=&quot;proposed-methodology-for-language-identification&quot;&gt;Proposed Methodology for Language Identification&lt;/h2&gt;

&lt;p&gt;In the proposed solution, the user will use an Intel AI Analytics Toolkit container environment to train a model and perform inference leveraging Intel-optimized libraries for PyTorch. There is also an option to quantize the trained model with Intel Neural Compressor to speed up inference.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;http://commonvoice.mozilla.org/en/datasets&quot;&gt;Common Voice&lt;/a&gt; dataset is used and for this code sample, specifically, Common Voice Corpus 11.0 for Japanese and Swedish. This dataset is used to train an &lt;a href=&quot;http://arxiv.org/abs/2005.07143&quot;&gt;Emphasized Channel Attention, Propagation and Aggregation Time Delay Neural Network (ECAPA-TDNN)&lt;/a&gt;, which is implemented using the &lt;a href=&quot;http://huggingface.co/SpeechBrain&quot;&gt;Hugging Face SpeechBrain&lt;/a&gt; library. Time Delay Neural Networks (TDNNs), aka one-dimensional Convolutional Neural Networks (1D CNNs), are multilayer artificial neural network architectures to classify patterns with shift-invariance and model context at each layer of the network. ECAPA-TDNN is a new TDNN-based speaker-embedding extractor for speaker verification; it is built upon the original x-vector architecture and puts more emphasis on channel attention, propagation, and aggregation.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;After downloading the Common Voice dataset, the data is preprocessed by converting the MP3 files into WAV format to avoid information loss and separated into training, validation, and testing sets.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;http://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa&quot;&gt;pretrained VoxLingua107 &lt;/a&gt;model is retrained with the Common Voice dataset using the Hugging Face SpeechBrain library to focus on the languages of interest. &lt;a href=&quot;http://bark.phon.ioc.ee/voxlingua107/&quot;&gt;VoxLingua107&lt;/a&gt; is a speech dataset used for training spoken language recognition models that work well with real-world and varying speech data. This dataset contains data for 107 languages. By default, Japanese and Swedish are used, and more languages can be included. This model is then used for inference on the testing dataset or a user-specified dataset. Also, there is an option to utilize SpeechBrain‚Äôs Voice Activity Detection (VAD) where only the speech segments from the audio files are extracted and combined before samples are randomly selected as input into the model. This &lt;a href=&quot;http://huggingface.co/speechbrain/vad-crdnn-libriparty&quot;&gt;link&lt;/a&gt; provides all the necessary tools to perform VAD. To improve performance, the user may quantize the trained model to integer-8 (INT8) using Intel Neural Compressor to decrease latency.&lt;/p&gt;

&lt;h4 id=&quot;training&quot;&gt;Training&lt;/h4&gt;

&lt;p&gt;The copies of training scripts are added to the current working directory, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_wds_shards.py&lt;/code&gt; - for creating the &lt;a href=&quot;http://github.com/webdataset/webdataset&quot;&gt;WebDataset&lt;/a&gt; shards, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt; - to perform the actual training procedure, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_ecapa.yaml&lt;/code&gt; - to configure the training options. The script to create WebDataset shards and YAML file are patched to work with the two languages chosen for this code sample.&lt;/p&gt;

&lt;p&gt;In the data preprocessing phase, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepareAllCommonVoice.py&lt;/code&gt; script is executed to randomly select a specified number of samples to convert the input from MP3 to WAV format. Here, 80% of these samples will be used for training, 10% for validation, and 10% for testing. At least 2000 samples are recommended as the number of input samples and is the default value.&lt;/p&gt;

&lt;p&gt;In the next step, WebDataset shards are created from the training and validation datasets. This stores the audio files as tar files which allows writing purely sequential I/O pipelines for large-scale deep learning in order to achieve high I/O rates from local storage‚Äîabout 3x-10x faster compared to random access.&lt;/p&gt;

&lt;p&gt;The YAML file will be modified by the user. This includes setting the value for the largest number for the WebDataset shards, output neurons to the number of languages of interest, number of epochs to train over the entire dataset, and the batch size. The batch size should be decreased if the CPU or GPU runs out of memory while running the training script.&lt;/p&gt;

&lt;p&gt;In this code sample, the training script will be executed with CPU. While running the script, ‚Äúcpu‚Äù will be passed as an input parameter. The configurations defined in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_ecapa.yaml&lt;/code&gt; are also passed as parameters.&lt;/p&gt;

&lt;p&gt;The command to run the script to train the model is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python train.py train_ecapa.yaml --device &quot;cpu&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the future, the training script train.py will be designed to work for Intel¬Æ GPUs such as the Intel¬Æ Data Center GPU Flex Series, Intel¬Æ Data Center GPU Max Series, and Intel¬Æ Arc‚Ñ¢ A-Series with updates from Intel Extension for PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification#train-the-model-with-languages&quot;&gt;Run the training script&lt;/a&gt; to learn how to train the models and execute the training script. The 4th Generation Intel¬Æ Xeon¬Æ Scalable Processor is recommended for this &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/transfer-learning.html&quot;&gt;transfer learning&lt;/a&gt; application because of its performance improvements through its Intel¬Æ Advanced Matrix Extensions (Intel¬Æ AMX) instruction set.&lt;/p&gt;

&lt;p&gt;After training, checkpoint files are available. These files are used to load the model for inference.&lt;/p&gt;

&lt;h4 id=&quot;inference&quot;&gt;Inference&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-inference-pipeline-language-identification.png&quot; alt=&quot;Inference Pipeline&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The crucial step before running inference is to patch the SpeechBrain library‚Äôs pretrained &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interfaces.py&lt;/code&gt; file so that PyTorch TorchScript* can be run to improve the runtime. TorchScript requires the output of the model to be only tensors.&lt;/p&gt;

&lt;p&gt;Users can choose to run inference using the testing set from Common Voice or their own custom data in WAV format. The following are the options the inference scripts (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inference_custom.py and inference_commonVoice.py&lt;/code&gt;) can be run with:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Input Option&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Description&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;-p
   &lt;/td&gt;
   &lt;td&gt;Specify the data path.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;-d
   &lt;/td&gt;
   &lt;td&gt;Specify the duration of wave sample. The default value is &lt;strong&gt;3&lt;/strong&gt;.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;-s
   &lt;/td&gt;
   &lt;td&gt;Specify size of sample waves, default is &lt;strong&gt;100&lt;/strong&gt;.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--vad
   &lt;/td&gt;
   &lt;td&gt;(`inference_custom.py` only) Enable VAD model to detect active speech. The VAD option will identify speech segments in the audio file and construct a new &lt;strong&gt;.wav&lt;/strong&gt; file containing only the speech segments. This improves the quality of speech data used as input into the language identification model.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--ipex
   &lt;/td&gt;
   &lt;td&gt;Run inference with optimizations from Intel Extension for PyTorch. This option will apply optimizations to the pretrained model. Using this option should result in performance improvements related to latency.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--ground_truth_compare
   &lt;/td&gt;
   &lt;td&gt;(`inference_custom.py` only) Enable comparison of prediction labels to ground truth values.
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;--verbose
   &lt;/td&gt;
   &lt;td&gt;Print additional debug information, like latency.
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The path to the data must be specified. By default, 100 audio samples of 3-seconds will be randomly selected from the original audio file and used as input to the language identification model.&lt;/p&gt;

&lt;p&gt;A small Convolutional Recurrent Deep Neural Network (CRDNN) pretrained on the &lt;a href=&quot;http://drive.google.com/file/d/1--cAS5ePojMwNY5fewioXAv9YlYAWzIJ/view&quot;&gt;LibriParty&lt;/a&gt; dataset is used to process audio samples and output the segments where speech activity is detected. This can be used in inference with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--vad&lt;/code&gt; option.&lt;/p&gt;

&lt;p&gt;From the figure below, the timestamps where speech will be detected is delivered from the CRDNN model, and these are used to construct a new, shorter audio file with only speech. Sampling from this new audio file will give a better prediction of the primary language spoken.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-timestamps-delivered-from-crdnn-model.png&quot; alt=&quot;Audio wave file visualization&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/End-to-end-Workloads/LanguageIdentification#run-inference&quot;&gt;Run the inference script&lt;/a&gt; yourself. An example command of running inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python inference_custom.py -p data_custom -d 3 -s 50 --vad
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will run inference on data you provide located inside the &lt;em&gt;data_custom&lt;/em&gt; folder. This command performs inference on 50 randomly selected 3-second audio samples with voice activity detection.&lt;/p&gt;

&lt;p&gt;If you want to run the code sample for other languages, download Common Voice Corpus 11.0 datasets for other languages.&lt;/p&gt;

&lt;h2 id=&quot;optimizations-with-intel-extension-for-pytorch-and-intel-neural-compressor&quot;&gt;Optimizations with Intel Extension for PyTorch and Intel Neural Compressor&lt;/h2&gt;

&lt;h3 id=&quot;pytorch&quot;&gt;PyTorch&lt;/h3&gt;

&lt;p&gt;The Intel extension expands PyTorch with up-to-date features and optimizations for an extra performance boost on Intel hardware. Check out &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch#installation&quot;&gt;how to install Intel Extension for PyTorch&lt;/a&gt;. The extension can be loaded as a Python module or linked as a C++ library. Python users can enable it dynamically by importing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel_extension_for_pytorch&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/cpu/latest/&quot;&gt;CPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel CPUs. Source code is available at the &lt;a href=&quot;https://github.com/intel/intel-extension-for-pytorch/tree/master&quot;&gt;master branch&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://intel.github.io/intel-extension-for-pytorch/xpu/latest/&quot;&gt;GPU tutorial&lt;/a&gt; gives detailed information about Intel Extension for PyTorch for Intel GPUs. Source code is available at the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/xpu-master&quot;&gt;xpu-master branch&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To optimize the model for inference using Intel Extension for PyTorch, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--ipex&lt;/code&gt;option can be passed in. The model is optimized using the plug-in. TorchScript speeds up inference because PyTorch is run in graph mode. The command to run with this optimization is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python inference_custom.py -p data_custom -d 3 -s 50 --vad --ipex --verbose
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--verbose&lt;/code&gt; option is required to view the latency measurements.&lt;/p&gt;

&lt;p&gt;Auto-mixed precision such as bfloat16 (BF16) support will be added in a future release of the code sample.&lt;/p&gt;

&lt;h3 id=&quot;intel-neural-compressor&quot;&gt;Intel Neural Compressor&lt;/h3&gt;

&lt;p&gt;This is an open-source Python library that runs on CPUs or GPUs, which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Performs model quantization to reduce the model size and increase the speed of deep learning inference for deployment.&lt;/li&gt;
  &lt;li&gt;Automates popular methods such as quantization, compression, pruning, and knowledge distillation across multiple deep-learning frameworks.&lt;/li&gt;
  &lt;li&gt;Is part of the AI Kit&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model can be quantized from float32 (FP32) precision to integer-8 (INT8) by running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_model.py&lt;/code&gt; script while passing in the path to the model and a validation dataset. The following code can be used to load this INT8 model for inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from neural_compressor.utils.pytorch import load
model_int8 = load(&quot;./lang_id_commonvoice_model_INT8&quot;, self.language_id)
signal = self.language_id.load_audio(data_path)
prediction = self.model_int8(signal)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the original model is required when loading the quantized model. The command to quantize the trained model from FP32 to INT8 by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_model.py&lt;/code&gt; is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python quantize_model.py -p ./lang_id_commonvoice_model -datapath $COMMON_VOICE_PATH/commonVoiceData/commonVoice/dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What‚Äôs Next?&lt;/h2&gt;

&lt;p&gt;Try out the above code sample by upgrading the hardware to a 4th Generation Intel Xeon Scalable Processor with Intel AMX and identify up to 93 different languages from Common Voice datasets.&lt;/p&gt;

&lt;p&gt;We encourage you to learn more about and incorporate Intel‚Äôs other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;AI/ML Framework optimizations&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;end-to-end portfolio of tools&lt;/a&gt; into your AI workflow. Also, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI &amp;amp; ML page&lt;/a&gt; covering Intel‚Äôs AI software development resources for preparing, building, deploying, and scaling your AI solutions.&lt;/p&gt;

&lt;p&gt;For more details about the new 4th Gen Intel Xeon Scalable processors, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;Intel‚Äôs AI Solution Platform portal&lt;/a&gt; where you can learn how Intel is empowering developers to run end-to-end AI pipelines on these powerful CPUs.&lt;/p&gt;

&lt;h3 id=&quot;useful-resources&quot;&gt;Useful resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;Intel AI Developer Tools and resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI unified programming model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-tensorflow.html&quot;&gt;Official documentation - Intel¬Æ Optimization for TensorFlow*&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&quot;&gt;Official documentation - Intel¬Æ Neural Compressor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/ai-solution-brief.html&quot;&gt;Accelerate AI Workloads with Intel¬Æ AMX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;explore-more-ai-code-samples&quot;&gt;Explore more AI code samples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;Optimize PyTorch Models using Intel¬Æ Extension for PyTorch (IPEX) Quantization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPyTorch_TrainingOptimizations_AMX_BF16&quot;&gt;PyTorch Training Optimizations with Advanced Matrix Extensions Bfloat16&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Getting-Started-Samples/INC-Sample-for-Tensorflow&quot;&gt;Intel¬Æ Neural Compressor TensorFlow* Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/code-samples.html&quot; class=&quot;btn btn-lg with-right-arrow&quot; data-cta=&quot;get-started&quot;&gt;See all code samples&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Language Identification is the process of identifying the primary language from multiple audio input samples. In natural language processing (NLP), language identification is an important problem and a challenging issue. There are many language-related tasks such as entering text on your phone, finding news articles you enjoy, or discovering answers to questions that you may have. All these tasks are powered by NLP models. To decide which model to invoke at a particular point in time, we must perform language identification.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing PyTorch Docathon 2023</title>
      <link href="https://pytorch.org/blog/announcing-docathon/" rel="alternate" type="text/html" title="Announcing PyTorch Docathon 2023" />
      <published>2023-05-03T00:00:00-07:00</published>
      <updated>2023-05-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/announcing-docathon</id>
      <content type="html" xml:base="https://pytorch.org/blog/announcing-docathon/">&lt;p&gt;&lt;img src=&quot;/assets/images/docathon-cover.jpg&quot; alt=&quot;PyTorch Docathon&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are excited to announce the first ever PyTorch Docathon! The Docathon is a hackathon-style event focused on improving the documentation by enlisting the help of the community. Documentation is a crucial aspect of any technology and by improving the documentation, we can make it easier for users to get started with PyTorch, help them understand how to use its features effectively, and ultimately accelerate research to production in the field of machine learning.&lt;/p&gt;

&lt;h2 id=&quot;why-participate&quot;&gt;WHY PARTICIPATE&lt;/h2&gt;

&lt;h3 id=&quot;low-barrier-to-entry&quot;&gt;Low Barrier to Entry&lt;/h3&gt;

&lt;p&gt;Many open-source projects require extensive knowledge of the codebase and prior contributions to the project to participate in any sort of hackathon events. The Docathon, on the other hand, is designed for newcomers. We do expect familiarity with Python, basic knowledge of PyTorch, and ML. But don‚Äôt fret, there are some tasks that are related to website issues that won‚Äôt require even that.&lt;/p&gt;

&lt;h3 id=&quot;tangible-results&quot;&gt;Tangible Results&lt;/h3&gt;

&lt;p&gt;One of the best things about the Docathon is that you can see the results of your efforts in real time. Improving documentation can have a huge impact on a project‚Äôs usability and accessibility and you‚Äôll be able to see those improvements firsthand. Plus having tangible results can be a great motivator to keep contributing.&lt;/p&gt;

&lt;h3 id=&quot;collaborative-environment&quot;&gt;Collaborative Environment&lt;/h3&gt;

&lt;p&gt;The Docathon is a collaborative event which means you‚Äôll have the opportunity to work with other contributors and PyTorch maintainers on improving the documentation. This can be a great way to learn from others, share ideas, and build connections.&lt;/p&gt;

&lt;h3 id=&quot;learning-opportunities&quot;&gt;Learning Opportunities&lt;/h3&gt;

&lt;p&gt;Finally, even if you are not an expert in PyTorch, the Docathon can be a great learning experience. You‚Äôll have the opportunity to explore the PyTorch modules and test some of the tutorials on your machine as well as in the CI.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;EVENT DETAILS&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;May 31&lt;/strong&gt;: Kick-off&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;May 31 - June 11&lt;/strong&gt;:  Submissions and Feedback&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 12 - June 13&lt;/strong&gt;: Final Reviews&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;June 15&lt;/strong&gt;: Winner Announcements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Details for the Docathon to be announced at the kick-off stream on May 31.&lt;/p&gt;

&lt;p&gt;Please register to join this year‚Äôs event: &lt;a href=&quot;https://community.linuxfoundation.org/e/mmbqqb/&quot;&gt;&lt;strong&gt;RSVP&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Image Segmentation using PyTorch</title>
      <link href="https://pytorch.org/blog/accelerated-image-seg/" rel="alternate" type="text/html" title="Accelerated Image Segmentation using PyTorch" />
      <published>2023-05-02T00:00:00-07:00</published>
      <updated>2023-05-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-image-seg</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-image-seg/">&lt;p&gt;&lt;em&gt;Using Intel¬Æ Extension for PyTorch to Boost Image Processing Performance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;PyTorch delivers great CPU performance, and it can be further accelerated with Intel¬Æ Extension for PyTorch. I trained an AI image segmentation model using PyTorch 1.13.1 (with ResNet34 + UNet architecture) to identify roads and speed limits from satellite images, all on the 4th Gen Intel¬Æ Xeon¬Æ Scalable processor.&lt;/p&gt;

&lt;p&gt;I will walk you through the steps to work with a satellite image dataset called SpaceNet5 and how I optimized the code to make deep learning workloads feasible on CPUs just by flipping a few key switches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Before we get started, some housekeeping‚Ä¶&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The code accompanying this article is available in the examples folder in the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/master/examples/cpu/usecase_spacenet5&quot;&gt;Intel Extension for PyTorch repository&lt;/a&gt;. I borrowed heavily from the &lt;a href=&quot;http://github.com/avanetten/cresi/&quot;&gt;City-Scale Road Extraction from Satellite Imagery (CRESI) repository&lt;/a&gt;. I adapted it for the 4th Gen Intel Xeon processors with PyTorch optimizations and &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel Extension for PyTorch&lt;/a&gt; optimizations. In particular, I was able to piece together a workflow using the &lt;a href=&quot;http://github.com/avanetten/cresi/tree/main/notebooks&quot;&gt;notebooks here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find the accompanying talk I gave &lt;a href=&quot;http://www.youtube.com/watch?v=LVZWm5GFvAw&quot;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also highly recommend these articles for a detailed explanation of how to get started with the SpaceNet5 data:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-1-imagery-and-label-preparation-598af46d485e&quot;&gt;The SpaceNet 5 Baseline ‚Äî Part 1: Imagery and Label Preparation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-2-training-a-road-speed-segmentation-model-2bc93de564d7&quot;&gt;The SpaceNet 5 Baseline ‚Äî Part 2: Training a Road Speed Segmentation Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/the-downlinq/the-spacenet-5-baseline-part-3-extracting-road-speed-vectors-from-satellite-imagery-5d07cd5e1d21&quot;&gt;The SpaceNet 5 Baseline ‚Äî Part 3: Extracting Road Speed Vectors from Satellite Imagery&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;SpaceNet 5 Winning Model Release: End of the Road&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I referenced two Hugging Face blogs by Julien Simon; he ran his tests on the AWS instance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r7iz.metal-16xl&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://huggingface.co/blog/intel-sapphire-rapids&quot;&gt;Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://huggingface.co/blog/intel-sapphire-rapids-inference&quot;&gt;Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The potential cost savings from using a CPU instance instead of a GPU instance on the major cloud service providers (CSP) can be significant. The latest processors are still being rolled out to the CSPs, so I‚Äôm using a 4th Gen Intel Xeon processor that is hosted on the Intel¬Æ Developer Cloud (you can sign up for the Beta here: &lt;a href=&quot;http://cloud.intel.com/&quot;&gt;cloud.intel.com&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;On AWS, you can select from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r7iz.*&lt;/code&gt; EC2 instances after you &lt;a href=&quot;http://pages.awscloud.com/R7iz-Preview.html&quot;&gt;sign up for the preview here&lt;/a&gt; (Figure 1). At the time of writing, the new AI-acceleration engine, Intel¬Æ Advanced Matrix Extensions (Intel¬Æ AMX), is only available on bare metal but it should soon be enabled on the virtual machines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f1-4th-gen-xeon-aws-instances.png&quot; alt=&quot;List of 4th Gen Xeon  instances on AWS EC2&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;. List of 4th Gen Xeon  instances on AWS EC2 (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;On Google Cloud* Platform, you can select from the 4th Gen Xeon Scalable processors C3 VMs (Figure 2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f2-4th-gen-xeon-googlecloud-instances.png&quot; alt=&quot;List of 4th Gen Intel Xeon Scalable processor instances on Google Cloud Platform&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. List of 4th Gen Intel Xeon Scalable processor instances on Google Cloud Platform (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;hardware-introduction-and-optimizations&quot;&gt;Hardware Introduction and Optimizations&lt;/h2&gt;

&lt;p&gt;The 4th Gen Intel Xeon processors were released January 2023, and the bare-metal instance I am using has two sockets (each with 56 physical cores), 504 GB of memory, and Intel AMX acceleration. I installed a few key libraries in the backend to take control and monitor the sockets, memory, and cores that I am using on the CPU:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get install numactl&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libjemalloc-dev&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get install libjemalloc&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel-openmp&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda install intel-openmp&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gperftools&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda install gperftools -c conda-forge&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Both PyTorch and Intel Extension for PyTorch have helper scripts so that one does not need to explicitly use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intel-openmp&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt;, but they do need to be installed in the backend. In case you want to set them up for other work, here is what I used for OpenMP* ‚Ä¶&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export OMP_NUM_THREADS=36
export KMP_AFFINITY=granularity=fine,compact,1,0
export KMP_BLOCKTIME=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;‚Ä¶ where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OMP_NUM_THREADS&lt;/code&gt; is the number of threads allocated to the job, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KMP_AFFINITY&lt;/code&gt; affects thread affinity settings (including packing threads close to each other, the state of pinning threads), and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KMP_BLOCKTIME&lt;/code&gt; sets the time in milliseconds that an idle thread should wait before going to sleep.&lt;/p&gt;

&lt;p&gt;Here‚Äôs what I used for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numactl&lt;/code&gt; ‚Ä¶&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;numactl -C 0-35 --membind=0 train.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;‚Ä¶where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-C&lt;/code&gt; specifies which cores to use and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--membind&lt;/code&gt; instructs the program to only use one socket (socket 0 in this case).&lt;/p&gt;

&lt;h2 id=&quot;spacenet-data&quot;&gt;SpaceNet Data&lt;/h2&gt;

&lt;p&gt;I am using a satellite image dataset from the &lt;a href=&quot;http://spacenet.ai/sn5-challenge/&quot;&gt;SpaceNet 5 Challenge&lt;/a&gt;. Different cities can be downloaded for free from an AWS S3 bucket:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws s3 ls s3://spacenet-dataset/spacenet/SN5_roads/tarballs/ --human-readable
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2019-09-03 20:59:32    5.8 GiB SN5_roads_test_public_AOI_7_Moscow.tar.gz
2019-09-24 08:43:02    3.2 GiB SN5_roads_test_public_AOI_8_Mumbai.tar.gz
2019-09-24 08:43:47    4.9 GiB SN5_roads_test_public_AOI_9_San_Juan.tar.gz
2019-09-14 13:13:26   35.0 GiB SN5_roads_train_AOI_7_Moscow.tar.gz
2019-09-14 13:13:34   18.5 GiB SN5_roads_train_AOI_8_Mumbai.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can use the following commands to download and unpack a file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws s3 cp s3://spacenet-dataset/spacenet/SN5_roads/tarballs/SN5_roads_train_AOI_7_Moscow.tar.gz .
tar -xvzf ~/spacenet5data/moscow/SN5_roads_train_AOI_7_Moscow.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dataset-preparation&quot;&gt;Dataset Preparation&lt;/h3&gt;

&lt;p&gt;I used the Moscow satellite image dataset, which consists of 1,352 images of 1,300 by 1,300 pixels with corresponding street labels in separate text files. The dataset contains both 8-band multispectral images and 3-band RGB images. Figure 3 shows four sample RGB satellite images and their corresponding generated masks. I used the &lt;a href=&quot;http://github.com/avanetten/cresi/blob/main/cresi/data_prep/speed_masks.py&quot;&gt;speed_masks.py&lt;/a&gt; script from the CRESI repository to generate the segmentation masks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f3-moscow-satellite-image-dataset.png&quot; alt=&quot;Satellite image 3-channel RGB chips from Moscow (top row) and corresponding pixel segmentation masks with varying speed limits&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;. Satellite image 3-channel RGB chips from Moscow (top row) and corresponding pixel segmentation masks with varying speed limits (bottom row) (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;There is a JSON configuration file that must be updated for all remaining components: training and validation split, training, and inference. &lt;a href=&quot;http://github.com/avanetten/cresi/blob/main/cresi/configs/sn5_baseline_aws.json.&quot;&gt;An example configuration can be found here&lt;/a&gt;. I perform an 80:20 training/validation split, making sure to point to the correct folder of satellite images and corresponding masks for training. The configuration parameters are explained in more in the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch/tree/master/examples/cpu/usecase_spacenet5&quot;&gt;notebook under examples in GitHub for Intel Extension for PyTorch here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;training-a-resnet34--unet-model&quot;&gt;Training a ResNet34 + UNet Model&lt;/h3&gt;

&lt;p&gt;I made some changes to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cresi&lt;/code&gt; code described below in order to run on a CPU and optimize the training. To run natively on a CPU, replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.model = nn.DataParallel(model).cuda()&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.model = nn.DataParallel(model)&lt;/code&gt; in the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/net/pytorch_utils/train.py&quot;&gt;train.py&lt;/a&gt; script. In the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/01_train.py&quot;&gt;01_train.py&lt;/a&gt; script, remove &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.randn(10).cuda()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To optimize training, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import intel_extension_for_pytorch as ipex&lt;/code&gt; to the import statements in the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/net/pytorch_utils/train.py&quot;&gt;train.py&lt;/a&gt; script. Just after defining the model and optimizer as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model = nn.DataParallel(model)
self.optimizer = optimizer(self.model.parameters(), lr=config.lr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipex.optimize&lt;/code&gt; line to use BF16 precision, instead of FP32: \&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model, self.optimizer = ipex.optimize(self.model, 
    optimizer=self.optimizer,dtype=torch.bfloat16)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add a line to do mixed-precision training just before running a forward pass and calculating the loss function:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.cpu.amp.autocast():
    if verbose:
        print(&quot;input.shape, target.shape:&quot;, input.shape, target.shape)
    output = self.model(input)
    meter = self.calculate_loss_single_channel(output, target, meter, training, iter_size)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that we have optimized our training code, we can move onto training our model.&lt;/p&gt;

&lt;p&gt;Like the &lt;a href=&quot;https://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;winner of the SpaceNet 5 competition&lt;/a&gt;, I trained a ResNet34 encoder + UNet decoder model. It is pretrained from ImageNet weights, and the backbone is left completely unfrozen during training. The training can be run with the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/01_train.py&quot;&gt;01_train.py&lt;/a&gt; script, but in order to control the use of hardware I used a helper script. There are actually two helper scripts: one that comes with stock PyTorch and one that comes with Intel Extension for PyTorch. They both accomplish the same thing, but the first one from stock is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.backends.xeon.run_cpu&lt;/code&gt;, and the second one from Intel Extension for PyTorch is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipexrun&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here is what I ran in the command-line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -m torch.backends.xeon.run_cpu --ninstances 1 \
  --ncores_per_instance 32 \
  --log_path /home/devcloud/spacenet5data/moscow/v10_xeon4_devcloud22.04/logs/run_cpu_logs \
  /home/devcloud/cresi/cresi/01_train.py \
  /home/devcloud/cresi/cresi/configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ipexrun --ninstances 1 \
--ncore_per_instance 32 \
/home/devcloud/cresi/cresi/01_train.py \
/home/devcloud/cresi/cresi/configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In both cases, I am asking PyTorch to run training on one socket with 32 cores. Upon running, I get a printout of what environment variables get set in the backend to understand how PyTorch is using the hardware:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INFO - Use TCMalloc memory allocator
INFO - OMP_NUM_THREADS=32
INFO - Using Intel OpenMP
INFO - KMP_AFFINITY=granularity=fine,compact,1,0
INFO - KMP_BLOCKTIME=1
INFO - LD_PRELOAD=/home/devcloud/.conda/envs/py39/lib/libiomp5.so:/home/devcloud/.conda/envs/py39/lib/libtcmalloc.so
INFO - numactl -C 0-31 -m 0 /home/devcloud/.conda/envs/py39/bin/python -u 01_train.py configs/ben/v10_xeon4_baseline_ben.json --fold=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;During training, I make sure that my total loss function is decreasing (i.e., the model is converging on a solution).&lt;/p&gt;

&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;

&lt;p&gt;After training a model, we can start to make predictions from satellite images alone. In the eval.py inference script, add import intel_extension_for_pytorch as ipex to the import statements. After loading the PyTorch model, use Intel Extension for PyTorch to optimize the model for BF16 inference:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.load(os.path.join(path_model_weights, 
    'fold{}_best.pth'.format(fold)), 
    map_location = lambda storage, 
    loc: storage)
model.eval()
model = ipex.optimize(model, dtype = torch.bfloat16)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just prior to running prediction, add two lines for mixed precision:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.no_grad():
    with torch.cpu.amp.autocast():
        for data in pbar:
            samples = torch.autograd.Variable(data['image'], volatile=True)
            predicted = predict(model, samples, flips=self.flips)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To run inference, we can use the &lt;a href=&quot;https://github.com/avanetten/cresi/blob/main/cresi/02_eval.py&quot;&gt;02_eval.py&lt;/a&gt; script. Now that we have a trained model, we can make predictions on satellite images (Figure 4). We can see that it does seem to map the roads closely to the image!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/f4-moscow-satellite-image-complete.png&quot; alt=&quot;Moscow satellite image and accompanying prediction of roads&quot; style=&quot;max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;. Moscow satellite image and accompanying prediction of roads (image by author)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;I realize that the model I‚Äôve trained is overfit to the Moscow image data and probably won‚Äôt generalize well to other cities. However, the &lt;a href=&quot;http://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c&quot;&gt;winning solution to this challenge&lt;/a&gt; used data from six cities (Las Vegas, Paris, Shanghai, Khartoum, Moscow, Mumbai) and performs well on new cities. In the future, one thing that would be worth testing is training on all six cities and running inference on another city to reproduce their results.&lt;/p&gt;

&lt;h2 id=&quot;note-on-post-processing&quot;&gt;Note on Post-Processing&lt;/h2&gt;

&lt;p&gt;There are further post-processing steps that can be performed to add the mask as graph features to maps. You can read more about the post-processing steps here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://medium.com/the-downlinq/the-spacenet-5-baseline-part-3-extracting-road-speed-vectors-from-satellite-imagery-5d07cd5e1d21&quot;&gt;The SpaceNet 5 Baseline ‚Äî Part 3: Extracting Road Speed Vectors from Satellite Imagery&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/avanetten/cresi/tree/main/cresi&quot;&gt;Post-processing scripts&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In summary, we:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Created 1,352 image training masks (with speed limits) to correspond to our training satellite image data (from .geojson text file labels)&lt;/li&gt;
  &lt;li&gt;Defined our configuration file for training and inference&lt;/li&gt;
  &lt;li&gt;Split up our data into training and validation sets&lt;/li&gt;
  &lt;li&gt;Optimized our code for CPU training, including using Intel Extension for PyTorch and BF16&lt;/li&gt;
  &lt;li&gt;Trained a performant ResNet34 + UNet model on a 4th Gen Intel Xeon CPU&lt;/li&gt;
  &lt;li&gt;Ran initial inference to see the prediction of a speed limit mask&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can find &lt;a href=&quot;http://edc.intel.com/content/www/us/en/products/performance/benchmarks/4th-generation-intel-xeon-scalable-processors/&quot;&gt;detailed benchmarks here for the 4th Gen Intel Xeon CPU here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Extend the optimizations on an Intel CPU by using the Intel Extension for PyTorch:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install intel-extension-for-pytorch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git clone https://github.com/intel/intel-extension-for-pytorch&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://linkedin.com/in/bconsolvo&quot;&gt;Get in touch with me on LinkedIn&lt;/a&gt; if you have any more questions!&lt;/p&gt;

&lt;p&gt;More information about the Intel Extension for PyTorch &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;can be found here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;get-the-software&quot;&gt;Get the Software&lt;/h3&gt;

&lt;p&gt;I encourage you to check out Intel‚Äôs other &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Framework&lt;/a&gt;&lt;/strong&gt; optimizations and learn about the open, standards-based &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt;&lt;/strong&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel‚Äôs AI software portfolio.&lt;/p&gt;

&lt;p&gt;For more details about 4th Gen Intel Xeon Scalable processor, visit &lt;strong&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI Platform&lt;/a&gt;&lt;/strong&gt; where you can learn about how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-resources&quot;&gt;PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Using Intel¬Æ Extension for PyTorch to Boost Image Processing Performance</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving</title>
      <link href="https://pytorch.org/blog/introducing-hidet/" rel="alternate" type="text/html" title="Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving" />
      <published>2023-04-27T00:00:00-07:00</published>
      <updated>2023-04-27T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/introducing-hidet</id>
      <content type="html" xml:base="https://pytorch.org/blog/introducing-hidet/">&lt;p&gt;&lt;a href=&quot;https://github.com/hidet-org/hidet&quot;&gt;Hidet&lt;/a&gt; is a powerful deep learning compiler that simplifies the process of implementing high-performing deep learning operators on modern accelerators (e.g., NVIDIA GPUs). With the new feature of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; in PyTorch 2.0, integrating a novel compiler into PyTorch is easier than ever - Hidet now can be used as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; backend to accelerate PyTorch models, making it an attractive option for PyTorch users who want to improve the inference performance of their models, especially for those who also need to implement extremely optimized custom operators.&lt;/p&gt;

&lt;h2 id=&quot;using-hidet-to-compile-a-pytorch-model&quot;&gt;Using Hidet to Compile A PyTorch Model&lt;/h2&gt;

&lt;p&gt;To use Hidet in PyTorch, you need to first install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet&lt;/code&gt; package via pip:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install hidet
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hidet is integrated with PyTorch as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(...)&lt;/code&gt; backend following the &lt;a href=&quot;https://pytorch.org/docs/stable/dynamo/custom-backends.html&quot;&gt;Custom Backends tutorial&lt;/a&gt;. You can specify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet&lt;/code&gt; as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backend&lt;/code&gt; when you compile a model. (Note: requires PyTorch version 2.0+):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.compile(..., backend='hidet')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hidet converts the given PyTorch model in the torch.fx.Graph format into its internal graph representation, and conducts a series of optimizations. Hidet provides a few options to configure the optimizations. For example, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet.torch.dynamo_config.use_tensor_core(True)&lt;/code&gt; to allow Hidet to generate CUDA kernels that leverage the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/tensor-cores/&quot;&gt;Tensor Cores on NVIDIA GPUs&lt;/a&gt;, and use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidet.torch.dynamo_config.search_space(2)&lt;/code&gt; to allow Hidet to search for the best operator schedule specific for your hardware and input sizes. More configurations can be found in &lt;a href=&quot;https://docs.hidet.org/stable/gallery/tutorials/optimize-pytorch-model.html&quot;&gt;Hidet‚Äôs documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here‚Äôs a complete example of how to use Hidet to compile and optimize a pre-trained ResNet50 model from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import hidet
import torch

# Load a pre-trained ResNet50 model
x = torch.randn(1, 3, 224, 224, device='cuda').half()
model = torch.hub.load(
    'pytorch/vision:v0.6.0', 'resnet50', pretrained=True
).cuda().half().eval()

# Configure hidet to use tensor core and enable tuning
hidet.torch.dynamo_config.use_tensor_core(True)
hidet.torch.dynamo_config.search_space(2) 

# Compile the model using Hidet
model_opt = torch.compile(model, backend='hidet')

# Check correctness
torch.testing.assert_close(actual=model_opt(x), expected=model(x), rtol=1e-2, atol=1e-2)

# Benchmark
from hidet.utils import benchmark_func
print('eager: {:2f}'.format(benchmark_func(lambda: model(x))))
print('hidet: {:2f}'.format(benchmark_func(lambda: model_opt(x))))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We encourage you to try out the above script on your own NVIDIA GPU(s)! If you run this script on an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws.g5.2xlarge&lt;/code&gt; instance, you would get the result shown in the following figure. Hidet achieves the speedup because it could automatically fuse multiple operators, tune operator schedules, and use CUDA Graph to reduce framework-level overhead. More results can be found in the &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3575693.3575702&quot;&gt;ASPLOS‚Äô23 publication of Hidet&lt;/a&gt; and our &lt;a href=&quot;https://github.com/hidet-org/hidet/issues/154&quot;&gt;performance tracking&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-4-27-hidet.png&quot; alt=&quot;Eager vs Hidet latency&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;using-hidet-script-to-write-custom-operators&quot;&gt;Using Hidet Script to Write Custom Operators&lt;/h2&gt;

&lt;p&gt;Hidet Script is one approach to implement tensor operators in Python. The following example shows how to implement a naive matrix multiplication using Hidet Script and integrate it as a PyTorch operator.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import hidet


def matmul(m_size, n_size, k_size):
    from hidet.lang import f32, attr
    from hidet.lang.cuda import threadIdx, blockIdx, blockDim

    with hidet.script_module() as script_module:
        @hidet.script
        def matmul(
            a: f32[m_size, k_size],
            b: f32[k_size, n_size],
            c: f32[m_size, n_size]
        ):
            attr.cuda_grid_dim = ((m_size + 31) // 32, (n_size + 31) // 32)
            attr.cuda_block_dim = (32, 32)
            i = threadIdx.x + blockIdx.x * blockDim.x
            j = threadIdx.y + blockIdx.y * blockDim.y
            if i &amp;lt; m_size and j &amp;lt; n_size:
                c[i, j] = 0.0
                for k in range(k_size):
                    c[i, j] += a[i, k] * b[k, j]

    ir_module = script_module.ir_module()
    func = hidet.driver.build_ir_module(ir_module)
    return func


class NaiveMatmul(torch.autograd.Function):
    @staticmethod
    def forward(ctx, a, b):
        m, k = a.shape
        k, n = b.shape
        c = torch.empty([m, n], dtype=a.dtype, device=a.device)
        func = matmul(m, n, k)
        func(a, b, c)
        return c


a = torch.randn([3, 4], device='cuda')
b = torch.randn([4, 5], device='cuda')
c = NaiveMatmul.apply(a, b)
cc = torch.matmul(a, b)
torch.testing.assert_close(c, cc)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;More optimizations can be applied, see the &lt;a href=&quot;https://docs.hidet.org/stable/gallery/developer-guides/hidet-script-dynamic-kernel.html&quot;&gt;example&lt;/a&gt; in our documentation to learn more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hidet Script vs. Triton&lt;/strong&gt;: Triton greatly simplifies the CUDA programming by introducing the tile-based programming model where the parallel execution unit is thread blocks instead of threads. However, this simplification also prevents the tensor program developers from manipulating the fine-grained computation and memory resources (e.g., warps, shared memory) in their preferred ways. It would be challenging to implement an optimization that requires fine-grained control of these resources using Triton if it has not been implemented by the Triton compiler itself. Hidet Script, on the other hand, simplifies tensor programming while still enabling users to implement their own optimizations with extensive flexibility. It‚Äôs worth noting that the more granular control of Hidet Script also brings added complexity compared to Triton.&lt;/p&gt;

&lt;h2 id=&quot;more-about-hidet&quot;&gt;More about Hidet&lt;/h2&gt;

&lt;p&gt;Hidet originates from a research project led by the &lt;a href=&quot;https://www.cs.toronto.edu/ecosystem/&quot;&gt;EcoSystem lab&lt;/a&gt; at the University of Toronto (UofT) and AWS. The authors propose a new way, named the task-mapping programming paradigm, to construct tensor programs. It aims to simplify the tensor programming without sacrificing any optimization opportunity. Now, Hidet is an open-source project, jointly supported by &lt;a href=&quot;https://centml.ai/&quot;&gt;CentML&lt;/a&gt; and the EcoSystem lab, that aims to provide an efficient solution to end-to-end inference on modern accelerators (e.g., NVIDIA GPUs).&lt;/p&gt;

&lt;h3 id=&quot;additional-resources&quot;&gt;Additional Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;GitHub Repository: &lt;a href=&quot;https://github.com/hidet-org/hidet&quot;&gt;https://github.com/hidet-org/hidet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hidet‚Äôs Documentation: &lt;a href=&quot;https://docs.hidet.org&quot;&gt;https://docs.hidet.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ASPLOS ‚Äô23 Publication: &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3575693.3575702&quot;&gt;https://dl.acm.org/doi/10.1145/3575693.3575702&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ASPLOS ‚Äô23 Tutorial: &lt;a href=&quot;https://centml.github.io/asplos23-tutorial/&quot;&gt;https://centml.github.io/asplos23-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;We would like to thank Jerry Park, Mark Saroufim, Jason Liang and Helen Suk for their valuable help on preparing the blog post and feedback on the text. We also would like to thank Nikita Shulga, Jason Ansel, and Dmytro Dzhulgakov for reviewing and improving our PR https://github.com/pytorch/pytorch/pull/93873 on the 3rd-party dynamo backend registration.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team Hidet</name>
        
        
      </author>

      

      

      
        <summary type="html">Hidet is a powerful deep learning compiler that simplifies the process of implementing high-performing deep learning operators on modern accelerators (e.g., NVIDIA GPUs). With the new feature of torch.compile(...) in PyTorch 2.0, integrating a novel compiler into PyTorch is easier than ever - Hidet now can be used as a torch.compile(...) backend to accelerate PyTorch models, making it an attractive option for PyTorch users who want to improve the inference performance of their models, especially for those who also need to implement extremely optimized custom operators.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Large Language Models with Accelerated Transformers</title>
      <link href="https://pytorch.org/blog/accelerating-large-language-models/" rel="alternate" type="text/html" title="Accelerating Large Language Models with Accelerated Transformers" />
      <published>2023-04-19T00:00:00-07:00</published>
      <updated>2023-04-19T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-large-language-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-large-language-models/">&lt;p&gt;&lt;strong&gt;TL;DR.&lt;/strong&gt; We show how to use Accelerated PyTorch 2.0 Transformers and the newly introduced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; method to accelerate Large Language Models on the example of &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt;, a compact open-source implementation of the GPT model from Andrej Karpathy. Using the new &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;scaled dot product attention operator&lt;/a&gt; introduced with Accelerated PT2 Transformers, we select the flash_attention custom kernel and achieve faster training time per batch (measured with Nvidia A100 GPUs), going from a ~143ms/batch baseline to ~113 ms/batch. In addition, the enhanced implementation using the SDPA operator offers better numerical stability. Finally, further optimizations are achieved using padded inputs, which when combined with flash attention lead to ~87ms/batch.&lt;/p&gt;

&lt;p&gt;Recent times have seen exponential adoption of large language models (LLMs) and Generative AI in everyday life. Tightly coupled with these ever-growing models is the ever-growing training cost - in terms of both time and hardware utilization. The PyTorch team has tackled these challenges head on with &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-2/&quot;&gt;Accelerated PyTorch 2 Transformers&lt;/a&gt; (previously known as ‚ÄúBetter Transformer‚Äù) and JIT Compilation in &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-release/&quot;&gt;PyTorch 2.0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post, we explore training optimizations gained by utilizing custom kernel implementations of SDPA - also known as scaled dot product attention - a critical layer in transformer models. The custom kernel for SDPA replaces several discrete sequential operations with one globally optimized kernel which avoids allocating a large amount of intermediate CUDA memory. This approach offers a number of advantages, including but not limited to:  higher performance computation of SDPA by reducing memory bandwidth bottleneck, reduced memory footprint to support larger batch sizes, and finally added numerical stability by prescaling input tensors. These optimizations are demonstrated on nanoGPT, an open-source implementation of GPT from Andrej Karpathy.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Scaled dot product attention is the fundamental building block of multihead attention, as introduced in &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;‚ÄúAttention is All You Need‚Äù&lt;/a&gt;, and has a wide range of applications in LLM and Generative AI models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/PyTorch_Better-Transformer_Figure-1.png&quot; alt=&quot;The Transformer model architecture&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; The Transformer model architecture based on &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;‚ÄúAttention is All You Need‚Äù&lt;/a&gt;. With the new PyTorch SDPA operator, Multi-Head Attention is efficiently implemented by a linear layer for the in-projection, the SDPA operator, and a linear layer for the out-projection.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;With the new scaled_dot_product_attention operator, multihead attention can be implemented in just 3 steps: in projection with a linear layer, SDPA, and out projection with a linear layer.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# In Projection
# variable descriptions:
# q,k,v = Query, Key, Value tensors
# bsz = batch size
# num_heads = Numner of heads for Multihead Attention
# tgt_len = Target length
# src_len = Source Length
# head_dim: Head Dimension
    q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)
    q = q.view(bsz, num_heads, tgt_len, head_dim)
    k = k.view(bsz, num_heads, src_len, head_dim)
    v = v.view(bsz, num_heads, src_len, head_dim)

    # Scaled Dot Product Attention
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)

    # Out Projection
    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)
    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
    attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch 2. supports multiple different kernels optimized for specific use cases, with specific requirements. A kernel picker picks the best kernel for a particular combination of input parameters. If no optimized ‚Äúcustom kernel‚Äù for a particular combination of input parameters can be identified, the kernel picker selects a general kernel that can handle all input combinations.&lt;/p&gt;

&lt;p&gt;While future releases may extend this set of operators, PyTorch 2.0 launches with 3 implementations for the SDPA operator:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A generic kernel which implements the mathematical equation of SDPA in the function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sdpa_math()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;An optimized kernel based on the paper ‚Äú&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;Flash Attention&lt;/a&gt;‚Äù, which supports evaluation of SDPA with 16 bit floating point data types on compute architecture SM80 (A100).&lt;/li&gt;
  &lt;li&gt;An optimized kernel based on the paper ‚Äú&lt;a href=&quot;https://arxiv.org/abs/2112.0568&quot;&gt;Self-Attention Does Not Need O(n^2) Memory&lt;/a&gt;‚Äù and implemented in &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormer&lt;/a&gt;, which supports both 32 and 16 bit floating data types on a wider range of architectures (SM40 and later). This blog post refers to this kernel as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mem_efficient&lt;/code&gt; kernel.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that both optimized kernels (two and three listed above), support a key padding mask and limit the supported attention mask to causal attention. Accelerated PyTorch 2.0 Transformers today only support the causal mask when it is specified using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_causal&lt;/code&gt; boolean. When a mask is specified, the general-purpose kernel will be selected because it is too expensive to analyze the contents of a provided mask to determine if it is the causal mask. Additional explanations on the constraints for each kernel can be found in the &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-2/&quot;&gt;Accelerated PT2 Transformer blog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;enabling-accelerated-transformers-with-nanogpt&quot;&gt;Enabling Accelerated Transformers with nanoGPT&lt;/h2&gt;

&lt;p&gt;The SDPA operator being a critical component of the GPT model,  we identified the open source nanoGPT model as an excellent candidate for both demonstrating the ease of implementation and benefits of PyTorch 2.0‚Äôs Accelerated Transformers. The following demonstrates the exact process by which Accelerated Transformers was enabled on nanoGPT.&lt;/p&gt;

&lt;p&gt;This process largely revolves around replacing the existing SDPA implementation with the newly added F.scaled_dot_product_attention operator from &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/df14650f0b14b80db132b0c1797dc595fbee1054/torch/nn/functional.py#L4834&quot;&gt;functional.py&lt;/a&gt;. This process can be easily adapted to enable the operator in many other LLMs. Alternatively, users can instead choose to call F.multi_head_attention_forward() or utilize the nn.MultiHeadAttention module directly where applicable. The following code snippets are adapted from Karpathy‚Äôs nanoGPT repository.&lt;/p&gt;

&lt;h3 id=&quot;step-1-identify-the-existing-sdpa-implementation&quot;&gt;Step 1: Identify the existing SDPA implementation&lt;/h3&gt;

&lt;p&gt;In the case of nanoGPT, SDPA is implemented in the model‚Äôs &lt;a href=&quot;https://github.com/karpathy/nanoGPT/blob/master/model.py#L37&quot;&gt;CausalSelfAttention&lt;/a&gt; class. The original implementation at time of writing is adapted below for this post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/causal_attention_step_1.png&quot; alt=&quot;The original implementation at time of writing&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-2-replace-with-torchs-scaled_dot_product_attention&quot;&gt;Step 2: Replace with Torch‚Äôs &lt;em&gt;scaled_dot_product_attention&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;At this point we can note the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lines 36 - 42 define the mathematical implementation of SDPA which we are replacing&lt;/li&gt;
  &lt;li&gt;The mask applied on line 39 is no longer relevant since we are using scaled_dot_product_attention‚Äôs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_causal&lt;/code&gt; flag.&lt;/li&gt;
  &lt;li&gt;The dropout layer used in line 41 is also now unnecessary.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Swapping out the SDPA implementation for torch‚Äôs scaled_dot_product_attention and removing the now redundant code yields the following implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/causal_attention_step_2.png&quot; alt=&quot;Swapping out the SDPA implementation for torch‚Äôs scaled_dot_product_attention and removing the now redundant code yields the following implementation.&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, the original mask can be passed into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attn_mask&lt;/code&gt; field however due to the mentioned kernel constraints that would limit the implementation to only support the generic &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sdpa_math&lt;/code&gt; kernel.&lt;/p&gt;

&lt;h3 id=&quot;step-3-bonus-faster-matmuls-with-padding&quot;&gt;Step 3 (Bonus): Faster matmuls with padding&lt;/h3&gt;

&lt;p&gt;On top of the performance improvements from SDPA, our analysis yielded a nice ancillary win.  In Andrej‚Äôs words ‚ÄúThe most dramatic optimization to nanoGPT so far (~25% speedup) is to simply increase the vocab size from 50257 to 50304 (nearest multiple of 64).‚Äù&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/tweet.png&quot; alt=&quot;Tweet by Andrej Karpathy&quot; style=&quot;max-height:800px; width:100%; max-width:600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The vocab size determines the dimensions of matmuls in the output layer of GPT, and these are so large that they were taking a &lt;em&gt;majority&lt;/em&gt; of the time for the entire training loop!  We discovered that they were achieving performance significantly below the peak throughput achievable on the A100 GPU, and guessed from &lt;a href=&quot;https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html&quot;&gt;NVIDIA‚Äôs matmul documentation&lt;/a&gt; that 64-element alignment would yield better results.  Indeed, padding these matmuls achieves nearly a 3x speedup!  The underlying cause is that unaligned memory accesses significantly reduce efficiency.  A deeper analysis can be found in &lt;a href=&quot;https://twitter.com/cHHillee/status/1630274804795445248&quot;&gt;this Twitter thread&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this optimization we were able to further reduce training time from ~113 ms (using flash attention) to ~87 ms per batch.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The figure below demonstrates the performance gained using Pytorch custom kernels. Here are the exact figures:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;baseline (nanoGPT implementation):  ~143ms&lt;/li&gt;
  &lt;li&gt;sdpa_math (generic): ~134ms (6.71% faster)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mem_efficient&lt;/code&gt; kernel: ~119ms (20.16% faster)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flash_attention&lt;/code&gt; kernel: ~113ms (26.54% faster)&lt;/li&gt;
  &lt;li&gt;flash_attention + padded vocab:  ~87ms (64.37% faster)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All code was run on an 8 x NVIDIA Corporation A100 server with 80 GB HBM [A100 SXM4 80GB], and for the purpose of this experiment dropout was set to 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/PyTorch_Better-Transformer_Chart-2.png&quot; alt=&quot;Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;enhancing-numerical-model-stability&quot;&gt;Enhancing Numerical Model Stability&lt;/h2&gt;

&lt;p&gt;In addition to being faster, PyTorch‚Äôs implementation offers increased numerical stability by avoiding loss of precision in many execution scenarios. There is a great explanation &lt;a href=&quot;https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/118&quot;&gt;here&lt;/a&gt;, but essentially the PyTorch implementation scales the Query and Key matrices &lt;em&gt;before&lt;/em&gt; multiplication, which is said to be more stable and avoid loss of precision. Because of the merged custom kernel architecture of SDPA, this scaling does not introduce additional overhead in the computation of the attention result.  In comparison, an implementation from the individual computational components would require separate pre-scaling at additional cost. For an additional explanation, see Appendix A.&lt;/p&gt;

&lt;h3 id=&quot;improved-memory-consumption&quot;&gt;Improved Memory Consumption&lt;/h3&gt;

&lt;p&gt;Yet another large advantage of using the torch SDPA kernels is the reduced memory footprint, which allows for the utilization of larger batch sizes. The following chart compares the best validation loss after one hour of training for both flash attention and the baseline implementations of causal attention. As can be seen, the maximum batch size achieved with the baseline causal attention implementation (on 8 x NVIDIA Corporation A100 server with 80 GB HBM) was 24, significantly less then the maximum achieved with flash attention, which was 39.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-18-accelerating-large-language-models/chart.png&quot; alt=&quot;Using Flash Attention enables the usage of larger batch sizes&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Using Flash Attention enables the usage of larger batch sizes, allowing users to achieve lower validation loss after one hour of training (smaller is better).&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Accelerated PyTorch 2 Transformers were designed to make the training and production deployment of state-of-the-art transformer models affordable and integrated with PyTorch 2.0 model JIT compilation.  The newly introduced PyTorch SDPA operator provides improved performance for training Transformer models and is particularly valuable for the expensive Large Language Model training. In this post we demonstrate a number of optimizations on the exemplary nanoGPT model  including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Over 26% training speedup, when compared against the baseline with constant batch size&lt;/li&gt;
  &lt;li&gt;An additional speedup achieved with padded vocabulary, bringing the total optimization to approximately 64% compared to the baseline&lt;/li&gt;
  &lt;li&gt;Additional numerical stability&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;appendix-a-analyzing-attention-numeric-stability&quot;&gt;Appendix A: Analyzing Attention Numeric Stability&lt;/h2&gt;

&lt;p&gt;In this section we provide a more in depth explanation of the previously mentioned enhanced numerical stability which is gained by prescaling SDPA‚Äôs input vectors. The following is a simplified version of nanoGPT‚Äôs mathematical implementation of SDPA. The important thing to note here is that the query undergoes matrix multiplication without being scaled.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# nanoGPT implementation of SDPA
# notice q (our query vector) is not scaled !
att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
att = F.softmax(att, dim=-1)

# Dropout is set to 0, so we can safely ignore this line in the implementation# att = self.attn_dropout(att) 

y_nanogpt = att @ v # (B, nh, T, T) x (B, nh, T, hs) -&amp;gt; (B, nh, T, hs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following is the equivalent mathematical implementation in torch‚Äôs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# PyTorch implementation of SDPA
embed_size = q.size(-1)
scaling_factor = math.sqrt(math.sqrt(embed_size))
q = q / scaling_factor 	# notice q _is_ scaled here !

# same as above, but with scaling factor
att = q @ (k.transpose(-2, -1) / scaling_factor)
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
att = F.softmax(att0, dim=-1)

# Dropout is set to 0, so we can safely ignore this line in the implementation# att = self.attn_dropout(att) 

y_scale_before = att @ v
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Mathematically both approaches should be equivalent, however our experimentation shows that in practice we receive different results from each approach.&lt;/p&gt;

&lt;p&gt;Using the approach above, we verified &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_scale_before&lt;/code&gt; matches the expected output from using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention &lt;/code&gt;method while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_nanogpt&lt;/code&gt; does not.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.allclose&lt;/code&gt; method was used to test equivalence. Specifically, we showed that:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y_sdpa = torch.nn.functional._scaled_dot_product_attention(
	q,
	k,
	v,
	attn_mask=self.bias[:,:,:T,:T] != 0,
	dropout_p=0.0,
	need_attn_weights=False,
	is_causal=False,
)

torch.allclose(y_sdpa, y_nanogpt) # False, indicating fp issues
torch.allclose(y_sdpa, y_scale_before) # True, as expected
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;appendix-b-reproducing-experiment-results&quot;&gt;Appendix B: Reproducing Experiment Results&lt;/h2&gt;

&lt;p&gt;Researchers seeking to reproduce these results should start with the following commit from Andrej‚Äôs nanoGPT repository - &lt;strong&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;b3c17c6c6a363357623f223aaa4a8b1e89d0a465&lt;/span&gt;&lt;/strong&gt;. This commit was used as the baseline when measuring the per batch speed improvements. For results which include padded vocabulary optimizations (which yielded the most significant improvements to batch speed), use the following commit - &lt;strong&gt;&lt;span style=&quot;text-decoration:underline;&quot;&gt;77e7e04c2657846ddf30c1ca2dd9f7cbb93ddeab&lt;/span&gt;&lt;/strong&gt;. From either checkout, selecting kernels for experimentation is made trivial with the use of the &lt;a href=&quot;https://pytorch.org/docs/stable/backends.html&quot;&gt;torch.backends&lt;/a&gt; API.&lt;/p&gt;

&lt;p&gt;The desired kernel can be selected via a context manager:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with torch.backends.cuda.sdp_kernel (
    enable_math = False,
    enable_flash = False,
    enable_mem_efficient = True
):
    train(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Lucas Pasqualin, Driss Guessous, Christian Puhrsch, Bertrand Maher, Michael Gschwind</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR. We show how to use Accelerated PyTorch 2.0 Transformers and the newly introduced torch.compile() method to accelerate Large Language Models on the example of nanoGPT, a compact open-source implementation of the GPT model from Andrej Karpathy. Using the new scaled dot product attention operator introduced with Accelerated PT2 Transformers, we select the flash_attention custom kernel and achieve faster training time per batch (measured with Nvidia A100 GPUs), going from a ~143ms/batch baseline to ~113 ms/batch. In addition, the enhanced implementation using the SDPA operator offers better numerical stability. Finally, further optimizations are achieved using padded inputs, which when combined with flash attention lead to ~87ms/batch.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Experience the power of PyTorch 2.0 on AMD Solutions</title>
      <link href="https://pytorch.org/blog/experience-power-pytorch-2.0/" rel="alternate" type="text/html" title="Experience the power of PyTorch 2.0 on AMD Solutions" />
      <published>2023-04-15T00:00:00-07:00</published>
      <updated>2023-04-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/experience-power-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/experience-power-pytorch-2.0/">&lt;p&gt;PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework.  The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that the PyTorch 2.0 stable release includes support for AMD Instinct‚Ñ¢ and Radeon‚Ñ¢ GPUs that are supported by the ROCm‚Ñ¢ software platform.&lt;/p&gt;

&lt;p&gt;With the stable PyTorch 2.0 release, PyTorch 2.0 introduces torch.compile as a beta feature underpinned by TorchInductor with support for AMD Instinct and Radeon GPUs through OpenAI Triton deep learning compiler. Through TorchInductor, developers can now generate low level kernels using Triton that are portable and performant to hand-written kernels on native hardware centric kernel programming models.&lt;/p&gt;

&lt;p&gt;OpenAI Triton is a language and compiler for blocked algorithms, which aims to provide an abstraction layer between CUDA/HIP and Torch at which developers can write efficient kernels more productively.  We have written a new backend which interfaces Triton‚Äôs custom MLIR dialects with our ROCm compiler stack.&lt;/p&gt;

&lt;p&gt;Triton can automatically optimize kernels generated by machine learning compilers such as TorchInductor for multiple AI accelerators including AMD Instinct GPU accelerator by leveraging hardware-specific features of the AMD CDNA‚Ñ¢ GPU architecture. This makes it easy for developers and users to switch seamlessly from any HW to AMD Instinct GPU accelerators and get great out of the box performance.&lt;/p&gt;

&lt;p&gt;In addition, compilers like Triton can also enable developers to use high-level programming languages, such as Python, to write machine learning code that can be efficiently compiled and executed on specialized hardware. This can help greatly improve the productivity of machine learning developers, as they can focus on the algorithmic aspects of their models and rely on the compiler to generate efficient code.&lt;/p&gt;

&lt;p&gt;By design, PyTorch 2.0 is backward compatible to earlier PyTorch releases. This holds true for the ROCm build of PyTorch 2.0 as well. Developers using PyTorch with AMD GPUs can migrate to PyTorch 2.0 with the confidence that their existing code will continue to work without any required changes, so there is no penalty to access the improvements that come with this release. On the other hand, using PyTorch 2.0 and TorchInductor can result in significant performance improvement over the default eager-mode as shown below.&lt;/p&gt;

&lt;p&gt;The initial results using AMD Instinct MI250 GPUs already shows strong performance improvement with minimal optimization on TorchInductor compared to the default eager-mode. We see an average performance increase of up to 1.54X on 44 out of the 45 models on HuggingFace benchmarks suite with CamemBert, DistillGPT2 and T5Small being a few of the standout models with up to 1.5X or more performance improvement over eager-mode. We are looking forward to continued engagement with members of the PyTorch team at Meta to enable further optimization on ROCm software stack and the additional performance improvement for future PyTorch releases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/t-vs-eager-mode.svg&quot; alt=&quot;Image 1: AMD MI250 GPU performance improvement for TorchInductor vs eager-mode using HuggingFace&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Image 1: AMD MI250 GPU performance improvement for TorchInductor vs eager-mode using HuggingFace &lt;sup&gt;MI200-89.&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.0 follows the same set of install options as before to build and install for supporting AMD GPUs. These include an installable Python package hosted at &lt;a href=&quot;https://pytorch.org/&quot;&gt;pytorch.org&lt;/a&gt;, AMD‚Äôs public PyTorch docker image, and of course the option to build from source using the upstream PyTorch repository. As with PyTorch builds for other platforms, the specific command line to be run for pip-based install is provided by the configurator at &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The GPUs supported by the ROCm software platform which forms the basis for PyTorch support on AMD GPUs are documented at &lt;a href=&quot;https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html&quot;&gt;https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 represents a major step in continuing to broaden support for ML developers by increasing performance while maintaining a simple, Pythonic interface. This performance uplift is made possible in large part by the new TorchInductor infrastructure, which in turn harnesses the Triton ML programming language and just-in-time compiler. AMD‚Äôs support for these technologies allows users to realize the full promise of the new PyTorch architecture.  Our GPU support in PyTorch 2.0 is just one manifestation of a larger vision around AI and machine learning. AI/ML plays an important role in multiple AMD product lines, including Instinct and Radeon GPUs, Alveo‚Ñ¢ data center accelerators, and both Ryzen‚Ñ¢ and EPYC processors. These hardware and software initiatives are all part of AMD‚Äôs Pervasive AI vision, and we look forward to addressing the many new challenges and opportunities of this dynamic space.&lt;/p&gt;

&lt;p&gt;MI200-89 ‚Äì PyTorch Inductor mode HuggingFace Transformers training speedup, running the standard PyTorch 2.0 test suite, over PyTorch eager-mode comparison based on AMD internal testing on a single GCD as of 3/10/2023 using a 2P AMD EPYC‚Ñ¢ 7763 production server with 4x AMD Instinct‚Ñ¢ MI250 (128GB HBM2e) 560W GPUs with Infinity Fabric‚Ñ¢ technology; host ROCm‚Ñ¢ 5.3, guest ROCm‚Ñ¢ 5.4.4, PyTorch 2.0.0, Triton 2.0. Server manufacturers may vary configurations, yielding different results. Performance may vary based on factors including use of latest drivers and optimizations.&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;¬© 2023 Advanced Micro Devices, Inc. All rights reserved. AMD, the AMD Arrow logo, AMD CDNA, AMD Instinct, EPYC, Radeon, ROCm, Ryzen, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective owners.&lt;/small&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>AMD</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework. The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that the PyTorch 2.0 stable release includes support for AMD Instinct‚Ñ¢ and Radeon‚Ñ¢ GPUs that are supported by the ROCm‚Ñ¢ software platform.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Generative Diffusion Models with PyTorch 2</title>
      <link href="https://pytorch.org/blog/accelerated-generative-diffusion-models/" rel="alternate" type="text/html" title="Accelerated Generative Diffusion Models with PyTorch 2" />
      <published>2023-04-14T00:00:00-07:00</published>
      <updated>2023-04-14T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-generative-diffusion-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-generative-diffusion-models/">&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;A large part of the recent progress in Generative AI came from denoising diffusion models, which allow producing high quality images and videos from text prompts. This family includes Imagen, DALLE, Latent Diffusion, and others. However, all models in this family share a common drawback: generation is rather slow, due to the iterative nature of the sampling process by which the images are produced. This makes it important to optimize the code running inside the sampling loop.&lt;/p&gt;

&lt;p&gt;We took an open source implementation of a popular text-to-image diffusion model as a starting point and accelerated its generation using two optimizations available in PyTorch 2: compilation and fast attention implementation. Together with a few minor memory processing improvements in the code these optimizations give up to 49% inference speedup relative to the original implementation without &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt;, and 39% inference speedup relative to using the original code with xFormers (excluding the compilation time), depending on the GPU architecture and batch size. Importantly, the speedup comes without a need to install xFormers or any other extra dependencies.&lt;/p&gt;

&lt;p&gt;The table below shows the improvement in runtime between the original implementation with xFormers installed and our optimized version with PyTorch-integrated memory efficient attention (originally developed for and released in the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; library)  and PyTorch compilation. The compilation time is excluded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Runtime improvement in % compared to original+xFormers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;See the absolute runtime numbers in section ‚ÄúBenchmarking setup and results summary‚Äù&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;GPU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 1&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 2&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Batch size 4&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;P100 (no compilation)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-3.8
   &lt;/td&gt;
   &lt;td&gt;0.44
   &lt;/td&gt;
   &lt;td&gt;5.47
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;2.12
   &lt;/td&gt;
   &lt;td&gt;10.51
   &lt;/td&gt;
   &lt;td&gt;14.2
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-2.34
   &lt;/td&gt;
   &lt;td&gt;8.99
   &lt;/td&gt;
   &lt;td&gt;10.57
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;18.63
   &lt;/td&gt;
   &lt;td&gt;6.39
   &lt;/td&gt;
   &lt;td&gt;10.43
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;38.5
   &lt;/td&gt;
   &lt;td&gt;20.33
   &lt;/td&gt;
   &lt;td&gt;12.17
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;One can notice the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The improvements are significant for powerful GPUs like A100 and V100. For those GPUs the improvement is most pronounced for batch size 1&lt;/li&gt;
  &lt;li&gt;For less powerful GPUs we observe smaller speedups (or in two cases slight regressions). The batch size trend is reversed here: improvement is larger for larger batches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following sections we describe the applied optimizations and provide detailed benchmarking data, comparing the generation time with various optimization features on/off.&lt;/p&gt;

&lt;p&gt;Specifically, we benchmark 5 configurations and the plots below compare their absolute performance for different GPUs and batch sizes. For definitions of these configurations see section ‚ÄúBenchmarking setup and results‚Äù.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models1.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 1&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models2.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 2&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models3.png&quot; alt=&quot;Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 1&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimizations&quot;&gt;Optimizations&lt;/h2&gt;

&lt;p&gt;Here we‚Äôll go into more detail about the optimizations introduced into the model code. These optimizations rely on features of PyTorch 2.0 which has been released recently.&lt;/p&gt;

&lt;h3 id=&quot;optimized-attention&quot;&gt;Optimized Attention&lt;/h3&gt;

&lt;p&gt;One part of the code which we optimized is the scaled dot-product attention. Attention is known to be a heavy operation: naive implementation materializes the attention matrix, leading to time and memory complexity quadratic in sequence length. It is common for diffusion models to use attention (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CrossAttention&lt;/code&gt;) as part of Transformer blocks in multiple parts of the U-Net. Since the U-Net runs at every sampling step, this becomes a critical point to optimize. Instead of custom attention implementation one can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.MultiheadAttention,&lt;/code&gt; which in PyTorch 2 has optimized attention implementation is integrated into it. This optimization schematically boils down to the following pseudocode:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class CrossAttention(nn.Module):
    def __init__(self, ...):
        # Create matrices: Q, K, V, out_proj
        ...
    def forward(self, x, context=None, mask=None):
       # Compute out = SoftMax(Q*K/sqrt(d))V
       # Return out_proj(out)
       ‚Ä¶
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gets replaced with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class CrossAttention(nn.Module):
    def __init__(self, ...):
        self.mha = nn.MultiheadAttention(...)
    def forward(self, x, context):
	return self.mha(x, context, context)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The optimized implementation of attention was available already in PyTorch 1.13 (see &lt;a href=&quot;https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/&quot;&gt;here&lt;/a&gt;) and widely adopted (see e.g. &lt;a href=&quot;https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2&quot;&gt;HuggingFace transformers library example&lt;/a&gt;). In particular, it integrates memory-efficient attention from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; library and flash attention from &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;. PyTorch 2.0 expands this to additional attention functions such as cross attention and custom kernels for further acceleration, making it applicable to diffusion models.&lt;/p&gt;

&lt;p&gt;Flash attention is available on GPUs with compute capability SM 7.5 or SM 8.x - for example, on T4, A10, and A100, which are included in our benchmark (you can check compute capability of each NVIDIA GPU &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot;&gt;here&lt;/a&gt;). However, in our tests on A100 the memory efficient attention performed better than flash attention for the particular case of diffusion models, due to the small number of attention heads and small batch size.  PyTorch understands this and in this case chooses memory efficient attention over flash attention when both are available (see the logic &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/d8e795ecd53670682bd3b2e5ff1f378402b147d5/aten/src/ATen/native/transformers/cuda/sdp_utils.h#L33-L71&quot;&gt;here&lt;/a&gt;). For full control over the attention backends (memory-efficient attention, flash attention, ‚Äúvanilla math‚Äù, or any future ones), power users can enable and disable them manually with the help of the context manager &lt;a href=&quot;https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel&quot;&gt;torch.backends.cuda.sdp_kernel&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;compilation&quot;&gt;Compilation&lt;/h3&gt;

&lt;p&gt;Compilation is a &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/#user-experience&quot;&gt;new feature of PyTorch 2.0&lt;/a&gt;, enabling significant speedups with a very simple user experience. To invoke the default behavior, simply wrap a PyTorch module or a function into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.compile(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch compiler then turns Python code into a set of instructions which can be executed efficiently without Python overhead. The compilation happens dynamically the first time the code is executed. With the default behavior, under the hood PyTorch utilized &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/index.html&quot;&gt;TorchDynamo&lt;/a&gt; to compile the code and &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747&quot;&gt;TorchInductor&lt;/a&gt; to further optimize it. See &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/dynamo_tutorial.html&quot;&gt;this tutorial&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Although the one-liner above is enough for compilation, certain modifications in the code can squeeze a larger speedup. In particular, one should avoid so-called graph breaks - places in the code which PyTorch can‚Äôt compile. As opposed to previous PyTorch compilation approaches (like TorchScript), PyTorch 2 compiler doesn‚Äôt break in this case. Instead it falls back on eager execution - so the code runs, but with reduced performance. We introduced a few minor changes to the model code to get rid of graph breaks. This included eliminating functions from libraries not supported by the compiler, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inspect.isfunction&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;einops.rearrange&lt;/code&gt;. See this &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&quot;&gt;doc&lt;/a&gt; to learn more about graph breaks and how to eliminate them.&lt;/p&gt;

&lt;p&gt;Theoretically, one can apply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile &lt;/code&gt;on the whole diffusion sampling loop. However, in practice it is enough to just compile the U-Net. The reason is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; doesn‚Äôt yet have a loop analyzer and would recompile the code for each iteration of the sampling loop. Moreover, compiled sampler code is likely to generate graph breaks - so one would need to adjust it if one wants to get a good performance from the compiled version.&lt;/p&gt;

&lt;p&gt;Note that compilation &lt;a href=&quot;https://github.com/openai/triton/blob/b5d32896b1f89fc44a82f8df3bb010934c53f4f5/README.md?plain=1#L66-L68&quot;&gt;requires GPU compute capability &amp;gt;= SM 7.0&lt;/a&gt; to run in non-eager mode. This covers all GPUs in our benchmarks -  T4, V100, A10, A100 - except for P100 (see the &lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot;&gt;full list&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;other-optimizations&quot;&gt;Other optimizations&lt;/h3&gt;

&lt;p&gt;In addition, we have improved efficiency of GPU memory operations by eliminating some common pitfalls, e.g. creating a tensor on GPU directly rather than creating it on CPU and later moving to GPU. The places where such optimizations were necessary were determined by line-profiling and looking at CPU/GPU traces and &lt;a href=&quot;https://github.com/brendangregg/FlameGraph&quot;&gt;Flame Graphs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-setup-and-results-summary&quot;&gt;Benchmarking setup and results summary&lt;/h2&gt;

&lt;p&gt;We have two versions of code to compare: &lt;em&gt;original&lt;/em&gt; and &lt;em&gt;optimized&lt;/em&gt;. On top of this, several optimization features (xFormers, PyTorch memory efficient attention, compilation) can be turned on/off. Overall, as mentioned in the introduction, we will be benchmarking 5 configurations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Original code without xFormers&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Original code with xFormers&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with vanilla math attention backend and no compilation&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with memory-efficient attention backend and no compilation&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimized code with memory-efficient attention backend and compilation&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As the &lt;em&gt;original version&lt;/em&gt; we took the version of the code which uses PyTorch 1.12 and a custom implementation of attention. The &lt;em&gt;optimized version&lt;/em&gt; uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.MultiheadAttention&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CrossAttention&lt;/code&gt; and PyTorch 2.0.0.dev20230111+cu117. It also has a few other minor optimizations in PyTorch-related code.&lt;/p&gt;

&lt;p&gt;The table below shows runtime of each version of the code in seconds, and the percentage improvement compared to the _original with xFormers. _The compilation time is excluded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 1. In parenthesis - relative improvement with respect to the ‚ÄúOriginal with xFormers‚Äù row&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;30.4s (-19.3%)
   &lt;/td&gt;
   &lt;td&gt;29.8s (-77.3%)
   &lt;/td&gt;
   &lt;td&gt;13.0s (-83.9%)
   &lt;/td&gt;
   &lt;td&gt;10.9s (-33.1%)
   &lt;/td&gt;
   &lt;td&gt;8.0s (-19.3%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;25.5s&lt;/strong&gt; (0.0%)
   &lt;/td&gt;
   &lt;td&gt;16.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;7.1s&lt;/strong&gt; (0.0%)
   &lt;/td&gt;
   &lt;td&gt;8.2s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;6.7s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;27.3s (-7.0%)
   &lt;/td&gt;
   &lt;td&gt;19.9s (-18.7%)
   &lt;/td&gt;
   &lt;td&gt;13.2s (-87.2%)
   &lt;/td&gt;
   &lt;td&gt;7.5s (8.7%)
   &lt;/td&gt;
   &lt;td&gt;5.7s (15.1%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;26.5s (-3.8%)
   &lt;/td&gt;
   &lt;td&gt;16.8s (0.2%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;7.1s&lt;/strong&gt; (-0.8%)
   &lt;/td&gt;
   &lt;td&gt;6.9s (16.0%)
   &lt;/td&gt;
   &lt;td&gt;5.3s (20.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;16.4s &lt;/strong&gt;(2.1%)
   &lt;/td&gt;
   &lt;td&gt;7.2s (-2.3%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;6.6s&lt;/strong&gt; (18.6%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;4.1s&lt;/strong&gt; (38.5%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 2&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;58.0s (-21.6%)
   &lt;/td&gt;
   &lt;td&gt;57.6s (-84.0%)
   &lt;/td&gt;
   &lt;td&gt;24.4s (-95.2%)
   &lt;/td&gt;
   &lt;td&gt;18.6s (-63.0%)
   &lt;/td&gt;
   &lt;td&gt;12.0s (-50.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;47.7s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;31.3s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;12.5s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;11.4s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;8.0s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;49.3s (-3.5%)
   &lt;/td&gt;
   &lt;td&gt;37.9s (-21.0%)
   &lt;/td&gt;
   &lt;td&gt;17.8s (-42.2%)
   &lt;/td&gt;
   &lt;td&gt;12.7s (-10.7%)
   &lt;/td&gt;
   &lt;td&gt;7.8s (1.8%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;47.5s &lt;/strong&gt;(0.4%)
   &lt;/td&gt;
   &lt;td&gt;31.2s (0.5%)
   &lt;/td&gt;
   &lt;td&gt;12.2s (2.6%)
   &lt;/td&gt;
   &lt;td&gt;11.5s (-0.7%)
   &lt;/td&gt;
   &lt;td&gt;7.0s (12.6%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;28.0s&lt;/strong&gt; (10.5%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;11.4s&lt;/strong&gt; (9.0%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;10.7s &lt;/strong&gt;(6.4%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;6.4s&lt;/strong&gt; (20.3%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Runtimes for batch size 4&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Configuration&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;P100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;V100&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;A100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original without xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;117.9s (-20.0%)
   &lt;/td&gt;
   &lt;td&gt;112.4s (-81.8%)
   &lt;/td&gt;
   &lt;td&gt;47.2s (-101.7%)
   &lt;/td&gt;
   &lt;td&gt;35.8s (-71.9%)
   &lt;/td&gt;
   &lt;td&gt;22.8s (-78.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Original with xFormers&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;98.3s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;61.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;23.4s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;20.8s (0.0%)
   &lt;/td&gt;
   &lt;td&gt;12.7s (0.0%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with vanilla math attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;101.1s (-2.9%)
   &lt;/td&gt;
   &lt;td&gt;73.0s (-18.0%)
   &lt;/td&gt;
   &lt;td&gt;28.3s (-21.0%)
   &lt;/td&gt;
   &lt;td&gt;23.3s (-11.9%)
   &lt;/td&gt;
   &lt;td&gt;14.5s (-13.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention, no compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;92.9s &lt;/strong&gt;(5.5%)
   &lt;/td&gt;
   &lt;td&gt;61.1s (1.2%)
   &lt;/td&gt;
   &lt;td&gt;23.9s (-1.9%)
   &lt;/td&gt;
   &lt;td&gt;20.8s (-0.1%)
   &lt;/td&gt;
   &lt;td&gt;12.8s (-0.9%)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimized with mem. efficient attention and compilation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;53.1s &lt;/strong&gt;(14.2%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;20.9s&lt;/strong&gt; (10.6%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;18.6s&lt;/strong&gt; (10.4%)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;11.2s&lt;/strong&gt; (12.2%)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To minimize fluctuations and external influence on the performance of the benchmarked code, we ran each version of the code one after another, and then repeated this sequence 10 times: A, B, C, D, E,  A, B, ‚Ä¶ So the results of a typical run would look like the one in the picture below.. Note that one shouldn‚Äôt rely on comparison of absolute run times between different graphs, but comparison of run times_ inside_ one graph is pretty reliable, thanks to our benchmarking setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-04-11-accelerated-generative-diffusion-models4.png&quot; alt=&quot;Denoising diffusion model generation benchmarks&quot; style=&quot;max-height:700px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each run of text-to-image generation script produces several batches, the number of which is regulated by the CLI parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--n_iter&lt;/code&gt;. In the benchmarks we used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_iter = 2&lt;/code&gt;, but introduced an additional ‚Äúwarm-up‚Äù iteration, which doesn‚Äôt contribute to the run time. This was necessary for the runs with compilation, because compilation happens the first time the code runs, and so the first iteration is much longer than all subsequent. To make comparison fair, we also introduced this additional ‚Äúwarm-up‚Äù iteration to all other runs.&lt;/p&gt;

&lt;p&gt;The numbers in the table above are for number of iterations 2 (plus a ‚Äúwarm-up one‚Äù), prompt ‚ÄùA photo‚Äù, seed 1, PLMS sampler, and autocast turned on.&lt;/p&gt;

&lt;p&gt;Benchmarks were done using P100, V100, A100, A10 and T4 GPUs. The T4 benchmarks were done in Google Colab Pro. The A10 benchmarks were done on g5.4xlarge AWS instances with 1 GPU.&lt;/p&gt;

&lt;h2 id=&quot;conclusions-and-next-steps&quot;&gt;Conclusions and next steps&lt;/h2&gt;

&lt;p&gt;We have shown that new features of PyTorch 2 - compiler and optimized attention implementation - give performance improvements exceeding or comparable with what previously required installation of an external dependency (xFormers). PyTorch achieved this, in particular, by integrating memory efficient attention from xFormers into its codebase. This is a significant improvement for user experience, given that xFormers, being a state-of-the-art library, in many scenarios requires custom installation process and long builds.&lt;/p&gt;

&lt;p&gt;There are a few natural directions in which this work can be continued:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The optimizations we implemented and described here are only benchmarked for text-to-image inference so far. It would be interesting to see how they affect training performance. PyTorch compilation can be directly applied to training; enabling training with PyTorch optimized attention is on the roadmap&lt;/li&gt;
  &lt;li&gt;We intentionally minimized changes to the original model code. Further profiling and optimization can probably bring more improvements&lt;/li&gt;
  &lt;li&gt;At the moment compilation is applied only to the U-Net model inside the sampler. Since there is a lot happening outside of U-Net (e.g. operations directly in the sampling loop), it would be beneficial to compile the whole sampler. However, this would require analysis of the compilation process to avoid recompilation at every sampling step&lt;/li&gt;
  &lt;li&gt;Current code only applies compilation within the PLMS sampler, but it should be trivial to extend it to other samplers&lt;/li&gt;
  &lt;li&gt;Besides text-to-image generation, diffusion models are also applied to other tasks - image-to-image and inpainting. It would be interesting to measure how their performance improves from PyTorch 2 optimizations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See if you can increase performance of open source diffusion models using the methods we described, and share the results!&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch 2.0 overview, which has a lot of information on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile:&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;https://pytorch.org/get-started/pytorch-2.0/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tutorial on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;: &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;General compilation troubleshooting: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/troubleshooting.html&quot;&gt;https://pytorch.org/docs/master/dynamo/troubleshooting.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Details on graph breaks: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&quot;&gt;https://pytorch.org/docs/master/dynamo/faq.html#identifying-the-cause-of-a-graph-break&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Details on guards: &lt;a href=&quot;https://pytorch.org/docs/master/dynamo/guards-overview.html&quot;&gt;https://pytorch.org/docs/master/dynamo/guards-overview.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Video deep dive on TorchDynamo &lt;a href=&quot;https://www.youtube.com/watch?v=egZB5Uxki0I&quot;&gt;https://www.youtube.com/watch?v=egZB5Uxki0I&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tutorial on optimized attention in PyTorch 1.12: &lt;a href=&quot;https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html&quot;&gt;https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank Geeta Chauhan, Natalia Gimelshein, Patrick Labatut, Bert Maher, Mark Saroufim, Michael Voznesensky and Francisco Massa for their valuable advice and early feedback on the text.&lt;/p&gt;

&lt;p&gt;Special thanks to Yudong Tao initiating the work on using PyTorch native attention in diffusion models.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Grigory Sizov, Michael Gschwind, Hamid Shojanazeri, Driss Guessous, Daniel Haziza, Christian Puhrsch</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new torch.compile() compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Celebrate PyTorch 2.0 with New Performance Features for AI Developers</title>
      <link href="https://pytorch.org/blog/celebrate-pytorch-2.0/" rel="alternate" type="text/html" title="Celebrate PyTorch 2.0 with New Performance Features for AI Developers" />
      <published>2023-04-07T00:00:00-07:00</published>
      <updated>2023-04-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/celebrate-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/celebrate-pytorch-2.0/">&lt;p&gt;Congratulations to the PyTorch Foundation for its release of &lt;strong&gt;PyTorch 2.0&lt;/strong&gt;! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;TorchInductor&lt;/li&gt;
  &lt;li&gt;GNN&lt;/li&gt;
  &lt;li&gt;INT8 Inference Optimization&lt;/li&gt;
  &lt;li&gt;oneDNN Graph API&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We at Intel are delighted to be part of the PyTorch community and appreciate the collaboration with and feedback from our colleagues at &lt;a href=&quot;http://www.meta.com/&quot;&gt;Meta&lt;/a&gt; as we co-developed these features.&lt;/p&gt;

&lt;p&gt;Let‚Äôs get started.&lt;/p&gt;

&lt;h2 id=&quot;1-torchinductor-cpu-fp32-inference-optimized&quot;&gt;1. TorchInductor CPU FP32 Inference Optimized&lt;/h2&gt;

&lt;p&gt;As part of the PyTorch 2.0 compilation stack, TorchInductor CPU backend optimization brings notable performance improvements via graph compilation over the PyTorch eager mode.&lt;/p&gt;

&lt;p&gt;The TorchInductor CPU backend is sped up by leveraging the technologies from the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel¬Æ Extension for PyTorch&lt;/a&gt; for Conv/GEMM ops with post-op fusion and weight prepacking, and PyTorch ATen CPU kernels for memory-bound ops with explicit vectorization on top of OpenMP*-based thread parallelization.&lt;/p&gt;

&lt;p&gt;With these optimizations on top of the powerful loop fusions in TorchInductor codegen, we achieved up to a &lt;strong&gt;1.7x&lt;/strong&gt; FP32 inference performance boost over three representative deep learning benchmarks: TorchBench, HuggingFace, and timm1. Training and low-precision support are under development.&lt;/p&gt;

&lt;h3 id=&quot;see-the-improvements&quot;&gt;See the Improvements&lt;/h3&gt;

&lt;p&gt;The performance improvements on various backends are tracked on this &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/93531#issuecomment-1457373890&quot;&gt;TouchInductor CPU Performance Dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;improve-graph-neural-network-gnn-in-pyg-for-inference-and-training-performance-on-cpu&quot;&gt;Improve Graph Neural Network (GNN) in PyG for Inference and Training Performance on CPU&lt;/h2&gt;

&lt;p&gt;GNN is a powerful tool to analyze graph structure data. This feature is designed to improve GNN inference and training performance on Intel¬Æ CPUs, including the new 4th Gen Intel¬Æ Xeon¬Æ Scalable processors.&lt;/p&gt;

&lt;p&gt;PyTorch Geometric (PyG) is a very popular library built upon PyTorch to perform GNN workflows. Currently on CPU, GNN models of PyG run slowly due to the lack of GNN-related sparse matrix multiplication operations (i.e., SpMM_reduce) and the lack of several critical kernel-level optimizations (scatter/gather, etc.) tuned for GNN compute.&lt;/p&gt;

&lt;p&gt;To address this, optimizations are provided for message passing between adjacent neural network nodes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;scatter_reduce:&lt;/strong&gt; performance hotspot in message-passing when the edge index is stored in coordinate format (COO).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;gather:&lt;/strong&gt; backward computation of scatter_reduce, specially tuned for the GNN compute when the index is an expanded tensor.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;torch.sparse.mm with reduce flag:&lt;/strong&gt; performance hotspot in message-passing when the edge index is stored in compressed sparse row (CSR). Supported reduce flag for: sum, mean, amax, amin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;End-to-end performance benchmark results for both inference and training on 3rd Gen Intel¬Æ Xeon¬Æ Scalable processors 8380 platform and on 4th Gen 8480+ platform are discussed in &lt;a href=&quot;http://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;Accelerating PyG on Intel CPUs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;optimize-int8-inference-with-unified-quantization-backend-for-x86-cpu-platforms&quot;&gt;Optimize int8 Inference with Unified Quantization Backend for x86 CPU Platforms&lt;/h2&gt;

&lt;p&gt;The new X86 quantization backend is a combination of &lt;a href=&quot;http://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt; (Facebook General Matrix-Matrix Multiplication) and &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneAPI Deep Neural Network Library (oneDNN&lt;/a&gt;) backends and replaces FBGEMM as the default quantization backend for x86 platforms. The result: better end-to-end int8 inference performance than FBGEMM.&lt;/p&gt;

&lt;p&gt;Users access the x86 quantization backend by default for x86 platforms, and the selection between different kernels is automatically done behind the scenes. The rules of selection are based on prior performance testing data done by Intel during feature development. Thus, the x86 backend replaces FBGEMM and may offer better performance, depending on the use case.&lt;/p&gt;

&lt;p&gt;The selection rules are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On platforms without VNNI (e.g., Intel¬Æ Core‚Ñ¢ i7 processors), FBGEMM is always used.&lt;/li&gt;
  &lt;li&gt;On platforms with VNNI (e.g., 2nd-4th Gen Intel¬Æ Xeon¬Æ Scalable processors and future platforms):
    &lt;ul&gt;
      &lt;li&gt;For linear, FBGEMM is always used.&lt;/li&gt;
      &lt;li&gt;For convolution layers, FBGEMM is used for depth-wise convolution whose layers &amp;gt; 100; otherwise, oneDNN is used.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that as the kernels continue to evolve.&lt;/p&gt;

&lt;p&gt;The selection rules above are subject to change to achieve better performance. Performance metrics for through-put speed-up ratios of unified x86 backend vs. pure FBGEMM are discussed in &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;[RFC] Unified quantization backend for x86 CPU platforms #83888&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;leverage-onednn-graph-api-to-accelerate-inference-on-cpu&quot;&gt;Leverage oneDNN Graph API to Accelerate Inference on CPU&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on Intel¬Æ AI hardware. It automatically identifies the graph partitions to be accelerated via fusion. The &lt;a href=&quot;http://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul, and their neighbor operations for both inference and training use cases.&lt;/p&gt;

&lt;p&gt;Currently, BFloat16 and Float32 datatypes are supported and only inference workloads can be optimized.  BF16 is only optimized on machines with Intel¬Æ Advanced Vector Extensions 512 (Intel¬Æ AVX-512) BF16 support.&lt;/p&gt;

&lt;p&gt;Few or no modifications are needed in PyTorch to support newer oneDNN Graph fusions/optimized kernels. To use oneDNN Graph, users can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Either use the API &lt;em&gt;torch.jit.enable_onednn_fusion(True)&lt;/em&gt; before JIT tracing a model, OR ‚Ä¶&lt;/li&gt;
  &lt;li&gt;Use its context manager, viz. &lt;em&gt;with torch.jit.fuser(‚Äúfuser3‚Äù).&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;For accelerating &lt;a href=&quot;http://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16 inference&lt;/a&gt;, we rely on eager-mode AMP (Automatic Mixed Precision) support in PyTorch and disable JIT mode‚Äôs AMP.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;PyTorch performance tuning guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;h3 id=&quot;get-the-software&quot;&gt;Get the Software&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://pytorch.org/get-started/locally/&quot;&gt;Try out PyTorch 2.0&lt;/a&gt; and realize the performance benefits for yourself from these Intel-contributed features.&lt;/p&gt;

&lt;p&gt;We encourage you to check out Intel‚Äôs other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Framework&lt;/a&gt; optimizations and learn about the open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel‚Äôs AI software portfolio.&lt;/p&gt;

&lt;p&gt;For more details about 4th Gen Intel Xeon Scalable processor, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI Platform&lt;/a&gt; where you can learn about how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-resources&quot;&gt;PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Congratulations to the PyTorch Foundation for its release of PyTorch 2.0! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:</summary>
      

      
      
    </entry>
  
</feed>


